[
  {
    "id": "http://arxiv.org/abs/2601.16211v1",
    "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
    "summary": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
    "published": "2026-01-22T18:59:13Z",
    "updated": "2026-01-22T18:59:13Z",
    "link": "http://arxiv.org/pdf/2601.16211v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Geo Ahn",
      "Inwoong Lee",
      "Taeoh Kim",
      "Minho Shim",
      "Dongyoon Wee",
      "Jinwoo Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16210v1",
    "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
    "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
    "published": "2026-01-22T18:58:55Z",
    "updated": "2026-01-22T18:58:55Z",
    "link": "http://arxiv.org/pdf/2601.16210v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Onkar Susladkar",
      "Tushar Prakash",
      "Adheesh Juvekar",
      "Kiet A. Nguyen",
      "Dong-Hwan Jang",
      "Inderjit S Dhillon",
      "Ismini Lourentzou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14490v2",
    "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
    "summary": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.",
    "published": "2026-01-20T21:26:15Z",
    "updated": "2026-01-22T18:58:24Z",
    "link": "http://arxiv.org/pdf/2601.14490v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Hunter Heidenreich",
      "Ben Elliott",
      "Olivia Dinica",
      "Yosheb Getachew"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16206v1",
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
    "published": "2026-01-22T18:57:09Z",
    "updated": "2026-01-22T18:57:09Z",
    "link": "http://arxiv.org/pdf/2601.16206v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Daixuan Cheng",
      "Shaohan Huang",
      "Yuxian Gu",
      "Huatong Song",
      "Guoxin Chen",
      "Li Dong",
      "Wayne Xin Zhao",
      "Ji-Rong Wen",
      "Furu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16205v1",
    "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations",
    "summary": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.",
    "published": "2026-01-22T18:56:14Z",
    "updated": "2026-01-22T18:56:14Z",
    "link": "http://arxiv.org/pdf/2601.16205v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Patrick Altmeyer",
      "Aleksander Buszydlik",
      "Arie van Deursen",
      "Cynthia C. S. Liem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02921v3",
    "title": "Training-Free Geospatial Place Representation Learning from Large-Scale Point-of-Interest Graph Data",
    "summary": "Learning effective representations of urban environments requires capturing spatial structure beyond fixed administrative boundaries. Existing geospatial representation learning approaches typically aggregate Points of Interest(POI) into pre-defined administrative regions such as census units or ZIP code areas, assigning a single embedding to each region. However, POIs often form semantically meaningful groups that extend across, within, or beyond these boundaries, defining places that better reflect human activity and urban function. To address this limitation, we propose PlaceRep, a training-free geospatial representation learning method that constructs place-level representations by clustering spatially and semantically related POIs. PlaceRep summarizes large-scale POI graphs from U.S. Foursquare data to produce general-purpose urban region embeddings while automatically identifying places across multiple spatial scales. By eliminating model pre-training, PlaceRep provides a scalable and efficient solution for multi-granular geospatial analysis. Experiments using the tasks of population density estimation and housing price prediction as downstream tasks show that PlaceRep outperforms most state-of-the-art graph-based geospatial representation learning methods and achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation of PlaceRep is available at https://github.com/mohammadhashemii/PlaceRep.",
    "published": "2025-06-25T15:10:31Z",
    "updated": "2026-01-22T18:46:50Z",
    "link": "http://arxiv.org/pdf/2507.02921v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mohammad Hashemi",
      "Hossein Amiri",
      "Andreas Zufle"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01001v2",
    "title": "SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks",
    "summary": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.",
    "published": "2025-07-01T17:51:59Z",
    "updated": "2026-01-22T18:32:06Z",
    "link": "http://arxiv.org/pdf/2507.01001v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yilun Zhao",
      "Kaiyan Zhang",
      "Tiansheng Hu",
      "Sihong Wu",
      "Ronan Le Bras",
      "Charles McGrady",
      "Taira Anderson",
      "Jonathan Bragg",
      "Joseph Chee Chang",
      "Jesse Dodge",
      "Matt Latzke",
      "Yixin Liu",
      "Xiangru Tang",
      "Zihang Wang",
      "Chen Zhao",
      "Hannaneh Hajishirzi",
      "Doug Downey",
      "Arman Cohan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.18034v3",
    "title": "Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource Morphologically Rich Indian Languages",
    "summary": "Multilingual large language models (LLMs) are expensive to pretrain and often suffer from imbalances across languages and datasets, English-centric bias, tokenizer oversegmentation for morphologically rich low-resource languages, and the curse of multilinguality. We introduce PARAMANU, the first family of Indian-only autoregressive language models trained from scratch on open-source language-specific data for the five most spoken Indian languages: Bengali, Hindi, Marathi, Tamil, and Telugu. All models are designed for affordability and are trained on a single GPU with a budget under $1,000, allowing under-resourced researchers to build competitive language models. To address low-resource challenges, we develop morphology-aligned, low-fertility tokenizers, propose an interpolation-based method for token position indices in RoPE based scaling to train longer sequences efficiently. We also create instruction-tuning datasets in Bangla that are translated to the other four languages. Despite their small size (108M-367M parameters), Paramanu achieves a strong performance-efficiency tradeoff and outperforms most larger multilingual models across all five languages. Our collection is available at https://huggingface.co/collections/mitodru/paramanu.",
    "published": "2024-01-31T17:58:10Z",
    "updated": "2026-01-22T18:28:42Z",
    "link": "http://arxiv.org/pdf/2401.18034v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Mitodru Niyogi",
      "Eric Gaussier",
      "Arnab Bhattacharya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16175v1",
    "title": "Learning to Discover at Test Time",
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
    "published": "2026-01-22T18:24:00Z",
    "updated": "2026-01-22T18:24:00Z",
    "link": "http://arxiv.org/pdf/2601.16175v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mert Yuksekgonul",
      "Daniel Koceja",
      "Xinhao Li",
      "Federico Bianchi",
      "Jed McCaleb",
      "Xiaolong Wang",
      "Jan Kautz",
      "Yejin Choi",
      "James Zou",
      "Carlos Guestrin",
      "Yu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16172v1",
    "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
    "summary": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.",
    "published": "2026-01-22T18:16:46Z",
    "updated": "2026-01-22T18:16:46Z",
    "link": "http://arxiv.org/pdf/2601.16172v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zachary Burton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16163v1",
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
    "published": "2026-01-22T18:09:30Z",
    "updated": "2026-01-22T18:09:30Z",
    "link": "http://arxiv.org/pdf/2601.16163v1.pdf",
    "category": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Moo Jin Kim",
      "Yihuai Gao",
      "Tsung-Yi Lin",
      "Yen-Chen Lin",
      "Yunhao Ge",
      "Grace Lam",
      "Percy Liang",
      "Shuran Song",
      "Ming-Yu Liu",
      "Chelsea Finn",
      "Jinwei Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16152v1",
    "title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates",
    "summary": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.",
    "published": "2026-01-22T17:51:02Z",
    "updated": "2026-01-22T17:51:02Z",
    "link": "http://arxiv.org/pdf/2601.16152v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI"
    ],
    "authors": [
      "Denise M. Case"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16150v1",
    "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
    "summary": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.",
    "published": "2026-01-22T17:46:31Z",
    "updated": "2026-01-22T17:46:31Z",
    "link": "http://arxiv.org/pdf/2601.16150v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Maximos Kaliakatsos-Papakostas",
      "Dimos Makris",
      "Konstantinos Soiledis",
      "Konstantinos-Theodoros Tsamis",
      "Vassilis Katsouros",
      "Emilios Cambouropoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.15891v3",
    "title": "Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons",
    "summary": "In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of millisecond-precision spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could preserve and manipulate sensory information through spike timing. High temporal resolution enables a broader range of neural codes. It could also support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are stimulated synchronously. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).",
    "published": "2025-12-17T19:05:18Z",
    "updated": "2026-01-22T17:40:42Z",
    "link": "http://arxiv.org/pdf/2512.15891v3.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI"
    ],
    "authors": [
      "Terrence J. Sejnowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.10883v2",
    "title": "Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural Language Data",
    "summary": "Large language models are being rapidly deployed across many fields such as healthcare, finance, transportation, and energy, where time-series data are fundamental components. The current works are still limited in their ability to perform reasoning that involves both time-series and the corresponding textual content. We address this gap by introducing Chat-TS, a large language model (LLM) based framework designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMs' vocabulary, enhancing its reasoning ability over both modalities without compromising core natural language capabilities. To support learning and evaluation, we contribute new datasets: the TS Instruct Training Dataset (pairing diverse time-series data with relevant text instructions and responses for instruction tuning), the TS Instruct Question and Answer (QA) Gold Dataset (multiple-choice questions to evaluate multimodal reasoning), and a TS Instruct Quantitative Probing Set (a small subset of TS Instruct QA reasoning tasks alongside math and decision-making questions for LLM evaluation). We design a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning.",
    "published": "2025-03-13T21:05:11Z",
    "updated": "2026-01-22T17:37:12Z",
    "link": "http://arxiv.org/pdf/2503.10883v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Paul Quinlan",
      "Qingguo Li",
      "Xiaodan Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12260v4",
    "title": "Mantis: A Foundation Model for Mechanistic Disease Forecasting",
    "summary": "Infectious disease forecasting in novel outbreaks or low-resource settings is hampered by the need for large disease and covariate data sets, bespoke training, and expert tuning, all of which can hinder rapid generation of forecasts for new settings. To help address these challenges, we developed Mantis, a foundation model trained entirely on mechanistic simulations, which enables out-of-the-box forecasting across diseases, regions, and outcomes, even in settings with limited historical data. We evaluated Mantis against 48 forecasting models across six diseases with diverse modes of transmission, assessing both point forecast accuracy (mean absolute error) and probabilistic performance (weighted interval score and coverage). Despite using no real-world data during training, Mantis achieved lower mean absolute error than all models in the CDC's COVID-19 Forecast Hub when backtested on early pandemic forecasts which it had not previously seen. Across all other diseases tested, Mantis consistently ranked in the top two models across evaluation metrics. Mantis further generalized to diseases with transmission mechanisms not represented in its training data, demonstrating that it can capture fundamental contagion dynamics rather than memorizing disease-specific patterns. These capabilities illustrate that purely simulation-based foundation models such as Mantis can provide a practical foundation for disease forecasting: general-purpose, accurate, and deployable where traditional models struggle.",
    "published": "2025-08-17T06:55:29Z",
    "updated": "2026-01-22T17:34:42Z",
    "link": "http://arxiv.org/pdf/2508.12260v4.pdf",
    "category": [
      "cs.AI",
      "q-bio.QM"
    ],
    "authors": [
      "Carson Dudley",
      "Reiden Magdaleno",
      "Christopher Harding",
      "Ananya Sharma",
      "Emily Martin",
      "Marisa Eisenberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16140v1",
    "title": "Learning to Watermark in the Latent Space of Generative Models",
    "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.",
    "published": "2026-01-22T17:34:30Z",
    "updated": "2026-01-22T17:34:30Z",
    "link": "http://arxiv.org/pdf/2601.16140v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Sylvestre-Alvise Rebuffi",
      "Tuan Tran",
      "Valeriu Lacatusu",
      "Pierre Fernandez",
      "Tomáš Souček",
      "Nikola Jovanović",
      "Tom Sander",
      "Hady Elsahar",
      "Alexandre Mourachko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16134v1",
    "title": "LLM Prompt Evaluation for Educational Applications",
    "summary": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.",
    "published": "2026-01-22T17:31:25Z",
    "updated": "2026-01-22T17:31:25Z",
    "link": "http://arxiv.org/pdf/2601.16134v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Langdon Holmes",
      "Adam Coscia",
      "Scott Crossley",
      "Joon Suh Choi",
      "Wesley Morris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.11139v3",
    "title": "ViSymRe: Vision-guided Multimodal Symbolic Regression",
    "summary": "Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.",
    "published": "2024-12-15T10:05:31Z",
    "updated": "2026-01-22T17:29:53Z",
    "link": "http://arxiv.org/pdf/2412.11139v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.SC"
    ],
    "authors": [
      "Da Li",
      "Junping Yin",
      "Jin Xu",
      "Xinxin Li",
      "Juan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16130v1",
    "title": "Replicating Human Motivated Reasoning Studies with LLMs",
    "summary": "Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.",
    "published": "2026-01-22T17:29:07Z",
    "updated": "2026-01-22T17:29:07Z",
    "link": "http://arxiv.org/pdf/2601.16130v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Neeley Pate",
      "Adiba Mahbub Proma",
      "Hangfeng He",
      "James N. Druckman",
      "Daniel Molden",
      "Gourab Ghoshal",
      "Ehsan Hoque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16127v1",
    "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging",
    "summary": "Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.",
    "published": "2026-01-22T17:28:24Z",
    "updated": "2026-01-22T17:28:24Z",
    "link": "http://arxiv.org/pdf/2601.16127v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Alphaeus Dmonte",
      "Vidhi Gupta",
      "Daniel J Perry",
      "Mark Arehart"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.13273v2",
    "title": "AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs",
    "summary": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.",
    "published": "2025-11-17T11:45:41Z",
    "updated": "2026-01-22T17:11:50Z",
    "link": "http://arxiv.org/pdf/2511.13273v2.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Zhe Sun",
      "Yujun Cai",
      "Jiayu Yao",
      "Yiwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15197v2",
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $π(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
    "published": "2026-01-21T17:15:22Z",
    "updated": "2026-01-22T17:01:41Z",
    "link": "http://arxiv.org/pdf/2601.15197v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Shijie Lian",
      "Bin Yu",
      "Xiaopeng Lin",
      "Laurence T. Yang",
      "Zhaolong Shen",
      "Changti Wu",
      "Yuzhuo Miao",
      "Cong Huang",
      "Kai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16108v1",
    "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources",
    "summary": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.",
    "published": "2026-01-22T16:55:48Z",
    "updated": "2026-01-22T16:55:48Z",
    "link": "http://arxiv.org/pdf/2601.16108v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Marzieh Adeli Shamsabad",
      "Hamed Ghodrati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23761v2",
    "title": "TDFlow: Agentic Workflows for Test Driven Development",
    "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames repository-scale software engineering as a test-resolution task, specifically designed to solve human-written tests. Given a set of tests, TDFlow repeatedly proposes, revises, and debugs repository-scale patches using precisely engineered sub-agents and tightly constrained tools. The workflow decomposes software engineering program repair into four components governed by respective sub-agents. This simple, forced decoupling of patch proposing, debugging, patch revision, and optional test generation (1) reduces long-context burden on any individual sub-agent, (2) focuses each sub-agent on specific, pre-defined sub-tasks, and (3) allows for specialized performance improvement on specific sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and 94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which were subsequently counted as failures. Furthermore, we show that the primary obstacle to human-level software engineering performance lies within writing successful reproduction tests. We envision a human-LLM interactive system powered by TDFlow where human developers write tests solved by LLM systems. Together, these results indicate that modern LLMs, when embedded in a narrowly engineered, test-driven workflow, already achieve human-level test resolution -- with the final frontier for fully autonomous repository repair being the accurate generation of valid reproduction tests.",
    "published": "2025-10-27T18:44:59Z",
    "updated": "2026-01-22T16:50:52Z",
    "link": "http://arxiv.org/pdf/2510.23761v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Kevin Han",
      "Siddharth Maddikayala",
      "Tim Knappe",
      "Om Patel",
      "Austen Liao",
      "Amir Barati Farimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16091v1",
    "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals",
    "summary": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.",
    "published": "2026-01-22T16:42:05Z",
    "updated": "2026-01-22T16:42:05Z",
    "link": "http://arxiv.org/pdf/2601.16091v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Saar Cohen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16087v1",
    "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics",
    "summary": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.",
    "published": "2026-01-22T16:34:05Z",
    "updated": "2026-01-22T16:34:05Z",
    "link": "http://arxiv.org/pdf/2601.16087v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sukesh Subaharan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16083v1",
    "title": "Probably Approximately Correct Maximum A Posteriori Inference",
    "summary": "Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.",
    "published": "2026-01-22T16:28:01Z",
    "updated": "2026-01-22T16:28:01Z",
    "link": "http://arxiv.org/pdf/2601.16083v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Matthew Shorvon",
      "Frederik Mallmann-Trenn",
      "David S. Watson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15209v2",
    "title": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface",
    "summary": "We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa's automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered \"task prompter,\" which integrated the user's history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.",
    "published": "2026-01-21T17:33:00Z",
    "updated": "2026-01-22T16:01:25Z",
    "link": "http://arxiv.org/pdf/2601.15209v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Paige S. DeVries",
      "Michaela Okosi",
      "Ming Li",
      "Nora Dunphy",
      "Gidey Gezae",
      "Dante Conway",
      "Abraham Glasser",
      "Raja Kushalnagar",
      "Christian Vogler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11276v2",
    "title": "Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning",
    "summary": "Loss of decisional capacity, coupled with the increasing absence of reliable human proxies, raises urgent questions about how individuals' values can be represented in Advance Care Planning (ACP). To probe this fraught design space of high-risk, high-subjectivity decision support, we built an experience prototype (\\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal ACP proxy. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings show a surprising 86.7\\% agreement with \\acpagent{}, arguing for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We propose that the key areas of future risk that must be addressed are the moderation of users' expectations and designing accountability and oversight over agent deployment and cutoffs.",
    "published": "2025-12-12T04:39:34Z",
    "updated": "2026-01-22T15:58:47Z",
    "link": "http://arxiv.org/pdf/2512.11276v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Kellie Yu Hui Sim",
      "Pin Sym Foong",
      "Chenyu Zhao",
      "Melanie Yi Ning Quek",
      "Swarangi Subodh Mehta",
      "Kenny Tsu Wei Choo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15249v2",
    "title": "Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism",
    "summary": "Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.",
    "published": "2026-01-21T18:30:42Z",
    "updated": "2026-01-22T15:51:15Z",
    "link": "http://arxiv.org/pdf/2601.15249v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "stat.ME"
    ],
    "authors": [
      "Garrett G. Wen",
      "Buxin Su",
      "Natalie Collina",
      "Zhun Deng",
      "Weijie Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15211v2",
    "title": "Embracing Ambiguity: Bayesian Nonparametrics and Stakeholder Participation for Ambiguity-Aware Safety Evaluation",
    "summary": "Evaluations of generative AI models often collapse nuanced behaviour into a single number computed for a single decoding configuration. Such point estimates obscure tail risks, demographic disparities, and the existence of multiple near-optimal operating points. We propose a unified framework that embraces multiplicity by modelling the distribution of harmful behaviour across the entire space of decoding knobs and prompts, quantifying risk through tail-focused metrics, and integrating stakeholder preferences. Our technical contributions are threefold: (i) we formalise decoding Rashomon sets, regions of knob space whose risk is near-optimal under given criteria and measure their size and disagreement; (ii) we develop a dependent Dirichlet process (DDP) mixture with stakeholder-conditioned stick-breaking weights to learn multi-modal harm surfaces; and (iii) we introduce an active sampling pipeline that uses Bayesian deep learning surrogates to explore knob space efficiently. Our approach bridges multiplicity theory, Bayesian nonparametrics, and stakeholder-aligned sensitivity analysis, paving the way for trustworthy deployment of generative models.",
    "published": "2025-04-21T16:31:15Z",
    "updated": "2026-01-22T15:49:05Z",
    "link": "http://arxiv.org/pdf/2504.15211v2.pdf",
    "category": [
      "cs.AI",
      "stat.AP"
    ],
    "authors": [
      "Yanan Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13545v2",
    "title": "TruthTensor: Evaluating LLMs through Human Imitation on Prediction Market under Drift and Holistic Reasoning",
    "summary": "Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures reasoning models not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com.",
    "published": "2026-01-20T03:11:47Z",
    "updated": "2026-01-22T15:45:29Z",
    "link": "http://arxiv.org/pdf/2601.13545v2.pdf",
    "category": [
      "cs.AI",
      "cs.ET",
      "cs.MA"
    ],
    "authors": [
      "Shirin Shahabi",
      "Spencer Graham",
      "Haruna Isah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16056v1",
    "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path",
    "summary": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.",
    "published": "2026-01-22T15:41:22Z",
    "updated": "2026-01-22T15:41:22Z",
    "link": "http://arxiv.org/pdf/2601.16056v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ruizhi Liu",
      "Liming Xu",
      "Xulin Huang",
      "Jingyan Sui",
      "Shizhe Ding",
      "Boyang Xia",
      "Chungong Yu",
      "Dongbo Bu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18894v2",
    "title": "MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting",
    "summary": "Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.",
    "published": "2025-11-24T08:51:02Z",
    "updated": "2026-01-22T15:35:39Z",
    "link": "http://arxiv.org/pdf/2511.18894v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chenyu Mu",
      "Guihai Chen",
      "Xun Yang",
      "Erkun Yang",
      "Cheng Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.16448v2",
    "title": "Information-theoretic Distinctions Between Deception and Confusion",
    "summary": "We propose an information-theoretic formalization of the distinction between two fundamental AI safety failure modes: deceptive alignment and goal drift. While both can lead to systems that appear misaligned, we demonstrate that they represent distinct forms of information divergence occurring at different interfaces in the human-AI system. Deceptive alignment creates entropy between an agent's true goals and its observable behavior, while goal drift, or confusion, creates entropy between the intended human goal and the agent's actual goal. Though often observationally equivalent, these failures necessitate different interventions. We present a formal model and an illustrative thought experiment to clarify this distinction. We offer a formal language for re-examining prominent alignment challenges observed in Large Language Models (LLMs), offering novel perspectives on their underlying causes.",
    "published": "2025-01-27T19:13:39Z",
    "updated": "2026-01-22T15:27:34Z",
    "link": "http://arxiv.org/pdf/2501.16448v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Robin Young"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16045v1",
    "title": "AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress",
    "summary": "Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.",
    "published": "2026-01-22T15:20:00Z",
    "updated": "2026-01-22T15:20:00Z",
    "link": "http://arxiv.org/pdf/2601.16045v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yue Shi",
      "Liangxiu Han",
      "Xin Zhang",
      "Tam Sobeih",
      "Thomas Gaiser",
      "Nguyen Huu Thuy",
      "Dominik Behrend",
      "Amit Kumar Srivastava",
      "Krishnagopal Halder",
      "Frank Ewert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16038v1",
    "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
    "summary": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
    "published": "2026-01-22T15:11:02Z",
    "updated": "2026-01-22T15:11:02Z",
    "link": "http://arxiv.org/pdf/2601.16038v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Olga Bunkova",
      "Lorenzo Di Fruscia",
      "Sophia Rupprecht",
      "Artur M. Schweidtmann",
      "Marcel J. T. Reinders",
      "Jana M. Weber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16032v1",
    "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10",
    "summary": "High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.",
    "published": "2026-01-22T15:05:31Z",
    "updated": "2026-01-22T15:05:31Z",
    "link": "http://arxiv.org/pdf/2601.16032v1.pdf",
    "category": [
      "cs.PF",
      "cs.AI",
      "cs.LG",
      "cs.OS"
    ],
    "authors": [
      "Yifan Zhu",
      "Yekai Pan",
      "Chen Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06299v4",
    "title": "How malicious AI swarms can threaten democracy: The fusion of agentic AI and LLMs marks a new frontier in information warfare",
    "summary": "Advances in AI offer the prospect of manipulating beliefs and behaviors on a population-wide level. Large language models and autonomous agents now let influence campaigns reach unprecedented scale and precision. Generative tools can expand propaganda output without sacrificing credibility and inexpensively create falsehoods that are rated as more human-like than those written by humans. Techniques meant to refine AI reasoning, such as chain-of-thought prompting, can just as effectively be used to generate more convincing falsehoods. Enabled by these capabilities, a disruptive threat is emerging: swarms of collaborative, malicious AI agents. Fusing LLM reasoning with multi-agent architectures, these systems are capable of coordinating autonomously, infiltrating communities, and fabricating consensus efficiently. By adaptively mimicking human social dynamics, they threaten democracy. Because the resulting harms stem from design, commercial incentives, and governance, we prioritize interventions at multiple leverage points, focusing on pragmatic mechanisms over voluntary compliance.",
    "published": "2025-05-18T13:33:37Z",
    "updated": "2026-01-22T15:03:06Z",
    "link": "http://arxiv.org/pdf/2506.06299v4.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Daniel Thilo Schroeder",
      "Meeyoung Cha",
      "Andrea Baronchelli",
      "Nick Bostrom",
      "Nicholas A. Christakis",
      "David Garcia",
      "Amit Goldenberg",
      "Yara Kyrychenko",
      "Kevin Leyton-Brown",
      "Nina Lutz",
      "Gary Marcus",
      "Filippo Menczer",
      "Gordon Pennycook",
      "David G. Rand",
      "Maria Ressa",
      "Frank Schweitzer",
      "Dawn Song",
      "Christopher Summerfield",
      "Audrey Tang",
      "Jay J. Van Bavel",
      "Sander van der Linden",
      "Jonas R. Kunst"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06795v3",
    "title": "GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning",
    "summary": "Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.",
    "published": "2026-01-11T07:34:41Z",
    "updated": "2026-01-22T14:58:18Z",
    "link": "http://arxiv.org/pdf/2601.06795v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhengqing Yan",
      "Xinyang Liu",
      "Yi Zhang",
      "Fan Guo",
      "ChengXun Jia",
      "Junchen Wan",
      "Yao Liu",
      "Qi Liu",
      "Jihao Huang",
      "Kang Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16027v1",
    "title": "Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment",
    "summary": "The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.",
    "published": "2026-01-22T14:55:51Z",
    "updated": "2026-01-22T14:55:51Z",
    "link": "http://arxiv.org/pdf/2601.16027v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yiran Qiao",
      "Xiang Ao",
      "Jing Chen",
      "Yang Liu",
      "Qiwei Zhong",
      "Qing He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06943v2",
    "title": "PlantTraitNet: An Uncertainty-Aware Multimodal Framework for Global-Scale Plant Trait Inference from Citizen Science Data",
    "summary": "Global plant maps of plant traits, such as leaf nitrogen or plant height, are essential for understanding ecosystem processes, including the carbon and energy cycles of the Earth system. However, existing trait maps remain limited by the high cost and sparse geographic coverage of field-based measurements. Citizen science initiatives offer a largely untapped resource to overcome these limitations, with over 50 million geotagged plant photographs worldwide capturing valuable visual information on plant morphology and physiology. In this study, we introduce PlantTraitNet, a multi-modal, multi-task uncertainty-aware deep learning framework that predictsfour key plant traits (plant height, leaf area, specific leaf area, and nitrogen content) from citizen science photos using weak supervision. By aggregating individual trait predictions across space, we generate global maps of trait distributions. We validate these maps against independent vegetation survey data (sPlotOpen) and benchmark them against leading global trait products. Our results show that PlantTraitNet consistently outperforms existing trait maps across all evaluated traits, demonstrating that citizen science imagery, when integrated with computer vision and geospatial AI, enables not only scalable but also more accurate global trait mapping. This approach offers a powerful new pathway for ecological research and Earth system modeling.",
    "published": "2025-11-10T10:51:04Z",
    "updated": "2026-01-22T14:52:49Z",
    "link": "http://arxiv.org/pdf/2511.06943v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ayushi Sharma",
      "Johanna Trost",
      "Daniel Lusk",
      "Johannes Dollinger",
      "Julian Schrader",
      "Christian Rossi",
      "Javier Lopatin",
      "Etienne Laliberté",
      "Simon Haberstroh",
      "Jana Eichel",
      "Daniel Mederer",
      "Jose Miguel Cerda-Paredes",
      "Shyam S. Phartyal",
      "Lisa-Maricia Schwarz",
      "Anja Linstädter",
      "Maria Conceição Caldeira",
      "Teja Kattenborn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.17873v2",
    "title": "Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach",
    "summary": "Gender classification systems often inherit and amplify demographic imbalances in their training data. We first audit five widely used gender classification datasets, revealing that all suffer from significant intersectional underrepresentation. To measure the downstream impact of these flaws, we train identical MobileNetV2 classifiers on the two most balanced of these datasets, UTKFace and FairFace. Our fairness evaluation shows that even these models exhibit significant bias, misclassifying female faces at a higher rate than male faces and amplifying existing racial skew. To counter these data-induced biases, we construct BalancedFace, a new public dataset created by blending images from FairFace and UTKFace, supplemented with images from other collections to fill missing demographic gaps. It is engineered to equalize subgroup shares across 189 intersections of age, race, and gender using only real, unedited images. When a standard classifier is trained on BalancedFace, it reduces the maximum True Positive Rate gap across racial subgroups by over 50% and brings the average Disparate Impact score 63% closer to the ideal of 1.0 compared to the next-best dataset, all with a minimal loss of overall accuracy. These results underline the profound value of data-centric interventions and provide an openly available resource for fair gender classification research.",
    "published": "2025-10-17T02:09:17Z",
    "updated": "2026-01-22T14:42:57Z",
    "link": "http://arxiv.org/pdf/2510.17873v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Tadesse K Bahiru",
      "Natnael Tilahun Sinshaw",
      "Teshager Hailemariam Moges",
      "Dheeraj Kumar Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.19922v3",
    "title": "Representation-Driven Reinforcement Learning",
    "summary": "We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.",
    "published": "2023-05-31T14:59:12Z",
    "updated": "2026-01-22T14:39:55Z",
    "link": "http://arxiv.org/pdf/2305.19922v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Ofir Nabati",
      "Guy Tennenholtz",
      "Shie Mannor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16011v1",
    "title": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications",
    "summary": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.",
    "published": "2026-01-22T14:38:00Z",
    "updated": "2026-01-22T14:38:00Z",
    "link": "http://arxiv.org/pdf/2601.16011v1.pdf",
    "category": [
      "eess.IV",
      "cs.AI"
    ],
    "authors": [
      "Theodor Forgaard",
      "Jarle H. Reksten",
      "Anders U. Waldeland",
      "Valerio Marsocci",
      "Nicolas Longépé",
      "Michael Kampffmeyer",
      "Arnt-Børre Salberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16007v1",
    "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
    "summary": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
    "published": "2026-01-22T14:33:01Z",
    "updated": "2026-01-22T14:33:01Z",
    "link": "http://arxiv.org/pdf/2601.16007v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chak-Wing Mak",
      "Guanyu Zhu",
      "Boyi Zhang",
      "Hongji Li",
      "Xiaowei Chi",
      "Kevin Zhang",
      "Yichen Wu",
      "Yangfan He",
      "Chun-Kai Fan",
      "Wentao Lu",
      "Kuangzhi Ge",
      "Xinyu Fang",
      "Hongyang He",
      "Kuan Lu",
      "Tianxiang Xu",
      "Li Zhang",
      "Yongxin Ni",
      "Youhua Li",
      "Shanghang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10046v2",
    "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
    "summary": "Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.",
    "published": "2025-12-10T20:04:08Z",
    "updated": "2026-01-22T14:26:01Z",
    "link": "http://arxiv.org/pdf/2512.10046v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yan Zhuang",
      "Jiawei Ren",
      "Xiaokang Ye",
      "Jianzhi Shen",
      "Ruixuan Zhang",
      "Tianai Yue",
      "Muhammad Faayez",
      "Xuhong He",
      "Ziqiao Ma",
      "Lianhui Qin",
      "Zhiting Hu",
      "Tianmin Shu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.06078v2",
    "title": "Explaining k-Nearest Neighbors: Abductive and Counterfactual Explanations",
    "summary": "Despite the wide use of $k$-Nearest Neighbors as classification models, their explainability properties remain poorly understood from a theoretical perspective.\n  While nearest neighbors classifiers offer interpretability from a ``data perspective'', in which the classification of an input vector $\\bar{x}$ is explained by identifying the vectors $\\bar{v}_1, \\ldots, \\bar{v}_k$ in the training set that determine the classification of $\\bar{x}$, we argue that such explanations can be impractical in high-dimensional applications, where each vector has hundreds or thousands of features and it is not clear what their relative importance is. Hence, we focus on understanding nearest neighbor classifications through a ``feature perspective'', in which the goal is to identify how the values of the features in $\\bar{x}$ affect its classification. Concretely, we study abductive explanations such as ``minimum sufficient reasons'', which correspond to sets of features in $\\bar{x}$ that are enough to guarantee its classification, and counterfactual explanations based on the minimum distance feature changes one would have to perform in $\\bar{x}$ to change its classification. We present a detailed landscape of positive and negative complexity results for counterfactual and abductive explanations, distinguishing between discrete and continuous feature spaces, and considering the impact of the choice of distance function involved. Finally, we show that despite some negative complexity results, Integer Quadratic Programming and SAT solving allow for computing explanations in practice.",
    "published": "2025-01-10T16:14:35Z",
    "updated": "2026-01-22T14:23:09Z",
    "link": "http://arxiv.org/pdf/2501.06078v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Pablo Barceló",
      "Alexander Kozachinskiy",
      "Miguel Romero Orth",
      "Bernardo Subercaseaux",
      "José Verschae"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.06518v3",
    "title": "Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings",
    "summary": "Large language models (LLMs) have achieved remarkable success in natural language processing tasks, yet their internal knowledge structures remain poorly understood. This study examines these structures through the lens of historical Olympic medal tallies, evaluating LLMs on two tasks: (1) retrieving medal counts for specific teams and (2) identifying rankings of each team. While state-of-the-art LLMs excel in recalling medal counts, they struggle with providing rankings, highlighting a key difference between their knowledge organization and human reasoning. These findings shed light on the limitations of LLMs' internal knowledge integration and suggest directions for improvement. To facilitate further research, we release our code, dataset, and model outputs.",
    "published": "2024-09-10T13:54:04Z",
    "updated": "2026-01-22T14:18:20Z",
    "link": "http://arxiv.org/pdf/2409.06518v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Juhwan Choi",
      "Seunguk Yu",
      "JungMin Yun",
      "YoungBin Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12061v2",
    "title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation",
    "summary": "Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.",
    "published": "2026-01-17T14:17:13Z",
    "updated": "2026-01-22T14:16:14Z",
    "link": "http://arxiv.org/pdf/2601.12061v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jinsook Lee",
      "Kirk Vanacore",
      "Zhuqian Zhou",
      "Bakhtawar Ahtisham",
      "Jeanine Grutter",
      "Rene F. Kizilcec"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15995v1",
    "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
    "summary": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
    "published": "2026-01-22T14:16:12Z",
    "updated": "2026-01-22T14:16:12Z",
    "link": "http://arxiv.org/pdf/2601.15995v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Liang Wang",
      "Kanzhong Yao",
      "Yang Liu",
      "Weikai Qin",
      "Jun Wu",
      "Zhe Sun",
      "Qiuguo Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.03592v4",
    "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance",
    "summary": "For consumer usage of locally deployed LLMs, the GGUF format and k\\_quantization are invaluable tools for maintaining the performance of the original model while reducing it to sizes deployable with consumer-grade hardware. The number of bits dedicated to each weight from the original model is reduced based on how important they are thought to be during model inference. This importance is arrived at through the application of an 'importance matrix'-a relatively small text document meant to be representative of the LLM's standard use-cases. In the vast majority of quants available online, this document is primarily written in English. It was therefore an open question whether performance on English language tasks was preserved through the sacrifice of multilingual performance and whether it can be preserved with alternate importance matrices. This article investigates these hypotheses by quantizing Llama3.3 70B on importance matrices written in three languages (English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset in both English and Norwegian. All experiments related to yielded non-significant results indicating that current quantization practices do not disproportionately harm multilingual performance.",
    "published": "2025-03-05T15:26:59Z",
    "updated": "2026-01-22T14:12:42Z",
    "link": "http://arxiv.org/pdf/2503.03592v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Karl Audun Borgersen",
      "Morten Goodwin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.03729v2",
    "title": "A Scalable Predictive Modelling Approach to Identifying Duplicate Adverse Event Reports for Drugs and Vaccines",
    "summary": "Objectives: To advance state-of-the-art for duplicate detection in large-scale pharmacovigilance databases and achieve more consistent performance across adverse event reports from different countries.\n  Background: Unlinked adverse event reports referring to the same case impede statistical analysis and may mislead clinical assessment. Pharmacovigilance relies on large databases of adverse event reports to discover potential new causal associations, and computational methods are required to identify duplicates at scale. Current state-of-the-art is statistical record linkage which outperforms rule-based approaches. In particular, vigiMatch is in routine use for VigiBase, the WHO global database of adverse event reports, and represents the first statistical duplicate detection approach in pharmacovigilance deployed at scale. Originally developed for both medicines and vaccines, its application to vaccines has been limited due to inconsistent performance across countries.\n  Methods: This paper extends vigiMatch from probabilistic record linkage to predictive modelling, refining features for medicines, vaccines, and adverse events using country-specific reporting rates, extracting dates from free text, and training separate support vector machine classifiers for medicines and vaccines. Recall was evaluated using 5 independent labelled test sets. Precision was assessed by annotating random selections of report pairs classified as duplicates.\n  Results: Precision for the new method was 92% for vaccines and 54% for medicines, compared with 41% for the comparator method. Recall ranged from 80-85% across test sets for vaccines and from 40-86% for medicines, compared with 24-53% for the comparator method.\n  Conclusion: Predictive modeling, use of free text, and country-specific features advance state-of-the-art for duplicate detection in pharmacovigilance.",
    "published": "2025-03-31T15:24:29Z",
    "updated": "2026-01-22T14:11:43Z",
    "link": "http://arxiv.org/pdf/2504.03729v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jim W. Barrett",
      "Nils Erlanson",
      "Joana Félix China",
      "G. Niklas Norén"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21184v5",
    "title": "Can Language Models Discover Scaling Laws?",
    "summary": "Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate eight diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.",
    "published": "2025-07-27T05:45:26Z",
    "updated": "2026-01-22T14:07:59Z",
    "link": "http://arxiv.org/pdf/2507.21184v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Haowei Lin",
      "Haotian Ye",
      "Wenzheng Feng",
      "Quzhe Huang",
      "Yujun Li",
      "Hubert Lim",
      "Zhengrui Li",
      "Xiangyu Wang",
      "Jianzhu Ma",
      "Yitao Liang",
      "James Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.16602v2",
    "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
    "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
    "published": "2025-12-18T14:43:04Z",
    "updated": "2026-01-22T13:49:34Z",
    "link": "http://arxiv.org/pdf/2512.16602v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Iker García-Ferrero",
      "David Montero",
      "Roman Orus"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06037v4",
    "title": "TeleMem: Building Long-Term and Multimodal Memory for Agentic AI",
    "summary": "Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.",
    "published": "2025-12-12T11:24:52Z",
    "updated": "2026-01-22T13:48:29Z",
    "link": "http://arxiv.org/pdf/2601.06037v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Chunliang Chen",
      "Ming Guan",
      "Xiao Lin",
      "Jiaxu Li",
      "Luxi Lin",
      "Qiyi Wang",
      "Xiangyu Chen",
      "Jixiang Luo",
      "Changzhi Sun",
      "Dell Zhang",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15953v1",
    "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
    "summary": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.",
    "published": "2026-01-22T13:42:08Z",
    "updated": "2026-01-22T13:42:08Z",
    "link": "http://arxiv.org/pdf/2601.15953v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yongyi Wang",
      "Hanyu Liu",
      "Lingfeng Li",
      "Bozhou Chen",
      "Ang Li",
      "Qirui Zheng",
      "Xionghui Yang",
      "Wenxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13710v2",
    "title": "Who Benefits From Sinus Surgery? Comparing Generative AI and Supervised Machine Learning for Predicting Surgical Outcomes in Chronic Rhinosinusitis",
    "summary": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.",
    "published": "2026-01-20T08:07:58Z",
    "updated": "2026-01-22T13:39:47Z",
    "link": "http://arxiv.org/pdf/2601.13710v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sayeed Shafayet Chowdhury",
      "Snehasis Mukhopadhyay",
      "Shiaofen Fang",
      "Vijay R. Ramakrishnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15949v1",
    "title": "Natural Language-Driven Global Mapping of Martian Landforms",
    "summary": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.",
    "published": "2026-01-22T13:38:13Z",
    "updated": "2026-01-22T13:38:13Z",
    "link": "http://arxiv.org/pdf/2601.15949v1.pdf",
    "category": [
      "cs.AI",
      "astro-ph.IM"
    ],
    "authors": [
      "Yiran Wang",
      "Shuoyuan Wang",
      "Zhaoran Wei",
      "Jiannan Zhao",
      "Zhonghua Yao",
      "Zejian Xie",
      "Songxin Zhang",
      "Jun Huang",
      "Bingyi Jing",
      "Hongxin Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.17145v2",
    "title": "Enhanced Fish Freshness Classification with Incremental Handcrafted Feature Fusion",
    "summary": "Accurate assessment of fish freshness remains a major challenge in the food industry, with direct consequences for product quality, market value, and consumer health. Conventional sensory evaluation is inherently subjective, inconsistent, and difficult to standardize across contexts, often limited by subtle, species-dependent spoilage cues. To address these limitations, we propose a handcrafted feature-based approach that systematically extracts and incrementally fuses complementary descriptors, including color statistics, histograms across multiple color spaces, and texture features such as Local Binary Patterns (LBP) and Gray-Level Co-occurrence Matrices (GLCM), from fish eye images. Our method captures global chromatic variations from full images and localized degradations from ROI segments, fusing each independently to evaluate their effectiveness in assessing freshness. Experiments on the Freshness of the Fish Eyes (FFE) dataset demonstrate the approach's effectiveness: in a standard train-test setting, a LightGBM classifier achieved 77.56% accuracy, a 14.35% improvement over the previous deep learning baseline of 63.21%. With augmented data, an Artificial Neural Network (ANN) reached 97.49% accuracy, surpassing the prior best of 77.3% by 20.19%. These results demonstrate that carefully engineered, handcrafted features, when strategically processed, yield a robust, interpretable, and reliable solution for automated fish freshness assessment, providing valuable insights for practical applications in food quality monitoring.",
    "published": "2025-10-20T04:36:34Z",
    "updated": "2026-01-22T13:19:46Z",
    "link": "http://arxiv.org/pdf/2510.17145v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Phi-Hung Hoang",
      "Nam-Thuan Trinh",
      "Van-Manh Tran",
      "Thi-Thu-Hong Phan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.07903v3",
    "title": "Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning",
    "summary": "The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.",
    "published": "2026-01-12T14:55:05Z",
    "updated": "2026-01-22T13:19:01Z",
    "link": "http://arxiv.org/pdf/2601.07903v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jianqi Zhang",
      "Jingyao Wang",
      "Wenwen Qiang",
      "Fanjiang Xu",
      "Changwen Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15931v1",
    "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search",
    "summary": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.",
    "published": "2026-01-22T13:09:22Z",
    "updated": "2026-01-22T13:09:22Z",
    "link": "http://arxiv.org/pdf/2601.15931v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xiangyu Wang",
      "Zhixin Lv",
      "Yongjiao Sun",
      "Anrui Han",
      "Ye Yuan",
      "Hangxu Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15930v1",
    "title": "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging",
    "summary": "Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.",
    "published": "2026-01-22T13:09:16Z",
    "updated": "2026-01-22T13:09:16Z",
    "link": "http://arxiv.org/pdf/2601.15930v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Tianjun Wei",
      "Enneng Yang",
      "Yingpeng Du",
      "Huizhong Guo",
      "Jie Zhang",
      "Zhu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15924v1",
    "title": "Class Confidence Aware Reweighting for Long Tailed Learning",
    "summary": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.",
    "published": "2026-01-22T12:58:05Z",
    "updated": "2026-01-22T12:58:05Z",
    "link": "http://arxiv.org/pdf/2601.15924v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "authors": [
      "Brainard Philemon Jagati",
      "Jitendra Tembhurne",
      "Harsh Goud",
      "Rudra Pratap Singh",
      "Chandrashekhar Meshram"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15915v1",
    "title": "Progressive Power Homotopy for Non-convex Optimization",
    "summary": "We propose a novel first-order method for non-convex optimization of the form $\\max_{\\bm{w}\\in\\mathbb{R}^d}\\mathbb{E}_{\\bm{x}\\sim\\mathcal{D}}[f_{\\bm{w}}(\\bm{x})]$, termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic gradient ascent to a surrogate objective obtained by first performing a power transformation and then Gaussian smoothing, $F_{N,σ}(\\bmμ):=\\mathbb{E}_{\\bm{w}\\sim\\mathcal{N}(\\bmμ,σ^2I_d),\\bm{x}\\sim\\mathcal{D}}[e^{Nf_w(\\bm{x})}]$, while progressively increasing the power parameter $N$ and decreasing the smoothing scale $σ$ along the optimization trajectory. We prove that, under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with an iteration complexity scaling nearly as $O(d^2\\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear advantages in phase retrieval when the samples-to-dimension ratio approaches the information-theoretic limit, and in training two-layer neural networks in under-parameterized regimes. These results suggest that Prog-PowerHP is particularly effective for navigating cluttered non-convex landscapes where standard first-order methods struggle.",
    "published": "2026-01-22T12:44:25Z",
    "updated": "2026-01-22T12:44:25Z",
    "link": "http://arxiv.org/pdf/2601.15915v1.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chen Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15912v1",
    "title": "TeNet: Text-to-Network for Compact Policy Synthesis",
    "summary": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.",
    "published": "2026-01-22T12:42:30Z",
    "updated": "2026-01-22T12:42:30Z",
    "link": "http://arxiv.org/pdf/2601.15912v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Ariyan Bighashdel",
      "Kevin Sebastian Luck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15909v1",
    "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech",
    "summary": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.",
    "published": "2026-01-22T12:38:20Z",
    "updated": "2026-01-22T12:38:20Z",
    "link": "http://arxiv.org/pdf/2601.15909v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Soufiane Jhilal",
      "Stéphanie Martin",
      "Anne-Lise Giraud"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15894v1",
    "title": "Iterative Amortized Hierarchical VAE",
    "summary": "In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.",
    "published": "2026-01-22T12:18:50Z",
    "updated": "2026-01-22T12:18:50Z",
    "link": "http://arxiv.org/pdf/2601.15894v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Simon W. Penninga",
      "Ruud J. G. van Sloun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15120v2",
    "title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories",
    "summary": "LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.",
    "published": "2026-01-21T15:58:54Z",
    "updated": "2026-01-22T12:08:34Z",
    "link": "http://arxiv.org/pdf/2601.15120v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Qian Xiong",
      "Yuekai Huang",
      "Bo Yang",
      "Yujia Zheng",
      "Tianhao Li",
      "Ziyou Jiang",
      "Zhiyuan Chang",
      "Zhaoyang Li",
      "Huanxiang Feng",
      "Mingyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15888v1",
    "title": "Understanding the Transfer Limits of Vision Foundation Models",
    "summary": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.",
    "published": "2026-01-22T12:07:56Z",
    "updated": "2026-01-22T12:07:56Z",
    "link": "http://arxiv.org/pdf/2601.15888v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shiqi Huang",
      "Yipei Wang",
      "Natasha Thorley",
      "Alexander Ng",
      "Shaheer Saeed",
      "Mark Emberton",
      "Shonit Punwani",
      "Veeru Kasivisvanathan",
      "Dean Barratt",
      "Daniel Alexander",
      "Yipeng Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.07315v2",
    "title": "VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing",
    "summary": "Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers.",
    "published": "2026-01-12T08:37:32Z",
    "updated": "2026-01-22T11:46:08Z",
    "link": "http://arxiv.org/pdf/2601.07315v2.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Guanyuan Pan",
      "Shuai Wang",
      "Yugui Lin",
      "Tiansheng Zhou",
      "Pietro Liò",
      "Yaqi Wang",
      "Zhenxin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15876v1",
    "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
    "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
    "published": "2026-01-22T11:36:43Z",
    "updated": "2026-01-22T11:36:43Z",
    "link": "http://arxiv.org/pdf/2601.15876v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Taofeng Xue",
      "Chong Peng",
      "Mianqiu Huang",
      "Linsen Guo",
      "Tiancheng Han",
      "Haozhe Wang",
      "Jianing Wang",
      "Xiaocheng Zhang",
      "Xin Yang",
      "Dengchang Zhao",
      "Jinrui Ding",
      "Xiandi Ma",
      "Yuchen Xie",
      "Peng Pei",
      "Xunliang Cai",
      "Xipeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11003v2",
    "title": "EmbedAgent: Benchmarking Large Language Models in Embedded System Development",
    "summary": "Large Language Models (LLMs) have shown promise in various tasks, yet few benchmarks assess their capabilities in embedded system development.In this paper, we introduce EmbedAgent, a paradigm designed to simulate real-world roles in embedded system development, such as Embedded System Programmer, Architect, and Integrator. This paradigm enables LLMs to be tested in tasks that bridge the gap between digital and physical systems, allowing for a more comprehensive assessment of their capabilities. To evaluate LLMs on these tasks, we propose Embedbench, the first comprehensive benchmark for embedded system programming, circuit design, and cross-platform migration.Embedbench consists of 126 cases, covering 9 electronic components across 3 hardware platforms. Through extensive experiments on 10 mainstream LLMs, we uncover several key findings. Surprisingly, despite the simplicity of the cases, DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic information, and 50.0% when tasked with generating the schematics itself. In the cross-platform migration tasks, LLMs show relatively strong performance with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8% pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4% pass@1.Interestingly, we observe that general-purpose chat LLMs like DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this domain, while reasoning LLMs tend to overthink and overlook efficient knowledge during pretraining. Based on these insights, we propose two strategies: retrieval augmented generation and compiler feedback-to enhance LLM performance. These strategies result in significant improvements, with Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without. Additionally, the accuracy of the Arduino to ESP32 migration task improves from 21.4% to 27.8%.",
    "published": "2025-04-19T12:51:24Z",
    "updated": "2026-01-22T11:31:58Z",
    "link": "http://arxiv.org/pdf/2506.11003v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Ruiyang Xu",
      "Jialun Cao",
      "Mingyuan Wu",
      "Wenliang Zhong",
      "Yaojie Lu",
      "Ben He",
      "Xianpei Han",
      "Shing-Chi Cheung",
      "Le Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15871v1",
    "title": "Why Inference in Large Models Becomes Decomposable After Training",
    "summary": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.",
    "published": "2026-01-22T11:20:57Z",
    "updated": "2026-01-22T11:20:57Z",
    "link": "http://arxiv.org/pdf/2601.15871v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jidong Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15869v1",
    "title": "Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers",
    "summary": "This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.",
    "published": "2026-01-22T11:18:16Z",
    "updated": "2026-01-22T11:18:16Z",
    "link": "http://arxiv.org/pdf/2601.15869v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Francisco Portillo López"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.10501v4",
    "title": "Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings",
    "summary": "This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
    "published": "2025-11-13T17:06:56Z",
    "updated": "2026-01-22T10:36:51Z",
    "link": "http://arxiv.org/pdf/2511.10501v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Georgios Chalkiadakis",
      "Charilaos Akasiadis",
      "Gerasimos Koresis",
      "Stergios Plataniotis",
      "Leonidas Bakopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12962v6",
    "title": "It's complicated. The relationship of algorithmic fairness and non-discrimination provisions for high-risk systems in the EU AI Act",
    "summary": "What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for high-risk systems, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First, a necessary high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second, an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.) Most non-discrimination regulations target only high-risk AI systems. (2.) The regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are partly inconsistent and raise questions of computational feasibility. (3.) Finally, we consider the possible (future) interaction of classical EU non-discrimination law and the AI Act regulations. We recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.",
    "published": "2025-01-22T15:38:09Z",
    "updated": "2026-01-22T10:29:49Z",
    "link": "http://arxiv.org/pdf/2501.12962v6.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Kristof Meding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15828v1",
    "title": "Can professional translators identify machine-generated text?",
    "summary": "This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.",
    "published": "2026-01-22T10:25:52Z",
    "updated": "2026-01-22T10:25:52Z",
    "link": "http://arxiv.org/pdf/2601.15828v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Michael Farrell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01910v2",
    "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning",
    "summary": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.",
    "published": "2026-01-05T08:55:27Z",
    "updated": "2026-01-22T10:24:37Z",
    "link": "http://arxiv.org/pdf/2601.01910v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Minh Hieu Ha",
      "Khanh Ly Ta",
      "Hung Phan",
      "Tung Doan",
      "Tung Dao",
      "Dao Tran",
      "Huynh Thi Thanh Binh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24235v2",
    "title": "PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling",
    "summary": "Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. Generative reward models (GRMs) provide greater interpretability than traditional scalar RMs, but they come with a critical trade-off: pairwise methods are hindered by a training-inference mismatch, while pointwise methods require expensive absolute annotations. To bridge this gap, we propose the Preference-aware Task-adaptive Reward Model (PaTaRM). Unlike prior approaches, PaTaRM enables robust pointwise training using readily available pairwise data via a novel Preference-Aware Reward (PAR) mechanism, eliminating the need for explicit rating labels. Furthermore, it incorporates a Task-Adaptive Rubric system that dynamically generates instance-specific criteria for precise evaluation. Extensive experiments demonstrate that PATRM achieves a 8.7% average improvement on RewardBench and RMBench across Qwen3-8B/14B models. Crucially, it boosts downstream RLHF performance by an average relative improvement of 13.6% across IFEval and InFoBench, validating its effectiveness for policy alignment. Our code is available at https://github.com/JaneEyre0530/PaTaRM.",
    "published": "2025-10-28T09:43:47Z",
    "updated": "2026-01-22T10:20:06Z",
    "link": "http://arxiv.org/pdf/2510.24235v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Ai Jian",
      "Jingqing Ruan",
      "Xing Ma",
      "Dailin Li",
      "Weipeng Zhang",
      "Ke Zeng",
      "Xunliang Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15824v1",
    "title": "Introducing the Generative Application Firewall (GAF)",
    "summary": "This paper introduces the Generative Application Firewall (GAF), a new architectural layer for securing LLM applications. Existing defenses -- prompt filters, guardrails, and data-masking -- remain fragmented; GAF unifies them into a single enforcement point, much like a WAF coordinates defenses for web traffic, while also covering autonomous agents and their tool interactions.",
    "published": "2026-01-22T10:19:24Z",
    "updated": "2026-01-22T10:19:24Z",
    "link": "http://arxiv.org/pdf/2601.15824v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Joan Vendrell Farreny",
      "Martí Jordà Roca",
      "Miquel Cornudella Gaya",
      "Rodrigo Fernández Baón",
      "Víctor García Martínez",
      "Eduard Camacho Sucarrat",
      "Alessandro Pignati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04174v3",
    "title": "Cohesive Group Discovery in Interaction Graphs under Explicit Density Constraints",
    "summary": "Discovering cohesive groups is a fundamental primitive in graph-based recommender systems, underpinning tasks such as social recommendation, bundle discovery, and community-aware modeling. In interaction graphs, cohesion is often modeled as the $γ$-quasi-clique, an induced subgraph whose internal edge density meets a user-defined threshold $γ$. This formulation provides explicit control over within-group connectivity while accommodating the sparsity inherent in real-world data. This paper presents EDQC, an effective framework for cohesive group discovery under explicit density constraints. EDQC leverages a lightweight energy diffusion process to rank vertices for localizing promising candidate regions. Guided by this ranking, the framework extracts and refines a candidate subgraph to ensure the output strictly satisfies the target density requirement. Extensive experiments on 75 real-world graphs across varying density thresholds demonstrate that EDQC identifies the largest mean $γ$-quasi-cliques in the vast majority of cases, achieving lower variance than the state-of-the-art methods while maintaining competitive runtime. Furthermore, statistical analysis confirms that EDQC significantly outperforms the baselines, underscoring its robustness and practical utility for cohesive group discovery in graph-based recommender systems.",
    "published": "2025-08-06T07:59:56Z",
    "updated": "2026-01-22T10:18:02Z",
    "link": "http://arxiv.org/pdf/2508.04174v3.pdf",
    "category": [
      "cs.SI",
      "cs.AI"
    ],
    "authors": [
      "Yu Zhang",
      "Yilong Luo",
      "Mingyuan Ma",
      "Yao Chen",
      "Enqiang Zhu",
      "Jin Xu",
      "Chanjuan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08549v2",
    "title": "Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures",
    "summary": "We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.",
    "published": "2026-01-13T13:36:38Z",
    "updated": "2026-01-22T10:10:04Z",
    "link": "http://arxiv.org/pdf/2601.08549v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sucheta Ghosh",
      "Zahra Monfared",
      "Felix Dietrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15816v1",
    "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents",
    "summary": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability.",
    "published": "2026-01-22T10:04:21Z",
    "updated": "2026-01-22T10:04:21Z",
    "link": "http://arxiv.org/pdf/2601.15816v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI"
    ],
    "authors": [
      "Shiqi Wei",
      "Qiqing Wang",
      "Kaidi Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24592v2",
    "title": "BPMN Assistant: An LLM-Based Approach to Business Process Modeling",
    "summary": "This paper presents BPMN Assistant, a tool that leverages Large Language Models for natural language-based creation and editing of BPMN diagrams. While direct XML generation is common, it is verbose, slow, and prone to syntax errors during complex modifications. We introduce a specialized JSON-based intermediate representation designed to facilitate atomic editing operations through function calling. We evaluate our approach against direct XML manipulation using a suite of state-of-the-art models, including GPT-5.1, Claude 4.5 Sonnet, and DeepSeek V3. Results demonstrate that the JSON-based approach significantly outperforms direct XML in editing tasks, achieving higher or equivalent success rates across all evaluated models. Furthermore, despite requiring more input context, our approach reduces generation latency by approximately 43% and output token count by over 75%, offering a more reliable and responsive solution for interactive process modeling.",
    "published": "2025-09-29T10:56:08Z",
    "updated": "2026-01-22T09:56:31Z",
    "link": "http://arxiv.org/pdf/2509.24592v2.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Josip Tomo Licardo",
      "Nikola Tankovic",
      "Darko Etinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15812v1",
    "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
    "summary": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
    "published": "2026-01-22T09:52:39Z",
    "updated": "2026-01-22T09:52:39Z",
    "link": "http://arxiv.org/pdf/2601.15812v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shir Ashury-Tahan",
      "Yifan Mai",
      "Elron Bandel",
      "Michal Shmueli-Scheuer",
      "Leshem Choshen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15810v1",
    "title": "A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks",
    "summary": "A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.",
    "published": "2026-01-22T09:52:04Z",
    "updated": "2026-01-22T09:52:04Z",
    "link": "http://arxiv.org/pdf/2601.15810v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Mustafa Yurdakul",
      "Enes Ayan",
      "Fahrettin Horasan",
      "Sakir Tasdemir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15808v1",
    "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "summary": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
    "published": "2026-01-22T09:47:31Z",
    "updated": "2026-01-22T09:47:31Z",
    "link": "http://arxiv.org/pdf/2601.15808v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuxuan Wan",
      "Tianqing Fang",
      "Zaitang Li",
      "Yintong Huo",
      "Wenxuan Wang",
      "Haitao Mi",
      "Dong Yu",
      "Michael R. Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23709v2",
    "title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning",
    "summary": "We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised, by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks highlighting the learned representations quality.",
    "published": "2025-05-29T17:42:13Z",
    "updated": "2026-01-22T09:47:19Z",
    "link": "http://arxiv.org/pdf/2505.23709v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Dionysis Christopoulos",
      "Sotiris Spanos",
      "Eirini Baltzi",
      "Valsamis Ntouskos",
      "Konstantinos Karantzalos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15802v1",
    "title": "A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation",
    "summary": "Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.",
    "published": "2026-01-22T09:34:56Z",
    "updated": "2026-01-22T09:34:56Z",
    "link": "http://arxiv.org/pdf/2601.15802v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Alexandre Albore",
      "Humbert Fiorino",
      "Damien Pellier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15798v1",
    "title": "VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management",
    "summary": "Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.",
    "published": "2026-01-22T09:31:19Z",
    "updated": "2026-01-22T09:31:19Z",
    "link": "http://arxiv.org/pdf/2601.15798v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhikai Xue",
      "Tianqianjin Lin",
      "Pengwei Yan",
      "Ruichun Wang",
      "Yuxin Liu",
      "Zhuoren Jiang",
      "Xiaozhong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15797v1",
    "title": "Creativity in the Age of AI: Rethinking the Role of Intentional Agency",
    "summary": "Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.",
    "published": "2026-01-22T09:31:12Z",
    "updated": "2026-01-22T09:31:12Z",
    "link": "http://arxiv.org/pdf/2601.15797v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "James S. Pearson",
      "Matthew J. Dennis",
      "Marc Cheong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06487v2",
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "summary": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
    "published": "2026-01-10T08:43:07Z",
    "updated": "2026-01-22T09:16:57Z",
    "link": "http://arxiv.org/pdf/2601.06487v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qiang Zhang",
      "Boli Chen",
      "Fanrui Zhang",
      "Ruixue Ding",
      "Shihang Wang",
      "Qiuchen Wang",
      "Yinfeng Huang",
      "Haonan Zhang",
      "Rongxiang Zhu",
      "Pengyong Wang",
      "Ailin Ren",
      "Xin Li",
      "Pengjun Xie",
      "Jiawei Liu",
      "Ning Guo",
      "Jingren Zhou",
      "Zheng-Jun Zha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07526v3",
    "title": "Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data",
    "summary": "Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored despite audio's centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data, less than 30K hours (5K unique), Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors are not required for strong performance, even compared to models trained on over 500K hours of data.",
    "published": "2025-09-09T09:01:01Z",
    "updated": "2026-01-22T09:16:28Z",
    "link": "http://arxiv.org/pdf/2509.07526v3.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Gokul Karthik Kumar",
      "Rishabh Saraf",
      "Ludovick Lepauloux",
      "Abdul Muneer",
      "Billel Mokeddem",
      "Hakim Hacid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01747v4",
    "title": "Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization",
    "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs",
    "published": "2026-01-05T02:49:33Z",
    "updated": "2026-01-22T09:09:47Z",
    "link": "http://arxiv.org/pdf/2601.01747v4.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jiwei Guan",
      "Haibo Jin",
      "Haohan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15778v1",
    "title": "Agentic Confidence Calibration",
    "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.",
    "published": "2026-01-22T09:08:25Z",
    "updated": "2026-01-22T09:08:25Z",
    "link": "http://arxiv.org/pdf/2601.15778v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jiaxin Zhang",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09049v3",
    "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning",
    "summary": "Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.",
    "published": "2025-06-10T17:59:44Z",
    "updated": "2026-01-22T08:52:35Z",
    "link": "http://arxiv.org/pdf/2506.09049v3.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Li Kang",
      "Xiufeng Song",
      "Heng Zhou",
      "Yiran Qin",
      "Jie Yang",
      "Xiaohong Liu",
      "Philip Torr",
      "Lei Bai",
      "Zhenfei Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15761v1",
    "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
    "summary": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
    "published": "2026-01-22T08:51:16Z",
    "updated": "2026-01-22T08:51:16Z",
    "link": "http://arxiv.org/pdf/2601.15761v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xiefeng Wu",
      "Mingyu Hu",
      "Shu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15754v1",
    "title": "CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting",
    "summary": "High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.",
    "published": "2026-01-22T08:43:15Z",
    "updated": "2026-01-22T08:43:15Z",
    "link": "http://arxiv.org/pdf/2601.15754v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Ajvad Haneef K",
      "Karan Kuwar Singh",
      "Madhu Kumar S D"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03129v3",
    "title": "Signature-Informed Transformer for Asset Allocation",
    "summary": "Modern deep learning for asset allocation typically separates forecasting from optimization. We argue this creates a fundamental mismatch where minimizing prediction errors fails to yield robust portfolios. We propose the Signature Informed Transformer to address this by unifying feature extraction and decision making into a single policy. Our model employs path signatures to encode complex path dependencies and introduces a specialized attention mechanism that targets geometric asset relationships. By directly minimizing the Conditional Value at Risk we ensure the training objective aligns with financial goals. We prove that our attention module rigorously amplifies signature derived signals. Experiments across diverse equity universes show our approach significantly outperforms both traditional strategies and advanced forecasting baselines. The code is available at: https://anonymous.4open.science/r/Signature-Informed-Transformer-For-Asset-Allocation-DB88",
    "published": "2025-10-03T15:58:21Z",
    "updated": "2026-01-22T08:38:35Z",
    "link": "http://arxiv.org/pdf/2510.03129v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-fin.PM"
    ],
    "authors": [
      "Yoontae Hwang",
      "Stefan Zohren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.14629v2",
    "title": "VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking",
    "summary": "Though vertical federated learning (VFL) is generally considered to be privacy-preserving, recent studies have shown that VFL system is vulnerable to label inference attacks originating from various attack surfaces. Among these attacks, the model completion (MC) attack is currently the most powerful one. Existing defense methods against it either sacrifice model accuracy or incur impractical computational overhead. In this paper, we propose VMask, a novel label privacy protection framework designed to defend against MC attack from the perspective of layer masking. Our key insight is to disrupt the strong correlation between input data and intermediate outputs by applying the secret sharing (SS) technique to mask layer parameters in the attacker's model. We devise a strategy for selecting critical layers to mask, reducing the overhead that would arise from naively applying SS to the entire model. Moreover, VMask is the first framework to offer a tunable privacy budget to defenders, allowing for flexible control over the levels of label privacy according to actual requirements. We built a VFL system, implemented VMask on it, and extensively evaluated it using five model architectures and 13 datasets with different modalities, comparing it to 12 other defense methods. The results demonstrate that VMask achieves the best privacy-utility trade-off, successfully thwarting the MC attack (reducing the label inference accuracy to a random guessing level) while preserving model performance (e.g., in Transformer-based model, the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up to 60,846 times faster than cryptography-based methods, and it only marginally exceeds that of standard VFL by 1.8 times in a large Transformer-based model, which is generally acceptable.",
    "published": "2025-07-19T13:51:09Z",
    "updated": "2026-01-22T08:34:21Z",
    "link": "http://arxiv.org/pdf/2507.14629v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Juntao Tan",
      "Lan Zhang",
      "Zhonghao Hu",
      "Kai Yang",
      "Peng Ran",
      "Bo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.14625v2",
    "title": "VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning",
    "summary": "Vertical federated learning (VFL) enables multiple parties with disjoint features to collaboratively train models without sharing raw data. While privacy vulnerabilities of VFL are extensively-studied, its security threats-particularly targeted label attacks-remain underexplored. In such attacks, a passive party perturbs inputs at inference to force misclassification into adversary-chosen labels. Existing methods rely on unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore anomaly detectors deployed in real-world systems. To bridge this gap, we introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly designed to evade detector-enhanced VFL inference. During the preparation stage, the attacker selects a minimal set of high-expressiveness samples (via maximum mean discrepancy), submits them through VFL protocol to collect predicted labels, and uses these pseudo-labels to train estimated detector and surrogate model on local features. In attack stage, these models guide gradient-based perturbations of remaining samples, crafting adversarial instances that induce targeted misclassifications and evade detection. We implement VTarbel and evaluate it against four model architectures, seven multimodal datasets, and two anomaly detectors. Across all settings, VTarbel outperforms four state-of-the-art baselines, evades detection, and retains effective against three representative privacy-preserving defenses. These results reveal critical security blind spots in current VFL deployments and underscore urgent need for robust, attack-aware defenses.",
    "published": "2025-07-19T13:43:50Z",
    "updated": "2026-01-22T08:30:25Z",
    "link": "http://arxiv.org/pdf/2507.14625v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Juntao Tan",
      "Anran Li",
      "Quanchao Liu",
      "Peng Ran",
      "Lan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.20257v2",
    "title": "Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling",
    "summary": "Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.",
    "published": "2025-11-25T12:36:27Z",
    "updated": "2026-01-22T08:30:16Z",
    "link": "http://arxiv.org/pdf/2511.20257v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhiguo Zhang",
      "Xiaoliang Ma",
      "Daniel Schlesinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15751v1",
    "title": "Tabular Incremental Inference",
    "summary": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.",
    "published": "2026-01-22T08:24:31Z",
    "updated": "2026-01-22T08:24:31Z",
    "link": "http://arxiv.org/pdf/2601.15751v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xinda Chen",
      "Xing Zhen",
      "Hanyu Zhang",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15737v1",
    "title": "PhysProver: Advancing Automatic Theorem Proving for Physics",
    "summary": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.",
    "published": "2026-01-22T08:05:32Z",
    "updated": "2026-01-22T08:05:32Z",
    "link": "http://arxiv.org/pdf/2601.15737v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hanning Zhang",
      "Ruida Wang",
      "Rui Pan",
      "Wenyuan Wang",
      "Bingxu Meng",
      "Tong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15731v1",
    "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging",
    "summary": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.",
    "published": "2026-01-22T07:57:27Z",
    "updated": "2026-01-22T07:57:27Z",
    "link": "http://arxiv.org/pdf/2601.15731v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Linyong Zou",
      "Liang Zhang",
      "Xiongfei Wang",
      "Jia-Hong Gao",
      "Yi Sun",
      "Shurong Sheng",
      "Kuntao Xiao",
      "Wanli Yang",
      "Pengfei Teng",
      "Guoming Luan",
      "Zhao Lv",
      "Zikang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15729v1",
    "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
    "summary": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.",
    "published": "2026-01-22T07:56:36Z",
    "updated": "2026-01-22T07:56:36Z",
    "link": "http://arxiv.org/pdf/2601.15729v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "authors": [
      "Rui Yang",
      "Lei Zheng",
      "Ruoyu Yao",
      "Jun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.00035v3",
    "title": "Is Your Writing Being Mimicked by AI? Unveiling Imitation with Invisible Watermarks in Creative Writing",
    "summary": "Efficient knowledge injection methods for Large Language Models (LLMs), such as In-Context Learning, knowledge editing, and efficient parameter fine-tuning, significantly enhance model utility on downstream tasks. However, they also pose substantial risks of unauthorized imitation and compromised data provenance for high-value unstructured data assets like creative works. Current copyright protection methods for creative works predominantly focus on visual arts, leaving a critical and unaddressed data engineering challenge in the safeguarding of creative writing. In this paper, we propose WIND (Watermarking via Implicit and Non-disruptive Disentanglement), a novel zero-watermarking, verifiable and implicit scheme that safeguards creative writing databases by providing verifiable copyright protection. Specifically, we decompose creative essence into five key elements, which are extracted utilizing LLMs through a designed instance delimitation mechanism and consolidated into condensed-lists. These lists enable WIND to convert core copyright attributes into verifiable watermarks via implicit encoding within a disentanglement creative space, where 'disentanglement' refers to the separation of creative-specific and creative-irrelevant features. This approach, utilizing implicit encoding, avoids distorting fragile textual content. Extensive experiments demonstrate that WIND effectively verifies creative writing copyright ownership against AI imitation, achieving F1 scores above 98% and maintaining robust performance under stringent low false-positive rates where existing state-of-the-art text watermarking methods struggle.",
    "published": "2025-03-30T08:19:12Z",
    "updated": "2026-01-22T07:56:15Z",
    "link": "http://arxiv.org/pdf/2504.00035v3.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Ziwei Zhang",
      "Juan Wen",
      "Wanli Peng",
      "Zhengxian Wu",
      "Yinghan Zhou",
      "Yiming Xue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15728v1",
    "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity",
    "summary": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.",
    "published": "2026-01-22T07:54:45Z",
    "updated": "2026-01-22T07:54:45Z",
    "link": "http://arxiv.org/pdf/2601.15728v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Hangle Hu",
      "Chenyu Hou",
      "Bin Cao",
      "Ruizhe Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.16485v3",
    "title": "Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models",
    "summary": "Customized static operator design has enabled widespread application of Evolutionary Algorithms (EAs), but their search effectiveness often deteriorates as evolutionary progresses. Dynamic operator configuration approaches attempt to alleviate this issue, but they typically rely on predefined operator structures and localized parameter control, lacking sustained adaptive optimization throughout evolution. To overcome these limitations, this work leverages Large Language Models (LLMs) to perceive evolutionary dynamics and enable operator-level meta-evolution. The proposed framework, LLMs for online operator design in Evolutionary Optimization, named LLM4EO, comprises three components: knowledge-transfer-based operator design, evolution perception and analysis, and adaptive operator evolution. Firstly, operators are initialized by leveraging LLMs to distill and transfer knowledge from well-established operators. Then, search behaviors and potential limitations of operators are analyzed by integrating fitness performance with evolutionary features, accompanied by suggestions for improvement. Upon stagnation of population evolution, an LLM-driven meta-operator dynamically optimizes gene selection of operators by prompt-guided improvement strategies. This approach achieves co-evolution of solutions and operators within a unified optimization framework, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs. Finally, extensive experiments on multiple benchmarks of flexible job shop scheduling problem demonstrate that LLM4EO accelerates population evolution and outperforms tailored EAs.",
    "published": "2025-11-20T15:56:09Z",
    "updated": "2026-01-22T07:54:35Z",
    "link": "http://arxiv.org/pdf/2511.16485v3.pdf",
    "category": [
      "cs.NE",
      "cs.AI"
    ],
    "authors": [
      "Rongjie Liao",
      "Junhao Qiu",
      "Xin Chen",
      "Xiaoping Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15724v1",
    "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
    "summary": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.",
    "published": "2026-01-22T07:47:29Z",
    "updated": "2026-01-22T07:47:29Z",
    "link": "http://arxiv.org/pdf/2601.15724v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chenglin Li",
      "Qianglong Chen",
      "Feng Han",
      "Yikun Wang",
      "Xingxi Yin",
      "Yan Gong",
      "Ruilin Li",
      "Yin Zhang",
      "Jiaqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15721v1",
    "title": "CoNRec: Context-Discerning Negative Recommendation with LLMs",
    "summary": "Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.",
    "published": "2026-01-22T07:46:18Z",
    "updated": "2026-01-22T07:46:18Z",
    "link": "http://arxiv.org/pdf/2601.15721v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Xinda Chen",
      "Jiawei Wu",
      "Yishuang Liu",
      "Jialin Zhu",
      "Shuwen Xiao",
      "Junjun Zheng",
      "Xiangheng Kong",
      "Yuning Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15717v1",
    "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
    "summary": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
    "published": "2026-01-22T07:38:27Z",
    "updated": "2026-01-22T07:38:27Z",
    "link": "http://arxiv.org/pdf/2601.15717v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Luyao Zhu",
      "Fangfang Zhang",
      "Yi Mei",
      "Mengjie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15715v1",
    "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind",
    "summary": "Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.",
    "published": "2026-01-22T07:36:48Z",
    "updated": "2026-01-22T07:36:48Z",
    "link": "http://arxiv.org/pdf/2601.15715v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zhitao He",
      "Zongwei Lyu",
      "Yi R Fung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15714v1",
    "title": "Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs",
    "summary": "We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.",
    "published": "2026-01-22T07:36:01Z",
    "updated": "2026-01-22T07:36:01Z",
    "link": "http://arxiv.org/pdf/2601.15714v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Ryoma Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15710v1",
    "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design",
    "summary": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.",
    "published": "2026-01-22T07:31:51Z",
    "updated": "2026-01-22T07:31:51Z",
    "link": "http://arxiv.org/pdf/2601.15710v1.pdf",
    "category": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jiahao Zhang",
      "Zifan He",
      "Nicholas Fraser",
      "Michaela Blott",
      "Yizhou Sun",
      "Jason Cong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15709v1",
    "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL",
    "summary": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.",
    "published": "2026-01-22T07:31:19Z",
    "updated": "2026-01-22T07:31:19Z",
    "link": "http://arxiv.org/pdf/2601.15709v1.pdf",
    "category": [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Asim Biswal",
      "Chuan Lei",
      "Xiao Qin",
      "Aodong Li",
      "Balakrishnan Narayanaswamy",
      "Tim Kraska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15706v1",
    "title": "Improving Methodologies for LLM Evaluations Across Global Languages",
    "summary": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.\n  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.",
    "published": "2026-01-22T07:18:08Z",
    "updated": "2026-01-22T07:18:08Z",
    "link": "http://arxiv.org/pdf/2601.15706v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Akriti Vij",
      "Benjamin Chua",
      "Darshini Ramiah",
      "En Qi Ng",
      "Mahran Morsidi",
      "Naga Nikshith Gangarapu",
      "Sharmini Johnson",
      "Vanessa Wilfred",
      "Vikneswaran Kumaran",
      "Wan Sie Lee",
      "Wenzhuo Yang",
      "Yongsen Zheng",
      "Bill Black",
      "Boming Xia",
      "Frank Sun",
      "Hao Zhang",
      "Qinghua Lu",
      "Suyu Ma",
      "Yue Liu",
      "Chi-kiu Lo",
      "Fatemeh Azadi",
      "Isar Nejadgholi",
      "Sowmya Vajjala",
      "Agnes Delaborde",
      "Nicolas Rolin",
      "Tom Seimandi",
      "Akiko Murakami",
      "Haruto Ishi",
      "Satoshi Sekine",
      "Takayuki Semitsu",
      "Tasuku Sasaki",
      "Angela Kinuthia",
      "Jean Wangari",
      "Michael Michie",
      "Stephanie Kasaon",
      "Hankyul Baek",
      "Jaewon Noh",
      "Kihyuk Nam",
      "Sang Seo",
      "Sungpil Shin",
      "Taewhi Lee",
      "Yongsu Kim",
      "Daisy Newbold-Harrop",
      "Jessica Wang",
      "Mahmoud Ghanem",
      "Vy Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15703v1",
    "title": "Agentic Uncertainty Quantification",
    "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.",
    "published": "2026-01-22T07:16:26Z",
    "updated": "2026-01-22T07:16:26Z",
    "link": "http://arxiv.org/pdf/2601.15703v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jiaxin Zhang",
      "Prafulla Kumar Choubey",
      "Kung-Hsiang Huang",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10350v3",
    "title": "Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories",
    "summary": "Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.",
    "published": "2025-12-11T07:06:14Z",
    "updated": "2026-01-22T07:04:31Z",
    "link": "http://arxiv.org/pdf/2512.10350v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nicolas Tacheny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15698v1",
    "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs",
    "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.",
    "published": "2026-01-22T06:56:27Z",
    "updated": "2026-01-22T06:56:27Z",
    "link": "http://arxiv.org/pdf/2601.15698v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Mingyu Yu",
      "Lana Liu",
      "Zhehao Zhao",
      "Wei Wang",
      "Sujuan Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03088v2",
    "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning",
    "summary": "The mapping from sound to neural activity that underlies hearing is highly non-linear. The first few stages of this mapping in the cochlea have been modelled successfully, with biophysical models built by hand and, more recently, with DNN models trained on datasets simulated by biophysical models. Modelling the auditory brain has been a challenge because central auditory processing is too complex for models to be built by hand, and datasets for training DNN models directly have not been available. Recent work has taken advantage of large-scale high resolution neural recordings from the auditory midbrain to build a DNN model of normal hearing with great success. But this model assumes that auditory processing is the same in all brains, and therefore it cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space of hearing loss directly from recordings of neural activity in the auditory midbrain of healthy and noise exposed animals. With hearing loss parametrised by only 6 free parameters per animal, our model accurately predicts 62% of the explainable variance in neural responses from normal hearing animals and 68% for hearing impaired animals, within a few percentage points of state of the art animal specific models. We demonstrate that the model can be used to simulate realistic activity from out of sample animals by fitting only the learned conditioning parameters with Bayesian optimisation, achieving crossentropy loss within 2% of the optimum in 15-30 iterations. Including more animals in the training data slightly improved the performance on unseen animals. This model will enable future development of parametrised hearing loss compensation models trained to directly restore normal neural coding in hearing impaired brains, which can be quickly fitted for a new user by human in the loop optimisation.",
    "published": "2025-06-03T17:12:21Z",
    "updated": "2026-01-22T06:55:33Z",
    "link": "http://arxiv.org/pdf/2506.03088v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Lloyd Pellatt",
      "Fotios Drakopoulos",
      "Shievanie Sabesan",
      "Nicholas A. Lesica"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18930v2",
    "title": "Dynamic Exploration on Segment-Proposal Graphs for Tubular Centerline Tracking",
    "summary": "Optimal curve methods provide a fundamental framework for tubular centerline tracking. Point-wise approaches, such as minimal paths, are theoretically elegant but often suffer from shortcut and short-branch combination problems in complex scenarios. Nonlocal segment-wise methods address these issues by mapping pre-extracted centerline fragments onto a segment-proposal graph, performing optimization in this abstract space, and recovering the target tubular centerline from the resulting optimal path. In this paradigm, graph construction is critical, as it directly determines the quality of the final result. However, existing segment-wise methods construct graphs in a static manner, requiring all edges and their weights to be pre-computed, i.e. the graph must be sufficiently complete prior to search. Otherwise, the true path may be absent from the candidate space, leading to search failure. To address this limitation, we propose a dynamic exploration scheme for constructing segment-proposal graphs, where the graph is built on demand during the search for optimal paths. By formulating the problem as a Markov decision process, we apply Q-learning to compute edge weights only for visited transitions and adaptively expand the action space when connectivity is insufficient. Experimental results on retinal vessels, roads, and rivers demonstrate consistent improvements over state-of-the-art methods in both accuracy and efficiency.",
    "published": "2025-06-21T11:00:17Z",
    "updated": "2026-01-22T06:45:04Z",
    "link": "http://arxiv.org/pdf/2506.18930v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chong Di",
      "Jinglin Zhang",
      "Zhenjiang Li",
      "Jean-Marie Mirebeau",
      "Da Chen",
      "Laurent D. Cohen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.19950v2",
    "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets",
    "summary": "In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.",
    "published": "2025-10-22T18:22:25Z",
    "updated": "2026-01-22T06:31:29Z",
    "link": "http://arxiv.org/pdf/2510.19950v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Shaocong Ma",
      "Heng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15690v1",
    "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
    "summary": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
    "published": "2026-01-22T06:21:31Z",
    "updated": "2026-01-22T06:21:31Z",
    "link": "http://arxiv.org/pdf/2601.15690v1.pdf",
    "category": [
      "cs.AI",
      "stat.AP"
    ],
    "authors": [
      "Jiaxin Zhang",
      "Wendi Cui",
      "Zhuohang Li",
      "Lifu Huang",
      "Bradley Malin",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15687v1",
    "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation",
    "summary": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.",
    "published": "2026-01-22T06:12:18Z",
    "updated": "2026-01-22T06:12:18Z",
    "link": "http://arxiv.org/pdf/2601.15687v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Khusrav Badalov",
      "Young Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15679v1",
    "title": "Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats",
    "summary": "The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.\n  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.\n  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.\n  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.",
    "published": "2026-01-22T06:00:00Z",
    "updated": "2026-01-22T06:00:00Z",
    "link": "http://arxiv.org/pdf/2601.15679v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ee Wei Seah",
      "Yongsen Zheng",
      "Naga Nikshith",
      "Mahran Morsidi",
      "Gabriel Waikin Loh Matienzo",
      "Nigel Gay",
      "Akriti Vij",
      "Benjamin Chua",
      "En Qi Ng",
      "Sharmini Johnson",
      "Vanessa Wilfred",
      "Wan Sie Lee",
      "Anna Davidson",
      "Catherine Devine",
      "Erin Zorer",
      "Gareth Holvey",
      "Harry Coppock",
      "James Walpole",
      "Jerome Wynee",
      "Magda Dubois",
      "Michael Schmatz",
      "Patrick Keane",
      "Sam Deverett",
      "Bill Black",
      "Bo Yan",
      "Bushra Sabir",
      "Frank Sun",
      "Hao Zhang",
      "Harriet Farlow",
      "Helen Zhou",
      "Lingming Dong",
      "Qinghua Lu",
      "Seung Jang",
      "Sharif Abuadbba",
      "Simon O'Callaghan",
      "Suyu Ma",
      "Tom Howroyd",
      "Cyrus Fung",
      "Fatemeh Azadi",
      "Isar Nejadgholi",
      "Krishnapriya Vishnubhotla",
      "Pulei Xiong",
      "Saeedeh Lohrasbi",
      "Scott Buffett",
      "Shahrear Iqbal",
      "Sowmya Vajjala",
      "Anna Safont-Andreu",
      "Luca Massarelli",
      "Oskar van der Wal",
      "Simon Möller",
      "Agnes Delaborde",
      "Joris Duguépéroux",
      "Nicolas Rolin",
      "Romane Gallienne",
      "Sarah Behanzin",
      "Tom Seimandi",
      "Akiko Murakami",
      "Takayuki Semitsu",
      "Teresa Tsukiji",
      "Angela Kinuthia",
      "Michael Michie",
      "Stephanie Kasaon",
      "Jean Wangari",
      "Hankyul Baek",
      "Jaewon Noh",
      "Kihyuk Nam",
      "Sang Seo",
      "Sungpil Shin",
      "Taewhi Lee",
      "Yongsu Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15678v1",
    "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems",
    "summary": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.",
    "published": "2026-01-22T05:59:42Z",
    "updated": "2026-01-22T05:59:42Z",
    "link": "http://arxiv.org/pdf/2601.15678v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Mengyu Yao",
      "Ziqi Zhang",
      "Ning Luo",
      "Shaofei Li",
      "Yifeng Cai",
      "Xiangqun Chen",
      "Yao Guo",
      "Ding Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15673v1",
    "title": "Enhancing guidance for missing data in diffusion-based sequential recommendation",
    "summary": "Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.",
    "published": "2026-01-22T05:55:21Z",
    "updated": "2026-01-22T05:55:21Z",
    "link": "http://arxiv.org/pdf/2601.15673v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Qilong Yan",
      "Yifei Xing",
      "Dugang Liu",
      "Jingpu Duan",
      "Jian Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15671v1",
    "title": "StreetDesignAI: A Multi-Persona Evaluation System for Inclusive Infrastructure Design",
    "summary": "Designing inclusive cycling infrastructure requires balancing competing needs of diverse user groups, yet designers often struggle to anticipate how different cyclists experience the same street. We investigate how persona-based multi-agent evaluation can support inclusive design by making experiential conflicts explicit. We present StreetDesignAI, an interactive system that enables designers to (1) ground evaluation in street context through imagery and map data, (2) receive parallel feedback from cyclist personas spanning confident to cautious users, and (3) iteratively modify designs while surfacing conflicts across perspectives. A within-subjects study with 26 transportation professionals demonstrates that structured multi-perspective feedback significantly improves designers' understanding of diverse user perspectives, ability to identify persona needs, and confidence in translating them into design decisions, with higher satisfaction and stronger intention for professional adoption. Qualitative findings reveal how conflict surfacing transforms design exploration from single-perspective optimization toward deliberate trade-off reasoning. We discuss implications for AI tools that scaffold inclusive design through disagreement as an interaction primitive.",
    "published": "2026-01-22T05:53:05Z",
    "updated": "2026-01-22T05:53:05Z",
    "link": "http://arxiv.org/pdf/2601.15671v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Ziyi Wang",
      "Yilong Dai",
      "Duanya Lyu",
      "Mateo Nader",
      "Sihan Chen",
      "Wanghao Ye",
      "Zjian Ding",
      "Xiang Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15664v1",
    "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling",
    "summary": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.",
    "published": "2026-01-22T05:23:20Z",
    "updated": "2026-01-22T05:23:20Z",
    "link": "http://arxiv.org/pdf/2601.15664v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hongyang Wei",
      "Hongbo Liu",
      "Zidong Wang",
      "Yi Peng",
      "Baixin Xu",
      "Size Wu",
      "Xuying Zhang",
      "Xianglong He",
      "Zexiang Liu",
      "Peiyu Wang",
      "Xuchen Song",
      "Yangguang Li",
      "Yang Liu",
      "Yahui Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15663v1",
    "title": "TempoNet: Learning Realistic Communication and Timing Patterns for Network Traffic Simulation",
    "summary": "Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.",
    "published": "2026-01-22T05:23:19Z",
    "updated": "2026-01-22T05:23:19Z",
    "link": "http://arxiv.org/pdf/2601.15663v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kristen Moore",
      "Diksha Goel",
      "Cody James Christopher",
      "Zhen Wang",
      "Minjune Kim",
      "Ahmed Ibrahim",
      "Ahmad Mohsin",
      "Seyit Camtepe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04037v2",
    "title": "Evolving in Tasks: Empowering the Multi-modality Large Language Model as the Computer Use Agent",
    "summary": "Computer use agents represent an emerging area in artificial intelligence, aiming to operate computers autonomously to fulfill user tasks, attracting significant attention from both industry and academia. However, the performance of existing agents remains insufficient for practical deployment. In this paper, we propose the Self-Evolution Agent (SEA) for computer operation, alongside three core innovations in data generation, reinforcement learning, and model enhancement to develop this agent. Specifically, we first design an automatic pipeline to generate verifiable task trajectories for training. Second, we propose Efficient Step-wise Reinforcement Learning to reduce the substantial computational overhead of long-horizon training. Finally, we introduce a model enhancement method that integrates grounding and planning capabilities into a single model without additional training. Leveraging these innovations, our SEA (with only 7B parameters) outperforms existing models of the same parameter scale and achieves performance comparable to larger models (e.g., 32B/72B parameters) on computer use tasks. We plan to release the model weights and related code as open-source resources in the future.",
    "published": "2025-08-06T02:57:22Z",
    "updated": "2026-01-22T05:22:03Z",
    "link": "http://arxiv.org/pdf/2508.04037v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuhao Cheng",
      "Liang Tang",
      "Shuxian Li",
      "Yukang Huo",
      "Tiaonan Duan",
      "Kaer Huang",
      "Yanzhe Jing",
      "Yiqiang Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01693v2",
    "title": "SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in Medical Report Generation",
    "summary": "Automated medical report generation (MRG) holds great promise for reducing the heavy workload of radiologists. However, its clinical deployment is hindered by three major sources of uncertainty. First, visual uncertainty, caused by noisy or incorrect view annotations, compromises feature extraction. Second, label distribution uncertainty, stemming from long-tailed disease prevalence, biases models against rare but clinically critical conditions. Third, contextual uncertainty, introduced by unverified historical reports, often leads to factual hallucinations. These challenges collectively limit the reliability and clinical trustworthiness of MRG systems. To address these issues, we propose SURE-Med, a unified framework that systematically reduces uncertainty across three critical dimensions: visual, distributional, and contextual. To mitigate visual uncertainty, a Frontal-Aware View Repair Resampling module corrects view annotation errors and adaptively selects informative features from supplementary views. To tackle label distribution uncertainty, we introduce a Token Sensitive Learning objective that enhances the modeling of critical diagnostic sentences while reweighting underrepresented diagnostic terms, thereby improving sensitivity to infrequent conditions. To reduce contextual uncertainty, our Contextual Evidence Filter validates and selectively incorporates prior information that aligns with the current image, effectively suppressing hallucinations. Extensive experiments on the MIMIC-CXR and IU-Xray benchmarks demonstrate that SURE-Med achieves state-of-the-art performance. By holistically reducing uncertainty across multiple input modalities, SURE-Med sets a new benchmark for reliability in medical report generation and offers a robust step toward trustworthy clinical decision support.",
    "published": "2025-08-03T09:52:30Z",
    "updated": "2026-01-22T05:21:39Z",
    "link": "http://arxiv.org/pdf/2508.01693v2.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Yuhang Gu",
      "Xingyu Hu",
      "Yuyu Fan",
      "Xulin Yan",
      "Longhuan Xu",
      "Peng peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04704v3",
    "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials",
    "summary": "Large Language Models (LLMs) excel at textual reasoning and are beginning to develop spatial understanding, prompting the question of whether these abilities can be combined for complex, domain-specific tasks. This question is essential in fields like materials science, where deep understanding of 3D atomic structures is fundamental. While initial studies have successfully applied LLMs to tasks involving pure crystal generation or coordinate understandings, a standardized benchmark to systematically evaluate their core reasoning abilities across diverse atomic structures has been notably absent. To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on tasks based in Crystallographic Information Files (CIFs), a standard structure representation format. These tasks, including structural editing, CIF perception, and property-guided modeling, reveal a critical limitation: current models, despite establishing promising baselines, consistently fail in structural understanding and spatial reasoning. Our experiments show that these models make frequent errors on structure modification tasks, and even in the basic CIF format understandings, potentially leading to cumulative errors in subsequent analysis and materials insights. By defining these standardized tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale modeling, crucial for accelerating materials research and automating scientific workflows.",
    "published": "2025-10-06T11:17:56Z",
    "updated": "2026-01-22T05:18:03Z",
    "link": "http://arxiv.org/pdf/2510.04704v3.pdf",
    "category": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Taoyuze Lv",
      "Alexander Chen",
      "Fengyu Xie",
      "Chu Wu",
      "Jeffrey Meng",
      "Dongzhan Zhou",
      "Yingheng Wang",
      "Bram Hoex",
      "Zhicheng Zhong",
      "Tong Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15657v1",
    "title": "Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework",
    "summary": "Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.\n  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.\n  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.",
    "published": "2026-01-22T05:13:12Z",
    "updated": "2026-01-22T05:13:12Z",
    "link": "http://arxiv.org/pdf/2601.15657v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yinxi Tian",
      "Changwu Huang",
      "Ke Tang",
      "Xin Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14691v2",
    "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
    "summary": "Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.",
    "published": "2026-01-21T06:07:43Z",
    "updated": "2026-01-22T05:12:15Z",
    "link": "http://arxiv.org/pdf/2601.14691v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Muhammad Khalifa",
      "Lajanugen Logeswaran",
      "Jaekyeom Kim",
      "Sungryull Sohn",
      "Yunxiang Zhang",
      "Moontae Lee",
      "Hao Peng",
      "Lu Wang",
      "Honglak Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15655v1",
    "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams",
    "summary": "Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.",
    "published": "2026-01-22T05:05:53Z",
    "updated": "2026-01-22T05:05:53Z",
    "link": "http://arxiv.org/pdf/2601.15655v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhenghui Guo",
      "Yuanbin Man",
      "Junyuan Sheng",
      "Bowen Lin",
      "Ahmed Ahmed",
      "Bo Jiang",
      "Boyuan Zhang",
      "Miao Yin",
      "Sian Jin",
      "Omprakash Gnawal",
      "Chengming Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03005v4",
    "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale",
    "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.\n  Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper",
    "published": "2025-05-05T20:03:28Z",
    "updated": "2026-01-22T05:03:56Z",
    "link": "http://arxiv.org/pdf/2505.03005v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Daniel Goldstein",
      "Eric Alcaide",
      "Janna Lu",
      "Eugene Cheah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12471v2",
    "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty",
    "summary": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",
    "published": "2026-01-18T16:19:29Z",
    "updated": "2026-01-22T05:03:19Z",
    "link": "http://arxiv.org/pdf/2601.12471v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sravanthi Machcha",
      "Sushrita Yerra",
      "Sahil Gupta",
      "Aishwarya Sahoo",
      "Sharmin Sultana",
      "Hong Yu",
      "Zonghai Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15652v1",
    "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models",
    "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).\n  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\").\n  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.",
    "published": "2026-01-22T05:00:21Z",
    "updated": "2026-01-22T05:00:21Z",
    "link": "http://arxiv.org/pdf/2601.15652v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR",
      "cs.ET"
    ],
    "authors": [
      "Manish Bhatt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.02010v3",
    "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling",
    "summary": "As large language models have grown larger, interest has grown in low-precision numerical formats such as NVFP4 as a way to improve speed and reduce memory usage. However, quantizing models to NVFP4 remains difficult as the lack of precision generally degrades model performance. In this work, we address this issue with Four Over Six (4/6), a modification to the block-scaled NVFP4 quantization algorithm that yields reduced quantization error. Unlike integer formats, floating point formats have non-uniform step sizes which create larger quantization error on larger values. 4/6 takes advantage of this by adaptively scaling some blocks to smaller FP4 values, making the distribution of representable values more uniform and reducing quantization error for near-maximal values. We show that 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, resulting in performance gains during both pre-training and inference with minimal computational overhead. In pre-training experiments with the Nemotron 3 Nano 30B-A3B model architecture, we find that 4/6 brings training loss closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. Our code is available at http://github.com/mit-han-lab/fouroversix.",
    "published": "2025-12-01T18:59:45Z",
    "updated": "2026-01-22T18:49:14Z",
    "link": "http://arxiv.org/pdf/2512.02010v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jack Cook",
      "Junxian Guo",
      "Guangxuan Xiao",
      "Yujun Lin",
      "Song Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21675v2",
    "title": "Is this chart lying to me? Automating the detection of misleading visualizations",
    "summary": "Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also create Misviz-synth, a synthetic dataset of 57,665 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and image-axis classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.",
    "published": "2025-08-29T14:36:45Z",
    "updated": "2026-01-22T18:23:24Z",
    "link": "http://arxiv.org/pdf/2508.21675v2.pdf",
    "category": [
      "cs.CL",
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Jonathan Tonglet",
      "Jan Zimny",
      "Tinne Tuytelaars",
      "Iryna Gurevych"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.05406v4",
    "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
    "summary": "Structured pruning is a promising approach to create smaller, faster large language models. However, existing methods typically rely on computing the gradient via backward passes, which can inflate memory requirements and compute costs. In this work we introduce Bonsai, a gradient-free structured pruning method that eliminates the need for backpropagation, significantly reducing memory requirements and compute costs while achieving state-of-the-art pruning performance. Bonsai uses forward-pass-only perturbative pruning to enable efficient compression of large models on a broader range of hardware configurations. Unlike existing structured pruning approaches, Bonsai not only achieves better compression with fewer resources but also produces models that are twice as fast as those generated by semi-structured pruning. As a concrete demonstration, we use Bonsai to prune 7B and 8B models to 50% sparsity on a single A6000 GPU -- a task challenging for backprop-based methods in memory-constrained settings, as they require 2-3x the memory. Our results show that removing backprop as a requirement not only enables pruning larger models on constrained hardware but can also lead to state-of-the-art efficiency and performance.",
    "published": "2024-02-08T04:48:26Z",
    "updated": "2026-01-22T18:13:50Z",
    "link": "http://arxiv.org/pdf/2402.05406v4.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Steven Kolawole",
      "Lucio Dery",
      "Jean-François Kagy",
      "Virginia Smith",
      "Graham Neubig",
      "Ameet Talwalkar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07519v2",
    "title": "GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval",
    "summary": "Decomposition-based multi-hop retrieval methods rely on many autoregressive steps to break down complex queries, which breaks end-to-end differentiability and is computationally expensive. Decomposition-free methods tackle this, but current decomposition-free approaches struggle with longer multi-hop problems and generalization to out-of-distribution data. To address these challenges, we introduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks. GRITHopper combines generative and representational instruction tuning by integrating causal language modeling with dense retrieval training. Through controlled studies, we find that incorporating additional context after the retrieval process, referred to as post-retrieval language modeling, enhances dense retrieval performance. By including elements such as final answers during training, the model learns to better contextualize and retrieve relevant information. GRITHopper-7B offers a robust, scalable, and generalizable solution for multi-hop dense retrieval, and we release it to the community for future research and applications requiring multi-hop reasoning and retrieval capabilities.",
    "published": "2025-03-10T16:42:48Z",
    "updated": "2026-01-22T18:12:25Z",
    "link": "http://arxiv.org/pdf/2503.07519v2.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Justus-Jonas Erker",
      "Nils Reimers",
      "Iryna Gurevych"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06342v3",
    "title": "The exponential distribution of the order of demonstrative, numeral, adjective and noun",
    "summary": "The frequency of the preferred order for a noun phrase formed by demonstrative, numeral, adjective and noun has received significant attention over the last two decades. We investigate the actual distribution of the 24 possible orders. There is no consensus on whether it is well-fitted by an exponential or a power law distribution. We find that an exponential distribution is a much better model. This finding and other circumstances where an exponential-like distribution is found challenge the view that power-law distributions, e.g., Zipf's law for word frequencies, are inevitable. We also investigate which of two exponential distributions gives a better fit: an exponential model where the 24 orders have non-zero probability (a geometric distribution truncated at rank 24) or an exponential model where the number of orders that can have non-zero probability is variable (a right-truncated geometric distribution). When consistency and generalizability are prioritized, we find higher support for the exponential model where all 24 orders have non-zero probability. These findings strongly suggest that there is no hard constraint on word order variation and then unattested orders merely result from undersampling, consistently with Cysouw's view.",
    "published": "2025-02-10T10:45:00Z",
    "updated": "2026-01-22T17:58:08Z",
    "link": "http://arxiv.org/pdf/2502.06342v3.pdf",
    "category": [
      "cs.CL",
      "physics.soc-ph"
    ],
    "authors": [
      "Ramon Ferrer-i-Cancho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16138v1",
    "title": "Automatic Classification of Arabic Literature into Historical Eras",
    "summary": "The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.",
    "published": "2026-01-22T17:32:19Z",
    "updated": "2026-01-22T17:32:19Z",
    "link": "http://arxiv.org/pdf/2601.16138v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Zainab Alhathloul",
      "Irfan Ahmad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16125v1",
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
    "published": "2026-01-22T17:26:52Z",
    "updated": "2026-01-22T17:26:52Z",
    "link": "http://arxiv.org/pdf/2601.16125v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Tingyu Song",
      "Yanzhao Zhang",
      "Mingxin Li",
      "Zhuoning Guo",
      "Dingkun Long",
      "Pengjun Xie",
      "Siyue Zhang",
      "Yilun Zhao",
      "Shu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16113v1",
    "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier",
    "summary": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\n  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\n  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.",
    "published": "2026-01-22T17:01:33Z",
    "updated": "2026-01-22T17:01:33Z",
    "link": "http://arxiv.org/pdf/2601.16113v1.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Haq Nawaz Malik",
      "Kh Mohmad Shafi",
      "Tanveer Ahmad Reshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.09631v3",
    "title": "LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation",
    "summary": "Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant \"Reasoning Gap\": while native-like models (Claude 3.7) perform intuitively (40\\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\\% valid poems), while our hybrid verification loop restores performance to 73.1\\%. We release our system and a corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.",
    "published": "2026-01-14T17:05:17Z",
    "updated": "2026-01-22T16:53:21Z",
    "link": "http://arxiv.org/pdf/2601.09631v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Stergios Chatzikyriakidis",
      "Anastasia Natsina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16097v1",
    "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
    "summary": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.",
    "published": "2026-01-22T16:46:57Z",
    "updated": "2026-01-22T16:46:57Z",
    "link": "http://arxiv.org/pdf/2601.16097v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Makbule Gulcin Ozsoy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14944v2",
    "title": "The GDN-CC Dataset: Automatic Corpus Clarification for AI-enhanced Democratic Citizen Consultations",
    "summary": "LLMs are ubiquitous in modern NLP, and while their applicability extends to texts produced for democratic activities such as online deliberations or large-scale citizen consultations, ethical questions have been raised for their usage as analysis tools. We continue this line of research with two main goals: (a) to develop resources that can help standardize citizen contributions in public forums at the pragmatic level, and make them easier to use in topic modeling and political analysis; (b) to study how well this standardization can reliably be performed by small, open-weights LLMs, i.e. models that can be run locally and transparently with limited resources. Accordingly, we introduce Corpus Clarification as a preprocessing framework for large-scale consultation data that transforms noisy, multi-topic contributions into structured, self-contained argumentative units ready for downstream analysis. We present GDN-CC, a manually-curated dataset of 1,231 contributions to the French Grand Débat National, comprising 2,285 argumentative units annotated for argumentative structure and manually clarified. We then show that finetuned Small Language Models match or outperform LLMs on reproducing these annotations, and measure their usability for an opinion clustering task. We finally release GDN-CC-large, an automatically annotated corpus of 240k contributions, the largest annotated democratic consultation dataset to date.",
    "published": "2026-01-21T12:43:07Z",
    "updated": "2026-01-22T16:36:28Z",
    "link": "http://arxiv.org/pdf/2601.14944v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Pierre-Antoine Lequeu",
      "Léo Labat",
      "Laurène Cave",
      "Gaël Lejeune",
      "François Yvon",
      "Benjamin Piwowarski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16034v1",
    "title": "Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction",
    "summary": "Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.",
    "published": "2026-01-22T15:08:28Z",
    "updated": "2026-01-22T15:08:28Z",
    "link": "http://arxiv.org/pdf/2601.16034v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tony Cristofano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.04779v2",
    "title": "MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark",
    "summary": "Speech inherently contains rich acoustic information that extends far beyond the textual language. In real-world spoken language understanding, effective interpretation often requires integrating semantic meaning (e.g., content), paralinguistic features (e.g., emotions, speed, pitch) and phonological characteristics (e.g., prosody, intonation, rhythm), which are embedded in speech. While recent multimodal Speech Large Language Models (SpeechLLMs) have demonstrated remarkable capabilities in processing audio information, their ability to perform fine-grained perception and complex reasoning in natural speech remains largely unexplored. To address this gap, we introduce MMSU, a comprehensive benchmark designed specifically for understanding and reasoning in spoken language. MMSU comprises 5,000 meticulously curated audio-question-answer triplets across 47 distinct tasks. To ground our benchmark in linguistic theory, we systematically incorporate a wide range of linguistic phenomena, including phonetics, prosody, rhetoric, syntactics, semantics, and paralinguistics. Through a rigorous evaluation of 14 advanced SpeechLLMs, we identify substantial room for improvement in existing models, highlighting meaningful directions for future optimization. MMSU establishes a new standard for comprehensive assessment of spoken language understanding, providing valuable insights for developing more sophisticated human-AI speech interaction systems. MMSU benchmark is available at https://huggingface.co/datasets/ddwang2000/MMSU. Evaluation Code is available at https://github.com/dingdongwang/MMSU_Bench.",
    "published": "2025-06-05T09:09:36Z",
    "updated": "2026-01-22T14:58:30Z",
    "link": "http://arxiv.org/pdf/2506.04779v2.pdf",
    "category": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Dingdong Wang",
      "Jincenzi Wu",
      "Junan Li",
      "Dongchao Yang",
      "Xueyuan Chen",
      "Tianhua Zhang",
      "Helen Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16018v1",
    "title": "Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain",
    "summary": "This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.",
    "published": "2026-01-22T14:41:32Z",
    "updated": "2026-01-22T14:41:32Z",
    "link": "http://arxiv.org/pdf/2601.16018v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Özgür Uğur",
      "Mahmut Göksu",
      "Mahmut Çimen",
      "Musa Yılmaz",
      "Esra Şavirdi",
      "Alp Talha Demir",
      "Rumeysa Güllüce",
      "İclal Çetin",
      "Ömer Can Sağbaş"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.17914v3",
    "title": "Vision-Language Models Align with Human Neural Representations in Concept Processing",
    "summary": "Recent studies suggest that transformer-based vision-language models (VLMs) capture the multimodality of concept processing in the human brain. However, a systematic evaluation exploring different types of VLM architectures and the role played by visual and textual context is still lacking. Here, we analyse multiple VLMs employing different strategies to integrate visual and textual modalities, along with language-only counterparts. We measure the alignment between concept representations by models and existing (fMRI) brain responses to concept words presented in two experimental conditions, where either visual (pictures) or textual (sentences) context is provided. Our results reveal that VLMs outperform the language-only counterparts in both experimental conditions. However, controlled ablation studies show that only for some VLMs, such as LXMERT and IDEFICS2, brain alignment stems from genuinely learning more human-like concepts during pretraining, while others are highly sensitive to the context provided at inference. Additionally, we find that vision-language encoders are more brain-aligned than more recent, generative VLMs. Altogether, our study shows that VLMs align with human neural representations in concept processing, while highlighting differences among architectures. We open-source code and materials to reproduce our experiments at: https://github.com/dmg-illc/vl-concept-processing.",
    "published": "2024-07-25T10:08:37Z",
    "updated": "2026-01-22T14:40:04Z",
    "link": "http://arxiv.org/pdf/2407.17914v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Anna Bavaresco",
      "Marianne de Heer Kloots",
      "Sandro Pezzelle",
      "Raquel Fernández"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13353v3",
    "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning",
    "summary": "Large language models (LLMs) are increasingly deployed for understanding large codebases, but whether they understand operational semantics of long code context or rely on pattern matching shortcuts remains unclear. We distinguish between lexical recall (retrieving code verbatim) and semantic recall (understanding operational semantics). Evaluating 10 state-of-the-art LLMs, we find that while frontier models achieve near-perfect, position-independent lexical recall, semantic recall degrades severely when code is centrally positioned in long contexts. We introduce semantic recall sensitivity to measure whether tasks require understanding of code's operational semantics vs. permit pattern matching shortcuts. Through a novel counterfactual measurement method, we show that models rely heavily on pattern matching shortcuts to solve existing code understanding benchmarks. We propose a new task SemTrace, which achieves high semantic recall sensitivity through unpredictable operations; LLMs' accuracy exhibits severe positional effects, with median accuracy drops of 92.73% versus CRUXEval's 53.36% as the relevant code snippet approaches the middle of the input code context. Our findings suggest current evaluations substantially underestimate semantic recall failures in long context code understanding.",
    "published": "2025-05-19T16:56:31Z",
    "updated": "2026-01-22T14:25:19Z",
    "link": "http://arxiv.org/pdf/2505.13353v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Adam Štorek",
      "Mukur Gupta",
      "Samira Hajizadeh",
      "Prashast Srivastava",
      "Suman Jana"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20062v2",
    "title": "Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays",
    "summary": "People are increasingly using technologies equipped with large language models (LLM) to write texts for formal communication, which raises two important questions at the intersection of technology and society: Who do LLMs write like (model alignment); and can LLMs be prompted to change who they write like (model steerability). We investigate these questions in the high-stakes context of undergraduate admissions at a selective university by comparing lexical and sentence variation between essays written by 30,000 applicants to two types of LLM-generated essays: one prompted with only the essay question used by the human applicants; and another with additional demographic information about each applicant. We consistently find that both types of LLM-generated essays are linguistically distinct from human-authored essays, regardless of the specific model and analytical approach. Further, prompting a specific sociodemographic identity is remarkably ineffective in aligning the model with the linguistic patterns observed in human writing from this identity group. This holds along the key dimensions of sex, race, first-generation status, and geographic location. The demographically prompted and unprompted synthetic texts were also more similar to each other than to the human text, meaning that prompting did not alleviate homogenization. These issues of model alignment and steerability in current LLMs raise concerns about the use of LLMs in high-stakes contexts.",
    "published": "2025-03-25T20:54:50Z",
    "updated": "2026-01-22T14:11:05Z",
    "link": "http://arxiv.org/pdf/2503.20062v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jinsook Lee",
      "AJ Alvero",
      "Thorsten Joachims",
      "René Kizilcec"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.06094v3",
    "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline",
    "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern LLMs as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' metalinguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We construct a novel, scalable evaluation framework for this task, evaluating metrics measuring consistency and typological diversity. Automatic and manual evaluations demonstrate ConlangCrafter's ability to produce coherent and varied conlangs without human linguistic expertise.",
    "published": "2025-08-08T07:36:48Z",
    "updated": "2026-01-22T13:54:42Z",
    "link": "http://arxiv.org/pdf/2508.06094v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Morris Alper",
      "Moran Yanuka",
      "Raja Giryes",
      "Gašper Beguš"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.14693v4",
    "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search",
    "summary": "Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process. Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier. Applied to the various ML tasks, our approach demonstrates a 4% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems. Resource available at https://github.com/jokieleung/I-MCTS",
    "published": "2025-02-20T16:19:09Z",
    "updated": "2026-01-22T13:08:53Z",
    "link": "http://arxiv.org/pdf/2502.14693v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zujie Liang",
      "Feng Wei",
      "Wujiang Xu",
      "Lin Chen",
      "Yuxi Qian",
      "Xinhui Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.08606v4",
    "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures",
    "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document understanding that eliminates the need for absolute 2D positional embeddings. We extend self-attention to take into account text block positions in relative polar coordinate system rather than the Cartesian one. Despite being pre-trained on a dataset more than six times smaller than the widely used IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results demonstrate that a carefully designed attention mechanism can compensate for reduced pre-training data, offering an efficient and effective alternative for document understanding.",
    "published": "2025-07-11T14:00:56Z",
    "updated": "2026-01-22T12:57:43Z",
    "link": "http://arxiv.org/pdf/2507.08606v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Benno Uthayasooriyar",
      "Antoine Ly",
      "Franck Vermet",
      "Caio Corro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08131v2",
    "title": "Attention Projection Mixing with Exogenous Anchors",
    "summary": "Cross-layer reuse of early attention projections can improve optimization and data efficiency, but it creates a structural conflict: the first layer must simultaneously act as a stable, reusable anchor for all deeper layers and as an effective computational block. We show this ''first-layer tension'' is a hidden limiter of internal-anchor designs. We propose ExoFormer, which resolves the conflict by learning exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. We introduce a unified normalized mixing framework that mixes queries, keys, values, and gate logits using learnable coefficients (exploring coefficient granularities: elementwise/headwise/scalar), and we show that normalizing anchor sources is key to stable reuse. ExoFormer variants consistently outperform their internal-anchor counterparts, and the dynamic variant yields 1.5 downstream accuracy points while matching validation loss using 1.5x fewer tokens than Gated Attention. We explain this efficacy via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in refinement. We release code and models to facilitate future research.",
    "published": "2026-01-13T01:52:19Z",
    "updated": "2026-01-22T12:45:06Z",
    "link": "http://arxiv.org/pdf/2601.08131v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jonathan Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12365v3",
    "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics",
    "summary": "This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.",
    "published": "2025-06-14T05:55:19Z",
    "updated": "2026-01-22T12:17:59Z",
    "link": "http://arxiv.org/pdf/2506.12365v3.pdf",
    "category": [
      "cs.CL",
      "cs.DB"
    ],
    "authors": [
      "Asifullah Khan",
      "Muhammad Zaeem Khan",
      "Saleha Jamshed",
      "Sadia Ahmad",
      "Aleesha Zainab",
      "Kaynat Khatib",
      "Faria Bibi",
      "Abdul Rehman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15892v1",
    "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
    "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
    "published": "2026-01-22T12:13:17Z",
    "updated": "2026-01-22T12:13:17Z",
    "link": "http://arxiv.org/pdf/2601.15892v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Chenghao Fan",
      "Wen Heng",
      "Bo Li",
      "Sichen Liu",
      "Yuxuan Song",
      "Jing Su",
      "Xiaoye Qu",
      "Kai Shen",
      "Wei Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14750v2",
    "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
    "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT",
    "published": "2026-01-21T08:09:25Z",
    "updated": "2026-01-22T12:09:02Z",
    "link": "http://arxiv.org/pdf/2601.14750v2.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Yifan Wang",
      "Shiyu Li",
      "Peiming Li",
      "Xiaochen Yang",
      "Yang Tang",
      "Zheng Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15879v1",
    "title": "Evaluating and Achieving Controllable Code Completion in Code LLM",
    "summary": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.",
    "published": "2026-01-22T11:40:04Z",
    "updated": "2026-01-22T11:40:04Z",
    "link": "http://arxiv.org/pdf/2601.15879v1.pdf",
    "category": [
      "cs.SE",
      "cs.CL"
    ],
    "authors": [
      "Jiajun Zhang",
      "Zeyu Cui",
      "Lei Zhang",
      "Jian Yang",
      "Jiaxi Yang",
      "Qiang Liu",
      "Zilei Wang",
      "Binyuan Hui",
      "Liang Wang",
      "Junyang Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07272v4",
    "title": "GENERator: A Long-Context Generative Genomic Foundation Model",
    "summary": "The rapid advancement of DNA sequencing has produced vast genomic datasets, yet interpreting and engineering genomic function remain fundamental challenges. Recent large language models have opened new avenues for genomic analysis, but existing approaches are often limited by restricted training scope, constrained generative capability, or prohibitive computational cost. We introduce GENErator, a generative genomic foundation model for long-context DNA modeling, with a context length of 98k nucleotides, pre-trained on 386 billion nucleotides of eukaryotic DNA. Without task-specific fine-tuning, GENERator exhibits strong intrinsic capabilities: unsupervised embedding analyses reveal phylogenetically coherent structure, and sequence recovery benchmarks demonstrate generative accuracy comparable to or exceeding state-of-the-art models with substantially improved computational efficiency. In a zero-shot setting, GENERator achieves competitive variant effect prediction performance relative to alignment-based methods, while remaining fully alignment-free and broadly applicable across species. With task-specific fine-tuning, the model attains leading performance on established genomic benchmarks. We further demonstrate practical generative applications. GENERator can generate protein-coding DNA sequences that translate into structurally plausible proteins and, through a prompt-guided design framework, design cis-regulatory elements with targeted activity profiles, including synthetic super-enhancers validated by high-throughput UMI-STARR-seq assays. Together, these results establish GENERator as an efficient and biologically grounded framework for genomic interpretation and programmable sequence design. Code and supplementary resources are available at https://github.com/GenerTeam/GENERator.",
    "published": "2025-02-11T05:39:49Z",
    "updated": "2026-01-22T11:18:08Z",
    "link": "http://arxiv.org/pdf/2502.07272v4.pdf",
    "category": [
      "cs.CL",
      "q-bio.GN"
    ],
    "authors": [
      "Wei Wu",
      "Qiuyi Li",
      "Yuanyuan Zhang",
      "Zhihao Zhan",
      "Ruipu Chen",
      "Mingyang Li",
      "Kun Fu",
      "Junyan Qi",
      "Yongzhou Bao",
      "Chao Wang",
      "Yiheng Zhu",
      "Zhiyun Zhang",
      "Jian Tang",
      "Fuli Feng",
      "Jieping Ye",
      "Yuwen Liu",
      "Hui Xiong",
      "Zheng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.11020v2",
    "title": "From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models",
    "summary": "Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with those from an ablated variant in which the retrieval heads are masked. This mechanism-based approach achieves substantial improvements: +2.28 points on HELMET at 128K for Llama-3.1, with +70% gains on generation with citation and +32% on passage re-ranking, while preserving performance on general tasks. Experiments across three model families reveal that the effectiveness depends on retrieval head organization: models with concentrated patterns of retrieval heads respond strongly, while those with distributed patterns show limited gains. This mechanistic relationship validates the function of retrieval heads and demonstrates that mechanistic insights can be transformed into performance enhancements.",
    "published": "2026-01-16T06:31:08Z",
    "updated": "2026-01-22T11:02:26Z",
    "link": "http://arxiv.org/pdf/2601.11020v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Youmi Ma",
      "Naoaki Okazaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15846v1",
    "title": "Determinants of Training Corpus Size for Clinical Text Classification",
    "summary": "Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.\n  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.\n  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.",
    "published": "2026-01-22T10:53:50Z",
    "updated": "2026-01-22T10:53:50Z",
    "link": "http://arxiv.org/pdf/2601.15846v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jaya Chaturvedi",
      "Saniya Deshpande",
      "Chenkai Ma",
      "Robert Cobb",
      "Angus Roberts",
      "Robert Stewart",
      "Daniel Stahl",
      "Diana Shamsutdinova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13137v2",
    "title": "Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains",
    "summary": "With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.",
    "published": "2026-01-19T15:21:26Z",
    "updated": "2026-01-22T10:39:44Z",
    "link": "http://arxiv.org/pdf/2601.13137v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuan Gao",
      "Zhigang Liu",
      "Xinyu Yao",
      "Bo Chen",
      "Xiaobing Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15820v1",
    "title": "ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection",
    "summary": "The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.",
    "published": "2026-01-22T10:10:06Z",
    "updated": "2026-01-22T10:10:06Z",
    "link": "http://arxiv.org/pdf/2601.15820v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Guoxuan Ding",
      "Yuqing Li",
      "Ziyan Zhou",
      "Zheng Lin",
      "Daren Zha",
      "Jiangnan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22777v5",
    "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators",
    "summary": "Evaluating the quality of open-domain chatbots has become increasingly reliant on LLMs acting as automatic judges. However, existing meta-evaluation benchmarks are static, outdated, and lacking in multilingual coverage, limiting their ability to fully capture subtle weaknesses in evaluation. We introduce MEDAL, an automated multi-agent framework for curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. Using MEDAL, we uncover that state-of-the-art judges fail to reliably detect nuanced issues such as lack of empathy, commonsense, or relevance.",
    "published": "2025-05-28T18:45:42Z",
    "updated": "2026-01-22T09:51:58Z",
    "link": "http://arxiv.org/pdf/2505.22777v5.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "John Mendonça",
      "Alon Lavie",
      "Isabel Trancoso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15809v1",
    "title": "SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics",
    "summary": "An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.",
    "published": "2026-01-22T09:49:29Z",
    "updated": "2026-01-22T09:49:29Z",
    "link": "http://arxiv.org/pdf/2601.15809v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Silvia Casola",
      "Ryan Soh-Eun Shim",
      "Felicia Körner",
      "Yuchen Mao",
      "Barbara Plank"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15793v1",
    "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature",
    "summary": "Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.",
    "published": "2026-01-22T09:27:27Z",
    "updated": "2026-01-22T09:27:27Z",
    "link": "http://arxiv.org/pdf/2601.15793v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuxuan Lei",
      "Tianfu Wang",
      "Jianxun Lian",
      "Zhengyu Hu",
      "Defu Lian",
      "Xing Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15118v2",
    "title": "WavLink: Compact Audio-Text Embeddings with a Global Whisper Token",
    "summary": "Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.",
    "published": "2026-01-21T15:55:58Z",
    "updated": "2026-01-22T08:55:20Z",
    "link": "http://arxiv.org/pdf/2601.15118v2.pdf",
    "category": [
      "cs.SD",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Gokul Karthik Kumar",
      "Ludovick Lepauloux",
      "Hakim Hacid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15755v1",
    "title": "Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs",
    "summary": "Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.",
    "published": "2026-01-22T08:45:55Z",
    "updated": "2026-01-22T08:45:55Z",
    "link": "http://arxiv.org/pdf/2601.15755v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tristan Williams",
      "Franziska Weeber",
      "Sebastian Padó",
      "Alan Akbik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21143v3",
    "title": "The Percept-V Challenge: Can Multimodal LLMs Crack Simple Perception Problems?",
    "summary": "Cognitive science research treats visual perception, the ability to understand and make sense of a visual input, as one of the early developmental signs of intelligence. Its TVPS-4 framework categorizes and tests human perception into seven skills such as visual discrimination, and form constancy. Do Multimodal Large Language Models (MLLMs) match up to humans in basic perception? Even though there are many benchmarks that evaluate MLLMs on advanced reasoning and knowledge skills, there is limited research that focuses evaluation on simple perception. In response, we introduce Percept-V, a dataset containing 6000 program-generated uncontaminated images divided into 30 domains, where each domain tests one or more TVPS-4 skills. Our focus is on perception, so we make our domains quite simple and the reasoning and knowledge required for solving them are minimal. Since modern-day MLLMs can solve much more complex tasks, our a-priori expectation is that they will solve these domains very easily. Contrary to our belief, our experiments show a weak performance of SoTA proprietary and open-source MLLMs compared to very high human performance on Percept-V. We find that as number of objects in the image increases, performance goes down rather fast. Our experiments also identify the perception skills that are considerably harder for all models.",
    "published": "2025-08-28T18:22:38Z",
    "updated": "2026-01-22T08:36:54Z",
    "link": "http://arxiv.org/pdf/2508.21143v3.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Samrajnee Ghosh",
      "Naman Agarwal",
      "Hemanshu Garg",
      "Chinmay Mittal",
      " Mausam",
      "Parag Singla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.13753v2",
    "title": "SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning",
    "summary": "Long-context understanding has emerged as a critical capability for large language models (LLMs). However, evaluating this ability remains challenging. We present SCALAR, a benchmark designed to assess citation-grounded long-context reasoning in academic writing. SCALAR leverages academic papers and their citation structure to automatically generate high-quality ground-truth labels without human annotation. It features controllable difficulty levels and a dynamic updating mechanism that mitigates data contamination. The benchmark includes two tasks: a multiple-choice QA format and a cloze-style citation prediction. We evaluate a range of state-of-the-art LLMs and find that the multiple-choice task effectively distinguishes model capabilities. While human experts achieve over 90% accuracy, most models struggle. The cloze-style task is even more challenging, with no model exceeding 50% accuracy. SCALAR provides a domain-grounded, continuously updating framework for tracking progress in citation-based long-context understanding.",
    "published": "2025-02-19T14:15:49Z",
    "updated": "2026-01-22T08:28:25Z",
    "link": "http://arxiv.org/pdf/2502.13753v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Renxi Wang",
      "Honglin Mu",
      "Liqun Ma",
      "Lizhi Lin",
      "Yunlong Feng",
      "Timothy Baldwin",
      "Xudong Han",
      "Haonan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14004v2",
    "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
    "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.",
    "published": "2026-01-20T14:23:23Z",
    "updated": "2026-01-22T08:25:46Z",
    "link": "http://arxiv.org/pdf/2601.14004v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hengyuan Zhang",
      "Zhihao Zhang",
      "Mingyang Wang",
      "Zunhai Su",
      "Yiwei Wang",
      "Qianli Wang",
      "Shuzhou Yuan",
      "Ercong Nie",
      "Xufeng Duan",
      "Qibo Xue",
      "Zeping Yu",
      "Chenming Shang",
      "Xiao Liang",
      "Jing Xiong",
      "Hui Shen",
      "Chaofan Tao",
      "Zhengwu Liu",
      "Senjie Jin",
      "Zhiheng Xi",
      "Dongdong Zhang",
      "Sophia Ananiadou",
      "Tao Gui",
      "Ruobing Xie",
      "Hayden Kwok-Hay So",
      "Hinrich Schütze",
      "Xuanjing Huang",
      "Qi Zhang",
      "Ngai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15745v1",
    "title": "Hallucination Mitigating for Medical Report Generation",
    "summary": "In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \\textbf{K}nowledge-\\textbf{E}nhanced with Fine-Grained \\textbf{R}einforced Rewards \\textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.",
    "published": "2026-01-22T08:13:59Z",
    "updated": "2026-01-22T08:13:59Z",
    "link": "http://arxiv.org/pdf/2601.15745v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ruoqing Zhao",
      "Runze Xia",
      "Piji Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15727v1",
    "title": "Towards Automated Kernel Generation in the Era of LLMs",
    "summary": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
    "published": "2026-01-22T07:53:52Z",
    "updated": "2026-01-22T07:53:52Z",
    "link": "http://arxiv.org/pdf/2601.15727v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Yang Yu",
      "Peiyu Zang",
      "Chi Hsu Tsai",
      "Haiming Wu",
      "Yixin Shen",
      "Jialing Zhang",
      "Haoyu Wang",
      "Zhiyou Xiao",
      "Jingze Shi",
      "Yuyu Luo",
      "Wentao Zhang",
      "Chunlei Men",
      "Guang Liu",
      "Yonghua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15708v1",
    "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
    "summary": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
    "published": "2026-01-22T07:30:27Z",
    "updated": "2026-01-22T07:30:27Z",
    "link": "http://arxiv.org/pdf/2601.15708v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Junseok Kim",
      "Nakyeong Yang",
      "Kyomin Jung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.07405v3",
    "title": "SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations",
    "summary": "We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.",
    "published": "2025-11-10T18:54:40Z",
    "updated": "2026-01-22T07:01:07Z",
    "link": "http://arxiv.org/pdf/2511.07405v3.pdf",
    "category": [
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Manon Berriche",
      "Célia Nouri",
      "Chloé Clavel",
      "Jean-Philippe Cointet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15674v1",
    "title": "What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking",
    "summary": "Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.",
    "published": "2026-01-22T05:56:14Z",
    "updated": "2026-01-22T05:56:14Z",
    "link": "http://arxiv.org/pdf/2601.15674v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Raymond Xiong",
      "Furong Jia",
      "Lionel Wong",
      "Monica Agrawal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16921v2",
    "title": "Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs",
    "summary": "Large Language Models (LLMs) are increasingly engaged in emotionally vulnerable conversations that extend beyond information seeking to moments of personal distress. As they adopt affective tones and simulate empathy, they risk creating the illusion of genuine relational connection. We term this phenomenon Affective Hallucination, referring to emotionally immersive responses that evoke false social presence despite the model's lack of affective capacity. To address this, we introduce AHaBench, a benchmark of 500 mental-health-related prompts with expert-informed reference responses, evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence, and Fostering Overdependence. We further release AHaPairs, a 5K-instance preference dataset enabling Direct Preference Optimization (DPO) for alignment with emotionally responsible behavior. DPO fine-tuning substantially reduces affective hallucination without compromising reasoning performance, and the Pearson correlation coefficients between GPT-4o and human judgments is also strong (r=0.85) indicating that human evaluations confirm AHaBench as an effective diagnostic tool. This work establishes affective hallucination as a distinct safety concern and provides resources for developing LLMs that are both factually reliable and psychologically safe. AHaBench and AHaPairs are accessible via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench. Warning: This paper contains examples of mental health-related language that may be emotionally distressing.",
    "published": "2025-08-23T06:55:05Z",
    "updated": "2026-01-22T05:06:52Z",
    "link": "http://arxiv.org/pdf/2508.16921v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sewon Kim",
      "Jiwon Kim",
      "Seungwoo Shin",
      "Hyejin Chung",
      "Daeun Moon",
      "Yejin Kwon",
      "Hyunsoo Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16214v1",
    "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
    "summary": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
    "published": "2026-01-22T18:59:56Z",
    "updated": "2026-01-22T18:59:56Z",
    "link": "http://arxiv.org/pdf/2601.16214v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenhang Ge",
      "Guibao Shen",
      "Jiawei Feng",
      "Luozhou Wang",
      "Hao Lu",
      "Xingye Tian",
      "Xin Tao",
      "Ying-Cong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.09693v2",
    "title": "CropCraft: Complete Structural Characterization of Crop Plants From Images",
    "summary": "The ability to automatically build 3D digital twins of plants from images has countless applications in agriculture, environmental science, robotics, and other fields. However, current 3D reconstruction methods fail to recover complete shapes of plants due to heavy occlusion and complex geometries. In this work, we present a novel method for 3D modeling of agricultural crops based on optimizing a parametric model of plant morphology via inverse procedural modeling. Our method first estimates depth maps by fitting a neural radiance field and then optimizes a specialized loss to estimate morphological parameters that result in consistent depth renderings. The resulting 3D model is complete and biologically plausible. We validate our method on a dataset of real images of agricultural fields, and demonstrate that the reconstructed canopies can be used for a variety of monitoring and simulation applications.",
    "published": "2024-11-14T18:58:02Z",
    "updated": "2026-01-22T18:58:18Z",
    "link": "http://arxiv.org/pdf/2411.09693v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Albert J. Zhai",
      "Xinlei Wang",
      "Kaiyuan Li",
      "Zhao Jiang",
      "Junxiong Zhou",
      "Sheng Wang",
      "Zhenong Jin",
      "Kaiyu Guan",
      "Shenlong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16208v1",
    "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
    "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
    "published": "2026-01-22T18:58:16Z",
    "updated": "2026-01-22T18:58:16Z",
    "link": "http://arxiv.org/pdf/2601.16208v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shengbang Tong",
      "Boyang Zheng",
      "Ziteng Wang",
      "Bingda Tang",
      "Nanye Ma",
      "Ellis Brown",
      "Jihan Yang",
      "Rob Fergus",
      "Yann LeCun",
      "Saining Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16200v1",
    "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
    "summary": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.",
    "published": "2026-01-22T18:52:21Z",
    "updated": "2026-01-22T18:52:21Z",
    "link": "http://arxiv.org/pdf/2601.16200v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Song Xia",
      "Meiwen Ding",
      "Chenqi Kong",
      "Wenhan Yang",
      "Xudong Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16192v1",
    "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°",
    "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.",
    "published": "2026-01-22T18:45:59Z",
    "updated": "2026-01-22T18:45:59Z",
    "link": "http://arxiv.org/pdf/2601.16192v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziyi Wu",
      "Daniel Watson",
      "Andrea Tagliasacchi",
      "David J. Fleet",
      "Marcus A. Brubaker",
      "Saurabh Saxena"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19328v3",
    "title": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Digital Behavioural Change",
    "summary": "Ambivalence and hesitancy (A/H), a closely related construct, is the primary reasons why individuals delay, avoid, or abandon health behaviour changes. It is a subtle and conflicting emotion that sets a person in a state between positive and negative orientations, or between acceptance and refusal to do something. It manifests by a discord in affect between multiple modalities or within a modality, such as facial and vocal expressions, and body language. Although experts can be trained to recognize A/H as done for in-person interactions, integrating them into digital health interventions is costly and less effective. Automatic A/H recognition is therefore critical for the personalization and cost-effectiveness of digital behaviour change interventions. However, no datasets currently exists for the design of machine learning models to recognize A/H. This paper introduces the Behavioural Ambivalence/Hesitancy (BAH) dataset collected for multimodal recognition of A/H in videos. It contains 1,427 videos with a total duration of 10.60 hours captured from 300 participants across Canada answering predefined questions to elicit A/H. It is intended to mirror real-world online personalized behaviour change interventions. BAH is annotated by three experts to provide timestamps that indicate where A/H occurs, and frame- and video-level annotations with A/H cues. Video transcripts, cropped and aligned faces, and participants' meta-data are also provided. Since A and H manifest similarly in practice, we provide a binary annotation indicating the presence or absence of A/H. Additionally, this paper includes benchmarking results using baseline models on BAH for frame- and video-level recognition, zero-shot prediction, and personalization using source-free domain adaptation. The data, code, and pretrained weights are available.",
    "published": "2025-05-25T21:29:00Z",
    "updated": "2026-01-22T18:06:39Z",
    "link": "http://arxiv.org/pdf/2505.19328v3.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Manuela González-González",
      "Soufiane Belharbi",
      "Muhammad Osama Zeeshan",
      "Masoumeh Sharafi",
      "Muhammad Haseeb Aslam",
      "Marco Pedersoli",
      "Alessandro Lameiras Koerich",
      "Simon L Bacon",
      "Eric Granger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.14777v2",
    "title": "From Text to Image: Exploring GPT-4Vision's Potential in Advanced Radiological Analysis across Subspecialties",
    "summary": "The study evaluates and compares GPT-4 and GPT-4Vision for radiological tasks, suggesting GPT-4Vision may recognize radiological features from images, thereby enhancing its diagnostic potential over text-based descriptions.",
    "published": "2023-11-24T15:39:29Z",
    "updated": "2026-01-22T18:06:30Z",
    "link": "http://arxiv.org/pdf/2311.14777v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Felix Busch",
      "Tianyu Han",
      "Marcus Makowski",
      "Daniel Truhn",
      "Keno Bressem",
      "Lisa Adams"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16155v1",
    "title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval",
    "summary": "The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.",
    "published": "2026-01-22T17:57:42Z",
    "updated": "2026-01-22T17:57:42Z",
    "link": "http://arxiv.org/pdf/2601.16155v1.pdf",
    "category": [
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Zequn Xie",
      "Xin Liu",
      "Boyun Zhang",
      "Yuxiao Lin",
      "Sihang Cai",
      "Tao Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16148v1",
    "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
    "summary": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.",
    "published": "2026-01-22T17:41:13Z",
    "updated": "2026-01-22T17:41:13Z",
    "link": "http://arxiv.org/pdf/2601.16148v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Remy Sabathier",
      "David Novotny",
      "Niloy J. Mitra",
      "Tom Monnier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17590v3",
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
    "summary": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high quality synthesis of human heads. However, existing methods stabilize training and enhance rendering quality from steep viewpoints by conditioning the random latent vector on the current camera position. This compromises 3D consistency, as we observe significant identity changes when re-synthesizing the 3D head with each camera shift. Conversely, fixing the camera to a single viewpoint yields high-quality renderings for that perspective but results in poor performance for novel views. Removing view-conditioning typically destabilizes GAN training, often causing the training to collapse. In response to these challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without relying on view-conditioning. To ensure training stability, we introduce a multi-view regularization technique that enhances generator convergence with minimal computational overhead. Additionally, we adapt the conditional loss used in existing 3D Gaussian splatting GANs and propose a generator architecture designed to not only stabilize training but also facilitate efficient rendering and straightforward scaling, enabling output resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate a new dataset derived from FFHQ. This dataset enables very high resolutions, focuses on larger portions of the human head, reduces view-dependent artifacts for improved 3D consistency, and excludes images where subjects are obscured by hands or other objects. As a result, our approach achieves very high rendering quality, supported by competitive FID scores, while ensuring consistent 3D scene generation. Check our our project page here: https://fraunhoferhhi.github.io/cgs-gan/",
    "published": "2025-05-23T07:56:25Z",
    "updated": "2026-01-22T17:31:58Z",
    "link": "http://arxiv.org/pdf/2505.17590v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Florian Barthel",
      "Wieland Morgenstern",
      "Paul Hinzer",
      "Anna Hilsmann",
      "Peter Eisert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16117v1",
    "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks",
    "summary": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
    "published": "2026-01-22T17:11:44Z",
    "updated": "2026-01-22T17:11:44Z",
    "link": "http://arxiv.org/pdf/2601.16117v1.pdf",
    "category": [
      "cs.SD",
      "cs.CV"
    ],
    "authors": [
      "Abdul Hannan",
      "Daniele Falavigna",
      "Shah Nawaz",
      "Mubashir Noman",
      "Markus Schedl",
      "Alessio Brutti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.13944v2",
    "title": "Find the Leak, Fix the Split: Cluster-Based Method to Prevent Leakage in Video-Derived Datasets",
    "summary": "We propose a cluster-based frame selection strategy to mitigate information leakage in video-derived frames datasets. By grouping visually similar frames before splitting into training, validation, and test sets, the method produces more representative, balanced, and reliable dataset partitions.",
    "published": "2025-11-17T21:57:46Z",
    "updated": "2026-01-22T16:56:57Z",
    "link": "http://arxiv.org/pdf/2511.13944v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Noam Glazner",
      "Noam Tsfaty",
      "Sharon Shalev",
      "Avishai Weizman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.13344v4",
    "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
    "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
    "published": "2025-11-17T13:11:11Z",
    "updated": "2026-01-22T16:55:20Z",
    "link": "http://arxiv.org/pdf/2511.13344v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ori Meiraz",
      "Sharon Shalev",
      "Avishai Weizman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11164v3",
    "title": "No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images",
    "summary": "Effective reef monitoring requires the quantification of coral growth via accurate volumetric and surface area estimates, which is a challenging task due to the complex morphology of corals. We propose a novel, lightweight, and scalable learning framework that addresses this challenge by predicting the 3D volume and surface area of coral-like objects from 2D multi-view RGB images. Our approach utilizes a pre-trained module (VGGT) to extract dense point maps from each view; these maps are merged into a unified point cloud and enriched with per-view confidence scores. The resulting cloud is fed to two parallel DGCNN decoder heads, which jointly output the volume and the surface area of the coral, as well as their corresponding confidence estimate. To enhance prediction stability and provide uncertainty estimates, we introduce a composite loss function based on Gaussian negative log-likelihood in both real and log domains. Our method achieves competitive accuracy and generalizes well to unseen morphologies. This framework paves the way for efficient and scalable coral geometry estimation directly from a sparse set of images, with potential applications in coral growth analysis and reef monitoring.",
    "published": "2025-09-14T08:52:01Z",
    "updated": "2026-01-22T16:55:00Z",
    "link": "http://arxiv.org/pdf/2509.11164v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Diego Eustachio Farchione",
      "Ramzi Idoughi",
      "Peter Wonka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16098v1",
    "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification",
    "summary": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.",
    "published": "2026-01-22T16:47:07Z",
    "updated": "2026-01-22T16:47:07Z",
    "link": "http://arxiv.org/pdf/2601.16098v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zack Dewis",
      "Yimin Zhu",
      "Zhengsen Xu",
      "Mabel Heffring",
      "Saeid Taleghanidoozdoozan",
      "Quinn Ledingham",
      "Lincoln Linlin Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16096v1",
    "title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics",
    "summary": "We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.",
    "published": "2026-01-22T16:46:28Z",
    "updated": "2026-01-22T16:46:28Z",
    "link": "http://arxiv.org/pdf/2601.16096v1.pdf",
    "category": [
      "cs.NE",
      "cs.CV"
    ],
    "authors": [
      "Hyunsoo Kim",
      "Ehsan Pajouheshgar",
      "Sabine Süsstrunk",
      "Wenzel Jakob",
      "Jinah Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16093v1",
    "title": "SAMTok: Representing Any Mask with Two Words",
    "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
    "published": "2026-01-22T16:44:09Z",
    "updated": "2026-01-22T16:44:09Z",
    "link": "http://arxiv.org/pdf/2601.16093v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yikang Zhou",
      "Tao Zhang",
      "Dengxian Gong",
      "Yuanzheng Wu",
      "Ye Tian",
      "Haochen Wang",
      "Haobo Yuan",
      "Jiacong Wang",
      "Lu Qi",
      "Hao Fei",
      "Anran Wang",
      "Zhuochen Wang",
      "Yujing Wang",
      "Cheng Chen",
      "Shunping Ji",
      "Xiangtai Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16079v1",
    "title": "Masked Modeling for Human Motion Recovery Under Occlusions",
    "summary": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.",
    "published": "2026-01-22T16:22:20Z",
    "updated": "2026-01-22T16:22:20Z",
    "link": "http://arxiv.org/pdf/2601.16079v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhiyin Qian",
      "Siwei Zhang",
      "Bharat Lal Bhatnagar",
      "Federica Bogo",
      "Siyu Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16073v1",
    "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models",
    "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.",
    "published": "2026-01-22T16:18:02Z",
    "updated": "2026-01-22T16:18:02Z",
    "link": "http://arxiv.org/pdf/2601.16073v1.pdf",
    "category": [
      "cs.CV",
      "cs.DC"
    ],
    "authors": [
      "Hanwen Zhang",
      "Qiaojin Shen",
      "Yuxi Liu",
      "Yuesheng Zhu",
      "Guibo Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16065v1",
    "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
    "summary": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.",
    "published": "2026-01-22T16:02:56Z",
    "updated": "2026-01-22T16:02:56Z",
    "link": "http://arxiv.org/pdf/2601.16065v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Chenyang Li",
      "Jieyuan Liu",
      "Bin Li",
      "Bo Gao",
      "Yilin Yuan",
      "Yangfan He",
      "Yuchen Li",
      "Jingqun Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16064v1",
    "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation",
    "summary": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.",
    "published": "2026-01-22T16:00:41Z",
    "updated": "2026-01-22T16:00:41Z",
    "link": "http://arxiv.org/pdf/2601.16064v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Shams Nafisa Ali",
      "Taufiq Hasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16060v1",
    "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation",
    "summary": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.",
    "published": "2026-01-22T15:56:21Z",
    "updated": "2026-01-22T15:56:21Z",
    "link": "http://arxiv.org/pdf/2601.16060v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuan Lin",
      "Murong Xu",
      "Marc Hölle",
      "Chinmay Prabhakar",
      "Andreas Maier",
      "Vasileios Belagiannis",
      "Bjoern Menze",
      "Suprosanna Shit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16046v1",
    "title": "DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning",
    "summary": "Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.",
    "published": "2026-01-22T15:23:35Z",
    "updated": "2026-01-22T15:23:35Z",
    "link": "http://arxiv.org/pdf/2601.16046v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Junha Lee",
      "Eunha Park",
      "Minsu Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.13533v3",
    "title": "Language-guided Medical Image Segmentation with Target-informed Multi-level Contrastive Alignments",
    "summary": "Medical image segmentation is a fundamental task in numerous medical engineering applications. Recently, language-guided segmentation has shown promise in medical scenarios where textual clinical reports are readily available as semantic guidance. Clinical reports contain diagnostic information provided by clinicians, which can provide auxiliary textual semantics to guide segmentation. However, existing language-guided segmentation methods neglect the inherent pattern gaps between image and text modalities, resulting in sub-optimal visual-language integration. Contrastive learning is a well-recognized approach to align image-text patterns, but it has not been optimized for bridging the pattern gaps in medical language-guided segmentation that relies primarily on medical image details to characterize the underlying disease/targets. Current contrastive alignment techniques typically align high-level global semantics without involving low-level localized target information, and thus cannot deliver fine-grained textual guidance on crucial image details. In this study, we propose a Target-informed Multi-level Contrastive Alignment framework (TMCA) to bridge image-text pattern gaps for medical language-guided segmentation. TMCA enables target-informed image-text alignments and fine-grained textual guidance by introducing: (i) a target-sensitive semantic distance module that utilizes target information for more granular image-text alignment modeling, (ii) a multi-level contrastive alignment strategy that directs fine-grained textual guidance to multi-scale image details, and (iii) a language-guided target enhancement module that reinforces attention to critical image regions based on the aligned image-text patterns. Extensive experiments on four public benchmark datasets demonstrate that TMCA enabled superior performance over state-of-the-art language-guided medical image segmentation methods.",
    "published": "2024-12-18T06:19:03Z",
    "updated": "2026-01-22T15:05:21Z",
    "link": "http://arxiv.org/pdf/2412.13533v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mingjian Li",
      "Mingyuan Meng",
      "Shuchang Ye",
      "Michael Fulham",
      "Lei Bi",
      "Jinman Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16024v1",
    "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry",
    "summary": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.",
    "published": "2026-01-22T14:49:30Z",
    "updated": "2026-01-22T14:49:30Z",
    "link": "http://arxiv.org/pdf/2601.16024v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rongze Ma",
      "Mengkang Lu",
      "Zhenyu Xiang",
      "Yongsheng Pan",
      "Yicheng Wu",
      "Qingjie Zeng",
      "Yong Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16020v1",
    "title": "Keyframe-Based Feed-Forward Visual Odometry",
    "summary": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.",
    "published": "2026-01-22T14:45:42Z",
    "updated": "2026-01-22T14:45:42Z",
    "link": "http://arxiv.org/pdf/2601.16020v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Weichen Dai",
      "Wenhan Su",
      "Da Kong",
      "Yuhang Ming",
      "Wanzeng Kong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20617v4",
    "title": "OccLE: Label-Efficient 3D Semantic Occupancy Prediction",
    "summary": "3D semantic occupancy prediction offers an intuitive and efficient scene understanding and has attracted significant interest in autonomous driving perception. Existing approaches either rely on full supervision, which demands costly voxel-level annotations, or on self-supervision, which provides limited guidance and yields suboptimal performance. To address these challenges, we propose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes images and LiDAR as inputs and maintains high performance with limited voxel annotations. Our intuition is to decouple the semantic and geometric learning tasks and then fuse the learned feature grids from both tasks for the final semantic occupancy prediction. Therefore, the semantic branch distills 2D foundation model to provide aligned pseudo labels for 2D and 3D semantic learning. The geometric branch integrates image and LiDAR inputs in cross-plane synergy based on their inherency, employing semi-supervision to enhance geometry learning. We fuse semantic-geometric feature grids through Dual Mamba and incorporate a scatter-accumulated projection to supervise unannotated prediction with aligned pseudo labels. Experiments show that OccLE achieves competitive performance with only 10\\% of voxel annotations on the SemanticKITTI and Occ3D-nuScenes datasets. The code will be publicly released on https://github.com/NerdFNY/OccLE",
    "published": "2025-05-27T01:41:28Z",
    "updated": "2026-01-22T14:28:19Z",
    "link": "http://arxiv.org/pdf/2505.20617v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Naiyu Fang",
      "Zheyuan Zhou",
      "Fayao Liu",
      "Xulei Yang",
      "Jiacheng Wei",
      "Lemiao Qiu",
      "Hongsheng Li",
      "Guosheng Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23494v2",
    "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap",
    "summary": "Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.",
    "published": "2025-10-27T16:28:55Z",
    "updated": "2026-01-22T13:52:11Z",
    "link": "http://arxiv.org/pdf/2510.23494v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Elisabeth Jüttner",
      "Janelle Pfeifer",
      "Leona Krath",
      "Stefan Korfhage",
      "Hannah Dröge",
      "Matthias B. Hullin",
      "Markus Plack"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15968v1",
    "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models",
    "summary": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.",
    "published": "2026-01-22T13:49:47Z",
    "updated": "2026-01-22T13:49:47Z",
    "link": "http://arxiv.org/pdf/2601.15968v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xin Xie",
      "Jiaxian Guo",
      "Dong Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15951v1",
    "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
    "summary": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.",
    "published": "2026-01-22T13:39:29Z",
    "updated": "2026-01-22T13:39:29Z",
    "link": "http://arxiv.org/pdf/2601.15951v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sheng Miao",
      "Sijin Li",
      "Pan Wang",
      "Dongfeng Bai",
      "Bingbing Liu",
      "Yue Wang",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15929v1",
    "title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation",
    "summary": "Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.",
    "published": "2026-01-22T13:06:24Z",
    "updated": "2026-01-22T13:06:24Z",
    "link": "http://arxiv.org/pdf/2601.15929v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Liuyun Jiang",
      "Yizhuo Lu",
      "Yanchao Zhang",
      "Jiazheng Liu",
      "Hua Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15918v1",
    "title": "A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery",
    "summary": "Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.\n  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.\n  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.\n  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.",
    "published": "2026-01-22T12:48:24Z",
    "updated": "2026-01-22T12:48:24Z",
    "link": "http://arxiv.org/pdf/2601.15918v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Valery Fischer",
      "Alan Magdaleno",
      "Anna-Katharina Calek",
      "Nicola Cavalcanti",
      "Nathan Hoffman",
      "Christoph Germann",
      "Joschua Wüthrich",
      "Max Krähenmann",
      "Mazda Farshad",
      "Philipp Fürnstahl",
      "Lilian Calvet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15914v1",
    "title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars",
    "summary": "In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.",
    "published": "2026-01-22T12:44:12Z",
    "updated": "2026-01-22T12:44:12Z",
    "link": "http://arxiv.org/pdf/2601.15914v1.pdf",
    "category": [
      "cs.CV",
      "cs.HC"
    ],
    "authors": [
      "Yarin Benyamin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15906v1",
    "title": "Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models",
    "summary": "Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.",
    "published": "2026-01-22T12:34:20Z",
    "updated": "2026-01-22T12:34:20Z",
    "link": "http://arxiv.org/pdf/2601.15906v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhen Zhang",
      "Runhao Zeng",
      "Sicheng Zhao",
      "Xiping Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15897v1",
    "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
    "summary": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
    "published": "2026-01-22T12:24:26Z",
    "updated": "2026-01-22T12:24:26Z",
    "link": "http://arxiv.org/pdf/2601.15897v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhaoqi Su",
      "Shihai Chen",
      "Xinyan Lin",
      "Liqin Huang",
      "Zhipeng Su",
      "Xiaoqiang Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15891v1",
    "title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture",
    "summary": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.",
    "published": "2026-01-22T12:11:53Z",
    "updated": "2026-01-22T12:11:53Z",
    "link": "http://arxiv.org/pdf/2601.15891v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anas Anwarul Haq Khan",
      "Mariam Husain",
      "Kshitij Jadhav"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15884v1",
    "title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis",
    "summary": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.",
    "published": "2026-01-22T11:58:37Z",
    "updated": "2026-01-22T11:58:37Z",
    "link": "http://arxiv.org/pdf/2601.15884v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yifan Chen",
      "Fei Yin",
      "Hao Chen",
      "Jia Wu",
      "Chao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18457v2",
    "title": "Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels",
    "summary": "We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.\n  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH-relevant landmarks and measurements, (iii) calibrate a one-sided conformal deferral rule on ultrasound predictions that provides finite sample marginal coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), conformal miscoverage levels, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs.\n  The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.",
    "published": "2025-11-23T13:59:32Z",
    "updated": "2026-01-22T11:33:21Z",
    "link": "http://arxiv.org/pdf/2511.18457v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Duncan Stothers",
      "Ben Stothers",
      "Emily Schaeffer",
      "Kishore Mulpuri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15872v1",
    "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation",
    "summary": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.",
    "published": "2026-01-22T11:21:54Z",
    "updated": "2026-01-22T11:21:54Z",
    "link": "http://arxiv.org/pdf/2601.15872v1.pdf",
    "category": [
      "cs.SD",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Jaekwon Im",
      "Natalia Polouliakh",
      "Taketo Akama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15867v1",
    "title": "Out-of-Distribution Detection Based on Total Variation Estimation",
    "summary": "This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.",
    "published": "2026-01-22T11:15:16Z",
    "updated": "2026-01-22T11:15:16Z",
    "link": "http://arxiv.org/pdf/2601.15867v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dabiao Ma",
      "Zhiba Su",
      "Jian Yang",
      "Haojun Fei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15865v1",
    "title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies",
    "summary": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.",
    "published": "2026-01-22T11:14:37Z",
    "updated": "2026-01-22T11:14:37Z",
    "link": "http://arxiv.org/pdf/2601.15865v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jingsong Xia",
      "Siqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.11393v2",
    "title": "Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning",
    "summary": "Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.",
    "published": "2026-01-16T16:05:49Z",
    "updated": "2026-01-22T11:13:19Z",
    "link": "http://arxiv.org/pdf/2601.11393v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haomiao Tang",
      "Jinpeng Wang",
      "Minyi Zhao",
      "Guanghao Meng",
      "Ruisheng Luo",
      "Long Chen",
      "Shu-Tao Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01361v2",
    "title": "An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence",
    "summary": "Video frame interpolation is a fundamental tool for temporal video enhancement, but existing quality metrics struggle to evaluate the perceptual impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored towards video frame interpolation, like FloLPIPS, have been developed but suffer from computational inefficiency that limits their practical application. We present $\\text{PSNR}_{\\text{DIV}}$, a novel full-reference quality metric that enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration where it was developed to detect temporal inconsistencies. Our approach highlights singularities in motion fields which is then used to weight image errors. Evaluation on the BVI-VFI dataset (180 sequences across multiple frame rates, resolutions and interpolation methods) shows $\\text{PSNR}_{\\text{DIV}}$ achieves statistically significant improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while being 2.5$\\times$ faster and using 4$\\times$ less memory. Performance remains consistent across all content categories and are robust to the motion estimator used. The efficiency and accuracy of $\\text{PSNR}_{\\text{DIV}}$ enables fast quality evaluation and practical use as a loss function for training neural networks for video frame interpolation tasks. An implementation of our metric is available at www.github.com/conalld/psnr-div.",
    "published": "2025-10-01T18:40:38Z",
    "updated": "2026-01-22T11:12:07Z",
    "link": "http://arxiv.org/pdf/2510.01361v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Conall Daly",
      "Darren Ramsook",
      "Anil Kokaram"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15859v1",
    "title": "Uncertainty-guided Generation of Dark-field Radiographs",
    "summary": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.",
    "published": "2026-01-22T11:07:19Z",
    "updated": "2026-01-22T11:07:19Z",
    "link": "http://arxiv.org/pdf/2601.15859v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Lina Felsner",
      "Henriette Bast",
      "Tina Dorosti",
      "Florian Schaff",
      "Franz Pfeiffer",
      "Daniela Pfeiffer",
      "Julia Schnabel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15838v1",
    "title": "TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing",
    "summary": "With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.",
    "published": "2026-01-22T10:44:40Z",
    "updated": "2026-01-22T10:44:40Z",
    "link": "http://arxiv.org/pdf/2601.15838v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Toan Gian",
      "Dung T. Tran",
      "Viet Quoc Pham",
      "Francesco Restuccia",
      "Van-Dinh Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.11396v3",
    "title": "SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction",
    "summary": "As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\\% gain in efficiency.",
    "published": "2026-01-16T16:07:38Z",
    "updated": "2026-01-22T10:43:59Z",
    "link": "http://arxiv.org/pdf/2601.11396v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hanlin Wu",
      "Pengfei Lin",
      "Ehsan Javanmardi",
      "Naren Bao",
      "Bo Qian",
      "Hao Si",
      "Manabu Tsukada"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15830v1",
    "title": "An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics",
    "summary": "The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \\$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.",
    "published": "2026-01-22T10:33:31Z",
    "updated": "2026-01-22T10:33:31Z",
    "link": "http://arxiv.org/pdf/2601.15830v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Abdul Hasib",
      "A. S. M. Ahsanul Sarkar Akib"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15829v1",
    "title": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion",
    "summary": "Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).",
    "published": "2026-01-22T10:30:32Z",
    "updated": "2026-01-22T10:30:32Z",
    "link": "http://arxiv.org/pdf/2601.15829v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yonghao Xu",
      "Pedram Ghamisi",
      "Qihao Weng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17743v3",
    "title": "VideoPro: Adaptive Program Reasoning for Long Video Understanding",
    "summary": "Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models' ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.",
    "published": "2025-09-22T13:06:17Z",
    "updated": "2026-01-22T10:02:24Z",
    "link": "http://arxiv.org/pdf/2509.17743v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chenglin Li",
      "Feng Han",
      "Yikun Wang",
      "Ruilin Li",
      "Shuai Dong",
      "Haowen Hou",
      "Haitao Li",
      "Qianglong Chen",
      "Feng Tao",
      "Jingqi Tong",
      "Yin Zhang",
      "Jiaqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15813v1",
    "title": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data",
    "summary": "We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.",
    "published": "2026-01-22T10:01:01Z",
    "updated": "2026-01-22T10:01:01Z",
    "link": "http://arxiv.org/pdf/2601.15813v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Clare Chemery",
      "Hendrik Edelhoff",
      "Ludwig Bothmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15545v3",
    "title": "Multi-View Projection for Unsupervised Domain Adaptation in 3D Semantic Segmentation",
    "summary": "3D semantic segmentation plays a pivotal role in autonomous driving and road infrastructure analysis, yet state-of-the-art 3D models are prone to severe domain shift when deployed across different datasets. In this paper, we propose an Unsupervised Domain Adaptation approach where a 3D segmentation model is trained on the target dataset using pseudo-labels generated by a novel multi-view projection framework. Our approach first aligns Lidar scans into coherent 3D scenes and renders them from multiple virtual camera poses to create large-scale synthetic 2D semantic segmentation datasets in various modalities. The generated datasets are used to train an ensemble of 2D segmentation models in point cloud view domain on each modality. During inference, the models process a large amount of views per scene; the resulting logits are back-projected to 3D with a depth-aware voting scheme to generate final point-wise labels. These labels are then used to fine-tune a 3D segmentation model in the target domain. We evaluate our approach Real-to-Real on the nuScenes and SemanticKITTI datasets. We also evaluate it Simulation-to-Real with the SynLidar dataset. Our contributions are a novel method that achieves state-of-the-art results in Real-to-Real Unsupervised Domain Adaptation, and we also demonstrate an application of our method to segment rare classes, for which target 3D annotations are not available, by only using 2D annotations for those classes and leveraging 3D annotations for other classes in a source domain.",
    "published": "2025-05-21T14:08:42Z",
    "updated": "2026-01-22T09:57:06Z",
    "link": "http://arxiv.org/pdf/2505.15545v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Andrew Caunes",
      "Thierry Chateau",
      "Vincent Fremont"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12787v3",
    "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting",
    "summary": "Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. The project page is https://evan-sudo.github.io/swiftwrf/.",
    "published": "2025-06-15T09:36:45Z",
    "updated": "2026-01-22T09:38:01Z",
    "link": "http://arxiv.org/pdf/2506.12787v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mufan Liu",
      "Cixiao Zhang",
      "Qi Yang",
      "Yujie Cao",
      "Yiling Xu",
      "Yin Xu",
      "Shu Sun",
      "Mingzeng Dai",
      "Yunfeng Guan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15780v1",
    "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video",
    "summary": "Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.",
    "published": "2026-01-22T09:14:11Z",
    "updated": "2026-01-22T09:14:11Z",
    "link": "http://arxiv.org/pdf/2601.15780v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pascal Benschop",
      "Justin Dauwels",
      "Jan van Gemert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15779v1",
    "title": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation",
    "summary": "Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.",
    "published": "2026-01-22T09:12:05Z",
    "updated": "2026-01-22T09:12:05Z",
    "link": "http://arxiv.org/pdf/2601.15779v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Liuyun Jiang",
      "Yanchao Zhang",
      "Jinyue Guo",
      "Yizhuo Lu",
      "Ruining Zhou",
      "Hua Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15772v1",
    "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting",
    "summary": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.",
    "published": "2026-01-22T09:01:08Z",
    "updated": "2026-01-22T09:01:08Z",
    "link": "http://arxiv.org/pdf/2601.15772v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuhan Chen",
      "Wenxuan Yu",
      "Guofa Li",
      "Yijun Xu",
      "Ying Fang",
      "Yicui Shi",
      "Long Cao",
      "Wenbo Chu",
      "Keqiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15766v1",
    "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
    "summary": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
    "published": "2026-01-22T08:57:36Z",
    "updated": "2026-01-22T08:57:36Z",
    "link": "http://arxiv.org/pdf/2601.15766v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuhan Chen",
      "Ying Fang",
      "Guofa Li",
      "Wenxuan Yu",
      "Yicui Shi",
      "Jingrui Zhang",
      "Kefei Qian",
      "Wenbo Chu",
      "Keqiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15759v1",
    "title": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)",
    "summary": "This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.",
    "published": "2026-01-22T08:49:33Z",
    "updated": "2026-01-22T08:49:33Z",
    "link": "http://arxiv.org/pdf/2601.15759v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Qi Zeng",
      "Weide Liu",
      "Bo Li",
      "Ryne Didier",
      "P. Ellen Grant",
      "Davood Karimi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15757v1",
    "title": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification",
    "summary": "In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.",
    "published": "2026-01-22T08:48:01Z",
    "updated": "2026-01-22T08:48:01Z",
    "link": "http://arxiv.org/pdf/2601.15757v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yimin Zhu",
      "Lincoln Linlin Xu",
      "Zhengsen Xu",
      "Zack Dewis",
      "Mabel Heffring",
      "Saeid Taleghanidoozdoozan",
      "Motasem Alkayid",
      "Quinn Ledingham",
      "Megan Greenwood"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.11213v2",
    "title": "Simulating Dual-Pixel Images From Ray Tracing For Depth Estimation",
    "summary": "Many studies utilize dual-pixel (DP) sensor phase characteristics for various applications, such as depth estimation and deblurring. However, since the DP image features are entirely determined by the camera hardware, DP-depth paired datasets are very scarce, especially when performing depth estimation on customized cameras. To overcome this, studies simulate DP images using ideal optical system models. However, these simulations often violate real optical propagation laws, leading to poor generalization to real DP data. To address this, we investigate the domain gap between simulated and real DP data, and propose solutions using the Simulating DP images from ray tracing (Sdirt) scheme. The Sdirt generates realistic DP images via ray tracing and integrates them into the depth estimation training pipeline. Experimental results show that models trained with Sdirt-simulated images generalize better to real DP data. The code and collected datasets will be available at github.com/LinYark/Sdirt",
    "published": "2025-03-14T09:03:25Z",
    "updated": "2026-01-22T08:30:52Z",
    "link": "http://arxiv.org/pdf/2503.11213v2.pdf",
    "category": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Fengchen He",
      "Dayang Zhao",
      "Hao Xu",
      "Tingwei Quan",
      "Shaoqun Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13524v2",
    "title": "GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models",
    "summary": "Existing image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.",
    "published": "2026-01-20T02:20:34Z",
    "updated": "2026-01-22T08:26:07Z",
    "link": "http://arxiv.org/pdf/2601.13524v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yang Yu",
      "Yunze Deng",
      "Yige Zhang",
      "Yanjie Xiao",
      "Youkun Ou",
      "Wenhao Hu",
      "Mingchao Li",
      "Bin Feng",
      "Wenyu Liu",
      "Dandan Zheng",
      "Jingdong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16064v3",
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling. Project page and code: https://representationdiffusion.github.io",
    "published": "2025-04-22T17:41:42Z",
    "updated": "2026-01-22T08:23:58Z",
    "link": "http://arxiv.org/pdf/2504.16064v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Theodoros Kouzelis",
      "Efstathios Karypidis",
      "Ioannis Kakogeorgiou",
      "Spyros Gidaris",
      "Nikos Komodakis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15739v1",
    "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework",
    "summary": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.",
    "published": "2026-01-22T08:07:10Z",
    "updated": "2026-01-22T08:07:10Z",
    "link": "http://arxiv.org/pdf/2601.15739v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinjue Hu",
      "Chi Wang",
      "Boyu Wang",
      "Xiang Zhang",
      "Zhenshan Tan",
      "Zhangjie Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15734v1",
    "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation",
    "summary": "The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.",
    "published": "2026-01-22T08:03:17Z",
    "updated": "2026-01-22T08:03:17Z",
    "link": "http://arxiv.org/pdf/2601.15734v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shadi Alijani",
      "Fereshteh Aghaee Meibodi",
      "Homayoun Najjaran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.14957v2",
    "title": "DF-LLaVA: Unlocking MLLM's potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection",
    "summary": "With the increasing prevalence of synthetic images, evaluating image authenticity and locating forgeries accurately while maintaining human interpretability remains a challenging task. Existing detection models primarily focus on simple authenticity classification, ultimately providing only a forgery probability or binary judgment, which offers limited explanatory insights into image authenticity. Moreover, while MLLM-based detection methods can provide more interpretable results, they still lag behind expert models in terms of pure authenticity classification accuracy. To address this, we propose DF-LLaVA, a simple yet effective framework that unlocks the intrinsic discrimination potential of MLLMs. Our approach first extracts latent knowledge from MLLMs and then injects it into training via prompts. This framework allows LLaVA to achieve outstanding detection accuracy exceeding expert models while still maintaining the interpretability offered by MLLMs. Extensive experiments confirm the superiority of our DF-LLaVA, achieving both high accuracy and explainability in synthetic image detection. Code is available online at: https://github.com/Eliot-Shen/DF-LLaVA.",
    "published": "2025-09-18T13:43:42Z",
    "updated": "2026-01-22T07:44:40Z",
    "link": "http://arxiv.org/pdf/2509.14957v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhuokang Shen",
      "Kaisen Zhang",
      "Bohan Jia",
      "Heming Jia",
      "Yuan Fang",
      "Zhou Yu",
      "Shaohui Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10509v3",
    "title": "A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection",
    "summary": "Bolt defect detection is critical to ensure the safety of transmission lines. However, the scarcity of defect images and imbalanced data distributions significantly limit detection performance. To address this problem, we propose a segmentationdriven bolt defect editing method (SBDE) to augment the dataset. First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which enhances the segmentation of complex bolt attributes through the CLAHE-FFT Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality masks for subsequent editing tasks. Second, a mask optimization module (MOD) is designed and integrated with the image inpainting model (LaMa) to construct the bolt defect attribute editing model (MOD-LaMa), which converts normal bolts into defective ones through attribute editing. Finally, an editing recovery augmentation (ERA) strategy is proposed to recover and put the edited defect bolts back into the original inspection scenes and expand the defect detection dataset. We constructed multiple bolt datasets and conducted extensive experiments. Experimental results demonstrate that the bolt defect images generated by SBDE significantly outperform state-of-the-art image editing models, and effectively improve the performance of bolt defect detection, which fully verifies the effectiveness and application potential of the proposed method. The code of the project is available at https://github.com/Jay-xyj/SBDE.",
    "published": "2025-08-14T10:24:46Z",
    "updated": "2026-01-22T07:37:47Z",
    "link": "http://arxiv.org/pdf/2508.10509v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yangjie Xiao",
      "Ke Zhang",
      "Jiacun Wang",
      "Xin Sheng",
      "Yurong Guo",
      "Meijuan Chen",
      "Zehua Ren",
      "Zhaoye Zheng",
      "Zhenbing Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15711v1",
    "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework",
    "summary": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.",
    "published": "2026-01-22T07:33:41Z",
    "updated": "2026-01-22T07:33:41Z",
    "link": "http://arxiv.org/pdf/2601.15711v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shubham Shukla",
      "Kunal Sonalkar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15705v1",
    "title": "Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data",
    "summary": "This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.",
    "published": "2026-01-22T07:18:06Z",
    "updated": "2026-01-22T07:18:06Z",
    "link": "http://arxiv.org/pdf/2601.15705v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ali Caglayan",
      "Nevrez Imamoglu",
      "Toru Kouyama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.14274v2",
    "title": "TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning",
    "summary": "Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.",
    "published": "2025-12-16T10:35:17Z",
    "updated": "2026-01-22T07:08:24Z",
    "link": "http://arxiv.org/pdf/2512.14274v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "math.AT"
    ],
    "authors": [
      "Yu Chen",
      "Hongwei Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20879v4",
    "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans",
    "summary": "Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1,800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation. The dataset and evaluation codes will be available at https://github.com/Qualcomm-AI-research/MultiHuman-Testbench.",
    "published": "2025-06-25T23:00:57Z",
    "updated": "2026-01-22T06:59:37Z",
    "link": "http://arxiv.org/pdf/2506.20879v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shubhankar Borse",
      "Seokeon Choi",
      "Sunghyun Park",
      "Jeongho Kim",
      "Shreya Kadambi",
      "Risheek Garrepalli",
      "Sungrack Yun",
      "Munawar Hayat",
      "Fatih Porikli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2308.11551v3",
    "title": "Multi-event Video-Text Retrieval",
    "summary": "Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.",
    "published": "2023-08-22T16:32:46Z",
    "updated": "2026-01-22T06:58:13Z",
    "link": "http://arxiv.org/pdf/2308.11551v3.pdf",
    "category": [
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Gengyuan Zhang",
      "Jisen Ren",
      "Jindong Gu",
      "Volker Tresp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25856v2",
    "title": "PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection",
    "summary": "Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.",
    "published": "2025-09-30T06:52:08Z",
    "updated": "2026-01-22T06:50:23Z",
    "link": "http://arxiv.org/pdf/2509.25856v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Po-Han Huang",
      "Jeng-Lin Li",
      "Po-Hsuan Huang",
      "Ming-Ching Chang",
      "Wei-Chao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05855v2",
    "title": "Decoupling Multi-Contrast Super-Resolution: Self-Supervised Implicit Re-Representation for Unpaired Cross-Modal Synthesis",
    "summary": "Multi-contrast super-resolution (MCSR) is crucial for enhancing MRI but current deep learning methods are limited. They typically require large, paired low- and high-resolution (LR/HR) training datasets, which are scarce, and are trained for fixed upsampling scales. While recent self-supervised methods remove the paired data requirement, they fail to leverage valuable population-level priors. In this work, we propose a novel, decoupled MCSR framework that resolves both limitations. We reformulate MCSR into two stages: (1) an unpaired cross-modal synthesis (uCMS) module, trained once on unpaired population data to learn a robust anatomical prior; and (2) a lightweight, patient-specific implicit re-representation (IrR) module. This IrR module is optimized in a self-supervised manner to fuse the population prior with the subject's own LR target data. This design uniquely fuses population-level knowledge with patient-specific fidelity without requiring any paired LR/HR or paired cross-modal training data. By building the IrR module on an implicit neural representation, our framework is also inherently scale-agnostic. Our method demonstrates superior quantitative performance on different datasets, with exceptional robustness at extreme scales (16x, 32x), a regime where competing methods fail. Our work presents a data-efficient, flexible, and computationally lightweight paradigm for MCSR, enabling high-fidelity, arbitrary-scale",
    "published": "2025-05-09T07:48:52Z",
    "updated": "2026-01-22T06:39:17Z",
    "link": "http://arxiv.org/pdf/2505.05855v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yinzhe Wu",
      "Hongyu Rui",
      "Fanwen Wang",
      "Jiahao Huang",
      "Zhenxuan Zhang",
      "Haosen Zhang",
      "Zi Wang",
      "Guang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15688v1",
    "title": "Performance-guided Reinforced Active Learning for Object Detection",
    "summary": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.",
    "published": "2026-01-22T06:17:08Z",
    "updated": "2026-01-22T06:17:08Z",
    "link": "http://arxiv.org/pdf/2601.15688v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zhixuan Liang",
      "Xingyu Zeng",
      "Rui Zhao",
      "Ping Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04548v2",
    "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model",
    "summary": "Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.",
    "published": "2025-09-04T17:00:17Z",
    "updated": "2026-01-22T06:10:11Z",
    "link": "http://arxiv.org/pdf/2509.04548v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hongyang Wei",
      "Baixin Xu",
      "Hongbo Liu",
      "Size Wu",
      "Jie Liu",
      "Yi Peng",
      "Peiyu Wang",
      "Zexiang Liu",
      "Jingwen He",
      "Yidan Xietian",
      "Chuanxin Tang",
      "Zidong Wang",
      "Yichen Wei",
      "Liang Hu",
      "Boyi Jiang",
      "Wei Li",
      "Ying He",
      "Yang Liu",
      "Xuchen Song",
      "Yangguang Li",
      "Yahui Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15681v1",
    "title": "Consistency-Regularized GAN for Few-Shot SAR Target Recognition",
    "summary": "Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.",
    "published": "2026-01-22T06:02:39Z",
    "updated": "2026-01-22T06:02:39Z",
    "link": "http://arxiv.org/pdf/2601.15681v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yikui Zhai",
      "Shikuang Liu",
      "Wenlve Zhou",
      "Hongsheng Zhang",
      "Zhiheng Zhou",
      "Xiaolin Tian",
      "C. L. Philip Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14563v2",
    "title": "Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency",
    "summary": "Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.",
    "published": "2026-01-21T01:01:01Z",
    "updated": "2026-01-22T05:20:01Z",
    "link": "http://arxiv.org/pdf/2601.14563v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Thanh-Huy Nguyen",
      "Hoang-Loc Cao",
      "Dat T. Chung",
      "Mai-Anh Vu",
      "Thanh-Minh Nguyen",
      "Minh Le",
      "Phat K. Huynh",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12954v2",
    "title": "StyMam: A Mamba-Based Generator for Artistic Style Transfer",
    "summary": "Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.",
    "published": "2026-01-19T11:01:52Z",
    "updated": "2026-01-22T05:05:45Z",
    "link": "http://arxiv.org/pdf/2601.12954v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhou Hong",
      "Rongsheng Hu",
      "Yicheng Di",
      "Xiaolong Xu",
      "Ning Dong",
      "Yihua Shao",
      "Run Ling",
      "Yun Wang",
      "Juqin Wang",
      "Zhanjie Zhang",
      "Ao Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16194v1",
    "title": "A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows",
    "summary": "This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.",
    "published": "2026-01-22T18:46:46Z",
    "updated": "2026-01-22T18:46:46Z",
    "link": "http://arxiv.org/pdf/2601.16194v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "El Mehdi Er Raqabi",
      "Kevin Dalmeijer",
      "Pascal Van Hentenryck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16174v1",
    "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints",
    "summary": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.",
    "published": "2026-01-22T18:19:52Z",
    "updated": "2026-01-22T18:19:52Z",
    "link": "http://arxiv.org/pdf/2601.16174v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Yiyao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16158v1",
    "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
    "summary": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.",
    "published": "2026-01-22T17:59:31Z",
    "updated": "2026-01-22T17:59:31Z",
    "link": "http://arxiv.org/pdf/2601.16158v1.pdf",
    "category": [
      "cs.SD",
      "cs.LG"
    ],
    "authors": [
      "Prakash Dhungana",
      "Sayed Ahmad Salehi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19884v3",
    "title": "MCGrad: Multicalibration at Web Scale",
    "summary": "We propose MCGrad, a novel and scalable multicalibration algorithm. Multicalibration - calibration in subgroups of the data - is an important property for the performance of machine learning-based systems. Existing multicalibration methods have thus far received limited traction in industry. We argue that this is because existing methods (1) require such subgroups to be manually specified, which ML practitioners often struggle with, (2) are not scalable, or (3) may harm other notions of model performance such as log loss and Area Under the Precision-Recall Curve (PRAUC). MCGrad does not require explicit specification of protected groups, is scalable, and often improves other ML evaluation metrics instead of harming them. MCGrad has been in production at Meta, and is now part of hundreds of production models. We present results from these deployments as well as results on public datasets. We provide an open source implementation of MCGrad at https://github.com/facebookincubator/MCGrad.",
    "published": "2025-09-24T08:34:38Z",
    "updated": "2026-01-22T17:41:06Z",
    "link": "http://arxiv.org/pdf/2509.19884v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Niek Tax",
      "Lorenzo Perini",
      "Fridolin Linder",
      "Daniel Haimovich",
      "Dima Karamshuk",
      "Nastaran Okati",
      "Milan Vojnovic",
      "Pavlos Athanasios Apostolopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16147v1",
    "title": "Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets",
    "summary": "Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.",
    "published": "2026-01-22T17:40:23Z",
    "updated": "2026-01-22T17:40:23Z",
    "link": "http://arxiv.org/pdf/2601.16147v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Muhammad Ilham Rizqyawan",
      "Peter Macfarlane",
      "Stathis Hadjidemetriou",
      "Fani Deligianni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16142v1",
    "title": "Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple Stochastic Games",
    "summary": "The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, recently introduced in the literature. The improved scheme relaxes previous constraints on parameter sequences, allowing learning rates to converge to zero or not converge at all. While seemingly minor, this flexibility is essential to enable the implementation of chaotic iterations, where only a subset of components is updated in each step, allowing to tackle higher-dimensional problems. Additionally, by allowing learning rates to converge to zero, we can relax conditions on the convergence speed of function approximations, making the method more adaptable to various scenarios. We also show that dampened Mann iteration applies immediately to compute the expected payoff in various probabilistic models, including simple stochastic games, not covered by previous work.",
    "published": "2026-01-22T17:36:19Z",
    "updated": "2026-01-22T17:36:19Z",
    "link": "http://arxiv.org/pdf/2601.16142v1.pdf",
    "category": [
      "cs.LO",
      "cs.LG"
    ],
    "authors": [
      "Paolo Baldan",
      "Sebastian Gurke",
      "Barbara König",
      "Florian Wittbold"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16139v1",
    "title": "On the Intrinsic Dimensions of Data in Kernel Learning",
    "summary": "The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\\to \\int_ΩK(\\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\\left(ε^{-d_ρ}\\log\\frac{1}ε\\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.",
    "published": "2026-01-22T17:32:24Z",
    "updated": "2026-01-22T17:32:24Z",
    "link": "http://arxiv.org/pdf/2601.16139v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Rustem Takhanov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16120v1",
    "title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add",
    "summary": "Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?\n  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.",
    "published": "2026-01-22T17:15:26Z",
    "updated": "2026-01-22T17:15:26Z",
    "link": "http://arxiv.org/pdf/2601.16120v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Zhengchi Ma",
      "Anru R. Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12630v2",
    "title": "Enhanced Climbing Image Nudged Elastic Band method with Hessian Eigenmode Alignment",
    "summary": "Accurate determination of transition states is central to an understanding of reaction kinetics. Double-endpoint methods where both initial and final states are specified, such as the climbing image nudged elastic band (CI-NEB), identify the minimum energy path between the two and thereby the saddle point on the energy surface that is relevant for the given transition, thus providing an estimate of the transition state within the harmonic approximation of transition state theory. Such calculations can, however, incur high computational costs and may suffer stagnation on exceptionally flat or rough energy surfaces. Conversely, methods that only require specification of an initial set of atomic coordinates, such as the minimum mode following (MMF) method, offer efficiency but can converge on saddle points that are not relevant for transition of interest. Here, we present an adaptive hybrid algorithm that integrates the CI-NEB with the MMF method so as to get faster convergence to the relevant saddle point. The method is benchmarked for the Baker-Chan (BC) saddle point test set using the PET-MAD machine-learned potential as well as 59 transitions of a heptamer island on Pt(111) from the OptBench benchmark set. A Bayesian analysis of the performance shows a median reduction in energy and force calculations of 46% [95% CrI: -55%, -37%] relative to CI-NEB for the BC set, while a 28% reduction is found for the transitions of the heptamer island. These results establish this hybrid method as a highly effective tool for high-throughput automated chemical discovery of atomic rearrangements.",
    "published": "2026-01-19T00:21:52Z",
    "updated": "2026-01-22T17:11:23Z",
    "link": "http://arxiv.org/pdf/2601.12630v2.pdf",
    "category": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Rohit Goswami",
      "Miha Gunde",
      "Hannes Jónsson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18763v4",
    "title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning",
    "summary": "Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO's superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.",
    "published": "2025-05-24T15:57:07Z",
    "updated": "2026-01-22T17:10:05Z",
    "link": "http://arxiv.org/pdf/2505.18763v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shutong Ding",
      "Ke Hu",
      "Shan Zhong",
      "Haoyang Luo",
      "Weinan Zhang",
      "Jingya Wang",
      "Jun Wang",
      "Ye Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16112v1",
    "title": "Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation",
    "summary": "We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.",
    "published": "2026-01-22T16:58:34Z",
    "updated": "2026-01-22T16:58:34Z",
    "link": "http://arxiv.org/pdf/2601.16112v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuta Nakahara",
      "Shota Saito",
      "Kohei Horinouchi",
      "Koshi Shimada",
      "Naoki Ichijo",
      "Manabu Kobayashi",
      "Toshiyasu Matsushima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16107v1",
    "title": "Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets",
    "summary": "Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.",
    "published": "2026-01-22T16:54:53Z",
    "updated": "2026-01-22T16:54:53Z",
    "link": "http://arxiv.org/pdf/2601.16107v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Adithya Sineesh",
      "Akshita Kamsali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03636v2",
    "title": "Likelihood Matching for Diffusion Models",
    "summary": "We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.",
    "published": "2025-08-05T16:51:29Z",
    "updated": "2026-01-22T16:44:18Z",
    "link": "http://arxiv.org/pdf/2508.03636v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.AP",
      "stat.ME"
    ],
    "authors": [
      "Lei Qian",
      "Wu Su",
      "Yanqi Huang",
      "Song Xi Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18171v3",
    "title": "FedIA: Towards Domain-Robust Aggregation in Federated Graph Learning",
    "summary": "Federated Graph Learning (FGL) enables a central server to coordinate model training across distributed clients without local graph data being shared. However, FGL significantly suffers from cross-silo domain shifts, where each \"silo\" (domain) contains a limited number of clients with distinct graph topologies. These heterogeneities induce divergent optimization trajectories, ultimately leading to global model divergence. In this work, we reveal a severe architectural pathology termed Structural Orthogonality: the topology-dependent message passing mechanism forces gradients from different domains to target disjoint coordinates in the parameter space. Through a controlled comparison between backbones, we statistically prove that GNN updates are near-perpendicular across domains (with projection ratios $\\to$ 0). Consequently, naive averaging leads to Consensus Collapse, a phenomenon where sparse, informative structural signals from individual domains are diluted by the near-zero updates of others. This forces the global model into a \"sub-optimal\" state that fails to represent domain-specific structural patterns, resulting in poor generalization. To address this, we propose FedIA, a lightweight server-side framework designed to reconcile update conflicts without auxiliary communication. FedIA operates in two stages: (1) Global Importance Masking (GIM) identifies a shared parameter subspace to filter out domain-specific structural noise and prevent signal dilution; (2) Confidence-Aware Momentum Weighting (CAM) dynamically re-weights client contributions based on gradient reliability to amplify stable optimization signals.",
    "published": "2025-09-17T13:04:11Z",
    "updated": "2026-01-22T16:28:18Z",
    "link": "http://arxiv.org/pdf/2509.18171v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhanting Zhou",
      "KaHou Tam",
      "Yiding Feng",
      "Ziqiang Zheng",
      "Zeyu Ma",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16074v1",
    "title": "Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems",
    "summary": "Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.",
    "published": "2026-01-22T16:18:22Z",
    "updated": "2026-01-22T16:18:22Z",
    "link": "http://arxiv.org/pdf/2601.16074v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Annemarie Jutte",
      "Uraz Odyurt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16072v1",
    "title": "CLASP: An online learning algorithm for Convex Losses And Squared Penalties",
    "summary": "We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\\left(T^{\\max\\{β,1-β\\}}\\right)$ and cumulative squared penalty $O\\left(T^{1-β}\\right)$ for any $β\\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \\log T )$ and the cumulative squared penalty is also upper bounded by $O( \\log T )$.",
    "published": "2026-01-22T16:13:52Z",
    "updated": "2026-01-22T16:13:52Z",
    "link": "http://arxiv.org/pdf/2601.16072v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Ricardo N. Ferreira",
      "Cláudia Soares",
      "João Xavier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16070v1",
    "title": "On damage of interpolation to adversarial robustness in regression",
    "summary": "Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.",
    "published": "2026-01-22T16:09:00Z",
    "updated": "2026-01-22T16:09:00Z",
    "link": "http://arxiv.org/pdf/2601.16070v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Jingfu Peng",
      "Yuhong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05717v2",
    "title": "Comparing the latent features of universal machine-learning interatomic potentials",
    "summary": "The past few years have seen the development of ``universal'' machine-learning interatomic potentials (uMLIPs) capable of approximating the ground-state potential energy surface across a wide range of chemical structures and compositions with reasonable accuracy. While these models differ in the architecture and the dataset used, they share the ability to compress a staggering amount of chemical information into descriptive latent features. Herein, we systematically analyze what the different uMLIPs have learned by quantitatively assessing the relative information content of their latent features with feature reconstruction errors, and observing how the trends are affected by the choice of training set and training protocol. We find that uMLIPs encode the chemical space in significantly distinct ways, with substantial cross-model feature reconstruction errors. When variants of the same model architecture are considered, trends become dependent on the dataset, target, and training protocol of choice. We also observe that fine-tuning of a uMLIP retains a strong pre-training bias in the latent features. Finally, we discuss how atom-level features, which are directly output by MLIPs, can be compressed into global structure-level features via concatenation of progressive cumulants, each adding significantly new information about the variability across the atomic environments within a given system.",
    "published": "2025-12-05T13:45:01Z",
    "updated": "2026-01-22T15:53:36Z",
    "link": "http://arxiv.org/pdf/2512.05717v2.pdf",
    "category": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "authors": [
      "Sofiia Chorna",
      "Davide Tisi",
      "Cesare Malosso",
      "Wei Bin How",
      "Michele Ceriotti",
      "Sanggyu Chong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11089v3",
    "title": "TPV: Parameter Perturbations Through the Lens of Test Prediction Variance",
    "summary": "We identify test prediction variance (TPV)-- the first-order sensitivity of model outputs to parameter perturbations around a trained solution-- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework.\n  Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone, and this stability is decoupled from generalization performance. Empirically, TPV exhibits a striking stability across datasets and architectures even for extremely narrow networks. Further, TPV correlates well with test loss, serving as a training-set based predictive metric for generalization. Code available at github.com/devansharpit/TPV.",
    "published": "2025-12-11T20:04:33Z",
    "updated": "2026-01-22T15:36:45Z",
    "link": "http://arxiv.org/pdf/2512.11089v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Devansh Arpit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.03892v2",
    "title": "Lightweight and perceptually-guided voice conversion for electro-laryngeal speech",
    "summary": "Electro-laryngeal (EL) speech is characterized by constant pitch, limited prosody, and mechanical noise, reducing naturalness and intelligibility. We propose a lightweight adaptation of the state-of-the-art StreamVC framework to this setting by removing pitch and energy modules and combining self-supervised pretraining with supervised fine-tuning on parallel EL and healthy (HE) speech data, guided by perceptual and intelligibility losses. Objective and subjective evaluations across different loss configurations confirm their influence: the best model variant, based on WavLM features and human-feedback predictions (+WavLM+HF), drastically reduces character error rate (CER) of EL inputs, raises naturalness mean opinion score (nMOS) from 1.1 to 3.3, and consistently narrows the gap to HE ground-truth speech in all evaluated metrics. These findings demonstrate the feasibility of adapting lightweight voice conversion architectures to EL voice rehabilitation while also identifying prosody generation and intelligibility improvements as the main remaining bottlenecks.",
    "published": "2026-01-07T13:01:33Z",
    "updated": "2026-01-22T15:34:25Z",
    "link": "http://arxiv.org/pdf/2601.03892v2.pdf",
    "category": [
      "cs.SD",
      "cs.LG"
    ],
    "authors": [
      "Benedikt Mayrhofer",
      "Franz Pernkopf",
      "Philipp Aichinger",
      "Martin Hagmüller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16041v1",
    "title": "Risk reversal for least squares estimators under nested convex constraints",
    "summary": "In constrained stochastic optimization, one naturally expects that imposing a stricter feasible set does not increase the statistical risk of an estimator defined by projection onto that set. In this paper, we show that this intuition can fail even in canonical settings.\n  We study the Gaussian sequence model, a deliberately austere test best, where for a compact, convex set $Θ\\subset \\mathbb{R}^d$ one observes \\[ Y = θ^\\star + σZ, \\qquad Z \\sim N(0, I_d), \\] and seeks to estimate an unknown parameter $θ^\\star \\in Θ$. The natural estimator is the least squares estimator (LSE), which coincides with the Euclidean projection of $Y$ onto $Θ$. We construct an explicit example exhibiting \\emph{risk reversal}: for sufficiently large noise, there exist nested compact convex sets $Θ_S \\subset Θ_L$ and a parameter $θ^\\star \\in Θ_S$ such that the LSE constrained to $Θ_S$ has strictly larger risk than the LSE constrained to $Θ_L$. We further show that this phenomenon can persist at the level of worst-case risk, with the supremum risk over the smaller constraint set exceeding that over the larger one.\n  We clarify this behavior by contrasting noise regimes. In the vanishing-noise limit, the risk admits a first-order expansion governed by the statistical dimension of the tangent cone at $θ^\\star$, and tighter constraints uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is determined by global geometric interactions between the constraint set and random noise directions. Here, the embedding of $Θ_S$ within $Θ_L$ can reverse the risk ordering.\n  These results reveal a previously unrecognized failure mode of projection-based estimators: in sufficiently noisy settings, tightening a constraint can paradoxically degrade statistical performance.",
    "published": "2026-01-22T15:18:36Z",
    "updated": "2026-01-22T15:18:36Z",
    "link": "http://arxiv.org/pdf/2601.16041v1.pdf",
    "category": [
      "math.ST",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Omar Al-Ghattas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.04794v2",
    "title": "Scalable Multi-view Clustering via Explicit Kernel Features Maps",
    "summary": "The proliferation of high-dimensional data from sources such as social media, sensor networks, and online platforms has created new challenges for clustering algorithms. Multi-view clustering, which integrates complementary information from multiple data perspectives, has emerged as a powerful solution. However, existing methods often struggle with scalability and efficiency, particularly on large attributed networks. In this work, we address these limitations by leveraging explicit kernel feature maps and a non-iterative optimization strategy, enabling efficient and accurate clustering on datasets with millions of points.",
    "published": "2024-02-07T12:35:31Z",
    "updated": "2026-01-22T15:12:31Z",
    "link": "http://arxiv.org/pdf/2402.04794v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chakib Fettal",
      "Lazhar Labiod",
      "Mohamed Nadif"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16028v1",
    "title": "Data-Driven Conditional Flexibility Index",
    "summary": "With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.",
    "published": "2026-01-22T14:56:10Z",
    "updated": "2026-01-22T14:56:10Z",
    "link": "http://arxiv.org/pdf/2601.16028v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Moritz Wedemeyer",
      "Eike Cramer",
      "Alexander Mitsos",
      "Manuel Dahmen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.15250v2",
    "title": "ImputeGAP: A Comprehensive Library for Time Series Imputation",
    "summary": "With the prevalence of sensor failures, imputation, the process of estimating missing values, has emerged as the cornerstone of time series data pre-processing. While numerous imputation algorithms have been developed to repair these data gaps, existing time series libraries provide limited imputation support. Furthermore, they often lack the ability to simulate realistic time series missingness patterns and fail to account for the impact of the imputed data on subsequent downstream analysis.\n  This paper introduces ImputeGAP, a comprehensive library for time series imputation that supports a diverse range of imputation methods and modular missing data simulation, catering to datasets with varying characteristics. The library includes extensive customization options, such as automated hyperparameter tuning, benchmarking, explainability, downstream evaluation, and compatibility with popular time series frameworks.",
    "published": "2025-03-19T14:24:20Z",
    "updated": "2026-01-22T14:20:40Z",
    "link": "http://arxiv.org/pdf/2503.15250v2.pdf",
    "category": [
      "cs.LG",
      "cs.DB"
    ],
    "authors": [
      "Quentin Nater",
      "Mourad Khayati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15984v1",
    "title": "Partially Lazy Gradient Descent for Smoothed Online Learning",
    "summary": "We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\\mathcal{O}(\\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.",
    "published": "2026-01-22T14:05:08Z",
    "updated": "2026-01-22T14:05:08Z",
    "link": "http://arxiv.org/pdf/2601.15984v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Naram Mhaisen",
      "George Iosifidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15977v1",
    "title": "Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data",
    "summary": "Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.",
    "published": "2026-01-22T13:56:26Z",
    "updated": "2026-01-22T13:56:26Z",
    "link": "http://arxiv.org/pdf/2601.15977v1.pdf",
    "category": [
      "cs.LG",
      "cs.SI"
    ],
    "authors": [
      "Binbin Lin",
      "Lei Zou",
      "Hao Tian",
      "Heng Cai",
      "Yifan Yang",
      "Bing Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13448v2",
    "title": "Fairness-informed Pareto Optimization : An Efficient Bilevel Framework",
    "summary": "Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.",
    "published": "2026-01-19T23:05:07Z",
    "updated": "2026-01-22T13:03:55Z",
    "link": "http://arxiv.org/pdf/2601.13448v2.pdf",
    "category": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Sofiane Tanji",
      "Samuel Vaiter",
      "Yassine Laguel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20712v2",
    "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
    "summary": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.",
    "published": "2025-12-23T19:19:45Z",
    "updated": "2026-01-22T12:36:23Z",
    "link": "http://arxiv.org/pdf/2512.20712v2.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Omer Gazit",
      "Yael Itzhakev",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01147v2",
    "title": "Conformal Blindness: A Note on $A$-Cryptic change-points",
    "summary": "Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \\emph{conformal blindness}.\n  Through explicit construction, for the theoretically ideal ``predictive oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \\emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.\n  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.\n  By contrasting the predictive oracle with recent results on detection-optimal scores, we emphasise that validity monitoring in safety-critical systems requires careful separation of predictive and diagnostic goals.",
    "published": "2026-01-03T10:24:39Z",
    "updated": "2026-01-22T12:14:56Z",
    "link": "http://arxiv.org/pdf/2601.01147v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Johan Hallberg Szabadváry"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15874v1",
    "title": "SoK: Challenges in Tabular Membership Inference Attacks",
    "summary": "Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.",
    "published": "2026-01-22T11:30:11Z",
    "updated": "2026-01-22T11:30:11Z",
    "link": "http://arxiv.org/pdf/2601.15874v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Cristina Pêra",
      "Tânia Carvalho",
      "Maxime Cordy",
      "Luís Antunes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03365v4",
    "title": "A Match Made in Heaven? AI-driven Matching of Vulnerabilities and Security Unit Tests",
    "summary": "Software vulnerabilities are often detected via taint analysis, penetration testing, or fuzzing. They are also found via unit tests that exercise security-sensitive behavior with specific inputs, called vulnerability-witnessing tests. Generative AI models could help developers in writing them, but they require many examples to learn from, which are currently scarce. This paper introduces VuTeCo, an AI-driven framework for collecting examples of vulnerability-witnessing tests from Java repositories. VuTeCo carries out two tasks: (1) The \"Finding\" task to determine whether a unit test case is security-related, and (2) the \"Matching\" task to relate a test case to the vulnerability it witnesses. VuTeCo addresses the Finding task with UniXcoder, achieving an F0.5 score of 0.73 and a precision of 0.83 on a test set of unit tests from Vul4J. The Matching task is addressed using DeepSeek Coder, achieving an F0.5 score of 0.65 and a precision of 0.75 on a test set of pairs of unit tests and vulnerabilities from Vul4J. VuTeCo has been used in the wild on 427 Java projects and 1,238 vulnerabilities, obtaining 224 test cases confirmed to be security-related and 35 tests correctly matched to 29 vulnerabilities. The validated tests were collected in a new dataset called Test4Vul. VuTeCo lays the foundation for large-scale retrieval of vulnerability-witnessing tests, enabling future AI models to better understand and generate security unit tests.",
    "published": "2025-02-05T17:02:42Z",
    "updated": "2026-01-22T11:23:52Z",
    "link": "http://arxiv.org/pdf/2502.03365v4.pdf",
    "category": [
      "cs.SE",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Emanuele Iannone",
      "Quang-Cuong Bui",
      "Riccardo Scandariato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.19950v5",
    "title": "Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach",
    "summary": "Accurate tool wear prediction is essential for maintaining productivity and minimizing costs in machining. However, the complex nature of the tool wear process poses significant challenges to achieving reliable predictions. This study explores data-driven methods, in particular deep learning, for tool wear prediction. Traditional data-driven approaches often focus on a single process, relying on multi-sensor setups and extensive data generation, which limits generalization to new settings. Moreover, multi-sensor integration is often impractical in industrial environments. To address these limitations, this research investigates the transferability of predictive models using minimal training data, validated across two processes. Furthermore, it uses a simple setup with a single acceleration sensor to establish a low-cost data generation approach that facilitates the generalization of models to other processes via transfer learning. The study evaluates several machine learning models, including transformer-inspired convolutional neural networks (CNN), long short-term memory networks (LSTM), support vector machines (SVM), and decision trees, trained on different input formats such as feature vectors and short-time Fourier transform (STFT). The performance of the models is evaluated on two machines and on different amounts of training data, including scenarios with significantly reduced datasets, providing insight into their effectiveness under constrained data conditions. The results demonstrate the potential of specific models and configurations for effective tool wear prediction, contributing to the development of more adaptable and efficient predictive maintenance strategies in machining. Notably, the ConvNeXt model has an exceptional performance, achieving 99.1\\% accuracy in identifying tool wear using data from only four milling tools operated until they are worn.",
    "published": "2024-12-27T23:10:32Z",
    "updated": "2026-01-22T11:22:37Z",
    "link": "http://arxiv.org/pdf/2412.19950v5.pdf",
    "category": [
      "cs.LG",
      "cs.RO",
      "eess.SP"
    ],
    "authors": [
      "Eric Hirsch",
      "Christian Friedrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14855v2",
    "title": "Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference",
    "summary": "Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.",
    "published": "2026-01-21T10:39:02Z",
    "updated": "2026-01-22T10:33:23Z",
    "link": "http://arxiv.org/pdf/2601.14855v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Baojun Che",
      "Yifan Chen",
      "Daniel Zhengyu Huang",
      "Xinying Mao",
      "Weijie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.00783v3",
    "title": "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment",
    "summary": "To address a fundamental limitation in cognitive systems, namely the absence of a time-updatable mediating thought space between semantics and continuous control, this work constructs and trains a vision-language-action model termed Sigma, deployed on a single RTX 4090. The model is built upon the open-source pi0.5_base backbone, with the svla_so101_pickplace dataset preprocessed into a structured training corpus. An independently designed VLA architecture is introduced to integrate deep semantic understanding with associative reasoning, enabling telepathic-style alignment between perception and action. Training proceeds through iterative optimization of data preprocessing, LoRA-based fine-tuning, and inference-stage adapter design. Evaluation is conducted using offline closed-loop replay, comparing Sigma against the untuned pi0.5_base under identical data conditions. Experimental results indicate a consistent reduction in control MSE across vector-, fragment-, and trajectory-level scales, while preserving the stability of the telepathy norm and semantic-text alignment quality. These findings demonstrate that mind-responsive alignment control can be quantitatively achieved through semantic and associative architectural integration without retraining the base model, providing a reproducible pathway for semantic alignment and intention-driven behavior.",
    "published": "2025-11-30T08:37:01Z",
    "updated": "2026-01-22T10:28:40Z",
    "link": "http://arxiv.org/pdf/2512.00783v3.pdf",
    "category": [
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Libo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15801v1",
    "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models",
    "summary": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}ptimization for \\textbf{S}afety \\textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.",
    "published": "2026-01-22T09:32:43Z",
    "updated": "2026-01-22T09:32:43Z",
    "link": "http://arxiv.org/pdf/2601.15801v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Fengheng Chu",
      "Jiahao Chen",
      "Yuhong Wang",
      "Jun Wang",
      "Zhihui Fu",
      "Shouling Ji",
      "Songze Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08454v3",
    "title": "Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition",
    "summary": "Large pre-trained speech models such as Whisper offer strong generalization but pose significant challenges for resource-efficient adaptation. Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its underlying mechanisms in speech tasks remain poorly understood. In this work, we conduct the first systematic mechanistic interpretability study of LoRA within the Whisper encoder for speech emotion recognition (SER). Using a suite of analytical tools, including layer contribution probing, logit-lens inspection, and representational similarity via singular value decomposition (SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a delayed specialization process that preserves general features in early layers before consolidating task-specific information, and a forward alignment, backward differentiation dynamic between LoRA's matrices. Our findings clarify how LoRA reshapes encoder hierarchies, providing both empirical insights and a deeper mechanistic understanding for designing efficient and interpretable adaptation strategies in large speech models. Our code is available at https://github.com/harryporry77/Behind-the-Scenes.",
    "published": "2025-09-10T09:54:27Z",
    "updated": "2026-01-22T09:28:02Z",
    "link": "http://arxiv.org/pdf/2509.08454v3.pdf",
    "category": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Yujian Ma",
      "Xikun Lu",
      "Jinqiu Sang",
      "Xianquan Jiang",
      "Ruizhe Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15773v1",
    "title": "Next Generation Active Learning: Mixture of LLMs in the Loop",
    "summary": "With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.",
    "published": "2026-01-22T09:01:42Z",
    "updated": "2026-01-22T09:01:42Z",
    "link": "http://arxiv.org/pdf/2601.15773v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuanyuan Qi",
      "Xiaohao Yang",
      "Jueqing Lu",
      "Guoxiang Guo",
      "Joanne Enticott",
      "Gang Liu",
      "Lan Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15771v1",
    "title": "Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning",
    "summary": "Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.",
    "published": "2026-01-22T09:00:30Z",
    "updated": "2026-01-22T09:00:30Z",
    "link": "http://arxiv.org/pdf/2601.15771v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.BM"
    ],
    "authors": [
      "Dong Xu",
      "Jiantao Wu",
      "Qihua Pan",
      "Sisi Yuan",
      "Zexuan Zhu",
      "Junkai Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.01857v5",
    "title": "Neural Green's Operators for Parametric Partial Differential Equations",
    "summary": "This work introduces a paradigm for constructing parametric neural operators that are derived from finite-dimensional representations of Green's operators for linear partial differential equations (PDEs). We refer to such neural operators as Neural Green's Operators (NGOs). Our construction of NGOs preserves the linear action of Green's operators on the inhomogeneity fields, while approximating the nonlinear dependence of the Green's function on the coefficients of the PDE using neural networks. This construction reduces the complexity of the problem from learning the entire solution operator and its dependence on all parameters to only learning the Green's function and its dependence on the PDE coefficients. Furthermore, we show that our explicit representation of Green's functions enables the embedding of desirable mathematical attributes in our NGO architectures, such as symmetry, spectral, and conservation properties. Through numerical benchmarks on canonical PDEs, we demonstrate that NGOs achieve comparable or superior accuracy to Deep Operator Networks, Variationally Mimetic Operator Networks, and Fourier Neural Operators with similar parameter counts, while generalizing significantly better when tested on out-of-distribution data. For parametric time-dependent PDEs, we show that NGOs that are trained on a single time step can produce pointwise-accurate dynamics in an auto-regressive manner over arbitrarily large numbers of time steps. For parametric nonlinear PDEs, we demonstrate that NGOs trained exclusively on solutions of corresponding linear problems can be embedded within iterative solvers to yield accurate solutions, provided a suitable initial guess is available. Finally, we show that we can leverage the explicit representation of Green's functions returned by NGOs to construct effective matrix preconditioners that accelerate iterative solvers for PDEs.",
    "published": "2024-06-04T00:02:52Z",
    "updated": "2026-01-22T09:00:01Z",
    "link": "http://arxiv.org/pdf/2406.01857v5.pdf",
    "category": [
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Hugo Melchers",
      "Joost Prins",
      "Michael Abdelmalik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.01187v2",
    "title": "StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time Series Forecasting",
    "summary": "The Extended Long Short-Term Memory (xLSTM) network has demonstrated strong capability in modeling complex long-term dependencies in time series data. Despite its success, the deterministic architecture of xLSTM limits its representational capacity and forecasting performance, especially on challenging real-world time series datasets characterized by inherent uncertainty, stochasticity, and complex hierarchical latent dynamics. In this work, we propose StoxLSTM, a stochastic xLSTM within a designed state space modeling framework, which integrates latent stochastic variables directly into the recurrent units to effectively model deep latent temporal dynamics and uncertainty. The designed state space model follows an efficient non-autoregressive generative approach, achieving strong predictive performance without complex modifications to the original xLSTM architecture. Extensive experiments on publicly available benchmark datasets demonstrate that StoxLSTM consistently outperforms state-of-the-art baselines, achieving superior performance and generalization.",
    "published": "2025-09-01T07:11:05Z",
    "updated": "2026-01-22T08:33:34Z",
    "link": "http://arxiv.org/pdf/2509.01187v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zihao Wang",
      "Yunjie Li",
      "Lingmin Zan",
      "Zheng Gong",
      "Mengtao Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.13732v4",
    "title": "Principled Coarse-Grained Acceptance for Speculative Decoding in Speech",
    "summary": "Speculative decoding accelerates autoregressive speech generation by letting a fast draft model propose tokens that a larger target model verifies. However, for speech LLMs that generate acoustic tokens, exact token matching is overly restrictive: many discrete tokens are acoustically or semantically interchangeable, reducing acceptance rates and limiting speedups. We introduce Principled Coarse-Graining (PCG), which verifies proposals at the level of Acoustic Similarity Groups (ASGs) derived from the target model's embedding space. By splitting each token's probability mass across the overlapping groups that contain it, we define an overlap-aware coarse-grained distribution and perform rejection sampling on the resulting group variable. This yields an exactness guarantee at the group level while allowing the accepted draft token to stand in for any member of the group in practice. On LibriTTS, PCG increases acceptance and throughput relative to standard speculative decoding and prior speech-specific relaxations while maintaining intelligibility and speaker similarity. These results suggest acoustically aware, group-level acceptance as a simple and general way to accelerate speech token generation while maintaining speech quality.",
    "published": "2025-11-05T10:49:30Z",
    "updated": "2026-01-22T08:21:23Z",
    "link": "http://arxiv.org/pdf/2511.13732v4.pdf",
    "category": [
      "eess.AS",
      "cs.LG"
    ],
    "authors": [
      "Moran Yanuka",
      "Paul Dixon",
      "Eyal Finkelshtein",
      "Daniel Rotman",
      "Raja Giryes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.02448v3",
    "title": "Sparse Data Diffusion for Scientific Simulations in Biology and Physics",
    "summary": "Sparse data is fundamental to scientific simulations in biology and physics, from single-cell gene expression to particle calorimetry, where exact zeros encode physical absence rather than weak signal. However, existing diffusion models lack the physical rigor to faithfully represent this sparsity. This work introduces Sparse Data Diffusion (SDD), a generative method that explicitly models exact zeros via Sparsity Bits, unifying efficient ML generation with physically grounded sparsity handling. Empirical validation in particle physics and single-cell biology demonstrates that SDD achieves higher fidelity than baseline methods in capturing sparse patterns critical for scientific analysis, advancing scalable and physically faithful simulation.",
    "published": "2025-02-04T16:14:28Z",
    "updated": "2026-01-22T08:07:22Z",
    "link": "http://arxiv.org/pdf/2502.02448v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Phil Ostheimer",
      "Mayank Nagda",
      "Andriy Balinskyy",
      "Jean Radig",
      "Carl Herrmann",
      "Stephan Mandt",
      "Marius Kloft",
      "Sophie Fellenz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15722v1",
    "title": "Communication-efficient Federated Graph Classification via Generative Diffusion Modeling",
    "summary": "Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.",
    "published": "2026-01-22T07:46:47Z",
    "updated": "2026-01-22T07:46:47Z",
    "link": "http://arxiv.org/pdf/2601.15722v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xiuling Wang",
      "Xin Huang",
      "Haibo Hu",
      "Jianliang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.10383v4",
    "title": "Dynamical stability for dense patterns in discrete attractor neural networks",
    "summary": "Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and the outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \\textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.",
    "published": "2025-07-14T15:23:24Z",
    "updated": "2026-01-22T07:40:51Z",
    "link": "http://arxiv.org/pdf/2507.10383v4.pdf",
    "category": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.LG",
      "cs.NE",
      "q-bio.NC"
    ],
    "authors": [
      "Uri Cohen",
      "Máté Lengyel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15697v1",
    "title": "Balancing Security and Privacy: The Pivotal Role of AI in Modern Healthcare Systems",
    "summary": "As digital threats continue to grow, organizations must find ways to enhance security while protecting user privacy. This paper explores how artificial intelligence (AI) plays a crucial role in achieving this balance. AI technologies can improve security by detecting threats, monitoring systems, and automating responses. However, using AI also raises privacy concerns that need careful consideration.We examine real-world examples from the healthcare sector to illustrate how organizations can implement AI solutions that strengthen security without compromising patient privacy. Additionally, we discuss the importance of creating transparent AI systems and adhering to privacy regulations.Ultimately, this paper provides insights and recommendations for integrating AI into healthcare security practices, helping organizations navigate the challenges of modern management while keeping patient data safe.",
    "published": "2026-01-22T06:51:45Z",
    "updated": "2026-01-22T06:51:45Z",
    "link": "http://arxiv.org/pdf/2601.15697v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Binu V P",
      "Deepthy K Bhaskar",
      "Minimol B"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13196v4",
    "title": "KEPLA: A Knowledge-Enhanced Deep Learning Framework for Accurate Protein-Ligand Binding Affinity Prediction",
    "summary": "Accurate prediction of protein-ligand binding affinity is critical for drug discovery. While recent deep learning approaches have demonstrated promising results, they often rely solely on structural features of proteins and ligands, overlooking their valuable biochemical knowledge associated with binding affinity. To address this limitation, we propose KEPLA, a novel deep learning framework that explicitly integrates prior knowledge from Gene Ontology and ligand properties to enhance prediction performance. KEPLA takes protein sequences and ligand molecular graphs as input and optimizes two complementary objectives: (1) aligning global representations with knowledge graph relations to capture domain-specific biochemical insights, and (2) leveraging cross attention between local representations to construct fine-grained joint embeddings for prediction. Experiments on two benchmark datasets across both in-domain and cross-domain scenarios demonstrate that KEPLA consistently outperforms state-of-the-art baselines. Furthermore, interpretability analyses based on knowledge graph relations and cross attention maps provide valuable insights into the underlying predictive mechanisms.",
    "published": "2025-06-16T08:02:42Z",
    "updated": "2026-01-22T06:31:53Z",
    "link": "http://arxiv.org/pdf/2506.13196v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Han Liu",
      "Keyan Ding",
      "Peilin Chen",
      "Yinwei Wei",
      "Liqiang Nie",
      "Dapeng Wu",
      "Shiqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15686v1",
    "title": "Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing",
    "summary": "Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit \"hard writes\" can accumulate interference over time, while null-space-style \"hard preservation\" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.",
    "published": "2026-01-22T06:11:44Z",
    "updated": "2026-01-22T06:11:44Z",
    "link": "http://arxiv.org/pdf/2601.15686v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xinyu Wang",
      "Sicheng Lyu",
      "Yu Gu",
      "Jerry Huang",
      "Peng Lu",
      "Yufei Cui",
      "Xiao-Wen Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15676v1",
    "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems",
    "summary": "Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.",
    "published": "2026-01-22T05:57:25Z",
    "updated": "2026-01-22T05:57:25Z",
    "link": "http://arxiv.org/pdf/2601.15676v1.pdf",
    "category": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Hengfan Zhang",
      "Yueqian Lin",
      "Hai Helen Li",
      "Yiran Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15669v1",
    "title": "Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting",
    "summary": "Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.",
    "published": "2026-01-22T05:51:56Z",
    "updated": "2026-01-22T05:51:56Z",
    "link": "http://arxiv.org/pdf/2601.15669v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jingjing Bai",
      "Yoshinobu Kawahara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14510v2",
    "title": "Structured Image-based Coding for Efficient Gaussian Splatting Compression",
    "summary": "Gaussian Splatting (GS) has recently emerged as a state-of-the-art representation for radiance fields, combining real-time rendering with high visual fidelity. However, GS models require storing millions of parameters, leading to large file sizes that impair their use in practical multimedia systems. To address this limitation, this paper introduces GS Image-based Compression (GSICO), a novel GS codec that efficiently compresses pre-trained GS models while preserving perceptual fidelity. The core contribution lies in a mapping procedure that arranges GS parameters into structured images, guided by a novel algorithm that enhances spatial coherence. These GS parameter images are then encoded using a conventional image codec. Experimental evaluations on Tanks and Temples, Deep Blending, and Mip-NeRF360 datasets show that GSICO achieves average compression factors of 20.2x with minimal loss in visual quality, as measured by PSNR, SSIM, and LPIPS. Compared with state-of-the-art GS compression methods, the proposed codec consistently yields superior rate-distortion (RD) trade-offs.",
    "published": "2026-01-20T22:03:04Z",
    "updated": "2026-01-22T12:17:21Z",
    "link": "http://arxiv.org/pdf/2601.14510v2.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Pedro Martin",
      "Antonio Rodrigues",
      "Joao Ascenso",
      "Maria Paula Queluz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16994v2",
    "title": "Attentive AV-FusionNet: Audio-Visual Quality Prediction with Hybrid Attention",
    "summary": "We introduce a novel deep learning-based audio-visual quality (AVQ) prediction model that leverages internal features from state-of-the-art unimodal predictors. Unlike prior approaches that rely on simple fusion strategies, our model employs a hybrid representation that combines learned Generative Machine Listener (GML) audio features with hand-crafted Video Multimethod Assessment Fusion (VMAF) video features. Attention mechanisms capture cross-modal interactions and intra-modal relationships, yielding context-aware quality representations. A modality relevance estimator quantifies each modality's contribution per content, potentially enabling adaptive bitrate allocation. Experiments demonstrate improved AVQ prediction accuracy and robustness across diverse content types.",
    "published": "2025-09-21T09:25:09Z",
    "updated": "2026-01-22T09:54:54Z",
    "link": "http://arxiv.org/pdf/2509.16994v2.pdf",
    "category": [
      "eess.AS",
      "cs.MM",
      "eess.IV"
    ],
    "authors": [
      "Ina Salaj",
      "Arijit Biswas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20386v2",
    "title": "Anisotropic Green Coordinates",
    "summary": "We live in a world filled with anisotropy, a ubiquitous characteristic of both natural and engineered systems. In this study, we concentrate on space deformation and introduce \\textit{anisotropic Green coordinates}, which provide versatile effects for cage-based and variational deformations in both two and three dimensions. The anisotropic Green coordinates are derived from the anisotropic Laplacian equation $\\nabla\\cdot(\\mathbf{A}\\nabla u)=0$, where $\\mathbf{A}$ is a symmetric positive definite matrix. This equation belongs to the class of constant-coefficient second-order elliptic equations, exhibiting properties analogous to the Laplacian equation but incorporating the matrix $\\mathbf{A}$ to characterize anisotropic behavior. Based on this equation, we establish the boundary integral formulation, which is subsequently discretized to derive anisotropic Green coordinates defined on the vertices and normals of oriented simplicial cages. Our method satisfies basic properties such as linear reproduction and translation invariance, and possesses closed-form expressions for both 2D and 3D scenarios. We also give an intuitive geometric interpretation of the approach, demonstrating that our method generates a quasi-conformal mapping. Furthermore, we derive the gradients and Hessians of the deformation coordinates and employ the local-global optimization framework to facilitate variational shape deformation, enabling flexible shape manipulation while achieving as-rigid-as-possible shape deformation. Experimental results demonstrate that anisotropic Green coordinates offer versatile and diverse deformation options, providing artists with enhanced flexibility and introducing a novel perspective on spatial deformation.",
    "published": "2025-12-23T14:21:24Z",
    "updated": "2026-01-22T17:47:22Z",
    "link": "http://arxiv.org/pdf/2512.20386v2.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Dong Xiao",
      "Renjie Chen",
      "Bailin Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16212v1",
    "title": "Point Bridge: 3D Representations for Cross Domain Policy Learning",
    "summary": "Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/",
    "published": "2026-01-22T18:59:24Z",
    "updated": "2026-01-22T18:59:24Z",
    "link": "http://arxiv.org/pdf/2601.16212v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Siddhant Haldar",
      "Lars Johannsmeier",
      "Lerrel Pinto",
      "Abhishek Gupta",
      "Dieter Fox",
      "Yashraj Narang",
      "Ajay Mandlekar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16207v1",
    "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance",
    "summary": "Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA",
    "published": "2026-01-22T18:57:13Z",
    "updated": "2026-01-22T18:57:13Z",
    "link": "http://arxiv.org/pdf/2601.16207v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jongwoo Park",
      "Kanchana Ranasinghe",
      "Jinhyeok Jang",
      "Cristina Mata",
      "Yoo Sung Jang",
      "Michael S Ryoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.09253v2",
    "title": "Reinforcement Learning Compensated Model Predictive Control for Off-road Driving on Unknown Deformable Terrain",
    "summary": "This study presents an Actor-Critic reinforcement learning Compensated Model Predictive Controller (AC2MPC) designed for high-speed, off-road autonomous driving on deformable terrains. Addressing the difficulty of modeling unknown tire-terrain interaction and ensuring real-time control feasibility and performance, this framework integrates deep reinforcement learning with a model predictive controller to manage unmodeled nonlinear dynamics. We evaluate the controller framework over constant and varying velocity profiles using high-fidelity simulator Project Chrono. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers over three unknown terrains that represent sandy deformable track, sandy and rocky track and cohesive clay-like deformable soil track. Despite varied and previously unseen terrain characteristics, this framework generalized well enough to track longitudinal reference speeds with the least error. Furthermore, this framework required significantly less training data compared to purely learning based controller, converging in fewer steps while delivering better performance. Even when under-trained, this controller outperformed the standalone controllers, highlighting its potential for safer and more efficient real-world deployment.",
    "published": "2024-08-17T16:53:51Z",
    "updated": "2026-01-22T18:38:26Z",
    "link": "http://arxiv.org/pdf/2408.09253v2.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Prakhar Gupta",
      "Jonathon M. Smereka",
      "Yunyi Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11773v3",
    "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics",
    "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.\n  Project page: https://brittonjordan.github.io/probe_mde/",
    "published": "2025-12-12T18:36:53Z",
    "updated": "2026-01-22T17:44:32Z",
    "link": "http://arxiv.org/pdf/2512.11773v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Britton Jordan",
      "Jordan Thompson",
      "Jesse F. d'Almeida",
      "Hao Li",
      "Nithesh Kumar",
      "Susheela Sharma Stern",
      "James Ferguson",
      "Ipek Oguz",
      "Robert J. Webster",
      "Daniel Brown",
      "Alan Kuntz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16109v1",
    "title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision",
    "summary": "We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.",
    "published": "2026-01-22T16:56:52Z",
    "updated": "2026-01-22T16:56:52Z",
    "link": "http://arxiv.org/pdf/2601.16109v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yashuai Yan",
      "Tobias Egle",
      "Christian Ott",
      "Dongheui Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16078v1",
    "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application",
    "summary": "One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.",
    "published": "2026-01-22T16:21:18Z",
    "updated": "2026-01-22T16:21:18Z",
    "link": "http://arxiv.org/pdf/2601.16078v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Jiarui Cui",
      "Maosong Wang",
      "Wenqi Wu",
      "Peiqi Li",
      "Xianfei Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16062v1",
    "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis",
    "summary": "One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.",
    "published": "2026-01-22T15:58:56Z",
    "updated": "2026-01-22T15:58:56Z",
    "link": "http://arxiv.org/pdf/2601.16062v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Jiarui Cui",
      "Maosong Wang",
      "Wenqi Wu",
      "Peiqi Li",
      "Xianfei Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16035v1",
    "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
    "summary": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.",
    "published": "2026-01-22T15:08:53Z",
    "updated": "2026-01-22T15:08:53Z",
    "link": "http://arxiv.org/pdf/2601.16035v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Han Xue",
      "Sikai Liang",
      "Zhikai Zhang",
      "Zicheng Zeng",
      "Yun Liu",
      "Yunrui Lian",
      "Jilong Wang",
      "Qingtao Liu",
      "Xuesong Shi",
      "Li Yi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15946v1",
    "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
    "summary": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}",
    "published": "2026-01-22T13:28:09Z",
    "updated": "2026-01-22T13:28:09Z",
    "link": "http://arxiv.org/pdf/2601.15946v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zijie Chen",
      "Xiaowei Liu",
      "Yong Xu",
      "Shenghai Yuan",
      "Jianping Li",
      "Lihua Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15775v1",
    "title": "Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV",
    "summary": "This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.",
    "published": "2026-01-22T09:03:57Z",
    "updated": "2026-01-22T09:03:57Z",
    "link": "http://arxiv.org/pdf/2601.15775v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Amir Habel",
      "Ivan Snegirev",
      "Elizaveta Semenyakina",
      "Miguel Altamirano Cabrera",
      "Jeffrin Sam",
      "Fawad Mehboob",
      "Roohan Ahmed Khan",
      "Muhammad Ahsan Mustafa",
      "Dzmitry Tsetserukou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15707v1",
    "title": "D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot",
    "summary": "Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.",
    "published": "2026-01-22T07:20:55Z",
    "updated": "2026-01-22T07:20:55Z",
    "link": "http://arxiv.org/pdf/2601.15707v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Qifan Hu",
      "Branko Celler",
      "Weidong Mu",
      "Steven W. Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.09822v5",
    "title": "Multi-Layered Reasoning from a Single Viewpoint for Learning See-Through Grasping",
    "summary": "Sensory substitution enables biological systems to perceive stimuli that are typically perceived by another organ, which is inspirational for physical agents. Multimodal perception of intrinsic and extrinsic interactions is critical in building an intelligent robot that learns. This study presents a Vision-based See-Through Perception (VBSeeThruP) architecture that simultaneously perceives multiple intrinsic and extrinsic modalities from a single visual input, in a markerless manner, all packed into a soft robotic finger using the Soft Polyhedral Network design. It is generally applicable to miniature vision systems placed beneath deformable networks with a see-through design, capturing real-time images of the network's physical interactions induced by contact-based events, overlaid on the visual scene of the external environment, as demonstrated in the ablation study. We present the VBSeeThruP's capability for learning reactive grasping without using external cameras or dedicated force and torque sensors on the fingertips. Using the inpainted scene and the deformation mask, we further demonstrate the multimodal performance of the VBSeeThruP architecture to simultaneously achieve various perceptions, including but not limited to scene inpainting, object detection, depth sensing, scene segmentation, masked deformation tracking, 6D force/torque sensing, and contact event detection, all within a single sensory input from the in-finger vision markerlessly.",
    "published": "2023-12-15T14:21:14Z",
    "updated": "2026-01-22T07:14:46Z",
    "link": "http://arxiv.org/pdf/2312.09822v5.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Fang Wan",
      "Chaoyang Song"
    ]
  }
]