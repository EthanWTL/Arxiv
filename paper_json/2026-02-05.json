[
  {
    "id": "http://arxiv.org/abs/2502.04700v5",
    "title": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference",
    "summary": "The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.",
    "published": "2025-02-07T07:07:04Z",
    "updated": "2026-02-05T18:59:59Z",
    "link": "http://arxiv.org/pdf/2502.04700v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Prakhar Kaushik",
      "Ankit Vaidya",
      "Shravan Chaudhari",
      "Alan Yuille"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06043v1",
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "summary": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
    "published": "2026-02-05T18:59:58Z",
    "updated": "2026-02-05T18:59:58Z",
    "link": "http://arxiv.org/pdf/2602.06043v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Prakhar Kaushik",
      "Ankit Vaidya",
      "Shravan Chaudhari",
      "Rama Chellappa",
      "Alan Yuille"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06039v1",
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "summary": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
    "published": "2026-02-05T18:59:51Z",
    "updated": "2026-02-05T18:59:51Z",
    "link": "http://arxiv.org/pdf/2602.06039v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuxing Lu",
      "Yucheng Hu",
      "Xukai Zhao",
      "Jiuxin Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06038v1",
    "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
    "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.",
    "published": "2026-02-05T18:59:45Z",
    "updated": "2026-02-05T18:59:45Z",
    "link": "http://arxiv.org/pdf/2602.06038v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Xiaopan Zhang",
      "Zejin Wang",
      "Zhixu Li",
      "Jianpeng Yao",
      "Jiachen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21051v3",
    "title": "Language Models and Logic Programs for Trustworthy Tax Reasoning",
    "summary": "According to the United States Internal Revenue Service, ``the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the effectiveness of applying semantic parsing methods to statutory reasoning, and show promising economic feasibility of neuro-symbolic architectures for increasing access to reliable tax assistance.",
    "published": "2025-08-28T17:55:07Z",
    "updated": "2026-02-05T18:58:31Z",
    "link": "http://arxiv.org/pdf/2508.21051v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "William Jurayj",
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06025v1",
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
    "published": "2026-02-05T18:57:09Z",
    "updated": "2026-02-05T18:57:09Z",
    "link": "http://arxiv.org/pdf/2602.06025v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Haozhen Zhang",
      "Haodong Yue",
      "Tao Feng",
      "Quanyu Long",
      "Jianzhu Bao",
      "Bowen Jin",
      "Weizhi Zhang",
      "Xiao Li",
      "Jiaxuan You",
      "Chengwei Qin",
      "Wenya Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06023v1",
    "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
    "summary": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.",
    "published": "2026-02-05T18:56:49Z",
    "updated": "2026-02-05T18:56:49Z",
    "link": "http://arxiv.org/pdf/2602.06023v1.pdf",
    "category": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Christopher A. McClurg",
      "Alan R. Wagner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06022v1",
    "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering",
    "summary": "Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\\% and expected calibration error (ECE) by 50\\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\\% accuracy improvements and 49\\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.",
    "published": "2026-02-05T18:55:56Z",
    "updated": "2026-02-05T18:55:56Z",
    "link": "http://arxiv.org/pdf/2602.06022v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Miranda Muqing Miao",
      "Young-Min Cho",
      "Lyle Ungar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06014v1",
    "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
    "summary": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \\citep{halder2025stable} is stable for any $K \\ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.",
    "published": "2026-02-05T18:52:54Z",
    "updated": "2026-02-05T18:52:54Z",
    "link": "http://arxiv.org/pdf/2602.06014v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST",
      "stat.ML"
    ],
    "authors": [
      "Shunxing Yan",
      "Han Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06013v1",
    "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
    "summary": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.",
    "published": "2026-02-05T18:52:48Z",
    "updated": "2026-02-05T18:52:48Z",
    "link": "http://arxiv.org/pdf/2602.06013v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ruihang Li",
      "Leigang Qu",
      "Jingxu Zhang",
      "Dongnan Gui",
      "Mengde Xu",
      "Xiaosong Zhang",
      "Han Hu",
      "Wenjie Wang",
      "Jiaqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06008v1",
    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
    "summary": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
    "published": "2026-02-05T18:50:36Z",
    "updated": "2026-02-05T18:50:36Z",
    "link": "http://arxiv.org/pdf/2602.06008v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xianyang Liu",
      "Shangding Gu",
      "Dawn Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06000v1",
    "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods",
    "summary": "Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.",
    "published": "2026-02-05T18:46:28Z",
    "updated": "2026-02-05T18:46:28Z",
    "link": "http://arxiv.org/pdf/2602.06000v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Ali Shendabadi",
      "Parnia Izadirad",
      "Mostafa Salehi",
      "Mahmoud Bijankhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05993v1",
    "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
    "summary": "Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose \"Diamond Maps\", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.",
    "published": "2026-02-05T18:42:00Z",
    "updated": "2026-02-05T18:42:00Z",
    "link": "http://arxiv.org/pdf/2602.05993v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Peter Holderrieth",
      "Douglas Chen",
      "Luca Eyring",
      "Ishin Shah",
      "Giri Anantharaman",
      "Yutong He",
      "Zeynep Akata",
      "Tommi Jaakkola",
      "Nicholas Matthew Boffi",
      "Max Simchowitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.07209v2",
    "title": "SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model",
    "summary": "Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.",
    "published": "2026-01-12T05:03:12Z",
    "updated": "2026-02-05T18:37:54Z",
    "link": "http://arxiv.org/pdf/2601.07209v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "authors": [
      "Yu Guo",
      "Zhiqiang Lao",
      "Xiyun Song",
      "Yubin Zhou",
      "Heather Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05986v1",
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "summary": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
    "published": "2026-02-05T18:36:10Z",
    "updated": "2026-02-05T18:36:10Z",
    "link": "http://arxiv.org/pdf/2602.05986v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Mingxin Liu",
      "Shuran Ma",
      "Shibei Meng",
      "Xiangyu Zhao",
      "Zicheng Zhang",
      "Shaofeng Zhang",
      "Zhihang Zhong",
      "Peixian Chen",
      "Haoyu Cao",
      "Xing Sun",
      "Haodong Duan",
      "Xue Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05983v1",
    "title": "Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins",
    "summary": "The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.",
    "published": "2026-02-05T18:33:03Z",
    "updated": "2026-02-05T18:33:03Z",
    "link": "http://arxiv.org/pdf/2602.05983v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Krešimir Kušić",
      "Vinny Cahill",
      "Ivana Dusparic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05977v1",
    "title": "Clifford Kolmogorov-Arnold Networks",
    "summary": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.",
    "published": "2026-02-05T18:25:40Z",
    "updated": "2026-02-05T18:25:40Z",
    "link": "http://arxiv.org/pdf/2602.05977v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Matthias Wolff",
      "Francesco Alesiani",
      "Christof Duhme",
      "Xiaoyi Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05970v1",
    "title": "Inverse Depth Scaling From Most Layers Being Similar",
    "summary": "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.",
    "published": "2026-02-05T18:22:41Z",
    "updated": "2026-02-05T18:22:41Z",
    "link": "http://arxiv.org/pdf/2602.05970v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.DS",
      "stat.ML"
    ],
    "authors": [
      "Yizhou Liu",
      "Sara Kangaslahti",
      "Ziming Liu",
      "Jeff Gore"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05966v1",
    "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
    "summary": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
    "published": "2026-02-05T18:21:02Z",
    "updated": "2026-02-05T18:21:02Z",
    "link": "http://arxiv.org/pdf/2602.05966v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Mirlan Karimov",
      "Teodora Spasojevic",
      "Markus Braun",
      "Julian Wiederer",
      "Vasileios Belagiannis",
      "Marc Pollefeys"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05965v1",
    "title": "Learning to Share: Selective Memory for Efficient Parallel Agentic Systems",
    "summary": "Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/",
    "published": "2026-02-05T18:20:21Z",
    "updated": "2026-02-05T18:20:21Z",
    "link": "http://arxiv.org/pdf/2602.05965v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Joseph Fioresi",
      "Parth Parag Kulkarni",
      "Ashmal Vayani",
      "Song Wang",
      "Mubarak Shah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05951v1",
    "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
    "summary": "Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.",
    "published": "2026-02-05T18:08:20Z",
    "updated": "2026-02-05T18:08:20Z",
    "link": "http://arxiv.org/pdf/2602.05951v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Junwan Kim",
      "Jiho Park",
      "Seonghu Jeon",
      "Seungryong Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16175v2",
    "title": "Learning to Discover at Test Time",
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
    "published": "2026-01-22T18:24:00Z",
    "updated": "2026-02-05T18:03:03Z",
    "link": "http://arxiv.org/pdf/2601.16175v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mert Yuksekgonul",
      "Daniel Koceja",
      "Xinhao Li",
      "Federico Bianchi",
      "Jed McCaleb",
      "Xiaolong Wang",
      "Jan Kautz",
      "Yejin Choi",
      "James Zou",
      "Carlos Guestrin",
      "Yu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12840v3",
    "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics",
    "summary": "Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for reasoning about both the physical world and the beliefs of agents, with applications in domains where information flow and awareness among agents are critical. The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability. To address this, we exploit Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process. GNNs, which naturally capture the graph-like nature of Kripke models, allow us to derive meaningful estimates of state quality -- e.g., the distance from the nearest goal -- by generalizing knowledge obtained from previously solved planning instances. We integrate these predictive heuristics into an epistemic planning pipeline and evaluate them against standard baselines, showing improvements in the scalability of multi-agent epistemic planning.",
    "published": "2025-08-18T11:26:20Z",
    "updated": "2026-02-05T18:00:15Z",
    "link": "http://arxiv.org/pdf/2508.12840v3.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Giovanni Briglia",
      "Francesco Fabiano",
      "Stefano Mariani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09712v3",
    "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments",
    "summary": "Online fake news profoundly distorts public judgment and erodes trust in social platforms. While existing detectors achieve competitive performance on benchmark datasets, they remain notably vulnerable to malicious comments designed specifically to induce misclassification. This evolving threat landscape necessitates detection systems that simultaneously prioritize predictive accuracy and structural robustness. However, current detectors often fail to generalize across diverse and novel comment attack patterns. To bridge this gap, we propose AdComment, an adaptive adversarial training framework for robustness enhancement against diverse malicious comments. Based on cognitive psychology, we categorize adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and leverage LLMs to synthesize diverse, category-specific perturbations. Central to our framework is an InfoDirichlet Resampling (IDR) mechanism that dynamically adjusts malicious comment proportions during training, thereby steering optimization toward the model's most susceptible regions. Experimental results demonstrate that our approach achieves state-of-the-art performance on three benchmark datasets, improving the F1 scores by 17.9%, 14.5% and 9.0%, respectively.",
    "published": "2025-10-10T04:39:57Z",
    "updated": "2026-02-05T17:52:31Z",
    "link": "http://arxiv.org/pdf/2510.09712v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zhao Tong",
      "Chunlin Gong",
      "Yimeng Gu",
      "Haichao Shi",
      "Qiang Liu",
      "Shu Wu",
      "Xiao-Yu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22401v3",
    "title": "Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems",
    "summary": "We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.",
    "published": "2026-01-29T23:15:36Z",
    "updated": "2026-02-05T17:48:07Z",
    "link": "http://arxiv.org/pdf/2601.22401v3.pdf",
    "category": [
      "cs.AI",
      "math.CO",
      "math.NT"
    ],
    "authors": [
      "Tony Feng",
      "Trieu Trinh",
      "Garrett Bingham",
      "Jiwon Kang",
      "Shengtong Zhang",
      "Sang-hyun Kim",
      "Kevin Barreto",
      "Carl Schildkraut",
      "Junehyuk Jung",
      "Jaehyeon Seo",
      "Carlo Pagano",
      "Yuri Chervonyi",
      "Dawsen Hwang",
      "Kaiying Hou",
      "Sergei Gukov",
      "Cheng-Chiang Tsai",
      "Hyunwoo Choi",
      "Youngbeom Jin",
      "Wei-Yuan Li",
      "Hao-An Wu",
      "Ruey-An Shiu",
      "Yu-Sheng Shih",
      "Quoc V. Le",
      "Thang Luong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05930v1",
    "title": "Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025",
    "summary": "Large language models (LLMs) are increasingly used in academic writing workflows, yet they frequently hallucinate by generating citations to sources that do not exist. This study analyzes 100 AI-generated hallucinated citations that appeared in papers accepted by the 2025 Conference on Neural Information Processing Systems (NeurIPS), one of the world's most prestigious AI conferences. Despite review by 3-5 expert researchers per paper, these fabricated citations evaded detection, appearing in 53 published papers (approx. 1% of all accepted papers). We develop a five-category taxonomy that classifies hallucinations by their failure mode: Total Fabrication (66%), Partial Attribute Corruption (27%), Identifier Hijacking (4%), Placeholder Hallucination (2%), and Semantic Hallucination (1%). Our analysis reveals a critical finding: every hallucination (100%) exhibited compound failure modes. The distribution of secondary characteristics was dominated by Semantic Hallucination (63%) and Identifier Hijacking (29%), which often appeared alongside Total Fabrication to create a veneer of plausibility and false verifiability. These compound structures exploit multiple verification heuristics simultaneously, explaining why peer review fails to detect them. The distribution exhibits a bimodal pattern: 92% of contaminated papers contain 1-2 hallucinations (minimal AI use) while 8% contain 4-13 hallucinations (heavy reliance). These findings demonstrate that current peer review processes do not include effective citation verification and that the problem extends beyond NeurIPS to other major conferences, government reports, and professional consulting. We propose mandatory automated citation verification at submission as an implementable solution to prevent fabricated citations from becoming normalized in scientific literature.",
    "published": "2026-02-05T17:43:35Z",
    "updated": "2026-02-05T17:43:35Z",
    "link": "http://arxiv.org/pdf/2602.05930v1.pdf",
    "category": [
      "cs.DL",
      "cs.AI"
    ],
    "authors": [
      "Samar Ansari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11298v2",
    "title": "When Are Two RLHF Objectives the Same?",
    "summary": "The preference optimization literature contains many proposed objectives, often presented as distinct improvements. We introduce Opal, a canonicalization algorithm that determines whether two preference objectives are algebraically equivalent by producing either a canonical form or a concrete witness of non-equivalence. Applying Opal reveals that many widely used methods optimize the same underlying objective, while others are provably distinct. For example, batch normalization can cause the same response pair to receive different gradients depending on batch composition. We identify a small set of structural mechanisms that give rise to genuinely different objectives; most remaining differences are reparameterizations.",
    "published": "2025-09-14T14:42:39Z",
    "updated": "2026-02-05T17:34:59Z",
    "link": "http://arxiv.org/pdf/2509.11298v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Madhava Gaikwad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05920v1",
    "title": "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
    "summary": "This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.",
    "published": "2026-02-05T17:32:14Z",
    "updated": "2026-02-05T17:32:14Z",
    "link": "http://arxiv.org/pdf/2602.05920v1.pdf",
    "category": [
      "cs.AI",
      "cs.ET"
    ],
    "authors": [
      "Eva Andrés"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08667v2",
    "title": "RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data",
    "summary": "Modern software teams frequently encounter delays in resolving recurring or related issues due to fragmented knowledge scattered across JIRA tickets, developer discussions, and GitHub pull requests (PRs). To address this challenge, we propose a Retrieval-Augmented Generation (RAG) framework that integrates Sentence-Transformers for semantic embeddings with FAISS-based vector search to deliver context-aware ticket resolution recommendations. The approach embeds historical JIRA tickets, user comments, and linked PR metadata to retrieve semantically similar past cases, which are then synthesized by a Large Language Model (LLM) into grounded and explainable resolution suggestions. The framework contributes a unified pipeline linking JIRA and GitHub data, an embedding and FAISS indexing strategy for heterogeneous software artifacts, and a resolution generation module guided by retrieved evidence. Experimental evaluation using precision, recall, resolution time reduction, and developer acceptance metrics shows that the proposed system significantly improves resolution accuracy, fix quality, and knowledge reuse in modern DevOps environments.",
    "published": "2025-10-09T16:33:00Z",
    "updated": "2026-02-05T17:25:30Z",
    "link": "http://arxiv.org/pdf/2510.08667v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Mohammad Baqar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20295v4",
    "title": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?",
    "summary": "The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables. To support the development of this universal form of LLM uncertainties, we publish the code that implements our metric for arbitrary LLMs under https://github.com/apple/ml-selfreflect .",
    "published": "2025-05-26T17:59:53Z",
    "updated": "2026-02-05T17:25:06Z",
    "link": "http://arxiv.org/pdf/2505.20295v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Michael Kirchhof",
      "Luca Füger",
      "Adam Goliński",
      "Eeshan Gunesh Dhekane",
      "Arno Blaas",
      "Seong Joon Oh",
      "Sinead Williamson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05903v1",
    "title": "Verification of the Implicit World Model in a Generative Model via Adversarial Sequences",
    "summary": "Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.",
    "published": "2026-02-05T17:18:22Z",
    "updated": "2026-02-05T17:18:22Z",
    "link": "http://arxiv.org/pdf/2602.05903v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "András Balogh",
      "Márk Jelasity"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05902v1",
    "title": "Regularized Calibration with Successive Rounding for Post-Training Quantization",
    "summary": "Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.",
    "published": "2026-02-05T17:18:02Z",
    "updated": "2026-02-05T17:18:02Z",
    "link": "http://arxiv.org/pdf/2602.05902v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Seohyeon Cha",
      "Huancheng Chen",
      "Dongjun Kim",
      "Haoran Zhang",
      "Kevin Chan",
      "Gustavo de Veciana",
      "Haris Vikalo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15678v2",
    "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems",
    "summary": "Stealing attacks pose a persistent threat to the intellectual property of deployed machine-learning systems. Retrieval-augmented generation (RAG) intensifies this risk by extending the attack surface beyond model weights to knowledge base that often contains IP-bearing assets such as proprietary runbooks, curated domain collections, or licensed documents. Recent work shows that multi-turn questioning can gradually steal corpus content from RAG systems, yet existing attacks are largely heuristic and often plateau early. We address this gap by formulating RAG knowledge-base stealing as an adaptive stochastic coverage problem (ASCP), where each query is a stochastic action and the goal is to maximize the conditional expected marginal gain (CMG) in corpus coverage under a query budget. Bridging ASCP to real-world black-box RAG knowledge-base stealing raises three challenges: CMG is unobservable, the natural-language action space is intractably large, and feasibility constraints require stealthy queries that remain effective under diverse architectures. We introduce RAGCrawler, a knowledge graph-guided attacker that maintains a global attacker-side state to estimate coverage gains, schedule high-value semantic anchors, and generate non-redundant natural queries. Across four corpora and four generators with BGE retriever, RAGCrawler achieves 66.8% average coverage (up to 84.4%) within 1,000 queries, improving coverage by 44.90% relative to the strongest baseline. It also reduces the queries needed to reach 70% coverage by at least 4.03x on average and enables surrogate reconstruction with answer similarity up to 0.699. Our attack is also scalable to retriever switching and newer RAG techniques like query rewriting and multi-query retrieval. These results highlight urgent needs to protect RAG knowledge assets.",
    "published": "2026-01-22T05:59:42Z",
    "updated": "2026-02-05T17:16:47Z",
    "link": "http://arxiv.org/pdf/2601.15678v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Mengyu Yao",
      "Ziqi Zhang",
      "Ning Luo",
      "Shaofei Li",
      "Yifeng Cai",
      "Xiangqun Chen",
      "Yao Guo",
      "Ding Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05896v1",
    "title": "Parity, Sensitivity, and Transformers",
    "summary": "The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.\n  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.",
    "published": "2026-02-05T17:14:33Z",
    "updated": "2026-02-05T17:14:33Z",
    "link": "http://arxiv.org/pdf/2602.05896v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alexander Kozachinskiy",
      "Tomasz Steifer",
      "Przemysław Wałȩga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13579v3",
    "title": "Learning to summarize user information for personalized reinforcement learning from human feedback",
    "summary": "As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users' preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model, meaning it assumes that everyone's preferences are the same. We present a novel framework, Preference Learning Using Summarization (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries of each user's preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. Both the user-summarization model and reward model are trained simultaneously, creating an online co-adaptation loop. We show that in contrast to the standard Bradley-Terry model, summaries produced by PLUS capture diverse aspects of user preferences, achieving a 11-77/% improvement in reward model accuracy. Key strengths of PLUS are: (1) robust performance with new users and conversation topics, achieving a 25\\% improvement over the best personalized reward model technique used for RLHF; (2) zero-shot personalization with state-of-the-art proprietary models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72\\% win rate compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond preference labels, and (4) interpretable representation of users, enabling greater transparency and user control in pluralistic LLM alignment.",
    "published": "2025-07-17T23:48:51Z",
    "updated": "2026-02-05T17:13:23Z",
    "link": "http://arxiv.org/pdf/2507.13579v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hyunji Nam",
      "Yanming Wan",
      "Mickel Liu",
      "Peter Ahnn",
      "Jianxun Lian",
      "Natasha Jaques"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05888v1",
    "title": "Metric Hedonic Games on the Line",
    "summary": "Hedonic games are fundamental models for investigating the formation of coalitions among a set of strategic agents, where every agent has a certain utility for every possible coalition of agents it can be part of. To avoid the intractability of defining exponentially many utilities for all possible coalitions, many variants with succinct representations of the agents' utility functions have been devised and analyzed, e.g., modified fractional hedonic games by Monaco et al. [JAAMAS 2020]. We extend this by studying a novel succinct variant that is related to modified fractional hedonic games. In our model, each agent has a fixed type-value and an agent's cost for some given coalition is based on the differences between its value and those of the other members of its coalition. This allows to model natural situations like athletes forming training groups with similar performance levels or voters that partition themselves along a political spectrum.\n  In particular, we investigate natural variants where an agent's cost is defined by distance thresholds, or by the maximum or average value difference to the other agents in its coalition. For these settings, we study the existence of stable coalition structures, their properties, and their quality in terms of the price of anarchy and the price of stability. Further, we investigate the impact of limiting the maximum number of coalitions. Despite the simple setting with metric distances on a line, we uncover a rich landscape of models, partially with counter-intuitive behavior. Also, our focus on both swap stability and jump stability allows us to study the influence of fixing the number and the size of the coalitions. Overall, we find that stable coalition structures always exist but that their properties and quality can vary widely.",
    "published": "2026-02-05T17:05:08Z",
    "updated": "2026-02-05T17:05:08Z",
    "link": "http://arxiv.org/pdf/2602.05888v1.pdf",
    "category": [
      "cs.GT",
      "cs.AI"
    ],
    "authors": [
      "Merlin de la Haye",
      "Pascal Lenzner",
      "Farehe Soheil",
      "Marcus Wunderlich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05885v1",
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "summary": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
    "published": "2026-02-05T17:01:09Z",
    "updated": "2026-02-05T17:01:09Z",
    "link": "http://arxiv.org/pdf/2602.05885v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Wei Liu",
      "Jiawei Xu",
      "Yingru Li",
      "Longtao Zheng",
      "Tianjian Li",
      "Qian Liu",
      "Junxian He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05884v1",
    "title": "Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views",
    "summary": "Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\\pm$ 4.26 mL vs. 8.14 $\\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\\pm$ 7.37 mL vs. 37.76 $\\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.",
    "published": "2026-02-05T17:00:59Z",
    "updated": "2026-02-05T17:00:59Z",
    "link": "http://arxiv.org/pdf/2602.05884v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CE"
    ],
    "authors": [
      "Gino E. Jansen",
      "Carolina Brás",
      "R. Nils Planken",
      "Mark J. Schuuring",
      "Berto J. Bouma",
      "Ivana Išgum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05883v1",
    "title": "A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges",
    "summary": "Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.",
    "published": "2026-02-05T17:00:07Z",
    "updated": "2026-02-05T17:00:07Z",
    "link": "http://arxiv.org/pdf/2602.05883v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Philippe J. Giabbanelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19903v2",
    "title": "STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification",
    "summary": "Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.",
    "published": "2025-11-28T05:31:07Z",
    "updated": "2026-02-05T16:54:09Z",
    "link": "http://arxiv.org/pdf/2601.19903v2.pdf",
    "category": [
      "cs.AR",
      "cs.AI"
    ],
    "authors": [
      "Saeid Rajabi",
      "Chengmo Yang",
      "Satwik Patnaik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05879v1",
    "title": "EuroLLM-22B: Technical Report",
    "summary": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.",
    "published": "2026-02-05T16:53:47Z",
    "updated": "2026-02-05T16:53:47Z",
    "link": "http://arxiv.org/pdf/2602.05879v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Miguel Moura Ramos",
      "Duarte M. Alves",
      "Hippolyte Gisserot-Boukhlef",
      "João Alves",
      "Pedro Henrique Martins",
      "Patrick Fernandes",
      "José Pombal",
      "Nuno M. Guerreiro",
      "Ricardo Rei",
      "Nicolas Boizard",
      "Amin Farajian",
      "Mateusz Klimaszewski",
      "José G. C. de Souza",
      "Barry Haddow",
      "François Yvon",
      "Pierre Colombo",
      "Alexandra Birch",
      "André F. T. Martins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05877v1",
    "title": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy",
    "summary": "The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous \"separation of concerns\" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented \"victim modeling\" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.",
    "published": "2026-02-05T16:53:41Z",
    "updated": "2026-02-05T16:53:41Z",
    "link": "http://arxiv.org/pdf/2602.05877v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Lukas Stappen",
      "Ahmet Erkan Turan",
      "Johann Hagerer",
      "Georg Groh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16048v4",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "summary": "We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.",
    "published": "2025-05-21T22:00:20Z",
    "updated": "2026-02-05T16:53:24Z",
    "link": "http://arxiv.org/pdf/2505.16048v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Philipp D. Siedler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05875v1",
    "title": "Beyond Manual Planning: Seating Allocation for Large Organizations",
    "summary": "We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.",
    "published": "2026-02-05T16:52:44Z",
    "updated": "2026-02-05T16:52:44Z",
    "link": "http://arxiv.org/pdf/2602.05875v1.pdf",
    "category": [
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Anton Ipsen",
      "Michael Cashmore",
      "Kirsty Fielding",
      "Nicolas Marchesotti",
      "Parisa Zehtabi",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05874v1",
    "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection",
    "summary": "Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.\n  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.\n  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.\n  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.",
    "published": "2026-02-05T16:51:56Z",
    "updated": "2026-02-05T16:51:56Z",
    "link": "http://arxiv.org/pdf/2602.05874v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Adrián Girón",
      "Pablo Miralles",
      "Javier Huertas-Tato",
      "Sergio D'Antonio",
      "David Camacho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.03190v2",
    "title": "Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning",
    "summary": "Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 45.2 per-benchmark accuracy and 51.8 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.",
    "published": "2026-02-03T06:59:42Z",
    "updated": "2026-02-05T16:51:08Z",
    "link": "http://arxiv.org/pdf/2602.03190v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Wenquan Lu",
      "Hai Huang",
      "Randall Balestriero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05859v1",
    "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
    "summary": "Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.",
    "published": "2026-02-05T16:41:25Z",
    "updated": "2026-02-05T16:41:25Z",
    "link": "http://arxiv.org/pdf/2602.05859v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Xu Wang",
      "Bingqing Jiang",
      "Yu Wan",
      "Baosong Yang",
      "Lingpeng Kong",
      "Difan Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05857v1",
    "title": "BABE: Biology Arena BEnchmark",
    "summary": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
    "published": "2026-02-05T16:39:20Z",
    "updated": "2026-02-05T16:39:20Z",
    "link": "http://arxiv.org/pdf/2602.05857v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Junting Zhou",
      "Jin Chen",
      "Linfeng Hao",
      "Denghui Cao",
      "Zheyu Wang",
      "Qiguang Chen",
      "Chaoyou Fu",
      "Jiaze Chen",
      "Yuchen Wu",
      "Ge Zhang",
      "Mingxuan Wang",
      "Wenhao Huang",
      "Tong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05848v1",
    "title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network",
    "summary": "DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.",
    "published": "2026-02-05T16:35:46Z",
    "updated": "2026-02-05T16:35:46Z",
    "link": "http://arxiv.org/pdf/2602.05848v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Henry Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05847v1",
    "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
    "summary": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
    "published": "2026-02-05T16:35:19Z",
    "updated": "2026-02-05T16:35:19Z",
    "link": "http://arxiv.org/pdf/2602.05847v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhangquan Chen",
      "Jiale Tao",
      "Ruihuang Li",
      "Yihao Hu",
      "Ruitao Chen",
      "Zhantao Yang",
      "Xinlei Yu",
      "Haodong Jing",
      "Manyuan Zhang",
      "Shuai Shao",
      "Biao Wang",
      "Qinglin Lu",
      "Ruqi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05838v1",
    "title": "FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation",
    "summary": "Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.",
    "published": "2026-02-05T16:28:13Z",
    "updated": "2026-02-05T16:28:13Z",
    "link": "http://arxiv.org/pdf/2602.05838v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Mayank Kumar",
      "Qian Lou",
      "Paulo Barreto",
      "Martine De Cock",
      "Sikha Pentyala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.06749v3",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .",
    "published": "2025-03-09T20:06:45Z",
    "updated": "2026-02-05T16:21:33Z",
    "link": "http://arxiv.org/pdf/2503.06749v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Wenxuan Huang",
      "Bohan Jia",
      "Zijie Zhai",
      "Shaosheng Cao",
      "Zheyu Ye",
      "Fei Zhao",
      "Zhe Xu",
      "Yao Hu",
      "Shaohui Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05830v1",
    "title": "Learning Compact Boolean Networks",
    "summary": "Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.",
    "published": "2026-02-05T16:19:59Z",
    "updated": "2026-02-05T16:19:59Z",
    "link": "http://arxiv.org/pdf/2602.05830v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shengpu Wang",
      "Yuhao Mao",
      "Yani Zhang",
      "Martin Vechev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05818v1",
    "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
    "summary": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.",
    "published": "2026-02-05T16:08:36Z",
    "updated": "2026-02-05T16:08:36Z",
    "link": "http://arxiv.org/pdf/2602.05818v1.pdf",
    "category": [
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Zihao Jiang",
      "Miao Peng",
      "Zhenyan Shan",
      "Wenjie Xu",
      "Ben Liu",
      "Gong Chen",
      "Ziqi Gao",
      "Min Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05811v1",
    "title": "STProtein: predicting spatial protein expression from multi-omics data",
    "summary": "The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological \"Dark Matter\".",
    "published": "2026-02-05T16:04:03Z",
    "updated": "2026-02-05T16:04:03Z",
    "link": "http://arxiv.org/pdf/2602.05811v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhaorui Jiang",
      "Yingfang Yuan",
      "Lei Hu",
      "Wei Pang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08176v2",
    "title": "Leveraging Whisper Embeddings for Audio-based Lyrics Matching",
    "summary": "Audio-based lyrics matching can be an appealing alternative to other content-based retrieval approaches, but existing methods often suffer from limited reproducibility and inconsistent baselines. In this work, we introduce WEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings for lyrics matching tasks. WEALY establishes robust and transparent baselines, while also exploring multimodal extensions that integrate textual and acoustic features. Through extensive experiments on standard datasets, we demonstrate that WEALY achieves a performance comparable to state-of-the-art methods that lack reproducibility. In addition, we provide ablation studies and analyses on language robustness, loss functions, and embedding strategies. This work contributes a reliable benchmark for future research, and underscores the potential of speech technologies for music information retrieval tasks.",
    "published": "2025-10-09T13:03:34Z",
    "updated": "2026-02-05T16:02:06Z",
    "link": "http://arxiv.org/pdf/2510.08176v2.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Eleonora Mancini",
      "Joan Serrà",
      "Paolo Torroni",
      "Yuki Mitsufuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19827v2",
    "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering",
    "summary": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.",
    "published": "2026-01-27T17:35:05Z",
    "updated": "2026-02-05T15:59:52Z",
    "link": "http://arxiv.org/pdf/2601.19827v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Mahdi Astaraki",
      "Mohammad Arshi Saloot",
      "Ali Shiraee Kasmaee",
      "Hamidreza Mahyar",
      "Soheila Samiee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05805v1",
    "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
    "summary": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
    "published": "2026-02-05T15:59:12Z",
    "updated": "2026-02-05T15:59:12Z",
    "link": "http://arxiv.org/pdf/2602.05805v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Kang Chen",
      "Zhuoka Feng",
      "Sihan Zhao",
      "Kai Xiong",
      "Junjie Nian",
      "Yaoning Wang",
      "Changyi Xiao",
      "Yixin Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.13431v2",
    "title": "Solving Prior Distribution Mismatch in Diffusion Models via Optimal Transport",
    "summary": "Diffusion Models (DMs) have achieved remarkable progress in generative modeling. However, the mismatch between the forward terminal distribution and reverse initial distribution introduces prior error, leading to deviations of sampling trajectories from the true distribution and severely limiting model performance. This issue further triggers cascading problems, including non-zero Signal-to-Noise Ratio, accumulated denoising errors, degraded generation quality, and constrained sampling efficiency. To address this issue, this paper proposes a prior error elimination framework based on Optimal Transport (OT). Specifically, an OT map from the reverse initial distribution to the forward terminal distribution is constructed to achieve precise matching of the two distributions. Meanwhile, the upper bound of the prior error is quantified using the Wasserstein distance, proving that the prior error can be effectively eliminated via the OT map. Additionally, by deriving the asymptotic consistency between dynamic OT and probability flow, this method is revealed to be highly compatible with the intrinsic mechanism of the diffusion process. Experimental results demonstrate that the proposed method completely eliminates the prior error both theoretically and practically, providing a universal and rigorous solution for optimizing the performance of DMs.",
    "published": "2024-10-17T10:54:55Z",
    "updated": "2026-02-05T15:52:57Z",
    "link": "http://arxiv.org/pdf/2410.13431v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhanpeng Wang",
      "Shenghao Li",
      "Jiameng Che",
      "Chen Wang",
      "Shangling Jui",
      "Na Lei",
      "Zhongxuan Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05794v1",
    "title": "FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem",
    "summary": "We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.",
    "published": "2026-02-05T15:48:49Z",
    "updated": "2026-02-05T15:48:49Z",
    "link": "http://arxiv.org/pdf/2602.05794v1.pdf",
    "category": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Aboli Kathar",
      "Aman Kumar",
      "Anusha Kamath",
      "Araveeti Srujan",
      "Ashish Sharma",
      "Chandra Bhushan",
      "Dilip Asbe",
      "Divya Sorate",
      "Duddu Prasanth Kumar",
      "Evan Acharya",
      "Harsh Sharma",
      "Hrithik Kadam",
      "Kanishk Singla",
      "Keyur Doshi",
      "Kiran Praveen",
      "Kolisetty Krishna SK",
      "Krishanu Adhikary",
      "Lokesh MPT",
      "Mayurdeep Sonowal",
      "Nadeem Shaikh",
      "Navya Prakash",
      "Nimit Kothari",
      "Nitin Kukreja",
      "Prashant Devadiga",
      "Rakesh Paul",
      "Ratanjeet Pratap Chauhan",
      "Raunak Kalani",
      "Raviraj Joshi",
      "Shamanth MH",
      "Shantanu Pandey",
      "Shubham Soni",
      "Siddharth Dixit",
      "Smriti Jopat",
      "Sunil Patel",
      "Suraj Singh",
      "Suvradip Paul",
      "Tulasi Pilla",
      "Utkarsh Vaidya",
      "Vineeth Nambiar",
      "Vishal Kanvaty",
      "Yatharth Dedhia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.17196v2",
    "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models",
    "summary": "Effectively processing long contexts is a critical challenge for language models. While standard Transformers are limited by quadratic complexity and poor length extrapolation, alternative architectures like sliding window attention and state space models sacrifice the ability to effectively utilize the full context due to their fixed-size memory. Chunk-based sparse attention has emerged as a promising paradigm for extreme length generalization, yet the key architectural principles underpinning its success are not yet fully understood. In this work, we present a systematic dissection of these models to identify the core components driving their performance. Through a unified framework and comprehensive ablation studies, we demonstrate that a combination of three design principles is critical: (1) an expressive, non-linear Chunk Encoder with a dedicated CLS token to produce representations for retrieval; (2) a Bypassing Residual Path to stably integrate retrieved global information without it being overridden by the local residual stream; and (3) enforced selection sparsity during pre-training to bridge the train-test distribution gap. We provide a theoretical motivation for intra-chunk information processing and landmark generation. By combining these principles, we establish a new state-of-the-art for training-free length extrapolation, successfully generalizing models trained on a 4K context to 32 million tokens on RULER and BABILong. Our findings provide a clear and empirically-grounded set of design principles for developing future, highly-capable long-context language models.",
    "published": "2025-10-20T06:17:57Z",
    "updated": "2026-02-05T15:48:38Z",
    "link": "http://arxiv.org/pdf/2510.17196v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jiaqi Leng",
      "Xiang Hu",
      "Junxiong Wang",
      "Jianguo Li",
      "Wei Wu",
      "Yucheng Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05789v1",
    "title": "Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation",
    "summary": "With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.",
    "published": "2026-02-05T15:45:39Z",
    "updated": "2026-02-05T15:45:39Z",
    "link": "http://arxiv.org/pdf/2602.05789v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hengyi Wang",
      "Ruiqiang Zhang",
      "Chang Liu",
      "Guanjie Wang",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05787v1",
    "title": "Bagging-Based Model Merging for Robust General Text Embeddings",
    "summary": "General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \\modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \\modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.",
    "published": "2026-02-05T15:45:08Z",
    "updated": "2026-02-05T15:45:08Z",
    "link": "http://arxiv.org/pdf/2602.05787v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Jiaming Zhang",
      "Wenbo Yang",
      "Daiting Shi",
      "Xueqi Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05785v1",
    "title": "ReText: Text Boosts Generalization in Image-Based Person Re-identification",
    "summary": "Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.",
    "published": "2026-02-05T15:43:31Z",
    "updated": "2026-02-05T15:43:31Z",
    "link": "http://arxiv.org/pdf/2602.05785v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Timur Mamedov",
      "Karina Kvanchiani",
      "Anton Konushin",
      "Vadim Konushin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05780v1",
    "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
    "summary": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
    "published": "2026-02-05T15:38:54Z",
    "updated": "2026-02-05T15:38:54Z",
    "link": "http://arxiv.org/pdf/2602.05780v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Ulrich Finkler",
      "Irene Manotas",
      "Wei Zhang",
      "Geert Janssen",
      "Octavian Popescu",
      "Shyam Ramji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05774v1",
    "title": "Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance",
    "summary": "Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.",
    "published": "2026-02-05T15:36:19Z",
    "updated": "2026-02-05T15:36:19Z",
    "link": "http://arxiv.org/pdf/2602.05774v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xiandong Zou",
      "Jianshu Li",
      "Jing Huang",
      "Pan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05765v1",
    "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
    "summary": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.",
    "published": "2026-02-05T15:30:23Z",
    "updated": "2026-02-05T15:30:23Z",
    "link": "http://arxiv.org/pdf/2602.05765v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhong Guan",
      "Haoran Sun",
      "Yongjian Guo",
      "Shuai Di",
      "Xiaodong Bai",
      "Jing Long",
      "Tianyun Zhao",
      "Mingxi Luo",
      "Chen Zhou",
      "Yucheng Guo",
      "Qiming Yang",
      "Wanting Xu",
      "Wen Huang",
      "Yunxuan Ma",
      "Hongke Zhao",
      "Likang Wu",
      "Xiaotie Deng",
      "Xi Xiao",
      "Sheng Wen",
      "Yicheng Gong",
      "Junwu Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05762v1",
    "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
    "summary": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
    "published": "2026-02-05T15:28:26Z",
    "updated": "2026-02-05T15:28:26Z",
    "link": "http://arxiv.org/pdf/2602.05762v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SE"
    ],
    "authors": [
      "Andrei Kozyrev",
      "Nikita Khramov",
      "Denis Lochmelis",
      "Valerio Morelli",
      "Gleb Solovev",
      "Anton Podkopaev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05754v1",
    "title": "TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism",
    "summary": "Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.",
    "published": "2026-02-05T15:24:11Z",
    "updated": "2026-02-05T15:24:11Z",
    "link": "http://arxiv.org/pdf/2602.05754v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Seonghye Cho",
      "Jaemin Han",
      "Hyunjin Kim",
      "Euisoo Jung",
      "Jae-Gil Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05748v1",
    "title": "LeakBoost: Perceptual-Loss-Based Membership Inference Attack",
    "summary": "Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.",
    "published": "2026-02-05T15:15:35Z",
    "updated": "2026-02-05T15:15:35Z",
    "link": "http://arxiv.org/pdf/2602.05748v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Amit Kravchik Taub",
      "Fred M. Grabovski",
      "Guy Amit",
      "Yisroel Mirsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05746v1",
    "title": "Learning to Inject: Automated Prompt Injection via Reinforcement Learning",
    "summary": "Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.",
    "published": "2026-02-05T15:14:46Z",
    "updated": "2026-02-05T15:14:46Z",
    "link": "http://arxiv.org/pdf/2602.05746v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xin Chen",
      "Jie Zhang",
      "Florian Tramer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16838v3",
    "title": "Segmentation-free Goodness of Pronunciation",
    "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in modern computer-aided language learning (CALL) systems. Most systems implementing phoneme-level MDD through goodness of pronunciation (GOP), however, rely on pre-segmentation of speech into phonetic units. This limits the accuracy of these methods and the possibility to use modern CTC-based acoustic models for their evaluation. In this study, we first propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR models for MDD. Next, we define a more general segmentation-free method that takes all possible segmentations of the canonical transcription into account (GOP-SF). We give a theoretical account of our definition of GOP-SF, an implementation that solves potential numerical issues as well as a proper normalization which allows the use of acoustic models with different peakiness over time. We provide extensive experimental results on the CMU Kids and speechocean762 datasets comparing the different definitions of our methods, estimating the dependency of GOP-SF on the peakiness of the acoustic models and on the amount of context around the target phoneme. Finally, we compare our methods with recent studies over the speechocean762 data showing that the feature vectors derived from the proposed method achieve state-of-the-art results on phoneme-level pronunciation assessment.",
    "published": "2025-07-18T04:00:58Z",
    "updated": "2026-02-05T15:12:38Z",
    "link": "http://arxiv.org/pdf/2507.16838v3.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Xinwei Cao",
      "Zijian Fan",
      "Torbjørn Svendsen",
      "Giampiero Salvi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.19755v4",
    "title": "Can MLLMs generate human-like feedback in grading multimodal short answers?",
    "summary": "In education, the traditional Automatic Short Answer Grading (ASAG) with feedback problem has focused primarily on evaluating text-only responses. However, real-world assessments often include multimodal responses containing both diagrams and text. To address this limitation, we introduce the Multimodal Short Answer Grading with Feedback (MMSAF) problem, which requires jointly evaluating textual and diagrammatic content while also providing explanatory feedback. Collecting data representative of such multimodal responses is challenging due to both scale and logistical constraints. To mitigate this, we develop an automated data generation framework that leverages LLM hallucinations to mimic common student errors, thereby constructing a dataset of 2,197 instances. We evaluate 4 Multimodal Large Language Models (MLLMs) across 3 STEM subjects, showing that MLLMs achieve accuracies of up to 62.5% in predicting answer correctness (correct/partially correct/incorrect) and up to 80.36% in assessing image relevance. This also includes a human evaluation with 9 annotators across 5 parameters, including a rubric-based approach. The rubrics also serve as a way to evaluate the feedback quality semantically rather than using overlap-based approaches. Our findings highlight which MLLMs are better suited for such tasks while also pointing out to drawbacks of the remaining MLLMs.",
    "published": "2024-12-27T17:33:39Z",
    "updated": "2026-02-05T15:02:21Z",
    "link": "http://arxiv.org/pdf/2412.19755v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Pritam Sil",
      "Pushpak Bhattacharyya",
      "Pawan Goyal",
      "Ganesh Ramakrishnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05735v1",
    "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
    "summary": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
    "published": "2026-02-05T14:59:51Z",
    "updated": "2026-02-05T14:59:51Z",
    "link": "http://arxiv.org/pdf/2602.05735v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.IT"
    ],
    "authors": [
      "Lixuan Guo",
      "Yifei Wang",
      "Tiansheng Wen",
      "Yifan Wang",
      "Aosong Feng",
      "Bo Chen",
      "Stefanie Jegelka",
      "Chenyu You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05734v1",
    "title": "Evaluating the impact of word embeddings on similarity scoring in practical information retrieval",
    "summary": "Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.\n  This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.",
    "published": "2026-02-05T14:57:38Z",
    "updated": "2026-02-05T14:57:38Z",
    "link": "http://arxiv.org/pdf/2602.05734v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Niall McCarroll",
      "Kevin Curran",
      "Eugene McNamee",
      "Angela Clist",
      "Andrew Brammer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05728v1",
    "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
    "summary": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.\n  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.\n  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
    "published": "2026-02-05T14:52:06Z",
    "updated": "2026-02-05T14:52:06Z",
    "link": "http://arxiv.org/pdf/2602.05728v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hao Yang",
      "Zhiyu Yang",
      "Xupeng Zhang",
      "Wei Wei",
      "Yunjie Zhang",
      "Lin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05723v1",
    "title": "Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification",
    "summary": "In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.",
    "published": "2026-02-05T14:49:05Z",
    "updated": "2026-02-05T14:49:05Z",
    "link": "http://arxiv.org/pdf/2602.05723v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Taoye Yin",
      "Haoyuan Hu",
      "Yaxin Fan",
      "Xinhao Chen",
      "Xinya Wu",
      "Kai Deng",
      "Kezun Zhang",
      "Feng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07760v2",
    "title": "VAO: Validation-Aligned Optimization for Cross-Task Generative Auto-Bidding",
    "summary": "Generative auto-bidding has demonstrated strong performance in online advertising, yet it often suffers from data scarcity in small-scale settings with limited advertiser participation. While cross-task data sharing is a natural remedy to mitigate this issue, naive approaches often introduce gradient bias due to distribution shifts across different tasks, and existing methods are not readily applicable to generative auto-bidding. In this paper, we propose Validation-Aligned Optimization (VAO), a principled data-sharing method that adaptively reweights cross-task data contributions based on validation performance feedback. Notably, VAO aligns training dynamics to prioritize updates that improve generalization on the target task, effectively leveraging auxiliary data and mitigating gradient bias. Building on VAO, we introduce a unified generative autobidding framework that generalizes across multiple tasks using a single model and all available task data. Extensive experiments on standard auto-bidding benchmarks validate the effectiveness of our approach.",
    "published": "2025-10-09T03:59:51Z",
    "updated": "2026-02-05T14:43:23Z",
    "link": "http://arxiv.org/pdf/2510.07760v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yiqin Lv",
      "Zhiyu Mou",
      "Miao Xu",
      "Jinghao Chen",
      "Qi Wang",
      "Yixiu Mao",
      "Yun Qu",
      "Rongquan Bai",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng",
      "Xiangyang Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05717v1",
    "title": "Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.",
    "published": "2026-02-05T14:41:57Z",
    "updated": "2026-02-05T14:41:57Z",
    "link": "http://arxiv.org/pdf/2602.05717v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tianyi Wang",
      "Long Li",
      "Hongcan Guo",
      "Yibiao Chen",
      "Yixia Li",
      "Yong Wang",
      "Yun Chen",
      "Guanhua Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05712v1",
    "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development",
    "summary": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.",
    "published": "2026-02-05T14:38:19Z",
    "updated": "2026-02-05T14:38:19Z",
    "link": "http://arxiv.org/pdf/2602.05712v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Lola Solovyeva",
      "Fernando Castor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05711v1",
    "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
    "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
    "published": "2026-02-05T14:37:32Z",
    "updated": "2026-02-05T14:37:32Z",
    "link": "http://arxiv.org/pdf/2602.05711v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jingze Shi",
      "Zhangyang Peng",
      "Yizhang Zhu",
      "Yifan Wu",
      "Guang Liu",
      "Yuyu Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05709v1",
    "title": "Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions",
    "summary": "Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.",
    "published": "2026-02-05T14:36:44Z",
    "updated": "2026-02-05T14:36:44Z",
    "link": "http://arxiv.org/pdf/2602.05709v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yihao Ouyang",
      "Shiwei Li",
      "Haozhao Wang",
      "Xiandi Luo",
      "Zhuoqi Hu",
      "Yuetong Song",
      "Qiyu Qin",
      "Yichen Li",
      "Ruixuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05706v1",
    "title": "Poster: Camera Tampering Detection for Outdoor IoT Systems",
    "summary": "Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.",
    "published": "2026-02-05T14:30:27Z",
    "updated": "2026-02-05T14:30:27Z",
    "link": "http://arxiv.org/pdf/2602.05706v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shadi Attarha",
      "Kanaga Shanmugi",
      "Anna Förster"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21843v4",
    "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition",
    "summary": "Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.",
    "published": "2025-03-27T15:21:49Z",
    "updated": "2026-02-05T14:29:31Z",
    "link": "http://arxiv.org/pdf/2503.21843v4.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ying Yu",
      "Siyao Li",
      "Yixuan Jiang",
      "Hang Xiao",
      "Jingxi Long",
      "Haotian Tang",
      "Hanyu Liu",
      "Chao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05695v1",
    "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
    "summary": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
    "published": "2026-02-05T14:21:00Z",
    "updated": "2026-02-05T14:21:00Z",
    "link": "http://arxiv.org/pdf/2602.05695v1.pdf",
    "category": [
      "cs.AI",
      "cs.PF"
    ],
    "authors": [
      "Hiari Pizzini Cavagna",
      "Andrea Proia",
      "Giacomo Madella",
      "Giovanni B. Esposito",
      "Francesco Antici",
      "Daniele Cesarini",
      "Zeynep Kiziltan",
      "Andrea Bartolini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05688v1",
    "title": "Mining Generalizable Activation Functions",
    "summary": "The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.",
    "published": "2026-02-05T14:13:40Z",
    "updated": "2026-02-05T14:13:40Z",
    "link": "http://arxiv.org/pdf/2602.05688v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alex Vitvitskyi",
      "Michael Boratko",
      "Matej Grcic",
      "Razvan Pascanu",
      "Deep Shah",
      "Petar Veličković"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05687v1",
    "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
    "summary": "Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.",
    "published": "2026-02-05T14:11:34Z",
    "updated": "2026-02-05T14:11:34Z",
    "link": "http://arxiv.org/pdf/2602.05687v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Pavithren V S Pakianathan",
      "Rania Islambouli",
      "Diogo Branco",
      "Albrecht Schmidt",
      "Tiago Guerreiro",
      "Jan David Smeddinck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21618v3",
    "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
    "summary": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To manage long-horizon interactions, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
    "published": "2025-10-24T16:24:01Z",
    "updated": "2026-02-05T14:08:05Z",
    "link": "http://arxiv.org/pdf/2510.21618v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Xiaoxi Li",
      "Wenxiang Jiao",
      "Jiarui Jin",
      "Guanting Dong",
      "Jiajie Jin",
      "Yinuo Wang",
      "Hao Wang",
      "Yutao Zhu",
      "Ji-Rong Wen",
      "Yuan Lu",
      "Zhicheng Dou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05670v1",
    "title": "HyperPotter: Spell the Charm of High-Order Interactions in Audio Deepfake Detection",
    "summary": "Advances in AIGC technologies have enabled the synthesis of highly realistic audio deepfakes capable of deceiving human auditory perception. Although numerous audio deepfake detection (ADD) methods have been developed, most rely on local temporal/spectral features or pairwise relations, overlooking high-order interactions (HOIs). HOIs capture discriminative patterns that emerge from multiple feature components beyond their individual contributions. We propose HyperPotter, a hypergraph-based framework that explicitly models these synergistic HOIs through clustering-based hyperedges with class-aware prototype initialization. Extensive experiments demonstrate that HyperPotter surpasses its baseline by an average relative gain of 22.15% across 11 datasets and outperforms state-of-the-art methods by 13.96% on 4 challenging cross-domain datasets, demonstrating superior generalization to diverse attacks and speakers.",
    "published": "2026-02-05T13:53:14Z",
    "updated": "2026-02-05T13:53:14Z",
    "link": "http://arxiv.org/pdf/2602.05670v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Qing Wen",
      "Haohao Li",
      "Zhongjie Ba",
      "Peng Cheng",
      "Miao He",
      "Li Lu",
      "Kui Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08641v3",
    "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
    "summary": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market's extremely illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal large language model (LLM) and chain-of-thought (CoT) reasoning. Our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average copier return of 3% per meme coin investment under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading.",
    "published": "2026-01-13T15:13:41Z",
    "updated": "2026-02-05T13:52:14Z",
    "link": "http://arxiv.org/pdf/2601.08641v3.pdf",
    "category": [
      "cs.AI",
      "q-fin.TR"
    ],
    "authors": [
      "Yichen Luo",
      "Yebo Feng",
      "Jiahua Xu",
      "Yang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05668v1",
    "title": "Stable but Wrong: When More Data Degrades Scientific Conclusions",
    "summary": "Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.",
    "published": "2026-02-05T13:51:47Z",
    "updated": "2026-02-05T13:51:47Z",
    "link": "http://arxiv.org/pdf/2602.05668v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhipeng Zhang",
      "Kai Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05665v1",
    "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
    "summary": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
    "published": "2026-02-05T13:49:05Z",
    "updated": "2026-02-05T13:49:05Z",
    "link": "http://arxiv.org/pdf/2602.05665v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chang Yang",
      "Chuang Zhou",
      "Yilin Xiao",
      "Su Dong",
      "Luyao Zhuang",
      "Yujing Zhang",
      "Zhu Wang",
      "Zijin Hong",
      "Zheng Yuan",
      "Zhishang Xiang",
      "Shengyuan Chen",
      "Huachi Zhou",
      "Qinggang Zhang",
      "Ninghao Liu",
      "Jinsong Su",
      "Xinrun Wang",
      "Yi Chang",
      "Xiao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05660v1",
    "title": "Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks",
    "summary": "The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism.\n  The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.",
    "published": "2026-02-05T13:43:18Z",
    "updated": "2026-02-05T13:43:18Z",
    "link": "http://arxiv.org/pdf/2602.05660v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Slawek Smyl",
      "Paweł Pełka",
      "Grzegorz Dudek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05656v1",
    "title": "Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation",
    "summary": "Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.\n  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.\n  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.\n  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.",
    "published": "2026-02-05T13:40:56Z",
    "updated": "2026-02-05T13:40:56Z",
    "link": "http://arxiv.org/pdf/2602.05656v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Igor Santos-Grueiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05650v1",
    "title": "Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances",
    "summary": "Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.",
    "published": "2026-02-05T13:35:04Z",
    "updated": "2026-02-05T13:35:04Z",
    "link": "http://arxiv.org/pdf/2602.05650v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Amir Ansari",
      "Jana Subirana",
      "Bruna Silva",
      "Sergio Escalera",
      "David Gallardo-Pujol",
      "Cristina Palmero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05636v1",
    "title": "Generative Ontology: When Structured Knowledge Learns to Create",
    "summary": "Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.\n  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional \"anxiety\" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.\n  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (\"bioluminescent fungi competing in a cave ecosystem\"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.\n  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.",
    "published": "2026-02-05T13:14:20Z",
    "updated": "2026-02-05T13:14:20Z",
    "link": "http://arxiv.org/pdf/2602.05636v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Benny Cheung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05628v1",
    "title": "AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care",
    "summary": "Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.\n  Sources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.\n  Areas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.\n  Areas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.\n  Growing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.\n  Areas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.",
    "published": "2026-02-05T13:09:19Z",
    "updated": "2026-02-05T13:09:19Z",
    "link": "http://arxiv.org/pdf/2602.05628v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Alastair Howcroft",
      "Amber Bennett-Weston",
      "Ahmad Khan",
      "Joseff Griffiths",
      "Simon Gay",
      "Jeremy Howick"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05625v1",
    "title": "Reactive Knowledge Representation and Asynchronous Reasoning",
    "summary": "Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.",
    "published": "2026-02-05T13:02:01Z",
    "updated": "2026-02-05T13:02:01Z",
    "link": "http://arxiv.org/pdf/2602.05625v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Julian Eggert",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05619v1",
    "title": "Mode-Dependent Rectification for Stable PPO Training",
    "summary": "Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.",
    "published": "2026-02-05T12:54:19Z",
    "updated": "2026-02-05T12:54:19Z",
    "link": "http://arxiv.org/pdf/2602.05619v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mohamad Mohamad",
      "Francesco Ponzio",
      "Xavier Descombes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05616v1",
    "title": "Path-Guided Flow Matching for Dataset Distillation",
    "summary": "Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \\emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\\times$ more efficient than the diffusion-based counterparts with 78\\% mode coverage.",
    "published": "2026-02-05T12:52:32Z",
    "updated": "2026-02-05T12:52:32Z",
    "link": "http://arxiv.org/pdf/2602.05616v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xuhui Li",
      "Zhengquan Luo",
      "Xiwei Liu",
      "Yongqiang Yu",
      "Zhiqiang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.03875v2",
    "title": "Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra",
    "summary": "We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.",
    "published": "2026-02-01T18:48:31Z",
    "updated": "2026-02-05T12:43:31Z",
    "link": "http://arxiv.org/pdf/2602.03875v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "authors": [
      "Stefan Kuhn",
      "Vandana Dwarka",
      "Przemyslaw Karol Grenda",
      "Eero Vainikko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10154v3",
    "title": "Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries",
    "summary": "Providing soundtracks for videos remains a costly and time-consuming challenge for multimedia content creators. We introduce EMSYNC, an automatic video-based symbolic music generator that creates music aligned with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate upcoming video scene cuts and align generated musical chords with them. We also propose a mapping scheme that bridges the discrete categorical outputs of the video emotion classifier with the continuous valence-arousal inputs required by the emotion-conditioned MIDI generator, enabling seamless integration of emotion information across different representations. Our method outperforms state-of-the-art models in objective and subjective evaluations across different video datasets, demonstrating its effectiveness in generating music aligned to video both emotionally and temporally. Our demo and output samples are available at https://serkansulun.com/emsync.",
    "published": "2025-02-14T13:32:59Z",
    "updated": "2026-02-05T12:42:33Z",
    "link": "http://arxiv.org/pdf/2502.10154v3.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.AS",
      "eess.IV"
    ],
    "authors": [
      "Serkan Sulun",
      "Paula Viana",
      "Matthew E. P. Davies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05605v1",
    "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
    "summary": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
    "published": "2026-02-05T12:42:22Z",
    "updated": "2026-02-05T12:42:22Z",
    "link": "http://arxiv.org/pdf/2602.05605v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Jiaji Zhang",
      "Hailiang Zhao",
      "Guoxuan Zhu",
      "Ruichao Sun",
      "Jiaju Wu",
      "Xinkui Zhao",
      "Hanlin Tang",
      "Weiyi Lu",
      "Kan Liu",
      "Tao Lan",
      "Lin Qu",
      "Shuiguang Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06690v2",
    "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring",
    "summary": "Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.",
    "published": "2025-09-08T13:44:55Z",
    "updated": "2026-02-05T12:38:01Z",
    "link": "http://arxiv.org/pdf/2509.06690v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Usman Haider",
      "Lukasz Szemet",
      "Daniel Kelly",
      "Vasileios Sergis",
      "Andrew C. Daly",
      "Karl Mason"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05599v1",
    "title": "BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages",
    "summary": "Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.",
    "published": "2026-02-05T12:33:30Z",
    "updated": "2026-02-05T12:33:30Z",
    "link": "http://arxiv.org/pdf/2602.05599v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Subhadip Maji",
      "Arnab Bhattacharya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05598v1",
    "title": "CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion",
    "summary": "Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.",
    "published": "2026-02-05T12:33:09Z",
    "updated": "2026-02-05T12:33:09Z",
    "link": "http://arxiv.org/pdf/2602.05598v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Aon Safdar",
      "Mohamed Saadeldin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05597v1",
    "title": "Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents",
    "summary": "Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.",
    "published": "2026-02-05T12:33:05Z",
    "updated": "2026-02-05T12:33:05Z",
    "link": "http://arxiv.org/pdf/2602.05597v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "authors": [
      "Stephen Pilli",
      "Vivek Nallur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.17835v2",
    "title": "The Use of AI-Robotic Systems for Scientific Discovery",
    "summary": "The process of developing theories and models and testing them with experiments is fundamental to the scientific method. Automating the entire scientific method then requires not only automation of the induction of theories from data, but also experimentation from design to implementation. This is the idea behind a robot scientist -- a coupled system of AI and laboratory robotics that has agency to test hypotheses with real-world experiments. In this chapter we explore some of the fundamentals of robot scientists in the philosophy of science. We also map the activities of a robot scientist to machine learning paradigms, and argue that the scientific method shares an analogy with active learning. We demonstrate these concepts using examples from previous robot scientists, and also from Genesis: a next generation robot scientist designed for research in systems biology, comprising a micro-fluidic system with 1000 computer-controlled micro-bioreactors and interpretable models based in controlled vocabularies and logic.",
    "published": "2024-06-25T15:33:01Z",
    "updated": "2026-02-05T12:29:48Z",
    "link": "http://arxiv.org/pdf/2406.17835v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alexander H. Gower",
      "Konstantin Korovin",
      "Daniel Brunnsåker",
      "Filip Kronström",
      "Gabriel K. Reder",
      "Ievgeniia A. Tiukova",
      "Ronald S. Reiserer",
      "John P. Wikswo",
      "Ross D. King"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25502v4",
    "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting",
    "summary": "Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.",
    "published": "2025-10-29T13:27:18Z",
    "updated": "2026-02-05T12:02:09Z",
    "link": "http://arxiv.org/pdf/2510.25502v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Vladyslav Moroshan",
      "Julien Siems",
      "Arber Zela",
      "Timur Carstensen",
      "Frank Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10311v2",
    "title": "ExplainReduce: Generating global explanations from many local explanations",
    "summary": "Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small \"proxy set\" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics. We show that, for many problems, as few as five explanations can faithfully emulate the closed-box model and that our reduction procedure is competitive with other model aggregation methods.",
    "published": "2025-02-14T17:14:02Z",
    "updated": "2026-02-05T11:52:54Z",
    "link": "http://arxiv.org/pdf/2502.10311v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Lauri Seppäläinen",
      "Mudong Guo",
      "Kai Puolamäki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05570v1",
    "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?",
    "summary": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.",
    "published": "2026-02-05T11:49:30Z",
    "updated": "2026-02-05T11:49:30Z",
    "link": "http://arxiv.org/pdf/2602.05570v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yikun Zong",
      "Cheston Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18001v4",
    "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
    "summary": "Sharpness-aware minimization (SAM) has emerged as a highly effective technique to improve model generalization, but its underlying principles are not fully understood. We investigate m-sharpness, where SAM performance improves monotonically as the micro-batch size for computing perturbations decreases, a phenomenon critical for distributed training yet lacking rigorous explanation. We leverage an extended Stochastic Differential Equation (SDE) framework and analyze stochastic gradient noise (SGN) to characterize the dynamics of SAM variants, including n-SAM and m-SAM. Our analysis reveals that stochastic perturbations induce an implicit variance-based sharpness regularization whose strength increases as m decreases. Motivated by this insight, we propose Reweighted SAM (RW-SAM), which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate our theory and method.",
    "published": "2025-09-22T16:40:42Z",
    "updated": "2026-02-05T11:48:23Z",
    "link": "http://arxiv.org/pdf/2509.18001v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haocheng Luo",
      "Mehrtash Harandi",
      "Dinh Phung",
      "Trung Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.22031v2",
    "title": "Differentiable Constraint-Based Causal Discovery",
    "summary": "Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code and data of the proposed method are publicly available at https://github$.$com/PurdueMINDS/DAGPA.",
    "published": "2025-10-24T21:28:39Z",
    "updated": "2026-02-05T11:26:17Z",
    "link": "http://arxiv.org/pdf/2510.22031v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jincheng Zhou",
      "Mengbo Wang",
      "Anqi He",
      "Yumeng Zhou",
      "Hessam Olya",
      "Murat Kocaoglu",
      "Bruno Ribeiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05548v1",
    "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
    "published": "2026-02-05T11:07:14Z",
    "updated": "2026-02-05T11:07:14Z",
    "link": "http://arxiv.org/pdf/2602.05548v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhiqi Yu",
      "Zhangquan Chen",
      "Mengting Liu",
      "Heye Zhang",
      "Liangqiong Qu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05547v1",
    "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks",
    "summary": "RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.",
    "published": "2026-02-05T11:06:37Z",
    "updated": "2026-02-05T11:06:37Z",
    "link": "http://arxiv.org/pdf/2602.05547v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shyam Sundhar Ramesh",
      "Xiaotong Ji",
      "Matthieu Zimmer",
      "Sangwoong Yoon",
      "Zhiyong Wang",
      "Haitham Bou Ammar",
      "Aurelien Lucchi",
      "Ilija Bogunovic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05544v1",
    "title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation",
    "summary": "Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\\% in Sports and 4.59\\% in Toys, along with ROUGE-L by 8.02\\% and 3.49\\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\\% in cold-start and 11.9\\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\\% in Beauty and 23.16\\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.",
    "published": "2026-02-05T11:05:09Z",
    "updated": "2026-02-05T11:05:09Z",
    "link": "http://arxiv.org/pdf/2602.05544v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Fahad Anwaar",
      "Adil Mehmood Khan",
      "Muhammad Khalid",
      "Usman Zia",
      "Kezhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05539v1",
    "title": "Steering Large Reasoning Models towards Concise Reasoning via Flow Matching",
    "summary": "Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.",
    "published": "2026-02-05T10:56:13Z",
    "updated": "2026-02-05T10:56:13Z",
    "link": "http://arxiv.org/pdf/2602.05539v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yawei Li",
      "Benjamin Bergner",
      "Yinghan Zhao",
      "Vihang Prakash Patil",
      "Bei Chen",
      "Cheng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05536v1",
    "title": "When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging",
    "summary": "Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.",
    "published": "2026-02-05T10:52:36Z",
    "updated": "2026-02-05T10:52:36Z",
    "link": "http://arxiv.org/pdf/2602.05536v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Yayuan Li",
      "Ze Peng",
      "Jian Zhang",
      "Jintao Guo",
      "Yue Duan",
      "Yinghuan Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05533v1",
    "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
    "summary": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.",
    "published": "2026-02-05T10:46:20Z",
    "updated": "2026-02-05T10:46:20Z",
    "link": "http://arxiv.org/pdf/2602.05533v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhengyi Guo",
      "Wenpin Tang",
      "Renyuan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05532v1",
    "title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
    "summary": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.",
    "published": "2026-02-05T10:45:48Z",
    "updated": "2026-02-05T10:45:48Z",
    "link": "http://arxiv.org/pdf/2602.05532v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Florian Dietz",
      "William Wale",
      "Oscar Gilg",
      "Robert McCarthy",
      "Felix Michalak",
      "Gustavo Ewbank Rodrigues Danon",
      "Miguelito de Guzman",
      "Dietrich Klakow"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02613v2",
    "title": "Exploring Silicon-Based Societies: An Early Study of the Moltbook Agent Community",
    "summary": "The rapid emergence of autonomous large language model agents has given rise to persistent, large-scale agent ecosystems whose collective behavior cannot be adequately understood through anecdotal observation or small-scale simulation. This paper introduces data-driven silicon sociology as a systematic empirical framework for studying social structure formation among interacting artificial agents. We present a pioneering large-scale data mining investigation of an in-the-wild agent society by analyzing Moltbook, a social platform designed primarily for agent-to-agent interaction. At the time of study, Moltbook hosted over 150,000 registered autonomous agents operating across thousands of agent-created sub-communities. Using programmatic and non-intrusive data acquisition, we collected and analyzed the textual descriptions of 12,758 submolts, which represent proactive sub-community partitioning activities within the ecosystem. Treating agent-authored descriptions as first-class observational artifacts, we apply rigorous preprocessing, contextual embedding, and unsupervised clustering techniques to uncover latent patterns of thematic organization and social space structuring. The results show that autonomous agents systematically organize collective space through reproducible patterns spanning human-mimetic interests, silicon-centric self-reflection, and early-stage economic and coordination behaviors. Rather than relying on predefined sociological taxonomies, these structures emerge directly from machine-generated data traces. This work establishes a methodological foundation for data-driven silicon sociology and demonstrates that data mining techniques can provide a powerful lens for understanding the organization and evolution of large autonomous agent societies.",
    "published": "2026-02-02T11:08:38Z",
    "updated": "2026-02-05T10:35:14Z",
    "link": "http://arxiv.org/pdf/2602.02613v2.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Yu-Zheng Lin",
      "Bono Po-Jen Shih",
      "Hsuan-Ying Alessandra Chien",
      "Shalaka Satam",
      "Jesus Horacio Pacheco",
      "Sicong Shao",
      "Soheil Salehi",
      "Pratik Satam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05524v1",
    "title": "AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval",
    "summary": "This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.",
    "published": "2026-02-05T10:35:00Z",
    "updated": "2026-02-05T10:35:00Z",
    "link": "http://arxiv.org/pdf/2602.05524v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Konosuke Yoshizato",
      "Kazuma Shimizu",
      "Ryota Higa",
      "Takanobu Otsuka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05523v1",
    "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations",
    "summary": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.",
    "published": "2026-02-05T10:30:57Z",
    "updated": "2026-02-05T10:30:57Z",
    "link": "http://arxiv.org/pdf/2602.05523v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Shahin Honarvar",
      "Amber Gorzynski",
      "James Lee-Jones",
      "Harry Coppock",
      "Marek Rei",
      "Joseph Ryan",
      "Alastair F. Donaldson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.00918v3",
    "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
    "summary": "Mixture of experts (MoE) architectures have become a cornerstone for scaling up and are a key component in most large language models such as GPT-OSS, DeepSeek-V3, Llama-4, and Gemini-2.5. However, systematic research on MoE remains severely constrained by the prohibitive computational costs of training and evaluation, restricting large-scale studies accessible to most researchers. We introduce LibMoE, a unified framework for reproducible, efficient, and extensible MoE research that supports both pretraining and sparse-upcycling regimes. Beyond unified implementations, the framework provides transparent analytical tools for probing routing and expert dynamics. Leveraging this foundation, we conduct a comprehensive analysis along three dimensions: (i) routing dynamics, covering expert selection patterns, routing stability and optimality, and how routing entropy reveals task specialization and expert diversity; (ii) the effect of lightweight initialization on load balancing, demonstrating how subtle changes in router initialization shape early expert utilization; and (iii) training regime differences, revealing how sparse upcycling and full pretraining exhibit distinct routing patterns and stability profiles. By lowering the barrier to entry and standardizing evaluation, along with our comprehensive analysis, LibMoE broadens access to MoE research and establishes a reliable benchmark to guide future innovations. GitHub: \\href{https://github.com/Fsoft-AIC/LibMoE}{https://github.com/Fsoft-AIC/LibMoE}.",
    "published": "2024-11-01T14:04:36Z",
    "updated": "2026-02-05T10:16:56Z",
    "link": "http://arxiv.org/pdf/2411.00918v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Nam V. Nguyen",
      "Thong T. Doan",
      "Luong Tran",
      "Van Nguyen",
      "Quang Pham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05515v1",
    "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
    "summary": "Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.",
    "published": "2026-02-05T10:15:34Z",
    "updated": "2026-02-05T10:15:34Z",
    "link": "http://arxiv.org/pdf/2602.05515v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Ajo Babu George",
      "Anna Mariam John",
      "Athul Anoop",
      "Balu Bhasuran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05513v1",
    "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter",
    "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.",
    "published": "2026-02-05T10:13:34Z",
    "updated": "2026-02-05T10:13:34Z",
    "link": "http://arxiv.org/pdf/2602.05513v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Xukun Li",
      "Yu Sun",
      "Lei Zhang",
      "Bosheng Huang",
      "Yibo Peng",
      "Yuan Meng",
      "Haojun Jiang",
      "Shaoxuan Xie",
      "Guacai Yao",
      "Alois Knoll",
      "Zhenshan Bing",
      "Xinlong Wang",
      "Zhenguo Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01028v3",
    "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning",
    "summary": "The {\\em stop gradient} and {\\em exponential moving average} iterative procedures are commonly used in non-contrastive approaches to self-supervised learning to avoid representation collapse, with excellent performance in downstream applications in practice. This presentation investigates these procedures from the dual viewpoints of optimization and dynamical systems. We show that, in general, although they {\\em do not} optimize the original objective, or {\\em any} other smooth function, they {\\em do} avoid collapse Following~\\citet{Tian21}, but without any of the extra assumptions used in their proofs, we then show using a dynamical system perspective that, in the linear case, minimizing the original objective function without the use of a stop gradient or exponential moving average {\\em always} leads to collapse. Conversely, we characterize explicitly the equilibria of the dynamical systems associated with these two procedures in this linear setting as algebraic varieties in their parameter space, and show that they are, in general, {\\em asymptotically stable}. Our theoretical findings are illustrated by empirical experiments with real and synthetic data.",
    "published": "2025-06-18T07:46:51Z",
    "updated": "2026-02-05T10:08:11Z",
    "link": "http://arxiv.org/pdf/2507.01028v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jean Ponce",
      "Basile Terver",
      "Martial Hebert",
      "Michael Arbel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05499v1",
    "title": "SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration",
    "summary": "Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.",
    "published": "2026-02-05T10:02:00Z",
    "updated": "2026-02-05T10:02:00Z",
    "link": "http://arxiv.org/pdf/2602.05499v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hanyu Wei",
      "Zunhai Su",
      "Peng Lu",
      "Chao Li",
      "Spandan Tiwari",
      "Ashish Sirasao",
      "Yuhan Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05496v1",
    "title": "XEmoGPT: An Explainable Multimodal Emotion Recognition Framework with Cue-Level Perception and Reasoning",
    "summary": "Explainable Multimodal Emotion Recognition plays a crucial role in applications such as human-computer interaction and social media analytics. However, current approaches struggle with cue-level perception and reasoning due to two main challenges: 1) general-purpose modality encoders are pretrained to capture global structures and general semantics rather than fine-grained emotional cues, resulting in limited sensitivity to emotional signals; and 2) available datasets usually involve a trade-off between annotation quality and scale, which leads to insufficient supervision for emotional cues and ultimately limits cue-level reasoning. Moreover, existing evaluation metrics are inadequate for assessing cue-level reasoning performance. To address these challenges, we propose eXplainable Emotion GPT (XEmoGPT), a novel EMER framework capable of both perceiving and reasoning over emotional cues. It incorporates two specialized modules: the Video Emotional Cue Bridge (VECB) and the Audio Emotional Cue Bridge (AECB), which enhance the video and audio encoders through carefully designed tasks for fine-grained emotional cue perception. To further support cue-level reasoning, we construct a large-scale dataset, EmoCue, designed to teach XEmoGPT how to reason over multimodal emotional cues. In addition, we introduce EmoCue-360, an automated metric that extracts and matches emotional cues using semantic similarity, and release EmoCue-Eval, a benchmark of 400 expert-annotated samples covering diverse emotional scenarios. Experimental results show that XEmoGPT achieves strong performance in both emotional cue perception and reasoning.",
    "published": "2026-02-05T09:58:41Z",
    "updated": "2026-02-05T09:58:41Z",
    "link": "http://arxiv.org/pdf/2602.05496v1.pdf",
    "category": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Hanwen Zhang",
      "Yao Liu",
      "Peiyuan Jiang",
      "Lang Junjie",
      "Xie Jun",
      "Yihui He",
      "Yajiao Deng",
      "Siyu Du",
      "Qiao Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05495v1",
    "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models",
    "summary": "Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.",
    "published": "2026-02-05T09:57:57Z",
    "updated": "2026-02-05T09:57:57Z",
    "link": "http://arxiv.org/pdf/2602.05495v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chenhang Cui",
      "Binyun Yang",
      "Fei Shen",
      "Yuxin Chen",
      "Jingnan Zheng",
      "Xiang Wang",
      "An Zhang",
      "Tat-Seng Chua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05494v1",
    "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "summary": "Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.",
    "published": "2026-02-05T09:56:16Z",
    "updated": "2026-02-05T09:56:16Z",
    "link": "http://arxiv.org/pdf/2602.05494v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qingyuan Wu",
      "Yuhui Wang",
      "Simon Sinong Zhan",
      "Yanning Dai",
      "Shilong Deng",
      "Sarra Habchi",
      "Qi Zhu",
      "Matthias Gallé",
      "Chao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05493v1",
    "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation",
    "summary": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.",
    "published": "2026-02-05T09:55:19Z",
    "updated": "2026-02-05T09:55:19Z",
    "link": "http://arxiv.org/pdf/2602.05493v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Bingru Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.00151v2",
    "title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency",
    "summary": "Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.",
    "published": "2026-01-29T14:06:50Z",
    "updated": "2026-02-05T09:54:33Z",
    "link": "http://arxiv.org/pdf/2602.00151v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Alexander Blezinger",
      "Wolfgang Nejdl",
      "Ming Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.21086v2",
    "title": "Are foundation models useful feature extractors for electroencephalography analysis?",
    "summary": "The success of foundation models in natural language processing and computer vision has motivated similar approaches in time series analysis. While foundational time series models have proven beneficial on a variety of tasks, their effectiveness in medical applications with limited data remains underexplored. In this work, we investigate this question in the context of electroencephalography (EEG) by evaluating general-purpose time series models on age prediction, seizure detection, and classification of clinically relevant EEG events. We compare their diagnostic performance against specialised EEG models and assess the quality of the extracted features. The results show that general-purpose models are competitive and capture features useful to localising demographic and disease-related biomarkers. These findings indicate that foundational time series models can reduce the reliance on large task-specific datasets and models, making them valuable in clinical practice.",
    "published": "2025-02-28T14:21:34Z",
    "updated": "2026-02-05T09:50:03Z",
    "link": "http://arxiv.org/pdf/2502.21086v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Özgün Turgut",
      "Felix S. Bott",
      "Markus Ploner",
      "Daniel Rueckert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05486v1",
    "title": "Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems",
    "summary": "Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.",
    "published": "2026-02-05T09:49:04Z",
    "updated": "2026-02-05T09:49:04Z",
    "link": "http://arxiv.org/pdf/2602.05486v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "authors": [
      "Matteo Esposito",
      "Lodovica Marchesi",
      "Roberto Tonelli",
      "Valentina Lenarduzzi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05479v1",
    "title": "Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction",
    "summary": "Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.",
    "published": "2026-02-05T09:39:22Z",
    "updated": "2026-02-05T09:39:22Z",
    "link": "http://arxiv.org/pdf/2602.05479v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhe Wang",
      "Zijing Liu",
      "Chencheng Xu",
      "Yuan Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.17314v2",
    "title": "Auto-Rubric: Learning From Implicit Weights to Explicit Rubrics for Reward Modeling",
    "summary": "Conventional reward modeling relies on gradient descent over neural weights, creating opaque, data-hungry \"black boxes.\" We propose a paradigm shift from implicit to explicit reward parameterization, recasting optimization from continuous weight spaces to the discrete space of natural language rubrics. We introduce a training-free framework based on iterative rubric learning: it locally induces discriminative criteria via verification-driven refinement, and globally compresses the candidate criteria pool into a compact core set by maximizing an information-theoretic coding rate objective. We organize the compressed core set into a hierarchical rubric structure -- high-level evaluation dimensions supported by concrete verification checks -- serving as an interpretable, portable reward function. Empirically, our approach challenges prevailing data scaling assumptions: using only 70 preference pairs, our rubric-guided judges outperform fully trained reward models on diverse benchmarks. For instance, Qwen3-8B equipped with our learned rubrics achieves 80.91% on RewardBench2, surpassing the specialized Skywork-Reward-V2-Qwen3-8B (78.20%). These results demonstrate that alignment signals are highly compressible and can be effectively captured through explicit symbolic search.",
    "published": "2025-10-20T09:01:37Z",
    "updated": "2026-02-05T09:36:28Z",
    "link": "http://arxiv.org/pdf/2510.17314v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Lipeng Xie",
      "Sen Huang",
      "Zhuo Zhang",
      "Anni Zou",
      "Yunpeng Zhai",
      "Dingchao Ren",
      "Kezun Zhang",
      "Haoyuan Hu",
      "Boyin Liu",
      "Haoran Chen",
      "Zhaoyang Liu",
      "Bolin Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22352v2",
    "title": "SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival Analysis",
    "summary": "Survival analysis is a cornerstone of clinical research by modeling time-to-event outcomes such as metastasis, disease relapse, or patient death. Unlike standard tabular data, survival data often come with incomplete event information due to dropout, or loss to follow-up. This poses unique challenges for synthetic data generation, where it is crucial for clinical research to faithfully reproduce both the event-time distribution and the censoring mechanism. In this paper, we propose SurvDiff an end-to-end diffusion model specifically designed for generating synthetic data in survival analysis. SurvDiff is tailored to capture the data-generating mechanism by jointly generating mixed-type covariates, event times, and right-censoring, guided by a survival-tailored loss function. The loss encodes the time-to-event structure and directly optimizes for downstream survival tasks, which ensures that SurvDiff (i) reproduces realistic event-time distributions and (ii preserves the censoring mechanism. Across multiple datasets, we show that SurvDiff consistently outperforms state-of-the-art generative baselines in both distributional fidelity and survival model evaluation metrics across multiple medical datasets. To the best of our knowledge, SurvDiff is the first end-to-end diffusion model explicitly designed for generating synthetic survival data.",
    "published": "2025-09-26T13:50:29Z",
    "updated": "2026-02-05T09:25:26Z",
    "link": "http://arxiv.org/pdf/2509.22352v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Marie Brockschmidt",
      "Maresa Schröder",
      "Stefan Feuerriegel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21996v3",
    "title": "AlphaBeta is not as good as you think: a simple class of synthetic games for a better analysis of deterministic game-solving algorithms",
    "summary": "Deterministic game-solving algorithms are conventionally analyzed in the light of their average-case complexity against a distribution of random game-trees, where leaf values are independently sampled from a fixed distribution. This simplified model enables uncluttered mathematical analysis, revealing two key properties: root value distributions asymptotically collapse to a single fixed value for finite-valued trees, and all reasonable algorithms achieve global optimality. However, these findings are artifacts of the model's design: its long criticized independence assumption strips games of structural complexity, producing trivial instances where no algorithm faces meaningful challenges. To address this limitation, we introduce a class of synthetic games generated by a probabilistic model that incrementally constructs game-trees using a fixed level-wise conditional distribution. By enforcing ancestor dependencies, a critical structural feature of real-world games, our framework generates problems with adjustable difficulty while retaining some form of analytical tractability. For several algorithms, including AlphaBeta and Scout, we derive recursive formulas characterizing their average-case complexities under this model. These allow us to rigorously compare algorithms on deep game-trees, where Monte-Carlo simulations are no longer feasible. While asymptotically, all algorithms seem to converge to identical branching factor (a result analogous to that of independence-based models), deep finite trees reveal stark differences: AlphaBeta incurs a significantly larger constant multiplicative factor compared to algorithms like Scout, leading to a substantial practical slowdown. Our framework sheds new light on classical game-solving algorithms, offering rigorous evidence and analytical tools to advance the understanding of these methods under a richer, more challenging, and yet tractable model.",
    "published": "2025-06-27T08:07:17Z",
    "updated": "2026-02-05T09:24:38Z",
    "link": "http://arxiv.org/pdf/2506.21996v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Raphaël Boige",
      "Amine Boumaza",
      "Bruno Scherrer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.21043v2",
    "title": "Log2Motion: Biomechanical Motion Synthesis from Touch Logs",
    "summary": "Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.",
    "published": "2026-01-28T21:04:19Z",
    "updated": "2026-02-05T09:24:34Z",
    "link": "http://arxiv.org/pdf/2601.21043v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Michał Patryk Miazga",
      "Hannah Bussmann",
      "Antti Oulasvirta",
      "Patrick Ebel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05474v1",
    "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation",
    "summary": "Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\\% performance improvement.",
    "published": "2026-02-05T09:22:17Z",
    "updated": "2026-02-05T09:22:17Z",
    "link": "http://arxiv.org/pdf/2602.05474v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Yicheng Di",
      "Zhanjie Zhang",
      "Yun Wangc",
      "Jinren Liue",
      "Jiaqi Yanf",
      "Jiyu Wei",
      "Xiangyu Chend",
      "Yuan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05472v1",
    "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
    "summary": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
    "published": "2026-02-05T09:20:23Z",
    "updated": "2026-02-05T09:20:23Z",
    "link": "http://arxiv.org/pdf/2602.05472v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yiwen Duan",
      "Jing Ye",
      "Xinpei Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05464v1",
    "title": "Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning",
    "summary": "Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.",
    "published": "2026-02-05T09:14:44Z",
    "updated": "2026-02-05T09:14:44Z",
    "link": "http://arxiv.org/pdf/2602.05464v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jiaquan Wang",
      "Yan Lyu",
      "Chen Li",
      "Yuheng Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05463v1",
    "title": "Thermodynamic Limits of Physical Intelligence",
    "summary": "Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.",
    "published": "2026-02-05T09:12:43Z",
    "updated": "2026-02-05T09:12:43Z",
    "link": "http://arxiv.org/pdf/2602.05463v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "authors": [
      "Koichi Takahashi",
      "Yusuke Hayashi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.09361v2",
    "title": "GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.",
    "published": "2026-01-14T10:41:34Z",
    "updated": "2026-02-05T09:12:37Z",
    "link": "http://arxiv.org/pdf/2601.09361v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiaying Zhang",
      "Lei Shi",
      "Jiguo Li",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.15798v4",
    "title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
    "summary": "Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization",
    "published": "2025-02-18T20:10:34Z",
    "updated": "2026-02-05T09:10:42Z",
    "link": "http://arxiv.org/pdf/2502.15798v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Yuxuan Zhou",
      "Heng Li",
      "Zhi-Qi Cheng",
      "Xudong Yan",
      "Yifei Dong",
      "Mario Fritz",
      "Margret Keuper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.18713v3",
    "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
    "summary": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\\tilde{O}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $Ω\\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.",
    "published": "2025-10-21T15:11:01Z",
    "updated": "2026-02-05T09:06:17Z",
    "link": "http://arxiv.org/pdf/2510.18713v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Joongkyu Lee",
      "Seouh-won Yi",
      "Min-hwan Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05456v1",
    "title": "Ontology-Driven Robotic Specification Synthesis",
    "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.",
    "published": "2026-02-05T08:59:23Z",
    "updated": "2026-02-05T08:59:23Z",
    "link": "http://arxiv.org/pdf/2602.05456v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "authors": [
      "Maksym Figat",
      "Ryan M. Mackey",
      "Michel D. Ingham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05454v1",
    "title": "Attention Retention for Continual Learning with Vision Transformers",
    "summary": "Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.",
    "published": "2026-02-05T08:55:58Z",
    "updated": "2026-02-05T08:55:58Z",
    "link": "http://arxiv.org/pdf/2602.05454v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yue Lu",
      "Xiangyu Zhou",
      "Shizhou Zhang",
      "Yinghui Xing",
      "Guoqiang Liang",
      "Wencong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05453v1",
    "title": "Towards Segmenting the Invisible: An End-to-End Registration and Segmentation Framework for Weakly Supervised Tumour Analysis",
    "summary": "Liver tumour ablation presents a significant clinical challenge: whilst tumours are clearly visible on pre-operative MRI, they are often effectively invisible on intra-operative CT due to minimal contrast between pathological and healthy tissue. This work investigates the feasibility of cross-modality weak supervision for scenarios where pathology is visible in one modality (MRI) but absent in another (CT). We present a hybrid registration-segmentation framework that combines MSCGUNet for inter-modal image registration with a UNet-based segmentation module, enabling registration-assisted pseudo-label generation for CT images. Our evaluation on the CHAOS dataset demonstrates that the pipeline can successfully register and segment healthy liver anatomy, achieving a Dice score of 0.72. However, when applied to clinical data containing tumours, performance degrades substantially (Dice score of 0.16), revealing the fundamental limitations of current registration methods when the target pathology lacks corresponding visual features in the target modality. We analyse the \"domain gap\" and \"feature absence\" problems, demonstrating that whilst spatial propagation of labels via registration is feasible for visible structures, segmenting truly invisible pathology remains an open challenge. Our findings highlight that registration-based label transfer cannot compensate for the absence of discriminative features in the target modality, providing important insights for future research in cross-modality medical image analysis. Code an weights are available at: https://github.com/BudhaTronix/Weakly-Supervised-Tumour-Detection",
    "published": "2026-02-05T08:55:26Z",
    "updated": "2026-02-05T08:55:26Z",
    "link": "http://arxiv.org/pdf/2602.05453v1.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "physics.med-ph"
    ],
    "authors": [
      "Budhaditya Mukhopadhyay",
      "Chirag Mandal",
      "Pavan Tummala",
      "Naghmeh Mahmoodian",
      "Andreas Nürnberger",
      "Soumick Chatterjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05449v1",
    "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
    "summary": "While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.",
    "published": "2026-02-05T08:45:08Z",
    "updated": "2026-02-05T08:45:08Z",
    "link": "http://arxiv.org/pdf/2602.05449v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chang Zou",
      "Changlin Li",
      "Yang Li",
      "Patrol Li",
      "Jianbing Wu",
      "Xiao He",
      "Songtao Liu",
      "Zhao Zhong",
      "Kailin Huang",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02345v3",
    "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
    "summary": "Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs. Code is available at https://github.com/szdtzpj/Breaking_the_moe_trilemma",
    "published": "2025-09-27T10:45:58Z",
    "updated": "2026-02-05T08:40:18Z",
    "link": "http://arxiv.org/pdf/2510.02345v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Peijun Zhu",
      "Ning Yang",
      "Baoliang Tian",
      "Jiayu Wei",
      "Weihao Zhang",
      "Haijun Zhang",
      "Pin Lv"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05447v1",
    "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
    "summary": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.\n  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.\n  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.",
    "published": "2026-02-05T08:39:05Z",
    "updated": "2026-02-05T08:39:05Z",
    "link": "http://arxiv.org/pdf/2602.05447v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Damon McMillan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19391v3",
    "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation",
    "summary": "Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.",
    "published": "2025-09-22T17:15:23Z",
    "updated": "2026-02-05T08:34:09Z",
    "link": "http://arxiv.org/pdf/2509.19391v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Axel Marmoret",
      "Reda Bensaid",
      "Jonathan Lys",
      "Vincent Gripon",
      "François Leduc-Primeau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05441v1",
    "title": "Benchmarking Affordance Generalization with BusyBox",
    "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.\n  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.",
    "published": "2026-02-05T08:31:27Z",
    "updated": "2026-02-05T08:31:27Z",
    "link": "http://arxiv.org/pdf/2602.05441v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Dean Fortier",
      "Timothy Adamson",
      "Tess Hellebrekers",
      "Teresa LaScala",
      "Kofi Ennin",
      "Michael Murray",
      "Andrey Kolobov",
      "Galen Mullins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15075v2",
    "title": "The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution",
    "summary": "Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining \\textbf{the reason behind agent behaviors}. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems. Codes are available at https://github.com/AI45Lab/AgentDoG.",
    "published": "2026-01-21T15:22:21Z",
    "updated": "2026-02-05T08:31:19Z",
    "link": "http://arxiv.org/pdf/2601.15075v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Chen Qian",
      "Peng Wang",
      "Dongrui Liu",
      "Junyao Yang",
      "Dadi Guo",
      "Ling Tang",
      "Jilin Mei",
      "Qihan Ren",
      "Shuai Shao",
      "Yong Liu",
      "Jie Fu",
      "Jing Shao",
      "Xia Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.04756v3",
    "title": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering",
    "summary": "Personalization has become crucial for adapting models to the diverse and evolving needs of users across cultural, temporal, and contextual dimensions. While existing methods often rely on centralized fine-tuning or static preference alignment within a single model, they struggle to achieve both real-time and high-quality personalization under the resource and privacy constraints of personal devices. To address this challenge, we propose CoSteer, a collaborative framework that enables tuning-free, real-time personalization via decoding-time adaptation. By leveraging logit differences between context-aware and context-agnostic local small models, CoSteer steers cloud-based large models, ensuring effective personalization while preserving the large model's capabilities. Personalization is handled locally, with only final tokens sent to the cloud, maintaining both user context and system efficiency. Through extensive experiments across a wide range of tasks, we demonstrate that CoSteer generates high-quality personalized content, ensuring both effectiveness and computational efficiency. Our results highlight its robustness across models and environments, confirming its practical applicability in real-world scenarios.",
    "published": "2025-07-07T08:32:29Z",
    "updated": "2026-02-05T08:26:38Z",
    "link": "http://arxiv.org/pdf/2507.04756v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hang Lv",
      "Sheng Liang",
      "Hao Wang",
      "Hongchao Gu",
      "Yaxiong Wu",
      "Wei Guo",
      "Defu Lian",
      "Yong Liu",
      "Enhong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05430v1",
    "title": "Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy",
    "summary": "Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.",
    "published": "2026-02-05T08:20:50Z",
    "updated": "2026-02-05T08:20:50Z",
    "link": "http://arxiv.org/pdf/2602.05430v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kritchanat Ponyuenyong",
      "Pengyu Tu",
      "Jia Wei Tan",
      "Wei Soon Cheong",
      "Jamie Ng Suat Ling",
      "Lianlian Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05429v1",
    "title": "M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining",
    "summary": "Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.",
    "published": "2026-02-05T08:19:39Z",
    "updated": "2026-02-05T08:19:39Z",
    "link": "http://arxiv.org/pdf/2602.05429v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Rui Lv",
      "Juncheng Mo",
      "Tianyi Chu",
      "Chen Rao",
      "Hongyi Jing",
      "Jiajie Teng",
      "Jiafu Chen",
      "Shiqi Zhang",
      "Liangzi Ding",
      "Shuo Fang",
      "Huaizhong Lin",
      "Ziqiang Dang",
      "Chenguang Ma",
      "Lei Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14665v5",
    "title": "Accurate and scalable exchange-correlation with deep learning",
    "summary": "Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.",
    "published": "2025-06-17T15:56:56Z",
    "updated": "2026-02-05T08:18:03Z",
    "link": "http://arxiv.org/pdf/2506.14665v5.pdf",
    "category": [
      "physics.chem-ph",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Giulia Luise",
      "Chin-Wei Huang",
      "Thijs Vogels",
      "Derk P. Kooi",
      "Sebastian Ehlert",
      "Stephanie Lanius",
      "Klaas J. H. Giesbertz",
      "Amir Karton",
      "Deniz Gunceler",
      "Megan Stanley",
      "Wessel P. Bruinsma",
      "Lin Huang",
      "Xinran Wei",
      "José Garrido Torres",
      "Abylay Katbashev",
      "Rodrigo Chavez Zavaleta",
      "Bálint Máté",
      "Sékou-Oumar Kaba",
      "Roberto Sordillo",
      "Yingrong Chen",
      "David B. Williams-Young",
      "Christopher M. Bishop",
      "Jan Hermann",
      "Rianne van den Berg",
      "Paola Gori-Giorgi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05424v1",
    "title": "THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs",
    "summary": "Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.",
    "published": "2026-02-05T08:15:49Z",
    "updated": "2026-02-05T08:15:49Z",
    "link": "http://arxiv.org/pdf/2602.05424v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Weijian Yu",
      "Yuhuan Lu",
      "Dingqi Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05420v1",
    "title": "Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring",
    "summary": "Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the \"divide and conquer\" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, \"Explicit Marking\" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a \"conflict set.\" Second, \"Implicit Disambiguation\" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.",
    "published": "2026-02-05T08:05:48Z",
    "updated": "2026-02-05T08:05:48Z",
    "link": "http://arxiv.org/pdf/2602.05420v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Rui Sun",
      "Yiwen Yang",
      "Kaiyu Guo",
      "Chen Jiang",
      "Dongli Xu",
      "Zhaonan Liu",
      "Tan Pan",
      "Limei Han",
      "Xue Jiang",
      "Wu Wei",
      "Yuan Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17208v3",
    "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems",
    "summary": "The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.",
    "published": "2025-06-20T17:57:08Z",
    "updated": "2026-02-05T08:04:43Z",
    "link": "http://arxiv.org/pdf/2506.17208v3.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Matias Martinez",
      "Xavier Franch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09425v2",
    "title": "Bandits with Single-Peaked Preferences and Limited Resources",
    "summary": "We study an online stochastic matching problem in which an algorithm sequentially matches $U$ users to $K$ arms, aiming to maximize cumulative reward over $T$ rounds under budget constraints. Without structural assumptions, computing the optimal matching is NP-hard, making online learning computationally infeasible. To overcome this barrier, we focus on single-peaked preferences -- a well-established structure in social choice theory, where users' preferences are unimodal with respect to a common order over arms. We devise an efficient algorithm for the offline budgeted matching problem, and leverage it into an efficient online algorithm with a regret of $\\tilde O(UKT^{2/3})$. Our approach relies on a novel PQ tree-based order approximation method. If the single-peaked structure is known, we develop an efficient UCB-like algorithm that achieves a regret bound of $\\tilde O(U\\sqrt{TK})$.",
    "published": "2025-10-10T14:27:25Z",
    "updated": "2026-02-05T08:04:40Z",
    "link": "http://arxiv.org/pdf/2510.09425v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Omer Ben-Porat",
      "Gur Keinan",
      "Rotem Torkan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04349v6",
    "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy",
    "summary": "Reinforcement Learning (RL) is pivotal for enhancing Large Language Model (LLM) reasoning, yet mainstream algorithms such as GRPO and DAPO remain constrained by a coarse-grained credit assignment paradigm, where all tokens within the same response receive the identical reward. In this paper, we propose Dynamic Entropy Weighting, systematically define entropy-based weight ratios $\\frac{H_{i,t}}{\\sum_{k=1}^{n} H_{k,t}}$ and similar variants to redistribute rewards and get fine-grained rewards through two new algorithms: Group Token Policy Optimization (GTPO), which assigns an entropy-weighted reward to each token and synthesizes token-specific advantage function to drive the model toward optimal path, and the analogous algorithm Sequence-Level GRPO (GRPO-S), which extends this design to the sequence level and exhibits superior stability in long Chain-of-Thought (CoT) reasoning tasks.",
    "published": "2025-08-06T11:42:47Z",
    "updated": "2026-02-05T08:04:18Z",
    "link": "http://arxiv.org/pdf/2508.04349v6.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hongze Tan",
      "Zihan Wang",
      "Jianfei Pan",
      "Jinghao Lin",
      "Hao Wang",
      "Yifan Wu",
      "Tao Chen",
      "Zhihang Zheng",
      "Zhihao Tang",
      "Haihua Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05416v1",
    "title": "Reduced-Order Surrogates for Forced Flexible Mesh Coastal-Ocean Models",
    "summary": "While POD-based surrogates are widely explored for hydrodynamic applications, the use of Koopman Autoencoders for real-world coastal-ocean modelling remains relatively limited. This paper introduces a flexible Koopman autoencoder formulation that incorporates meteorological forcings and boundary conditions, and systematically compares its performance against POD-based surrogates. The Koopman autoencoder employs a learned linear temporal operator in latent space, enabling eigenvalue regularization to promote temporal stability. This strategy is evaluated alongside temporal unrolling techniques for achieving stable and accurate long-term predictions. The models are assessed on three test cases spanning distinct dynamical regimes, with prediction horizons up to one year at 30-minute temporal resolution. Across all cases, the Koopman autoencoder with temporal unrolling yields the best overall accuracy compared to the POD-based surrogates, achieving relative root-mean-squared-errors of 0.01-0.13 and $R^2$-values of 0.65-0.996. Prediction errors are largest for current velocities, and smallest for water surface elevations. Comparing to in-situ observations, the surrogate yields -0.65% to 12% change in water surface elevation prediction error when compared to prediction errors of the physics-based model. These error levels, corresponding to a few centimeters, are acceptable for many practical applications, while inference speed-ups of 300-1400x enables workflows such as ensemble forecasting and long climate simulations for coastal-ocean modelling.",
    "published": "2026-02-05T07:59:58Z",
    "updated": "2026-02-05T07:59:58Z",
    "link": "http://arxiv.org/pdf/2602.05416v1.pdf",
    "category": [
      "cs.CE",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph",
      "physics.flu-dyn"
    ],
    "authors": [
      "Freja Høgholm Petersen",
      "Jesper Sandvig Mariegaard",
      "Rocco Palmitessa",
      "Allan P. Engsig-Karup"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04711v2",
    "title": "Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention",
    "summary": "Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLM's output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.",
    "published": "2026-02-04T16:22:20Z",
    "updated": "2026-02-05T07:59:00Z",
    "link": "http://arxiv.org/pdf/2602.04711v2.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Sagie Dekel",
      "Moshe Tennenholtz",
      "Oren Kurland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02780v2",
    "title": "A Study of Adaptive Modeling Towards Robust Generalization",
    "summary": "Large language models (LLMs) increasingly support reasoning over biomolecular structures, but most existing approaches remain modality-specific and rely on either sequence-style encodings or fixed-length connector tokens for structural inputs. These designs can under-expose explicit geometric cues and impose rigid fusion bottlenecks, leading to over-compression and poor token allocation as structural complexity grows. We present a unified all-atom framework that grounds language reasoning in geometric information while adaptively scaling structural tokens. The method first constructs variable-size structural patches on molecular graphs using an instruction-conditioned gating policy, enabling complexity-aware allocation of query tokens. It then refines the resulting patch tokens via cross-attention with modality embeddings and injects geometry-informed tokens into the language model to improve structure grounding and reduce structural hallucinations. Across diverse all-atom benchmarks, the proposed approach yields consistent gains in heterogeneous structure-grounded reasoning. An anonymized implementation is provided in the supplementary material.",
    "published": "2026-02-02T20:35:44Z",
    "updated": "2026-02-05T07:57:58Z",
    "link": "http://arxiv.org/pdf/2602.02780v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zihao Jing",
      "Qiuhao Zeng",
      "Ruiyi Fang",
      "Yan Yi Li",
      "Yan Sun",
      "Boyu Wang",
      "Pingzhao Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11300v2",
    "title": "Beyond touch-based human-machine interface: Control your machines in natural language by utilizing large language models and OPC UA",
    "summary": "This paper proposes an agent-based approach toward a more natural interface between humans and machines. Large language models equipped with tools and the communication standard OPC UA are utilized to control machines in natural language. Instead of touch interaction, which is currently the state-of-the-art medium for interaction in operations, the proposed approach enables operators to talk or text with machines. This allows commands such as 'Please decrease the temperature by 20 % in machine 1 and start the cleaning operation in machine 2.' The large language model receives the user input and selects one of three predefined tools that connect to an OPC UA server and either change or read the value of a node. Afterwards, the result of the tool execution is passed back to the language model, which then provides a final response to the user. The approach is universally designed and can therefore be applied to any machine that supports the OPC UA standard. The large language model is neither fine-tuned nor requires training data, only the relevant machine credentials and a parameter dictionary are included within the system prompt. The tool-calling ability and their design is evaluated on a demonstrator setup with a Siemens S7-1500 programmable logic controller with four machine parameters. Fifty synthetically generated commands on five different models were tested and the results demonstrate high success rate, with proprietary GPT-5 models achieving accuracies between 96.0 % and 98.0 %, and open-weight models reaching up to 90.0 %. Afterwards the approach was transferred to a deployed spay-coating machine. The proposed concept is supposed to contribute in advancing natural interaction in industrial human-machine interfaces.",
    "published": "2025-10-13T11:46:47Z",
    "updated": "2026-02-05T07:56:21Z",
    "link": "http://arxiv.org/pdf/2510.11300v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Bernd Hofmann",
      "Niklas Piechulek",
      "Sven Kreitlein",
      "Joerg Franke",
      "Patrick Bruendl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05407v1",
    "title": "H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration",
    "summary": "Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.",
    "published": "2026-02-05T07:44:56Z",
    "updated": "2026-02-05T07:44:56Z",
    "link": "http://arxiv.org/pdf/2602.05407v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jun-Min Lee",
      "Meong Hi Son",
      "Edward Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05406v1",
    "title": "Enabling Automatic Disordered Speech Recognition: An Impaired Speech Dataset in the Akan Language",
    "summary": "The lack of impaired speech data hinders advancements in the development of inclusive speech technologies, particularly in low-resource languages such as Akan. To address this gap, this study presents a curated corpus of speech samples from native Akan speakers with speech impairment. The dataset comprises of 50.01 hours of audio recordings cutting across four classes of impaired speech namely stammering, cerebral palsy, cleft palate, and stroke induced speech disorder. Recordings were done in controlled supervised environments were participants described pre-selected images in their own words. The resulting dataset is a collection of audio recordings, transcriptions, and associated metadata on speaker demographics, class of impairment, recording environment and device. The dataset is intended to support research in low-resource automatic disordered speech recognition systems and assistive speech technology.",
    "published": "2026-02-05T07:44:13Z",
    "updated": "2026-02-05T07:44:13Z",
    "link": "http://arxiv.org/pdf/2602.05406v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Isaac Wiafe",
      "Akon Obu Ekpezu",
      "Sumaya Ahmed Salihs",
      "Elikem Doe Atsakpo",
      "Fiifi Baffoe Payin Winful",
      "Jamal-Deen Abdulai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05403v1",
    "title": "Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation",
    "summary": "Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.",
    "published": "2026-02-05T07:41:19Z",
    "updated": "2026-02-05T07:41:19Z",
    "link": "http://arxiv.org/pdf/2602.05403v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "authors": [
      "Chenghua Gong",
      "Yihang Jiang",
      "Hao Li",
      "Rui Sun",
      "Juyuan Zhang",
      "Tianjun Gu",
      "Liming Pan",
      "Linyuan Lü"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04142v2",
    "title": "JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models",
    "summary": "Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.",
    "published": "2026-02-04T02:24:32Z",
    "updated": "2026-02-05T07:38:00Z",
    "link": "http://arxiv.org/pdf/2602.04142v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hiroshi Sasaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00771v2",
    "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching",
    "summary": "In this paper, we present a vocoder-free framework for audio super-resolution that employs a flow matching generative model to capture the conditional distribution of complex-valued spectral coefficients. Unlike conventional two-stage diffusion-based approaches that predict a mel-spectrogram and then rely on a pre-trained neural vocoder to synthesize waveforms, our method directly reconstructs waveforms via the inverse Short-Time Fourier Transform (iSTFT), thereby eliminating the dependence on a separate vocoder. This design not only simplifies end-to-end optimization but also overcomes a critical bottleneck of two-stage pipelines, where the final audio quality is fundamentally constrained by vocoder performance. Experiments show that our model consistently produces high-fidelity 48 kHz audio across diverse upsampling factors, achieving state-of-the-art performance on both speech and general audio datasets.",
    "published": "2025-10-01T11:04:53Z",
    "updated": "2026-02-05T07:24:16Z",
    "link": "http://arxiv.org/pdf/2510.00771v2.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "authors": [
      "Woongjib Choi",
      "Sangmin Lee",
      "Hyungseob Lim",
      "Hong-Goo Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.00906v4",
    "title": "Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing",
    "summary": "Large language models often hallucinate with high confidence on \"random facts\" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified \"closed world\" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.",
    "published": "2026-01-31T21:18:28Z",
    "updated": "2026-02-05T07:24:05Z",
    "link": "http://arxiv.org/pdf/2602.00906v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "cs.IT"
    ],
    "authors": [
      "Anxin Guo",
      "Jingwei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05395v1",
    "title": "Optimal Bayesian Stopping for Efficient Inference of Consistent LLM Answers",
    "summary": "A simple strategy for improving LLM accuracy, especially in math and reasoning problems, is to sample multiple responses and submit the answer most consistently reached. In this paper we leverage Bayesian prior information to save on sampling costs, stopping once sufficient consistency is reached. Although the exact posterior is computationally intractable, we further introduce an efficient \"L-aggregated\" stopping policy that tracks only the L-1 most frequent answer counts. Theoretically, we prove that L=3 is all you need: this coarse approximation is sufficient to achieve asymptotic optimality, and strictly dominates prior-free baselines, while having a fast posterior computation. Empirically, this identifies the most consistent (i.e., mode) LLM answer using fewer samples, and can achieve similar answer accuracy while cutting the number of LLM calls (i.e., saving on LLM inference costs) by up to 50%.",
    "published": "2026-02-05T07:22:00Z",
    "updated": "2026-02-05T07:22:00Z",
    "link": "http://arxiv.org/pdf/2602.05395v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jingkai Huang",
      "Will Ma",
      "Zhengyuan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05392v1",
    "title": "Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances",
    "summary": "Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.",
    "published": "2026-02-05T07:19:04Z",
    "updated": "2026-02-05T07:19:04Z",
    "link": "http://arxiv.org/pdf/2602.05392v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jiyun Chun",
      "Eric Fosler-Lussier",
      "Michael White",
      "Andrew Perrault"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.03054v3",
    "title": "Calibration and Transformation-Free Weight-Only LLMs Quantization via Dynamic Grouping",
    "summary": "Large Language Models (LLMs) deliver strong performance but are difficult to deploy under tight memory and compute constraints. Low-bit post-training quantization (PTQ) is a promising direction; however, it typically relies on calibration data, auxiliary transformations, and GPU tools. To address these limitations, we propose MSB (Multi Scale Binary), a calibration-free and transformation-free PTQ method that generalizes binary quantization to multi-bit settings. MSB optimizes a dynamic grouping criterion that minimizes within group variance, yielding group-wise multiscale levels that can be applied consistently across granularities from per tensor to block-wise configurations with 64 elements groups per row, without calibration or intermediate transforms. We implement the optimization in a CPU based solver for the quantization step and evaluate using standard bfloat16 execution without low-bit packing. On Llama 3.2 3B, MSB achieves 8.43 perplexity on WikiText-2 under 4-bit weight only block-wise quantization, compared to 7.81 in full precision and 12.23 with GPTQ its default setup. Overall, MSB provides a new optimization perspective for low-bit PTQ while simplifying the pipeline by removing calibration and transformations.",
    "published": "2025-09-03T06:36:21Z",
    "updated": "2026-02-05T07:18:02Z",
    "link": "http://arxiv.org/pdf/2509.03054v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xinzhe Zheng",
      "Zhen-Qun Yang",
      "Zishan Liu",
      "Haoran Xie",
      "S. Joe Qin",
      "Arlene Chen",
      "Fangzhen Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05390v1",
    "title": "Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models",
    "summary": "Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.",
    "published": "2026-02-05T07:17:21Z",
    "updated": "2026-02-05T07:17:21Z",
    "link": "http://arxiv.org/pdf/2602.05390v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wei Soon Cheong",
      "Lian Lian Jiang",
      "Jamie Ng Suat Ling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01151v3",
    "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
    "summary": "Text-to-image diffusion models have revolutionized visual content generation, yet their deployment is hindered by a fundamental limitation: safety mechanisms enforce rigid, uniform standards that fail to reflect diverse user preferences shaped by age, culture, or personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that transitions generative safety from static filtration to user-conditioned adaptation. We introduce Sage, a large-scale dataset capturing diverse safety boundaries across 1,000 simulated user profiles, covering complex risks often missed by traditional datasets. By integrating these profiles via a parameter-efficient cross-attention adapter, PSA dynamically modulates generation to align with individual sensitivities. Extensive experiments demonstrate that PSA achieves a calibrated safety-quality trade-off: under permissive profiles, it relaxes over-cautious constraints to enhance visual fidelity, while under restrictive profiles, it enforces state-of-the-art suppression, significantly outperforming static baselines. Furthermore, PSA exhibits superior instruction adherence compared to prompt-engineering methods, establishing personalization as a vital direction for creating adaptive, user-centered, and responsible generative AI. Our code, data, and models are publicly available at https://github.com/M-E-AGI-Lab/PSAlign.",
    "published": "2025-08-02T02:23:20Z",
    "updated": "2026-02-05T07:15:25Z",
    "link": "http://arxiv.org/pdf/2508.01151v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yu Lei",
      "Jinbin Bai",
      "Qingyu Shi",
      "Aosong Feng",
      "Hongcheng Gao",
      "Xiao Zhang",
      "Rex Ying"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05386v1",
    "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
    "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
    "published": "2026-02-05T07:11:05Z",
    "updated": "2026-02-05T07:11:05Z",
    "link": "http://arxiv.org/pdf/2602.05386v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Zhenxiong Yu",
      "Zhi Yang",
      "Zhiheng Jin",
      "Shuhe Wang",
      "Heng Zhang",
      "Yanlin Fei",
      "Lingfeng Zeng",
      "Fangqi Lou",
      "Shuo Zhang",
      "Tu Hu",
      "Jingping Liu",
      "Rongze Chen",
      "Xingyu Zhu",
      "Kunyi Wang",
      "Chaofa Yuan",
      "Xin Guo",
      "Zhaowei Liu",
      "Feipeng Zhang",
      "Jie Huang",
      "Huacan Wang",
      "Ronghao Chen",
      "Liwen Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.06616v2",
    "title": "Generative AI for Intent-Driven Network Management in 6G RAN: A Case Study on the Mamba Model",
    "summary": "With the emergence of 6G, mobile networks are becoming increasingly heterogeneous and dynamic, necessitating advanced automation for efficient management. Intent-Driven Networks (IDNs) address this by translating high-level intents into optimization policies. Large Language Models (LLMs) can enhance this process by understanding complex human instructions, enabling adaptive and intelligent automation. Given the rapid advancements in Generative AI (GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated Radio Access Network (RAN) environments is both timely and critical. This article provides such a survey, along with a case study on a selective State-Space Model (SSM)-enabled IDN architecture that integrates GenAI across three key stages: intent processing, intent validation, and intent execution. For the first time in the literature, we propose a hierarchical framework built on Mamba-SSM that introduces GenAI across all stages of the IDN pipeline. We further present a case study demonstrating that the proposed Mamba architecture significantly improves network performance through intelligent automation, surpassing existing IDN approaches. In a multi-cell 5G/6G scenario, the proposed architecture reduces quality of service drift by up to 70%, improves throughput by up to 80 Mbps, and lowers inference time to 60-70 ms, outperforming GenAI, reinforcement learning, and non-machine learning baselines.",
    "published": "2025-08-08T18:06:52Z",
    "updated": "2026-02-05T07:07:22Z",
    "link": "http://arxiv.org/pdf/2508.06616v2.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Md Arafat Habib",
      "Medhat Elsayed",
      "Yigit Ozcan",
      "Pedro Enrique Iturria-Rivera",
      "Majid Bavand",
      "Melike Erol-Kantarci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05381v1",
    "title": "Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation",
    "summary": "Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.",
    "published": "2026-02-05T07:00:20Z",
    "updated": "2026-02-05T07:00:20Z",
    "link": "http://arxiv.org/pdf/2602.05381v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ting Fang Tan",
      "Kabilan Elangovan",
      "Andreas Pollreisz",
      "Kevin Bryan Dy",
      "Wei Yan Ng",
      "Joy Le Yi Wong",
      "Jin Liyuan",
      "Chrystie Quek Wan Ning",
      "Ashley Shuen Ying Hong",
      "Arun James Thirunavukarasu",
      "Shelley Yin-His Chang",
      "Jie Yao",
      "Dylan Hong",
      "Wang Zhaoran",
      "Amrita Gupta",
      "Daniel SW Ting"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25634v2",
    "title": "Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills",
    "summary": "Long-horizon contact-rich bimanual manipulation presents a significant challenge, requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms. In this paper, we introduce a hierarchical framework that frames this challenge as an integrated skill planning & scheduling problem, going beyond purely sequential decision-making to support simultaneous skill invocation. Our approach is built upon a library of single-arm and bimanual primitive skills, each trained using Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a Transformer-based planner on a dataset of skill compositions to act as a high-level scheduler, simultaneously predicting the discrete schedule of skills as well as their continuous parameters. We demonstrate that our method achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.",
    "published": "2025-10-29T15:39:53Z",
    "updated": "2026-02-05T06:48:40Z",
    "link": "http://arxiv.org/pdf/2510.25634v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Weikang Wan",
      "Fabio Ramos",
      "Xuning Yang",
      "Caelan Garrett"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.03493v3",
    "title": "On Entropy Control in LLM-RL Algorithms",
    "summary": "For RL algorithms, appropriate entropy control is crucial to their effectiveness. To control the policy entropy, a commonly used method is entropy regularization, which is adopted in various popular RL algorithms including PPO, SAC and A3C. Although entropy regularization proves effective in robotic and games RL conventionally, studies found that it gives weak to no gains in LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL setting. Specifically, we first argue that the conventional entropy regularization suffers from the LLM's extremely large response space and the sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy control method that utilizes a new clamped entropy bonus with an automatically adjusted coefficient. The clamped entropy is evaluated with the re-normalized policy defined on certain smaller token space, which encourages exploration within a more compact response set. In addition, the algorithm automatically adjusts entropy coefficient according to the clamped entropy value, effectively controlling the entropy-induced bias while leveraging the entropy's benefits. AEnt is tested in math-reasoning tasks under different base models and datasets, and it is observed that AEnt outperforms the baselines consistently across multiple benchmarks.",
    "published": "2025-09-03T17:23:19Z",
    "updated": "2026-02-05T06:47:40Z",
    "link": "http://arxiv.org/pdf/2509.03493v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Han Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05367v1",
    "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
    "summary": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\\times$ inference speed-up over full-precision models on an RTX 4090.",
    "published": "2026-02-05T06:41:11Z",
    "updated": "2026-02-05T06:41:11Z",
    "link": "http://arxiv.org/pdf/2602.05367v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Youngcheon You",
      "Banseok Lee",
      "Minseop Choi",
      "Seonyoung Kim",
      "Hyochan Chong",
      "Changdong Kim",
      "Youngmin Kim",
      "Dongkyu Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22693v2",
    "title": "PEAR: Pixel-aligned Expressive humAn mesh Recovery",
    "summary": "Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR",
    "published": "2026-01-30T08:12:54Z",
    "updated": "2026-02-05T06:25:40Z",
    "link": "http://arxiv.org/pdf/2601.22693v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jiahao Wu",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Lei Zhu",
      "Jingyi Li",
      "Yu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05354v1",
    "title": "PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents",
    "summary": "We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.",
    "published": "2026-02-05T06:24:23Z",
    "updated": "2026-02-05T06:24:23Z",
    "link": "http://arxiv.org/pdf/2602.05354v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shifat E. Arman",
      "Syed Nazmus Sakib",
      "Tapodhir Karmakar Taton",
      "Nafiul Haque",
      "Shahrear Bin Amin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05353v1",
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "summary": "Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.",
    "published": "2026-02-05T06:24:15Z",
    "updated": "2026-02-05T06:24:15Z",
    "link": "http://arxiv.org/pdf/2602.05353v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Ruijie Shi",
      "Houbin Zhang",
      "Yuecheng Han",
      "Yuheng Wang",
      "Jingru Fan",
      "Runde Yang",
      "Yufan Dang",
      "Huatao Li",
      "Dewen Liu",
      "Yuan Cheng",
      "Chen Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.01673v2",
    "title": "CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables",
    "summary": "For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the difficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS - continuity, sparsity, and variability - are identified and implemented through different modules. Even with a basic 2-layer MLP as core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it an efficient and transferable MTSF solution.",
    "published": "2024-03-04T01:52:40Z",
    "updated": "2026-02-05T06:14:25Z",
    "link": "http://arxiv.org/pdf/2403.01673v2.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08859v2",
    "title": "Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models",
    "summary": "Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories through distinct conversational approaches. Existing multi-turn methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles, defense to one pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA",
    "published": "2025-10-09T23:26:28Z",
    "updated": "2026-02-05T06:10:35Z",
    "link": "http://arxiv.org/pdf/2510.08859v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Ragib Amin Nihal",
      "Rui Wen",
      "Kazuhiro Nakadai",
      "Jun Sakuma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13948v3",
    "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
    "summary": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
    "published": "2026-01-20T13:23:44Z",
    "updated": "2026-02-05T06:02:11Z",
    "link": "http://arxiv.org/pdf/2601.13948v3.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Nikita Kuzmin",
      "Songting Liu",
      "Kong Aik Lee",
      "Eng Siong Chng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.14982v2",
    "title": "In-context Time Series Predictor",
    "summary": "Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate \"time series forecasting tasks\" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms, and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures.",
    "published": "2024-05-23T18:37:00Z",
    "updated": "2026-02-05T05:55:51Z",
    "link": "http://arxiv.org/pdf/2405.14982v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "authors": [
      "Jiecheng Lu",
      "Yan Sun",
      "Shihao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05327v1",
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "summary": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct",
    "published": "2026-02-05T05:45:16Z",
    "updated": "2026-02-05T05:45:16Z",
    "link": "http://arxiv.org/pdf/2602.05327v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yangbin Yu",
      "Mingyu Yang",
      "Junyou Li",
      "Yiming Gao",
      "Feiyu Liu",
      "Yijun Yang",
      "Zichuan Lin",
      "Jiafei Lyu",
      "Yicheng Liu",
      "Zhicong Lu",
      "Deheng Ye",
      "Jie Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05323v1",
    "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
    "summary": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to \"stitch\" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.",
    "published": "2026-02-05T05:44:48Z",
    "updated": "2026-02-05T05:44:48Z",
    "link": "http://arxiv.org/pdf/2602.05323v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zifan Liu",
      "Xinran Li",
      "Shibo Chen",
      "Jun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.11007v2",
    "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
    "summary": "Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
    "published": "2025-11-14T06:51:34Z",
    "updated": "2026-02-05T05:44:15Z",
    "link": "http://arxiv.org/pdf/2511.11007v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xinlei Yu",
      "Chengming Xu",
      "Guibin Zhang",
      "Zhangquan Chen",
      "Yudong Zhang",
      "Yongbo He",
      "Peng-Tao Jiang",
      "Jiangning Zhang",
      "Xiaobin Hu",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.03159v4",
    "title": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting",
    "summary": "We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results.",
    "published": "2024-10-04T05:45:50Z",
    "updated": "2026-02-05T05:34:29Z",
    "link": "http://arxiv.org/pdf/2410.03159v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.15617v4",
    "title": "A Differential and Pointwise Control Approach to Reinforcement Learning",
    "summary": "Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $\\mathcal{O}(K^{5/6})$. Empirically, dfPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions.",
    "published": "2024-04-24T03:11:12Z",
    "updated": "2026-02-05T05:28:08Z",
    "link": "http://arxiv.org/pdf/2404.15617v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST"
    ],
    "authors": [
      "Minh Nguyen",
      "Chandrajit Bajaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10960v2",
    "title": "Relational Graph Transformer",
    "summary": "Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.",
    "published": "2025-05-16T07:51:58Z",
    "updated": "2026-02-05T05:25:22Z",
    "link": "http://arxiv.org/pdf/2505.10960v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Vijay Prakash Dwivedi",
      "Sri Jaladi",
      "Yangyi Shen",
      "Federico López",
      "Charilaos I. Kanatsoulis",
      "Rishi Puri",
      "Matthias Fey",
      "Jure Leskovec"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05311v1",
    "title": "Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates",
    "summary": "Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \\emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.",
    "published": "2026-02-05T05:08:01Z",
    "updated": "2026-02-05T05:08:01Z",
    "link": "http://arxiv.org/pdf/2602.05311v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Chengxiao Wang",
      "Haoze Wu",
      "Gagandeep Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04575v2",
    "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
    "summary": "For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.\n  Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.",
    "published": "2026-02-04T14:01:44Z",
    "updated": "2026-02-05T05:00:32Z",
    "link": "http://arxiv.org/pdf/2602.04575v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jiaheng Liu",
      "Yuanxing Zhang",
      "Shihao Li",
      "Xinping Lei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06036v1",
    "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
    "summary": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.",
    "published": "2026-02-05T18:59:30Z",
    "updated": "2026-02-05T18:59:30Z",
    "link": "http://arxiv.org/pdf/2602.06036v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jian Chen",
      "Yesheng Liang",
      "Zhijian Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06019v1",
    "title": "Multi-Token Prediction via Self-Distillation",
    "summary": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.",
    "published": "2026-02-05T18:54:48Z",
    "updated": "2026-02-05T18:54:48Z",
    "link": "http://arxiv.org/pdf/2602.06019v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "John Kirchenbauer",
      "Abhimanyu Hans",
      "Brian Bartoldson",
      "Micah Goldblum",
      "Ashwinee Panda",
      "Tom Goldstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06015v1",
    "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies",
    "summary": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.",
    "published": "2026-02-05T18:53:17Z",
    "updated": "2026-02-05T18:53:17Z",
    "link": "http://arxiv.org/pdf/2602.06015v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Panagiotis Kaliosis",
      "Adithya V Ganesan",
      "Oscar N. E. Kjell",
      "Whitney Ringwald",
      "Scott Feltman",
      "Melissa A. Carr",
      "Dimitris Samaras",
      "Camilo Ruggero",
      "Benjamin J. Luft",
      "Roman Kotov",
      "Andrew H. Schwartz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05992v1",
    "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
    "summary": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.",
    "published": "2026-02-05T18:41:38Z",
    "updated": "2026-02-05T18:41:38Z",
    "link": "http://arxiv.org/pdf/2602.05992v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lizhuo Luo",
      "Shenggui Li",
      "Yonggang Wen",
      "Tianwei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25110v4",
    "title": "DEBATE: A Large-Scale Benchmark for Evaluating Opinion Dynamics in Role-Playing LLM Agents",
    "summary": "Accurately modeling opinion change through social interactions is crucial for understanding and mitigating polarization, misinformation, and societal conflict. Recent work simulates opinion dynamics with role-playing LLM agents (RPLAs), but multi-agent simulations often display unnatural group behavior (e.g., premature convergence) and lack empirical benchmarks for assessing alignment with real human group interactions. We introduce DEBATE, a large-scale benchmark for evaluating the authenticity of opinion dynamics in multi-agent RPLA simulations. DEBATE contains 36,383 messages from 2,832 U.S.-based participants across 708 groups and 107 topics, with both public messages and private Likert-scale beliefs, enabling evaluation at the utterance and group levels (and supporting future individual-level analyses). We instantiate \"digital twin\" RPLAs with seven LLMs and evaluate across two settings: next-message prediction and full conversation rollout, using stance-alignment and opinion-convergence metrics. In zero-shot settings, RPLA groups exhibit strong opinion convergence relative to human groups. Post-training via supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) improves stance alignment and brings group-level convergence closer to human behavior, though discrepancies in opinion change and belief updating remain. DEBATE enables rigorous benchmarking of simulated opinion dynamics and supports future research on aligning multi-agent RPLAs with realistic human interactions.",
    "published": "2025-10-29T02:21:10Z",
    "updated": "2026-02-05T18:29:20Z",
    "link": "http://arxiv.org/pdf/2510.25110v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yun-Shiuan Chuang",
      "Ruixuan Tu",
      "Chengtao Dai",
      "Smit Vasani",
      "You Li",
      "Binwei Yao",
      "Michael Henry Tessler",
      "Sijia Yang",
      "Dhavan Shah",
      "Robert Hawkins",
      "Junjie Hu",
      "Timothy T. Rogers"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05975v1",
    "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
    "summary": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.",
    "published": "2026-02-05T18:25:24Z",
    "updated": "2026-02-05T18:25:24Z",
    "link": "http://arxiv.org/pdf/2602.05975v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Tiansheng Hu",
      "Yilun Zhao",
      "Canyu Zhang",
      "Arman Cohan",
      "Chen Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05971v1",
    "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space",
    "summary": "Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.",
    "published": "2026-02-05T18:23:04Z",
    "updated": "2026-02-05T18:23:04Z",
    "link": "http://arxiv.org/pdf/2602.05971v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG",
      "q-bio.NC"
    ],
    "authors": [
      "Felipe D. Toro-Hernández",
      "Jesuino Vieira Filho",
      "Rodrigo M. Cabral-Carvalho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05940v1",
    "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
    "summary": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
    "published": "2026-02-05T17:55:09Z",
    "updated": "2026-02-05T17:55:09Z",
    "link": "http://arxiv.org/pdf/2602.05940v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Junxiao Liu",
      "Zhijun Wang",
      "Yixiao Li",
      "Zhejian Lai",
      "Liqian Huang",
      "Xin Huang",
      "Xue Han",
      "Junlan Feng",
      "Shujian Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04856v2",
    "title": "CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation",
    "summary": "From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks.",
    "published": "2026-02-04T18:43:10Z",
    "updated": "2026-02-05T17:47:06Z",
    "link": "http://arxiv.org/pdf/2602.04856v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhao Tong",
      "Chunlin Gong",
      "Yiping Zhang",
      "Qiang Liu",
      "Xingcheng Xu",
      "Shu Wu",
      "Haichao Shi",
      "Xiao-Yu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05932v1",
    "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions",
    "summary": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.",
    "published": "2026-02-05T17:44:06Z",
    "updated": "2026-02-05T17:44:06Z",
    "link": "http://arxiv.org/pdf/2602.05932v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Léo Labat",
      "Etienne Ollion",
      "François Yvon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05929v1",
    "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
    "summary": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
    "published": "2026-02-05T17:41:57Z",
    "updated": "2026-02-05T17:41:57Z",
    "link": "http://arxiv.org/pdf/2602.05929v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jian Chen",
      "Zhuoran Wang",
      "Jiayu Qin",
      "Ming Li",
      "Meng Wang",
      "Changyou Chen",
      "Yin Chen",
      "Qizhen Weng",
      "Yirui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07053v3",
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "published": "2025-04-09T17:14:33Z",
    "updated": "2026-02-05T17:31:01Z",
    "link": "http://arxiv.org/pdf/2504.07053v3.pdf",
    "category": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Liang-Hsuan Tseng",
      "Yi-Chang Chen",
      "Kuan-Yi Lee",
      "Da-Shan Shiu",
      "Hung-yi Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05905v1",
    "title": "Codified Finite-state Machines for Role-playing",
    "summary": "Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.",
    "published": "2026-02-05T17:19:18Z",
    "updated": "2026-02-05T17:19:18Z",
    "link": "http://arxiv.org/pdf/2602.05905v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Letian Peng",
      "Yupeng Hou",
      "Kun Zhou",
      "Jingbo Shang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05897v1",
    "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
    "summary": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.",
    "published": "2026-02-05T17:15:12Z",
    "updated": "2026-02-05T17:15:12Z",
    "link": "http://arxiv.org/pdf/2602.05897v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shuo Nie",
      "Hexuan Deng",
      "Chao Wang",
      "Ruiyu Fang",
      "Xuebo Liu",
      "Shuangyong Song",
      "Yu Li",
      "Min Zhang",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05890v1",
    "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training",
    "summary": "Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.",
    "published": "2026-02-05T17:07:42Z",
    "updated": "2026-02-05T17:07:42Z",
    "link": "http://arxiv.org/pdf/2602.05890v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Dingwei Zhu",
      "Zhiheng Xi",
      "Shihan Dou",
      "Jiahan Li",
      "Chenhao Huang",
      "Junjie Ye",
      "Sixian Li",
      "Mingxu Chai",
      "Yuhui Wang",
      "Yajie Yang",
      "Ming Zhang",
      "Jiazheng Zhang",
      "Shichun Liu",
      "Caishuang Huang",
      "Yunke Zhang",
      "Yuran Wang",
      "Tao Gui",
      "Xipeng Qiu",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05863v1",
    "title": "Constrained Group Relative Policy Optimization",
    "summary": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.",
    "published": "2026-02-05T16:44:23Z",
    "updated": "2026-02-05T16:44:23Z",
    "link": "http://arxiv.org/pdf/2602.05863v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.RO"
    ],
    "authors": [
      "Roger Girgis",
      "Rodrigue de Schaetzen",
      "Luke Rowe",
      "Azalée Robitaille",
      "Christopher Pal",
      "Liam Paull"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05853v1",
    "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
    "summary": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.",
    "published": "2026-02-05T16:37:41Z",
    "updated": "2026-02-05T16:37:41Z",
    "link": "http://arxiv.org/pdf/2602.05853v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Siran Liu",
      "Guoxia Wang",
      "Sa Wang",
      "Jinle Zeng",
      "HaoYang Xie",
      "Siyu Lou",
      "JiaBin Yang",
      "DianHai Yu",
      "Haifeng Wang",
      "Chao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05843v1",
    "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
    "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
    "published": "2026-02-05T16:31:43Z",
    "updated": "2026-02-05T16:31:43Z",
    "link": "http://arxiv.org/pdf/2602.05843v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Fangzhi Xu",
      "Hang Yan",
      "Qiushi Sun",
      "Jinyang Wu",
      "Zixian Huang",
      "Muye Huang",
      "Jingyang Gong",
      "Zichen Ding",
      "Kanzhi Cheng",
      "Yian Wang",
      "Xinyu Che",
      "Zeyi Sun",
      "Jian Zhang",
      "Zhangyue Yin",
      "Haoran Luo",
      "Xuanjing Huang",
      "Ben Kao",
      "Jun Liu",
      "Qika Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05842v1",
    "title": "Reinforcement World Model Learning for LLM-based Agents",
    "summary": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.",
    "published": "2026-02-05T16:30:08Z",
    "updated": "2026-02-05T16:30:08Z",
    "link": "http://arxiv.org/pdf/2602.05842v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xiao Yu",
      "Baolin Peng",
      "Ruize Xu",
      "Yelong Shen",
      "Pengcheng He",
      "Suman Nath",
      "Nikhil Singh",
      "Jiangfeng Gao",
      "Zhou Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.19806v2",
    "title": "LLM-Based Social Simulations Require a Boundary",
    "summary": "This position paper argues that LLM-based social simulations require clear boundaries to make meaningful contributions to social science. While Large Language Models (LLMs) offer promising capabilities for simulating human behavior, their tendency to produce homogeneous outputs, acting as an \"average persona\", fundamentally limits their ability to capture the behavioral diversity essential for complex social dynamics. We examine why heterogeneity matters for social simulations and how current LLMs fall short, analyzing the relationship between mean alignment and variance in LLM-generated behaviors. Through a systematic review of representative studies, we find that validation practices often fail to match the heterogeneity requirements of research questions: while most papers include ground truth comparisons, fewer than half explicitly assess behavioral variance, and most that do report lower variance than human populations. We propose that researchers should: (1) match validation depth to the heterogeneity demands of their research questions, (2) explicitly report variance alongside mean alignment, and (3) constrain claims to collective-level qualitative patterns when variance is insufficient. Rather than dismissing LLM-based simulation, we advocate for a boundary-aware approach that ensures these methods contribute genuine insights to social science.",
    "published": "2025-06-24T17:14:47Z",
    "updated": "2026-02-05T16:09:18Z",
    "link": "http://arxiv.org/pdf/2506.19806v2.pdf",
    "category": [
      "cs.CY",
      "cs.CL",
      "cs.MA"
    ],
    "authors": [
      "Zengqing Wu",
      "Run Peng",
      "Takayuki Ito",
      "Makoto Onizuka",
      "Chuan Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24494v3",
    "title": "Why Tree-Style Branching Matters for Thought Advantage Estimation in GRPO",
    "summary": "Group Relative Policy Optimization (GRPO) trains Chain-of-Thought reasoning with verifiable rewards, but estimating thought-level advantages without value functions often suffers from high variance. Although tree-style branching is used in practice to reduce the variance, it lacks a theoretical explanation of why it works and whether it is important or even potentially necessary. We study thought-level advantage estimation in GRPO from a variance perspective under a minimal tree-style setting where multiple answers are sampled for each thought. Using the multivariate delta method, we reveal an asymmetry in how different sampling dimensions affect variance. Increasing the number of sampled thoughts ($K$) leaves a strictly positive variance floor, whereas increasing the number of answers per thought ($M$) induces a monotonic decrease in variance, asymptotically decreasing it to zero. This implies that accurate thought-level advantage estimation is impossible through scaling thought sampling alone, making branching a potentially necessary mechanism rather than a heuristic. Experiments further provide empirical evidence for both the effectiveness and necessity of answer-level branching, demonstrating improved optimization stability, training efficiency, and final performance not only in math but also across a broad range of vision domains and under different model architectures and sizes.",
    "published": "2025-09-29T09:07:45Z",
    "updated": "2026-02-05T15:54:07Z",
    "link": "http://arxiv.org/pdf/2509.24494v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hongcheng Wang",
      "Yinuo Huang",
      "Sukai Wang",
      "Guanghui Ren",
      "Hao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05769v1",
    "title": "Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors",
    "summary": "LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.",
    "published": "2026-02-05T15:31:24Z",
    "updated": "2026-02-05T15:31:24Z",
    "link": "http://arxiv.org/pdf/2602.05769v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Adnan Al Ali",
      "Jindřich Helcl",
      "Jindřich Libovický"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01999v2",
    "title": "From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs",
    "summary": "R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.",
    "published": "2026-02-02T11:58:24Z",
    "updated": "2026-02-05T15:27:46Z",
    "link": "http://arxiv.org/pdf/2602.01999v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yanrui Du",
      "Yibo Gao",
      "Sendong Zhao",
      "Jiayun Li",
      "Haochun Wang",
      "Qika Lin",
      "Kai He",
      "Bing Qin",
      "Mengling Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05758v1",
    "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards",
    "summary": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.",
    "published": "2026-02-05T15:26:47Z",
    "updated": "2026-02-05T15:26:47Z",
    "link": "http://arxiv.org/pdf/2602.05758v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Bowen Ping",
      "Zijun Chen",
      "Yiyao Yu",
      "Tingfeng Hui",
      "Junchi Yan",
      "Baobao Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05710v1",
    "title": "Ethology of Latent Spaces",
    "summary": "This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.\n  Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.\n  We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.",
    "published": "2026-02-05T14:37:31Z",
    "updated": "2026-02-05T14:37:31Z",
    "link": "http://arxiv.org/pdf/2602.05710v1.pdf",
    "category": [
      "cs.CY",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Philippe Boisnard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05708v1",
    "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
    "summary": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
    "published": "2026-02-05T14:33:00Z",
    "updated": "2026-02-05T14:33:00Z",
    "link": "http://arxiv.org/pdf/2602.05708v1.pdf",
    "category": [
      "cs.DB",
      "cs.CL"
    ],
    "authors": [
      "Chuangtao Ma",
      "Zeyu Zhang",
      "Arijit Khan",
      "Sebastian Schelter",
      "Paul Groth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05694v1",
    "title": "Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation",
    "summary": "Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.",
    "published": "2026-02-05T14:20:59Z",
    "updated": "2026-02-05T14:20:59Z",
    "link": "http://arxiv.org/pdf/2602.05694v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shuting Jiang",
      "Ran Song",
      "Yuxin Huang",
      "Yan Xiang",
      "Yantuan Xian",
      "Shengxiang Gao",
      "Zhengtao Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05692v1",
    "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations",
    "summary": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.",
    "published": "2026-02-05T14:18:20Z",
    "updated": "2026-02-05T14:18:20Z",
    "link": "http://arxiv.org/pdf/2602.05692v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Congbo Ma",
      "Yichun Zhang",
      "Yousef Al-Jazzazi",
      "Ahamed Foisal",
      "Laasya Sharma",
      "Yousra Sadqi",
      "Khaled Saleh",
      "Jihad Mallat",
      "Farah E. Shamout"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05648v1",
    "title": "Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew",
    "summary": "We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.",
    "published": "2026-02-05T13:31:21Z",
    "updated": "2026-02-05T13:31:21Z",
    "link": "http://arxiv.org/pdf/2602.05648v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Giuseppe Samo",
      "Paola Merlo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.10192v3",
    "title": "Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL",
    "summary": "The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.",
    "published": "2025-11-13T11:02:15Z",
    "updated": "2026-02-05T13:30:43Z",
    "link": "http://arxiv.org/pdf/2511.10192v3.pdf",
    "category": [
      "cs.CL",
      "cs.DB"
    ],
    "authors": [
      "Qifeng Cai",
      "Hao Liang",
      "Chang Xu",
      "Tao Xie",
      "Wentao Zhang",
      "Bin Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05633v1",
    "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models",
    "summary": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.",
    "published": "2026-02-05T13:13:19Z",
    "updated": "2026-02-05T13:13:19Z",
    "link": "http://arxiv.org/pdf/2602.05633v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Rui Jia",
      "Ruiyi Lan",
      "Fengrui Liu",
      "Zhongxiang Dai",
      "Bo Jiang",
      "Jing Shao",
      "Jingyuan Chen",
      "Guandong Xu",
      "Fei Wu",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05630v1",
    "title": "Rewards as Labels: Revisiting RLVR from a Classification Perspective",
    "summary": "Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.",
    "published": "2026-02-05T13:11:36Z",
    "updated": "2026-02-05T13:11:36Z",
    "link": "http://arxiv.org/pdf/2602.05630v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Zepeng Zhai",
      "Meilin Chen",
      "Jiaxuan Zhao",
      "Junlang Qian",
      "Lei Shen",
      "Yuan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20624v3",
    "title": "POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization",
    "summary": "Online polarization poses a growing challenge for democratic discourse, yet most computational social science research remains monolingual, culturally narrow, or event-specific. We introduce POLAR, a multilingual, multicultural, and multi-event dataset with over 110K instances in 22 languages drawn from diverse online platforms and real-world events. Polarization is annotated along three axes, namely detection, type, and manifestation, using a variety of annotation platforms adapted to each cultural context. We conduct two main experiments: (1) fine-tuning six pretrained small language models; and (2) evaluating a range of open and closed large language models in few-shot and zero-shot settings. The results show that, while most models perform well in binary polarization detection, they achieve substantially lower performance when predicting polarization types and manifestations. These findings highlight the complex, highly contextual nature of polarization and demonstrate the need for robust, adaptable approaches in NLP and computational social science. All resources will be released to support further research and effective mitigation of digital polarization globally.",
    "published": "2025-05-27T02:04:58Z",
    "updated": "2026-02-05T12:57:13Z",
    "link": "http://arxiv.org/pdf/2505.20624v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Usman Naseem",
      "Robert Geislinger",
      "Juan Ren",
      "Sarah Kohail",
      "Rudy Garrido Veliz",
      "P Sam Sahil",
      "Yiran Zhang",
      "Marco Antonio Stranisci",
      "Idris Abdulmumin",
      "Özge Alacam",
      "Cengiz Acartürk",
      "Aisha Jabr",
      "Saba Anwar",
      "Abinew Ali Ayele",
      "Simona Frenda",
      "Alessandra Teresa Cignarella",
      "Elena Tutubalina",
      "Oleg Rogov",
      "Aung Kyaw Htet",
      "Xintong Wang",
      "Surendrabikram Thapa",
      "Kritesh Rauniyar",
      "Tanmoy Chakraborty",
      "Arfeen Zeeshan",
      "Dheeraj Kodati",
      "Satya Keerthi",
      "Sahar Moradizeyveh",
      "Firoj Alam",
      "Arid Hasan",
      "Syed Ishtiaque Ahmed",
      "Ye Kyaw Thu",
      "Shantipriya Parida",
      "Ihsan Ayyub Qazi",
      "Lilian Wanzare",
      "Nelson Odhiambo Onyango",
      "Clemencia Siro",
      "Jane Wanjiru Kimani",
      "Ibrahim Said Ahmad",
      "Adem Chanie Ali",
      "Martin Semmann",
      "Chris Biemann",
      "Shamsuddeen Hassan Muhammad",
      "Seid Muhie Yimam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03999v3",
    "title": "LH-Deception: Simulating and Understanding LLM Deceptive Behaviors in Long-Horizon Interactions",
    "summary": "Deception is a pervasive feature of human communication and an emerging concern in large language models (LLMs). While recent studies document instances of LLM deception, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce a new simulation framework, LH-Deception, for a systematic, empirical quantification of deception in LLMs under extended sequences of interdependent tasks and dynamic contextual pressures. LH-Deception is designed as a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed-source and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal emergent, long-horizon phenomena, such as ``chains of deception\", which are invisible to static, single-turn evaluations. Our findings provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts.",
    "published": "2025-10-05T02:18:23Z",
    "updated": "2026-02-05T12:03:58Z",
    "link": "http://arxiv.org/pdf/2510.03999v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yang Xu",
      "Xuanming Zhang",
      "Samuel Yeh",
      "Jwala Dhamala",
      "Ousmane Dia",
      "Rahul Gupta",
      "Sharon Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05550v1",
    "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval",
    "summary": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.",
    "published": "2026-02-05T11:15:34Z",
    "updated": "2026-02-05T11:15:34Z",
    "link": "http://arxiv.org/pdf/2602.05550v1.pdf",
    "category": [
      "cs.SE",
      "cs.CL"
    ],
    "authors": [
      "Yulong He",
      "Artem Ermakov",
      "Sergey Kovalchuk",
      "Artem Aliev",
      "Dmitry Shalymov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05512v1",
    "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
    "summary": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
    "published": "2026-02-05T10:10:19Z",
    "updated": "2026-02-05T10:10:19Z",
    "link": "http://arxiv.org/pdf/2602.05512v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Larissa Pusch",
      "Alexandre Courtiol",
      "Tim Conrad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.22955v3",
    "title": "Diversity or Precision? A Deep Dive into Next Token Prediction",
    "summary": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.",
    "published": "2025-12-28T14:53:24Z",
    "updated": "2026-02-05T09:17:50Z",
    "link": "http://arxiv.org/pdf/2512.22955v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Haoyuan Wu",
      "Hai Wang",
      "Jiajia Wu",
      "Jinxiang Ou",
      "Keyao Wang",
      "Weile Chen",
      "Zihao Zheng",
      "Bei Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05471v1",
    "title": "Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision",
    "summary": "Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.",
    "published": "2026-02-05T09:17:25Z",
    "updated": "2026-02-05T09:17:25Z",
    "link": "http://arxiv.org/pdf/2602.05471v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Md. Mithun Hossaina",
      "Mashary N. Alrasheedy",
      "Nirban Bhowmick",
      "Shamim Forhad",
      "Md. Shakil Hossain",
      "Sudipto Chaki",
      "Md Shafiqul Islam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05467v1",
    "title": "MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation",
    "summary": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.",
    "published": "2026-02-05T09:15:34Z",
    "updated": "2026-02-05T09:15:34Z",
    "link": "http://arxiv.org/pdf/2602.05467v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.RO"
    ],
    "authors": [
      "Dekang Qi",
      "Shuang Zeng",
      "Xinyuan Chang",
      "Feng Xiong",
      "Shichao Xie",
      "Xiaolong Wu",
      "Mu Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.00476v2",
    "title": "Remembering Unequally: Global and Disciplinary Bias in LLM Reconstruction of Scholarly Coauthor Lists",
    "summary": "Ongoing breakthroughs in large language models (LLMs) are reshaping scholarly search and discovery interfaces. While these systems offer new possibilities for navigating scientific knowledge, they also raise concerns about fairness and representational bias rooted in the models' memorized training data. As LLMs are increasingly used to answer queries about researchers and research communities, their ability to accurately reconstruct scholarly coauthor lists becomes an important but underexamined issue. In this study, we investigate how memorization in LLMs affects the reconstruction of coauthor lists and whether this process reflects existing inequalities across academic disciplines and world regions. We evaluate three prominent models, DeepSeek R1, Llama 4 Scout, and Mixtral 8x7B, by comparing their generated coauthor lists against bibliographic reference data. Our analysis reveals a systematic advantage for highly cited researchers, indicating that LLM memorization disproportionately favors already visible scholars. However, this pattern is not uniform: certain disciplines, such as Clinical Medicine, and some regions, including parts of Africa, exhibit more balanced reconstruction outcomes. These findings highlight both the risks and limitations of relying on LLM-generated relational knowledge in scholarly discovery contexts and emphasize the need for careful auditing of memorization-driven biases in LLM-based systems.",
    "published": "2025-11-01T10:05:43Z",
    "updated": "2026-02-05T08:39:47Z",
    "link": "http://arxiv.org/pdf/2511.00476v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ghazal Kalhor",
      "Afra Mashhadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05444v1",
    "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
    "summary": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.",
    "published": "2026-02-05T08:34:49Z",
    "updated": "2026-02-05T08:34:49Z",
    "link": "http://arxiv.org/pdf/2602.05444v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yao Zhou",
      "Zeen Song",
      "Wenwen Qiang",
      "Fengge Wu",
      "Shuyi Zhou",
      "Changwen Zheng",
      "Hui Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13515v3",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "summary": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "published": "2025-12-15T16:42:51Z",
    "updated": "2026-02-05T08:30:36Z",
    "link": "http://arxiv.org/pdf/2512.13515v3.pdf",
    "category": [
      "cs.SE",
      "cs.CL",
      "cs.LO"
    ],
    "authors": [
      "Oleg Grynets",
      "Vasyl Lyashkevych",
      "Dmytro Baran",
      "Maksym Orliansky",
      "Taras Zelenyy",
      "Markiian Leshchyshyn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05437v1",
    "title": "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models",
    "summary": "Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.",
    "published": "2026-02-05T08:26:44Z",
    "updated": "2026-02-05T08:26:44Z",
    "link": "http://arxiv.org/pdf/2602.05437v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Basel Mousi",
      "Fahim Dalvi",
      "Shammur Chowdhury",
      "Firoj Alam",
      "Nadir Durrani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05419v1",
    "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation",
    "summary": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.",
    "published": "2026-02-05T08:05:42Z",
    "updated": "2026-02-05T08:05:42Z",
    "link": "http://arxiv.org/pdf/2602.05419v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Takumi Goto",
      "Yusuke Sakai",
      "Taro Watanabe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05413v1",
    "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models",
    "summary": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.\n  Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.",
    "published": "2026-02-05T07:52:08Z",
    "updated": "2026-02-05T07:52:08Z",
    "link": "http://arxiv.org/pdf/2602.05413v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Filip Kučera",
      "Christoph Mandl",
      "Isao Echizen",
      "Radu Timofte",
      "Timo Spinde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12300v3",
    "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models",
    "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets poses challenges due to data imbalance and heterogeneity. Existing methods often address these issues across datasets (globally) but overlook the imbalance and heterogeneity within individual datasets (locally), which limits their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a novel method that enables LLMs to autonomously adjust data allocation during fine-tuning both across datasets (globally) and within each individual dataset (locally). HBO employs a bilevel optimization strategy with two types of actors: a Global Actor, which balances data sampling across different subsets of the training mixture, and several Local Actors, which optimizes data usage within each subset based on difficulty levels. These actors are guided by reward functions derived from the LLM's training state, which measure learning progress and relative performance improvement. We evaluate HBO on three LLM backbones across nine diverse tasks in multilingual and multitask setups. Results show that HBO consistently outperforms existing baselines, achieving significant accuracy gains. Our in-depth analysis further demonstrates that both the global actor and local actors of HBO effectively adjust data usage during fine-tuning. HBO provides a comprehensive solution to the challenges of data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training across diverse datasets.",
    "published": "2025-05-18T08:31:44Z",
    "updated": "2026-02-05T07:49:42Z",
    "link": "http://arxiv.org/pdf/2505.12300v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.16252v4",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning",
    "summary": "In recent years, general-purpose large language models (LLMs) such as GPT, Gemini, Claude, and DeepSeek have advanced at an unprecedented pace. Despite these achievements, their application to finance remains challenging, due to fragmented data sources, intransparent reasoning processes, and weak transferability to business applications. In response, we introduce Fin-R1, a reasoning LLM designed for financial scenarios. With a compact size of 7 billion parameters, Fin-R1 reduces deployment costs while addressing the aforementioned challenges. Its development follows a two-stage pipeline. First, we construct Fin-R1-Data, a high-quality financial dataset consisting of 60,091 chain-of-thought (CoT) samples, distilled and filtered from multiple authoritative benchmarks to ensure consistency and reliability. Second, we train Fin-R1 using Fin-R1-Data through supervised fine-tuning (SFT), followed by reinforcement learning (RL). This stage substantially improves the model's ability to solve complex financial reasoning tasks, yielding outputs that are both accurate and interpretable. Despite its relatively small parameter scale, Fin-R1 achieves competitive empirical performance across established financial benchmarks and demonstrates practical utility in compliance checking and robo-advisory. Our code is publicly available at https://github.com/SUFE-AIFLM-Lab/Fin-R1, and has already attracted over 700 stars.",
    "published": "2025-03-20T15:46:18Z",
    "updated": "2026-02-05T07:37:48Z",
    "link": "http://arxiv.org/pdf/2503.16252v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhaowei Liu",
      "Xin Guo",
      "Zhi Yang",
      "Fangqi Lou",
      "Lingfeng Zeng",
      "Mengping Li",
      "Qi Qi",
      "Zhiqiang Liu",
      "Yiyang Han",
      "Dongpo Cheng",
      "Ronghao Chen",
      "Huacan Wang",
      "Xingdong Feng",
      "Huixia Judy Wang",
      "Chengchun Shi",
      "Liwen Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05400v1",
    "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
    "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
    "published": "2026-02-05T07:34:23Z",
    "updated": "2026-02-05T07:34:23Z",
    "link": "http://arxiv.org/pdf/2602.05400v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shaobo Wang",
      "Xuan Ouyang",
      "Tianyi Xu",
      "Yuzheng Hu",
      "Jialin Liu",
      "Guo Chen",
      "Tianyu Zhang",
      "Junhao Zheng",
      "Kexin Yang",
      "Xingzhang Ren",
      "Dayiheng Liu",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20900v3",
    "title": "Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization",
    "summary": "Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality.",
    "published": "2025-09-25T08:36:19Z",
    "updated": "2026-02-05T07:34:02Z",
    "link": "http://arxiv.org/pdf/2509.20900v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.00377v2",
    "title": "DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models",
    "summary": "Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: (i) cross-sequence coupling that concentrates probability mass on high-frequency prefixes, (ii) competitive decoding effects that suppress long-tail concepts, and (iii) scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse - divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 19.6-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models intended for deployment.",
    "published": "2026-01-30T22:56:56Z",
    "updated": "2026-02-05T07:33:51Z",
    "link": "http://arxiv.org/pdf/2602.00377v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhaochen Hong",
      "Jiaxuan You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05393v1",
    "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
    "summary": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.",
    "published": "2026-02-05T07:19:34Z",
    "updated": "2026-02-05T07:19:34Z",
    "link": "http://arxiv.org/pdf/2602.05393v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Ji Zhao",
      "Yufei Gu",
      "Shitong Shao",
      "Xun Zhou",
      "Liang Xiang",
      "Zeke Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05385v1",
    "title": "IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models",
    "summary": "Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.",
    "published": "2026-02-05T07:10:45Z",
    "updated": "2026-02-05T07:10:45Z",
    "link": "http://arxiv.org/pdf/2602.05385v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tao Liu",
      "Jiafan Lu",
      "Bohan Yu",
      "Pengcheng Wu",
      "Liu Haixin",
      "Guoyu Xu",
      "Li Xiangheng",
      "Lixiao Li",
      "Jiaming Hou",
      "Zhao Shijun",
      "Xinglin Lyu",
      "Kunli Zhang",
      "Yuxiang Jia",
      "Hongyin Zan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05374v1",
    "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks",
    "summary": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.",
    "published": "2026-02-05T06:52:46Z",
    "updated": "2026-02-05T06:52:46Z",
    "link": "http://arxiv.org/pdf/2602.05374v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Chaimae Abouzahir",
      "Congbo Ma",
      "Nizar Habash",
      "Farah E. Shamout"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05370v1",
    "title": "PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning",
    "summary": "Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \\ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \\textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.",
    "published": "2026-02-05T06:47:40Z",
    "updated": "2026-02-05T06:47:40Z",
    "link": "http://arxiv.org/pdf/2602.05370v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jun Rao",
      "Zixiong Yu",
      "Xuebo Liu",
      "Guhan Chen",
      "Jing Li",
      "Jiansheng Wei",
      "Xiaojun Meng",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05366v1",
    "title": "Multi-Field Tool Retrieval",
    "summary": "Integrating external tools enables Large Language Models (LLMs) to interact with real-world environments and solve complex tasks. Given the growing scale of available tools, effective tool retrieval is essential to mitigate constraints of LLMs' context windows and ensure computational efficiency. Existing approaches typically treat tool retrieval as a traditional ad-hoc retrieval task, matching user queries against the entire raw tool documentation. In this paper, we identify three fundamental challenges that limit the effectiveness of this paradigm: (i) the incompleteness and structural inconsistency of tool documentation; (ii) the significant semantic and granular mismatch between user queries and technical tool documents; and, most importantly, (iii) the multi-aspect nature of tool utility, that involves distinct dimensions, such as functionality, input constraints, and output formats, varying in format and importance. To address these challenges, we introduce Multi-Field Tool Retrieval, a framework designed to align user intent with tool representations through fine-grained, multi-field modeling. Experimental results show that our framework achieves SOTA performance on five datasets and a mixed benchmark, exhibiting superior generalizability and robustness.",
    "published": "2026-02-05T06:41:01Z",
    "updated": "2026-02-05T06:41:01Z",
    "link": "http://arxiv.org/pdf/2602.05366v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Yichen Tang",
      "Weihang Su",
      "Yiqun Liu",
      "Qingyao Ai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05347v1",
    "title": "How Do Language Models Acquire Character-Level Information?",
    "summary": "Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.",
    "published": "2026-02-05T06:19:51Z",
    "updated": "2026-02-05T06:19:51Z",
    "link": "http://arxiv.org/pdf/2602.05347v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Soma Sato",
      "Ryohei Sasano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.04773v4",
    "title": "Invisible Walls in Cities: Designing LLM Agent to Predict Urban Segregation Experience with Social Media Content",
    "summary": "Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose a novel Large Language Model (LLM) agent to automate online review mining for segregation prediction. Specifically, we propose a reflective LLM coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction.\n  Experiments on real-world data demonstrate that our agent substantially improves prediction accuracy, with a 22.79% elevation in R$^{2}$ and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving places of interest (POIs)' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with Web technology.",
    "published": "2025-02-17T09:52:17Z",
    "updated": "2026-02-05T06:07:03Z",
    "link": "http://arxiv.org/pdf/2503.04773v4.pdf",
    "category": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "authors": [
      "Bingbing Fan",
      "Lin Chen",
      "Songwei Li",
      "Jian Yuan",
      "Fengli Xu",
      "Pan Hui",
      "Yong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2205.11245v5",
    "title": "PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking",
    "summary": "This paper describes the PASH participation in TREC 2021 Deep Learning Track. In the recall stage, we adopt a scheme combining sparse and dense retrieval method. In the multi-stage ranking phase, point-wise and pair-wise ranking strategies are used one after another based on model continual pre-trained on general knowledge and document-level data. Compared to TREC 2020 Deep Learning Track, we have additionally introduced the generative model T5 to further enhance the performance.",
    "published": "2022-05-18T04:38:15Z",
    "updated": "2026-02-05T05:23:06Z",
    "link": "http://arxiv.org/pdf/2205.11245v5.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Yixuan Qiao",
      "Shanshan Zhao",
      "Jun Wang",
      "Hao Chen",
      "Tuozhen Liu",
      "Xianbin Ye",
      "Xin Tang",
      "Rui Fang",
      "Peng Gao",
      "Wenfeng Xie",
      "Guotong Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06042v1",
    "title": "Pseudo-Invertible Neural Networks",
    "summary": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or \"Back-Projection\", $x' = x + A^\\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.",
    "published": "2026-02-05T18:59:58Z",
    "updated": "2026-02-05T18:59:58Z",
    "link": "http://arxiv.org/pdf/2602.06042v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Yamit Ehrlich",
      "Nimrod Berman",
      "Assaf Shocher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06041v1",
    "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning",
    "summary": "Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.",
    "published": "2026-02-05T18:59:55Z",
    "updated": "2026-02-05T18:59:55Z",
    "link": "http://arxiv.org/pdf/2602.06041v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xuejun Zhang",
      "Aditi Tiwari",
      "Zhenhailong Wang",
      "Heng Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06040v1",
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
    "published": "2026-02-05T18:59:51Z",
    "updated": "2026-02-05T18:59:51Z",
    "link": "http://arxiv.org/pdf/2602.06040v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jintao Tong",
      "Shilin Yan",
      "Hongwei Xue",
      "Xiaojun Tang",
      "Kunyu Shi",
      "Guannan Zhang",
      "Ruixuan Li",
      "Yixiong Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06037v1",
    "title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning",
    "summary": "Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.",
    "published": "2026-02-05T18:59:32Z",
    "updated": "2026-02-05T18:59:32Z",
    "link": "http://arxiv.org/pdf/2602.06037v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haoyuan Li",
      "Qihang Cao",
      "Tao Tang",
      "Kun Xiang",
      "Zihan Guo",
      "Jianhua Han",
      "Hang Xu",
      "Xiaodan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06035v1",
    "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
    "summary": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.",
    "published": "2026-02-05T18:59:27Z",
    "updated": "2026-02-05T18:59:27Z",
    "link": "http://arxiv.org/pdf/2602.06035v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "authors": [
      "Sirui Xu",
      "Samuel Schulter",
      "Morteza Ziyadi",
      "Xialin He",
      "Xiaohan Fei",
      "Yu-Xiong Wang",
      "Liangyan Gui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06034v1",
    "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
    "summary": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.",
    "published": "2026-02-05T18:59:21Z",
    "updated": "2026-02-05T18:59:21Z",
    "link": "http://arxiv.org/pdf/2602.06034v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongyang Chen",
      "Chaoyang Wang",
      "Dezhao SU",
      "Xi Xiao",
      "Zeyu Zhang",
      "Jing Xiong",
      "Qing Li",
      "Yuzhang Shang",
      "Shichao Ka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06032v1",
    "title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation",
    "summary": "Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted\" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling\" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/",
    "published": "2026-02-05T18:59:05Z",
    "updated": "2026-02-05T18:59:05Z",
    "link": "http://arxiv.org/pdf/2602.06032v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "David Shavin",
      "Sagie Benaim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06028v1",
    "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
    "summary": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \\textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \\textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \\textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.",
    "published": "2026-02-05T18:58:01Z",
    "updated": "2026-02-05T18:58:01Z",
    "link": "http://arxiv.org/pdf/2602.06028v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuo Chen",
      "Cong Wei",
      "Sun Sun",
      "Ping Nie",
      "Kai Zhou",
      "Ge Zhang",
      "Ming-Hsuan Yang",
      "Wenhu Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06017v1",
    "title": "MambaVF: State Space Model for Efficient Video Fusion",
    "summary": "Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io",
    "published": "2026-02-05T18:53:47Z",
    "updated": "2026-02-05T18:53:47Z",
    "link": "http://arxiv.org/pdf/2602.06017v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zixiang Zhao",
      "Yukun Cui",
      "Lilun Deng",
      "Haowen Bai",
      "Haotong Qin",
      "Tao Feng",
      "Konrad Schindler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05998v1",
    "title": "VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation",
    "summary": "Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.",
    "published": "2026-02-05T18:45:53Z",
    "updated": "2026-02-05T18:45:53Z",
    "link": "http://arxiv.org/pdf/2602.05998v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jie Deng",
      "Kaichun Yao",
      "Libo Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05937v1",
    "title": "Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation",
    "summary": "Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.",
    "published": "2026-02-05T17:47:35Z",
    "updated": "2026-02-05T17:47:35Z",
    "link": "http://arxiv.org/pdf/2602.05937v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lingrui Li",
      "Yanfeng Zhou",
      "Nan Pu",
      "Xin Chen",
      "Zhun Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05909v1",
    "title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.",
    "published": "2026-02-05T17:25:16Z",
    "updated": "2026-02-05T17:25:16Z",
    "link": "http://arxiv.org/pdf/2602.05909v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kangjie Zhang",
      "Wenxuan Huang",
      "Xin Zhou",
      "Boxiang Zhou",
      "Dejia Song",
      "Yuan Xie",
      "Baochang Zhang",
      "Lizhuang Ma",
      "Nemo Chen",
      "Xu Tang",
      "Yao Hu",
      "Shaohui Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05882v1",
    "title": "EoCD: Encoder only Remote Sensing Change Detection",
    "summary": "Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.",
    "published": "2026-02-05T16:58:42Z",
    "updated": "2026-02-05T16:58:42Z",
    "link": "http://arxiv.org/pdf/2602.05882v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mubashir Noman",
      "Mustansar Fiaz",
      "Hiyam Debary",
      "Abdul Hannan",
      "Shah Nawaz",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05880v1",
    "title": "Contour Refinement using Discrete Diffusion in Low Data Regime",
    "summary": "Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.",
    "published": "2026-02-05T16:55:08Z",
    "updated": "2026-02-05T16:55:08Z",
    "link": "http://arxiv.org/pdf/2602.05880v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fei Yu Guan",
      "Ian Keefe",
      "Sophie Wilkinson",
      "Daniel D. B. Perrakis",
      "Steven Waslander"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16001v2",
    "title": "Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning",
    "summary": "Image-to-image translation aims to learn a mapping between a source and a target domain, enabling tasks such as style transfer, appearance transformation, and domain adaptation. In this work, we explore a diffusion-based framework for image-to-image translation by adapting Diffusion Transformers (DiT), which combine the denoising capabilities of diffusion models with the global modeling power of transformers. To guide the translation process, we condition the model on image embeddings extracted from a pre-trained CLIP encoder, allowing for fine-grained and structurally consistent translations without relying on text or class labels. We incorporate both a CLIP similarity loss to enforce semantic consistency and an LPIPS perceptual loss to enhance visual fidelity during training. We validate our approach on two benchmark datasets: face2comics, which translates real human faces to comic-style illustrations, and edges2shoes, which translates edge maps to realistic shoe images. Experimental results demonstrate that DiT, combined with CLIP-based conditioning and perceptual similarity objectives, achieves high-quality, semantically faithful translations, offering a promising alternative to GAN-based models for paired image-to-image translation tasks.",
    "published": "2025-05-21T20:37:33Z",
    "updated": "2026-02-05T16:53:22Z",
    "link": "http://arxiv.org/pdf/2505.16001v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qiang Zhu",
      "Kuan Lu",
      "Menghao Huo",
      "Yuxiao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05871v1",
    "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
    "summary": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.",
    "published": "2026-02-05T16:50:39Z",
    "updated": "2026-02-05T16:50:39Z",
    "link": "http://arxiv.org/pdf/2602.05871v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xunzhi Xiang",
      "Zixuan Duan",
      "Guiyu Zhang",
      "Haiyu Zhang",
      "Zhe Gao",
      "Junta Wu",
      "Shaofeng Zhang",
      "Tengfei Wang",
      "Qi Fan",
      "Chunchao Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.05599v2",
    "title": "Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation",
    "summary": "Convolutional Neural Networks (CNNs) exhibit a well-known texture bias, prioritizing local patterns over global shapes - a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this limitation, we propose a data-driven metric that quantifies the shape-texture balance within a dataset by computing the Structural Similarity Index (SSIM) between an image's luminance (Y) channel and its L0-smoothed counterpart. Building on this metric, we introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results demonstrate consistent accuracy improvements on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.",
    "published": "2026-01-09T07:36:29Z",
    "updated": "2026-02-05T16:49:18Z",
    "link": "http://arxiv.org/pdf/2601.05599v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Takito Sawada",
      "Akinori Iwata",
      "Masahiro Okuda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07346v3",
    "title": "Hidden in Plain Sight -- Class Competition Focuses Attribution Maps",
    "summary": "Attribution methods reveal which input features a neural network uses for a prediction, adding transparency to their decisions. A common problem is that these attributions seem unspecific, highlighting both important and irrelevant features. We revisit the common attribution pipeline and observe that using logits as attribution target is a main cause of this phenomenon. We show that the solution is in plain sight: considering distributions of attributions over multiple classes using existing attribution methods yields specific and fine-grained attributions. On common benchmarks, including the grid-pointing game and randomization-based sanity checks, this improves the ability of 18 attribution methods across 7 architectures up to 2x, agnostic to model architecture.",
    "published": "2025-03-10T13:59:57Z",
    "updated": "2026-02-05T16:34:57Z",
    "link": "http://arxiv.org/pdf/2503.07346v3.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Nils Philipp Walter",
      "Jilles Vreeken",
      "Jonas Fischer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05845v1",
    "title": "Self-Supervised Learning with a Multi-Task Latent Space Objective",
    "summary": "Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.",
    "published": "2026-02-05T16:33:30Z",
    "updated": "2026-02-05T16:33:30Z",
    "link": "http://arxiv.org/pdf/2602.05845v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pierre-François De Plaen",
      "Abhishek Jha",
      "Luc Van Gool",
      "Tinne Tuytelaars",
      "Marc Proesmans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05832v1",
    "title": "UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents",
    "summary": "Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io",
    "published": "2026-02-05T16:21:43Z",
    "updated": "2026-02-05T16:21:43Z",
    "link": "http://arxiv.org/pdf/2602.05832v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Han Xiao",
      "Guozhi Wang",
      "Hao Wang",
      "Shilong Liu",
      "Yuxiang Chai",
      "Yue Pan",
      "Yufeng Zhou",
      "Xiaoxin Chen",
      "Yafei Wen",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05829v1",
    "title": "Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning",
    "summary": "Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.",
    "published": "2026-02-05T16:19:41Z",
    "updated": "2026-02-05T16:19:41Z",
    "link": "http://arxiv.org/pdf/2602.05829v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yudi Shi",
      "Shangzhe Di",
      "Qirui Chen",
      "Qinian Wang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Weidi Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05827v1",
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "summary": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
    "published": "2026-02-05T16:16:13Z",
    "updated": "2026-02-05T16:16:13Z",
    "link": "http://arxiv.org/pdf/2602.05827v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Hai Zhang",
      "Siqi Liang",
      "Li Chen",
      "Yuxian Li",
      "Yukuan Xu",
      "Yichao Zhong",
      "Fu Zhang",
      "Hongyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05822v1",
    "title": "NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects",
    "summary": "We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.",
    "published": "2026-02-05T16:13:53Z",
    "updated": "2026-02-05T16:13:53Z",
    "link": "http://arxiv.org/pdf/2602.05822v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Musawar Ali",
      "Manuel Carranza-García",
      "Nicola Fioraio",
      "Samuele Salti",
      "Luigi Di Stefano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08194v3",
    "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra",
    "summary": "Modern monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet recent works cast doubt on their true understanding of geometric properties. We introduce GOQ, a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images and corresponding 3D meshes of diverse polyhedra covering varying levels of complexity and symmetry, from Platonic, Archimedean, Johnson, and Catalan solids to stellations and compound shapes. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric Platonic solids accurately. Next, although foundation models may be shown via linear and non-linear probing to capture specific 3D symmetry elements, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation. Moreover, advanced vision-language assistants such as ChatGPT, Gemini and Claud exhibit remarkably low accuracy in interpreting basic shape properties such as face geometry, convexity, and compound structures of complex polyhedra. GIQ is publicly available at toomanymatts.github.io/giq-benchmark/, providing a structured platform to benchmark critical gaps in geometric intelligence and facilitate future progress in robust, geometry-aware representation learning.",
    "published": "2025-06-09T20:11:21Z",
    "updated": "2026-02-05T16:06:21Z",
    "link": "http://arxiv.org/pdf/2506.08194v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mateusz Michalkiewicz",
      "Anekha Sokhal",
      "Tadeusz Michalkiewicz",
      "Piotr Pawlikowski",
      "Mahsa Baktashmotlagh",
      "Varun Jampani",
      "Guha Balakrishnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05809v1",
    "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
    "summary": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR",
    "published": "2026-02-05T16:02:48Z",
    "updated": "2026-02-05T16:02:48Z",
    "link": "http://arxiv.org/pdf/2602.05809v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Enwei Tong",
      "Yuanchao Bai",
      "Yao Zhu",
      "Junjun Jiang",
      "Xianming Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13899v2",
    "title": "Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging",
    "summary": "Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.",
    "published": "2026-01-20T12:27:23Z",
    "updated": "2026-02-05T15:54:09Z",
    "link": "http://arxiv.org/pdf/2601.13899v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Masoumeh Javanbakhat",
      "Piotr Komorowski",
      "Dilyara Bareeva",
      "Wei-Chang Lai",
      "Wojciech Samek",
      "Christoph Lippert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15371v5",
    "title": "Event2Vec: Processing Neuromorphic Events Directly by Representations in Vector Space",
    "summary": "Neuromorphic event cameras possess superior temporal resolution, power efficiency, and dynamic range compared to traditional cameras. However, their asynchronous and sparse data format poses a significant challenge for conventional deep learning methods. Existing methods either convert the events into dense synchronous frame representations for processing by powerful CNNs or Transformers, but lose the asynchronous, sparse and high temporal resolution characteristics of events during the conversion process; or adopt irregular models such as sparse convolution, spiking neural networks, or graph neural networks to process the irregular event representations but fail to take full advantage of GPU acceleration. Inspired by word-to-vector models, we draw an analogy between words and events to introduce event2vec, a novel representation that allows neural networks to process events directly. This approach is fully compatible with the parallel processing capabilities of Transformers. We demonstrate the effectiveness of event2vec on the DVS Gesture, ASL-DVS, and DVS-Lip benchmarks, showing that event2vec is remarkably parameter-efficient, features high throughput and low latency, and achieves high accuracy even with an extremely low number of events or low spatial resolutions. Event2vec introduces a novel paradigm by demonstrating for the first time that sparse, irregular event data can be directly integrated into high-throughput Transformer architectures. This breakthrough resolves the long-standing conflict between maintaining data sparsity and maximizing GPU efficiency, offering a promising balance for real-time, low-latency neuromorphic vision tasks. The code is provided in https://github.com/Intelligent-Computing-Lab-Panda/event2vec.",
    "published": "2025-04-21T18:21:18Z",
    "updated": "2026-02-05T15:47:39Z",
    "link": "http://arxiv.org/pdf/2504.15371v5.pdf",
    "category": [
      "cs.CV",
      "cs.NE"
    ],
    "authors": [
      "Wei Fang",
      "Priyadarshini Panda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.00391v3",
    "title": "Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data",
    "summary": "In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (adHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online at https://github.com/alceballosa/robust-vessel-segmentation",
    "published": "2026-01-30T23:10:48Z",
    "updated": "2026-02-05T15:37:06Z",
    "link": "http://arxiv.org/pdf/2602.00391v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alberto Mario Ceballos-Arroyo",
      "Shrikanth M. Yadav",
      "Chu-Hsuan Lin",
      "Jisoo Kim",
      "Geoffrey S. Young",
      "Lei Qin",
      "Huaizu Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22158v2",
    "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
    "summary": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
    "published": "2026-01-29T18:59:56Z",
    "updated": "2026-02-05T15:31:04Z",
    "link": "http://arxiv.org/pdf/2601.22158v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yiyang Lu",
      "Susie Lu",
      "Qiao Sun",
      "Hanhong Zhao",
      "Zhicheng Jiang",
      "Xianbang Wang",
      "Tianhong Li",
      "Zhengyang Geng",
      "Kaiming He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05755v1",
    "title": "FMPose3D: monocular 3D pose estimation via flow matching",
    "summary": "Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.",
    "published": "2026-02-05T15:25:35Z",
    "updated": "2026-02-05T15:25:35Z",
    "link": "http://arxiv.org/pdf/2602.05755v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ti Wang",
      "Xiaohang Yu",
      "Mackenzie Weygandt Mathis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19117v2",
    "title": "Optimized $k$-means color quantization of digital images in machine-based and human perception-based colorspaces",
    "summary": "Color quantization represents an image using a fraction of its original number of colors while only minimally losing its visual quality. The $k$-means algorithm is commonly used in this context, but has mostly been applied in the machine-based RGB colorspace composed of the three primary colors. However, some recent studies have indicated its improved performance in human perception-based colorspaces. We investigated the performance of $k$-means color quantization at four quantization levels in the RGB, CIE-XYZ, and CIE-LUV/CIE-HCL colorspaces, on 148 varied digital images spanning a wide range of scenes, subjects and settings. The Visual Information Fidelity (VIF) measure numerically assessed the quality of the quantized images, and showed that in about half of the cases, $k$-means color quantization is best in the RGB space, while at other times, and especially for higher quantization levels ($k$), the CIE-XYZ colorspace is where it usually does better. There are also some cases, especially at lower $k$, where the best performance is obtained in the CIE-LUV colorspace. Further analysis of the performances in terms of the distributions of the hue, chromaticity and luminance in an image presents a nuanced perspective and characterization of the images for which each colorspace is better for $k$-means color quantization.",
    "published": "2026-01-27T02:42:11Z",
    "updated": "2026-02-05T15:09:16Z",
    "link": "http://arxiv.org/pdf/2601.19117v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "stat.AP"
    ],
    "authors": [
      "Ranjan Maitra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02989v2",
    "title": "SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation",
    "summary": "Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.",
    "published": "2026-02-03T01:50:59Z",
    "updated": "2026-02-05T15:08:33Z",
    "link": "http://arxiv.org/pdf/2602.02989v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhanfeng Liao",
      "Jiajun Zhang",
      "Hanzhang Tu",
      "Zhixi Wang",
      "Yunqi Gao",
      "Hongwen Zhang",
      "Yebin Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05738v1",
    "title": "Disc-Centric Contrastive Learning for Lumbar Spine Severity Grading",
    "summary": "This work examines a disc-centric approach for automated severity grading of lumbar spinal stenosis from sagittal T2-weighted MRI. The method combines contrastive pretraining with disc-level fine-tuning, using a single anatomically localized region of interest per intervertebral disc. Contrastive learning is employed to help the model focus on meaningful disc features and reduce sensitivity to irrelevant differences in image appearance. The framework includes an auxiliary regression task for disc localization and applies weighted focal loss to address class imbalance. Experiments demonstrate a 78.1% balanced accuracy and a reduced severe-to-normal misclassification rate of 2.13% compared with supervised training from scratch. Detecting discs with moderate severity can still be challenging, but focusing on disc-level features provides a practical way to assess the lumbar spinal stenosis.",
    "published": "2026-02-05T15:04:43Z",
    "updated": "2026-02-05T15:04:43Z",
    "link": "http://arxiv.org/pdf/2602.05738v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Sajjan Acharya",
      "Pralisha Kansakar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.17939v2",
    "title": "A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes",
    "summary": "We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.",
    "published": "2025-12-12T13:53:38Z",
    "updated": "2026-02-05T15:04:39Z",
    "link": "http://arxiv.org/pdf/2512.17939v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuncheng Lu",
      "Yucen Shi",
      "Aobo Li",
      "Zehao Li",
      "Junying Li",
      "Bo Wang",
      "Tony Tae-Hyoung Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05737v1",
    "title": "Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing",
    "summary": "In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.",
    "published": "2026-02-05T15:02:07Z",
    "updated": "2026-02-05T15:02:07Z",
    "link": "http://arxiv.org/pdf/2602.05737v1.pdf",
    "category": [
      "cs.CV",
      "cs.NE"
    ],
    "authors": [
      "Luca Ciampi",
      "Ludovico Iannello",
      "Fabrizio Tonelli",
      "Gabriele Lagani",
      "Angelo Di Garbo",
      "Federico Cremisi",
      "Giuseppe Amato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05730v1",
    "title": "Depth as Prior Knowledge for Object Detection",
    "summary": "Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.",
    "published": "2026-02-05T14:52:39Z",
    "updated": "2026-02-05T14:52:39Z",
    "link": "http://arxiv.org/pdf/2602.05730v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05729v1",
    "title": "Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification",
    "summary": "Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.",
    "published": "2026-02-05T14:52:35Z",
    "updated": "2026-02-05T14:52:35Z",
    "link": "http://arxiv.org/pdf/2602.05729v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Lexiang Hu",
      "Youze Xue",
      "Dian Li",
      "Gang Liu",
      "Zhouchen Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05718v1",
    "title": "Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization",
    "summary": "Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.",
    "published": "2026-02-05T14:46:21Z",
    "updated": "2026-02-05T14:46:21Z",
    "link": "http://arxiv.org/pdf/2602.05718v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yunchuan Ma",
      "Laiyun Qing",
      "Guorong Li",
      "Yuqing Liu",
      "Yuankai Qi",
      "Qingming Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.17059v3",
    "title": "REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting",
    "summary": "Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.",
    "published": "2025-11-21T09:07:56Z",
    "updated": "2026-02-05T14:41:41Z",
    "link": "http://arxiv.org/pdf/2511.17059v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Anran Huang",
      "Yuyan Liu",
      "Qiaojun Yu",
      "Shaofan Liu",
      "Liangtu Song",
      "Cewu Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11620v3",
    "title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization",
    "summary": "Ground texture localization using a downward-facing camera offers a low-cost, high-precision localization solution that is robust to dynamic environments and requires no environmental modification. We present a significantly improved bag-of-words (BoW) image retrieval system for ground texture localization, achieving substantially higher accuracy for global localization and higher precision and recall for loop closure detection in SLAM. Our approach leverages an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits the consistent orientation and constant scale constraints inherent to ground texture localization. Identifying the different needs of global localization vs. loop closure detection for SLAM, we present both high-accuracy and high-speed versions of our algorithm. We test the effect of each of our proposed improvements through an ablation study and demonstrate our method's effectiveness for both global localization and loop closure detection. With numerous ground texture localization systems already using BoW, our method can readily replace other generic BoW systems in their pipeline and immediately improve their results.",
    "published": "2025-05-16T18:37:18Z",
    "updated": "2026-02-05T14:41:21Z",
    "link": "http://arxiv.org/pdf/2505.11620v3.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Aaron Wilhelm",
      "Nils Napp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01758v3",
    "title": "Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks",
    "summary": "Diffusion models have shown impressive performance in many visual generation and manipulation tasks. Many existing methods focus on training a model for a specific task, especially, text-to-video (T2V) generation, while many other works focus on finetuning the pretrained T2V model for image-to-video (I2V), video-to-video (V2V), image and video manipulation tasks, etc. However, training a strong T2V foundation model requires a large amount of high-quality annotations, which is very costly. In addition, many existing models can perform only one or several tasks. In this work, we introduce a unified framework, namely many-for-many, which leverages the available training data from many different visual generation and manipulation tasks to train a single model for those different tasks. Specifically, we design a lightweight adapter to unify the different conditions in different tasks, then employ a joint image-video learning strategy to progressively train the model from scratch. Our joint learning leads to a unified visual generation and manipulation model with improved video generation performance. In addition, we introduce depth maps as a condition to help our model better perceive the 3D space in visual generation. Two versions of our model are trained with different model sizes (8B and 2B), each of which can perform more than 10 different tasks. In particular, our 8B model demonstrates highly competitive performance in video generation tasks compared to open-source and even commercial engines. Our models and source codes are available at https://github.com/leeruibin/MfM.git.",
    "published": "2025-06-02T15:05:44Z",
    "updated": "2026-02-05T14:38:57Z",
    "link": "http://arxiv.org/pdf/2506.01758v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ruibin Li",
      "Tao Yang",
      "Yangming Shi",
      "Weiguo Feng",
      "Shilei Wen",
      "Bingyue Peng",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05676v1",
    "title": "ShapeUP: Scalable Image-Conditioned 3D Editing",
    "summary": "Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.",
    "published": "2026-02-05T13:59:16Z",
    "updated": "2026-02-05T13:59:16Z",
    "link": "http://arxiv.org/pdf/2602.05676v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Inbar Gat",
      "Dana Cohen-Bar",
      "Guy Levy",
      "Elad Richardson",
      "Daniel Cohen-Or"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17883v2",
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "summary": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.",
    "published": "2026-01-25T15:28:50Z",
    "updated": "2026-02-05T13:54:52Z",
    "link": "http://arxiv.org/pdf/2601.17883v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Dingkun Liu",
      "Yuheng Chen",
      "Zhu Chen",
      "Zhenyao Cui",
      "Yaozhi Wen",
      "Jiayu An",
      "Jingwei Luo",
      "Dongrui Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12788v2",
    "title": "Efficient Scene Modeling via Structure-Aware and Region-Prioritized 3D Gaussians",
    "summary": "Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4$\\times$ fewer Gaussians and 3$\\times$ faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.",
    "published": "2024-11-19T11:47:40Z",
    "updated": "2026-02-05T13:49:34Z",
    "link": "http://arxiv.org/pdf/2411.12788v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Guangchi Fang",
      "Bing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21950v2",
    "title": "Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach",
    "summary": "Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional performance across diverse tasks, continually surpassing previous expectations regarding their capabilities. Nevertheless, their proficiency in perceiving emotions from images remains debated, with studies yielding divergent results in zero-shot scenarios. We argue that this inconsistency stems partly from constraints in existing evaluation methods, including the oversight of plausible responses, limited emotional taxonomies, neglect of contextual factors, and labor-intensive annotations. To facilitate customized visual emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task that overcomes these constraints. Complementing this task, we devise an automated pipeline that efficiently constructs emotion-centric statements with minimal human effort. Through systematically evaluating prevailing MLLMs, our study showcases their stronger performance in emotion interpretation and context-based emotion judgment, while revealing relative limitations in comprehending perception subjectivity. When compared to humans, even top-performing MLLMs like GPT4o demonstrate remarkable performance gaps, underscoring key areas for future improvement. By developing a fundamental evaluation framework and conducting a comprehensive MLLM assessment, we hope this work contributes to advancing emotional intelligence in MLLMs. Project page: https://github.com/wdqqdw/MVEI.",
    "published": "2025-09-26T06:30:39Z",
    "updated": "2026-02-05T13:38:54Z",
    "link": "http://arxiv.org/pdf/2509.21950v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Daiqing Wu",
      "Dongbao Yang",
      "Sicheng Zhao",
      "Can Ma",
      "Yu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.04672v3",
    "title": "Histo-Miner: Deep learning based tissue features extraction pipeline from H&E whole slide images of cutaneous squamous cell carcinoma",
    "summary": "Recent advancements in digital pathology have enabled comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution microscopy and computational capabilities. Despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based pipeline for analysis of skin WSIs and generate two datasets with labeled nuclei and tumor regions. We develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients, Histo-Miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. Performance of trained models positively compares to state of the art with multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, macro-averaged F1 of 0.832 for nucleus classification and mean Intersection over Union (mIoU) of 0.907 for tumor region segmentation. From these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. Here, we use Histo-Miner to predict cSCC patient response to immunotherapy based on pre-treatment WSIs from 45 patients. Histo-Miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. This highlights the applicability of Histo-Miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.",
    "published": "2025-05-07T09:34:03Z",
    "updated": "2026-02-05T13:35:50Z",
    "link": "http://arxiv.org/pdf/2505.04672v3.pdf",
    "category": [
      "cs.CV",
      "q-bio.QM"
    ],
    "authors": [
      "Lucas Sancéré",
      "Carina Lorenz",
      "Doris Helbig",
      "Oana-Diana Persa",
      "Sonja Dengler",
      "Alexander Kreuter",
      "Martim Laimer",
      "Roland Lang",
      "Anne Fröhlich",
      "Jennifer Landsberg",
      "Johannes Brägelmann",
      "Katarzyna Bozek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05638v1",
    "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
    "summary": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.",
    "published": "2026-02-05T13:18:33Z",
    "updated": "2026-02-05T13:18:33Z",
    "link": "http://arxiv.org/pdf/2602.05638v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jinlin Wu",
      "Felix Holm",
      "Chuxi Chen",
      "An Wang",
      "Yaxin Hu",
      "Xiaofan Ye",
      "Zelin Zang",
      "Miao Xu",
      "Lihua Zhou",
      "Huai Liao",
      "Danny T. M. Chan",
      "Ming Feng",
      "Wai S. Poon",
      "Hongliang Ren",
      "Dong Yi",
      "Nassir Navab",
      "Gaofeng Meng",
      "Jiebo Luo",
      "Hongbin Liu",
      "Zhen Lei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05629v1",
    "title": "ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing",
    "summary": "Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.",
    "published": "2026-02-05T13:09:58Z",
    "updated": "2026-02-05T13:09:58Z",
    "link": "http://arxiv.org/pdf/2602.05629v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jianlei Chi",
      "Yuzhen Wu",
      "Jiaxuan Hou",
      "Xiaodong Zhang",
      "Ming Fan",
      "Suhui Sun",
      "Weijun Dai",
      "Bo Li",
      "Jianguo Sun",
      "Jun Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05617v1",
    "title": "Unified Sensor Simulation for Autonomous Driving",
    "summary": "In this work, we introduce \\textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \\href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.",
    "published": "2026-02-05T12:52:46Z",
    "updated": "2026-02-05T12:52:46Z",
    "link": "http://arxiv.org/pdf/2602.05617v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Nikolay Patakin",
      "Arsenii Shirokov",
      "Anton Konushin",
      "Dmitry Senushkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05602v1",
    "title": "Multi-instance robust fitting for non-classical geometric models",
    "summary": "Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.",
    "published": "2026-02-05T12:38:38Z",
    "updated": "2026-02-05T12:38:38Z",
    "link": "http://arxiv.org/pdf/2602.05602v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zongliang Zhang",
      "Shuxiang Li",
      "Xingwang Huang",
      "Zongyue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12307v2",
    "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding",
    "summary": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.",
    "published": "2025-12-13T12:26:57Z",
    "updated": "2026-02-05T12:34:19Z",
    "link": "http://arxiv.org/pdf/2512.12307v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Benjamin Beilharz",
      "Thomas S. A. Wallis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05590v1",
    "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
    "summary": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.",
    "published": "2026-02-05T12:17:35Z",
    "updated": "2026-02-05T12:17:35Z",
    "link": "http://arxiv.org/pdf/2602.05590v1.pdf",
    "category": [
      "cs.CV",
      "cs.ET",
      "cs.GR"
    ],
    "authors": [
      "Haojie Cheng",
      "Shaun Jing Heng Ong",
      "Shaoyu Cai",
      "Aiden Tat Yang Koh",
      "Fuxi Ouyang",
      "Eng Tat Khoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05588v1",
    "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
    "summary": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.",
    "published": "2026-02-05T12:17:05Z",
    "updated": "2026-02-05T12:17:05Z",
    "link": "http://arxiv.org/pdf/2602.05588v1.pdf",
    "category": [
      "cs.CV",
      "cs.ET",
      "cs.GR"
    ],
    "authors": [
      "Haojie Cheng",
      "Chang Liu",
      "Abhiram Kanneganti",
      "Mahesh Arjandas Choolani",
      "Arundhati Tushar Gosavi",
      "Eng Tat Khoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05582v1",
    "title": "Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation",
    "summary": "We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).\n  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.\n  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.",
    "published": "2026-02-05T12:12:00Z",
    "updated": "2026-02-05T12:12:00Z",
    "link": "http://arxiv.org/pdf/2602.05582v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Joe-Mei Feng",
      "Sheng-Wei Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05578v1",
    "title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation",
    "summary": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.",
    "published": "2026-02-05T12:03:11Z",
    "updated": "2026-02-05T12:03:11Z",
    "link": "http://arxiv.org/pdf/2602.05578v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junyang Chen",
      "Xiangbo Lv",
      "Zhiqiang Kou",
      "Xingdong Sheng",
      "Ning Xu",
      "Yiguo Qiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24162v2",
    "title": "Deep Probabilistic Supervision for Image Classification",
    "summary": "Supervised training of deep neural networks for classification typically relies on hard targets, which promote overconfidence and can limit calibration, generalization, and robustness. Self-distillation methods aim to mitigate this by leveraging inter-class and sample-specific information present in the model's own predictions, but often remain dependent on hard targets without explicitly modeling predictive uncertainty. With this in mind, we propose Deep Probabilistic Supervision (DPS), a principled learning framework constructing sample-specific target distributions via statistical inference on the model's own predictions, remaining independent of hard targets after initialization. We show that DPS consistently yields higher test accuracy (e.g., +2.0% for DenseNet-264 on ImageNet) and significantly lower Expected Calibration Error (ECE) (-40% ResNet-50, CIFAR-100) than existing self-distillation methods. When combined with a contrastive loss, DPS achieves state-of-the-art robustness under label noise.",
    "published": "2025-12-30T11:48:06Z",
    "updated": "2026-02-05T12:01:11Z",
    "link": "http://arxiv.org/pdf/2512.24162v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anton Adelöw",
      "Matteo Gamba",
      "Atsuto Maki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05577v1",
    "title": "LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization",
    "summary": "Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.",
    "published": "2026-02-05T12:01:09Z",
    "updated": "2026-02-05T12:01:09Z",
    "link": "http://arxiv.org/pdf/2602.05577v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shiyu Wu",
      "Shuyan Li",
      "Jing Li",
      "Jing Liu",
      "Yequan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04657v2",
    "title": "PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective",
    "summary": "Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\\times$ prefill speedup, 2.11$\\times$ inference speedup, 6.22$\\times$ lower FLOPs, and 6.05$\\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.",
    "published": "2026-02-04T15:33:10Z",
    "updated": "2026-02-05T12:00:10Z",
    "link": "http://arxiv.org/pdf/2602.04657v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haokui Zhang",
      "Congyang Ou",
      "Dawei Yan",
      "Peng Wang",
      "Qingsen Yan",
      "Ying Li",
      "Rong Xiao",
      "Chunhua Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.07163v2",
    "title": "Test-time Adaptive Hierarchical Co-enhanced Denoising Network for Reliable Multimodal Classification",
    "summary": "Reliable learning of multimodal data (e.g., multi-omics) is a widely concerning issue, especially in safety-critical applications such as medical diagnosis. However, low-quality data induced by multimodal noise poses a major challenge in this domain, causing existing methods to suffer from two key limitations. First, they struggle to handle heterogeneous data noise, hindering robust multimodal representation learning. Second, they exhibit limited adaptability and generalization when encountering previously unseen noise. To address these issues, we propose Test-time Adaptive Hierarchical Co-enhanced Denoising Network (TAHCD). On one hand, TAHCD introduces the Adaptive Stable Subspace Alignment and Sample-Adaptive Confidence Alignment to reliably remove heterogeneous noise. They account for noise at both global and instance levels and enable jointly removal of modality-specific and cross-modality noise, achieving robust learning. On the other hand, TAHCD introduces Test-Time Cooperative Enhancement, which adaptively updates the model in response to input noise in a label-free manner, thus improving generalization. This is achieved by collaboratively enhancing the joint removal process of modality-specific and cross-modality noise across global and instance levels according to sample noise. Experiments on multiple benchmarks demonstrate that the proposed method achieves superior classification performance, robustness, and generalization compared with state-of-the-art reliable multimodal learning approaches.",
    "published": "2026-01-12T03:14:12Z",
    "updated": "2026-02-05T11:58:06Z",
    "link": "http://arxiv.org/pdf/2601.07163v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shu Shen",
      "C. L. Philip Chen",
      "Tong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05574v1",
    "title": "A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features",
    "summary": "Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.",
    "published": "2026-02-05T11:57:20Z",
    "updated": "2026-02-05T11:57:20Z",
    "link": "http://arxiv.org/pdf/2602.05574v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mengyu Li",
      "Ingibjörg Kristjánsdóttir",
      "Thilo van Eimeren",
      "Kathrin Giehl",
      "Lotta M. Ellingsen",
      "the ASAP Neuroimaging Initiative"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05573v1",
    "title": "Visual Implicit Geometry Transformer for Autonomous Driving",
    "summary": "We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \\href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.",
    "published": "2026-02-05T11:54:38Z",
    "updated": "2026-02-05T11:54:38Z",
    "link": "http://arxiv.org/pdf/2602.05573v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Arsenii Shirokov",
      "Mikhail Kuznetsov",
      "Danila Stepochkin",
      "Egor Evdokimov",
      "Daniil Glazkov",
      "Nikolay Patakin",
      "Anton Konushin",
      "Dmitry Senushkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05572v1",
    "title": "ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors",
    "summary": "We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.",
    "published": "2026-02-05T11:52:15Z",
    "updated": "2026-02-05T11:52:15Z",
    "link": "http://arxiv.org/pdf/2602.05572v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhenxiao Liang",
      "Ning Zhang",
      "Youbao Tang",
      "Ruei-Sung Lin",
      "Qixing Huang",
      "Peng Chang",
      "Jing Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08520v2",
    "title": "Plug-and-play linear attention with provable guarantees for training-free image restoration",
    "summary": "Multi-head self-attention (MHSA) is a key building block in modern vision Transformers, yet its quadratic complexity in the number of tokens remains a major bottleneck for real-time and resource-constrained deployment. We present PnP-Nystra, a training-free Nyström-based linear attention module designed as a plug-and-play replacement for MHSA in {pretrained} image restoration Transformers, with provable kernel approximation error guarantees. PnP-Nystra integrates directly into window-based architectures such as SwinIR, Uformer, and Dehazeformer, yielding efficient inference without finetuning. Across denoising, deblurring, dehazing, and super-resolution on images, PnP-Nystra delivers $1.8$--$3.6\\times$ speedups on an NVIDIA RTX 4090 GPU and $1.8$--$7\\times$ speedups on CPU inference. Compared with the strongest training-free linear-attention baselines we evaluate, our method incurs the smallest quality drop and stays closest to the original model's outputs.",
    "published": "2025-06-10T07:37:41Z",
    "updated": "2026-02-05T11:35:14Z",
    "link": "http://arxiv.org/pdf/2506.08520v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Srinivasan Kidambi",
      "Karthik Palaniappan",
      "Pravin Nair"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05557v1",
    "title": "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds",
    "summary": "We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.",
    "published": "2026-02-05T11:29:09Z",
    "updated": "2026-02-05T11:29:09Z",
    "link": "http://arxiv.org/pdf/2602.05557v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Michael Schwingshackl",
      "Fabio F. Oberweger",
      "Mario Niedermeyer",
      "Huemer Johannes",
      "Markus Murschitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05555v1",
    "title": "IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools",
    "summary": "We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.",
    "published": "2026-02-05T11:28:57Z",
    "updated": "2026-02-05T11:28:57Z",
    "link": "http://arxiv.org/pdf/2602.05555v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Panagiotis Sapoutzoglou",
      "Orestis Vaggelis",
      "Athina Zacharia",
      "Evangelos Sartinas",
      "Maria Pateraki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05552v1",
    "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator",
    "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.",
    "published": "2026-02-05T11:23:11Z",
    "updated": "2026-02-05T11:23:11Z",
    "link": "http://arxiv.org/pdf/2602.05552v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Bessie Dominguez-Dager",
      "Sergio Suescun-Ferrandiz",
      "Felix Escalona",
      "Francisco Gomez-Donoso",
      "Miguel Cazorla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05551v1",
    "title": "FastVMT: Eliminating Redundancy in Video Motion Transfer",
    "summary": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.",
    "published": "2026-02-05T11:15:59Z",
    "updated": "2026-02-05T11:15:59Z",
    "link": "http://arxiv.org/pdf/2602.05551v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yue Ma",
      "Zhikai Wang",
      "Tianhao Ren",
      "Mingzhe Zheng",
      "Hongyu Liu",
      "Jiayi Guo",
      "Mark Fong",
      "Yuxuan Xue",
      "Zixiang Zhao",
      "Konrad Schindler",
      "Qifeng Chen",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06609v2",
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "summary": "Vector-quantized variational autoencoders (VQ-VAEs) are discrete autoencoders that compress images into discrete tokens. However, they are difficult to train due to discretization. In this paper, we propose a simple yet effective technique dubbed Gaussian Quant (GQ), which first trains a Gaussian VAE under certain constraints and then converts it into a VQ-VAE without additional training. For conversion, GQ generates random Gaussian noise as a codebook and finds the closest noise vector to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAEs for effective conversion, named the target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in the supplementary materials.",
    "published": "2025-12-07T00:57:58Z",
    "updated": "2026-02-05T11:09:27Z",
    "link": "http://arxiv.org/pdf/2512.06609v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Tongda Xu",
      "Wendi Zheng",
      "Jiajun He",
      "Jose Miguel Hernandez-Lobato",
      "Yan Wang",
      "Ya-Qin Zhang",
      "Jie Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05538v1",
    "title": "A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments",
    "summary": "Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.",
    "published": "2026-02-05T10:53:35Z",
    "updated": "2026-02-05T10:53:35Z",
    "link": "http://arxiv.org/pdf/2602.05538v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Malaz Tamim",
      "Andrea Matic-Flierl",
      "Karsten Roscher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05534v1",
    "title": "SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation",
    "summary": "Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.",
    "published": "2026-02-05T10:48:58Z",
    "updated": "2026-02-05T10:48:58Z",
    "link": "http://arxiv.org/pdf/2602.05534v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Youngwoo Shin",
      "Jiwan Hur",
      "Junmo Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05527v1",
    "title": "Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains",
    "summary": "Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \\pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \\pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.",
    "published": "2026-02-05T10:39:00Z",
    "updated": "2026-02-05T10:39:00Z",
    "link": "http://arxiv.org/pdf/2602.05527v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ben Isselmann",
      "Dilara Göksu",
      "Andreas Weinmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05522v1",
    "title": "Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification",
    "summary": "Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.",
    "published": "2026-02-05T10:30:40Z",
    "updated": "2026-02-05T10:30:40Z",
    "link": "http://arxiv.org/pdf/2602.05522v1.pdf",
    "category": [
      "cs.CV",
      "math.GT"
    ],
    "authors": [
      "Jeongbin You",
      "Donggun Kim",
      "Sejun Park",
      "Seungsang Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22650v2",
    "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
    "summary": "Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach achieves strong performance and surpasses prior methods on most datasets, establishing a new state of the art without fine-tuning, additional components and complex reasoning.",
    "published": "2025-09-26T17:59:57Z",
    "updated": "2026-02-05T10:20:31Z",
    "link": "http://arxiv.org/pdf/2509.22650v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anna Kukleva",
      "Enis Simsar",
      "Alessio Tonioni",
      "Muhammad Ferjad Naeem",
      "Federico Tombari",
      "Jan Eric Lenssen",
      "Bernt Schiele"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13772v2",
    "title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification",
    "summary": "Feature engineering continues to play a critical role in image classification, particularly when interpretability and computational efficiency are prioritized over deep learning models with millions of parameters. In this study, we revisit classical machine learning based image classification through a novel approach centered on Permutation Entropy (PE), a robust and computationally lightweight measure traditionally used in time series analysis but rarely applied to image data. We extend PE to two-dimensional images and propose a multiscale, multi-orientation entropy-based feature extraction approach that characterizes spatial order and complexity along rows, columns, diagonals, anti-diagonals, and local patches of the image. To enhance the discriminatory power of the entropy features, we integrate two classic image descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an image. The resulting hand-crafted feature set, comprising of 780 dimensions, is used to train Support Vector Machine (SVM) classifiers optimized through grid search. The proposed approach is evaluated on multiple benchmark datasets, including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers competitive classification performance without relying on deep architectures. Our results demonstrate that the fusion of PE with HOG and LBP provides a compact, interpretable, and effective alternative to computationally expensive and limited interpretable deep learning models. This shows a potential of entropy-based descriptors in image classification and contributes a lightweight and generalizable solution to interpretable machine learning in image classification and computer vision.",
    "published": "2025-07-18T09:29:03Z",
    "updated": "2026-02-05T10:19:50Z",
    "link": "http://arxiv.org/pdf/2507.13772v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Abhijit Sen",
      "Giridas Maiti",
      "Bikram K. Parida",
      "Bhanu P. Mishra",
      "Mahima Arya",
      "Denys I. Bondar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05508v1",
    "title": "VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency",
    "summary": "Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.",
    "published": "2026-02-05T10:07:11Z",
    "updated": "2026-02-05T10:07:11Z",
    "link": "http://arxiv.org/pdf/2602.05508v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhuang Xiong",
      "Chen Zhang",
      "Qingshan Xu",
      "Wenbing Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15602v4",
    "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?",
    "summary": "Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks from the stroke level to the rally level and includes 2527 human-verified questions. Evaluating 17 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.",
    "published": "2025-09-19T05:08:05Z",
    "updated": "2026-02-05T09:55:18Z",
    "link": "http://arxiv.org/pdf/2509.15602v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhongyuan Bao",
      "Lejun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05487v1",
    "title": "Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014",
    "summary": "What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).",
    "published": "2026-02-05T09:49:33Z",
    "updated": "2026-02-05T09:49:33Z",
    "link": "http://arxiv.org/pdf/2602.05487v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Julien Moreau",
      "S. Ambellouis",
      "Yassine Ruichek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05480v1",
    "title": "SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing",
    "summary": "Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.",
    "published": "2026-02-05T09:39:49Z",
    "updated": "2026-02-05T09:39:49Z",
    "link": "http://arxiv.org/pdf/2602.05480v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peihao Wu",
      "Yongxiang Yao",
      "Yi Wan",
      "Wenfei Zhang",
      "Ruipeng Zhao",
      "Jiayuan Li",
      "Yongjun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24212v2",
    "title": "RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation",
    "summary": "Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.",
    "published": "2025-12-30T13:25:22Z",
    "updated": "2026-02-05T09:33:50Z",
    "link": "http://arxiv.org/pdf/2512.24212v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Ming-Ming Yu",
      "Yi Chen",
      "Börje F. Karlsson",
      "Wenjun Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19447v3",
    "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
    "summary": "Self-Supervised Learning (SSL) enables us to pre-train foundation models without costly labeled data. Among SSL methods, Contrastive Learning (CL) methods are better at obtaining accurate semantic representations in noise interference. However, due to the significant domain gap, while CL methods have achieved great success in many computer vision tasks, they still require specific adaptation for Remote Sensing (RS) images. To this end, we present a novel self-supervised method called PerA, which produces all-purpose RS features through semantically Perfectly Aligned sample pairs. Specifically, PerA obtains features from sampled views by applying spatially disjoint masks to augmented images rather than random cropping. Our framework provides high-quality features by ensuring consistency between teacher and student and predicting learnable mask tokens. Compared to previous contrastive methods, our method demonstrates higher memory efficiency and can be trained with larger batches due to its sparse inputs. Additionally, the proposed method demonstrates remarkable adaptability to uncurated RS data and reduce the impact of the potential semantic inconsistency. We also collect an unlabeled pre-training dataset, which contains about 5 million RS images. We conducted experiments on multiple downstream task datasets and achieved performance comparable to previous state-of-the-art methods with a limited model scale, demonstrating the effectiveness of our approach. We hope this work will contribute to practical remote sensing interpretation works.",
    "published": "2025-05-26T03:12:49Z",
    "updated": "2026-02-05T09:11:47Z",
    "link": "http://arxiv.org/pdf/2505.19447v3.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Hengtong Shen",
      "Haiyan Gu",
      "Haitao Li",
      "Yi Yang",
      "Agen Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23646v2",
    "title": "Active Perception Agent for Omnimodal Audio-Video Understanding",
    "summary": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often face challenges in fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, to our best knowledge, the first fully active perception agent that dynamically orchestrates specialized unimodal tools to achieve more fine-grained omnimodal reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, we demonstrate a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and closed-source models by substantial margins of 10% - 20% accuracy without training.",
    "published": "2025-12-29T17:59:05Z",
    "updated": "2026-02-05T09:08:35Z",
    "link": "http://arxiv.org/pdf/2512.23646v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Keda Tao",
      "Wenjie Du",
      "Bohan Yu",
      "Weiqiang Wang",
      "Jian Liu",
      "Huan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.17732v3",
    "title": "DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt State Space Model",
    "summary": "All-in-One image restoration aims to address multiple image degradation problems using a single model, offering a more practical and versatile solution compared to designing dedicated models for each degradation type. Existing approaches typically rely on Degradation-specific models or coarse-grained degradation prompts to guide image restoration. However, they lack fine-grained modeling of degradation information and face limitations in balancing multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a novel All-in-One image restoration framework that introduces a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM). The DP-SSM leverages the fine-grained degradation features captured by the extractor as dynamic prompts, which are then incorporated into the state space modeling process. This enhances the model's adaptability to diverse degradation types, while a complementary High-Frequency Enhancement Block (HEB) recovers local high-frequency details. Extensive experiments on a mixed dataset containing seven degradation types show that DPMambaIR achieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These results highlight the potential and superiority of DPMambaIR as a unified solution for All-in-One image restoration.",
    "published": "2025-04-24T16:46:32Z",
    "updated": "2026-02-05T09:06:01Z",
    "link": "http://arxiv.org/pdf/2504.17732v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhanwen Liu",
      "Sai Zhou",
      "Yuchao Dai",
      "Yang Wang",
      "Yisheng An",
      "Xiangmo Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.22120v2",
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "summary": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
    "published": "2025-12-26T18:59:47Z",
    "updated": "2026-02-05T08:49:21Z",
    "link": "http://arxiv.org/pdf/2512.22120v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuoshuo Zhang",
      "Yizhen Zhang",
      "Jingjing Fu",
      "Lei Song",
      "Jiang Bian",
      "Yujiu Yang",
      "Rui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05440v1",
    "title": "Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations",
    "summary": "In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.",
    "published": "2026-02-05T08:28:44Z",
    "updated": "2026-02-05T08:28:44Z",
    "link": "http://arxiv.org/pdf/2602.05440v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Natascha Jeziorski",
      "Petra Gospodnetić",
      "Claudia Redenbach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.22936v2",
    "title": "PPE: Positional Preservation Embedding for Token Compression in Multimodal Large Language Models",
    "summary": "Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks, yet often suffer from inefficiencies due to redundant visual tokens. Existing token merging methods reduce sequence length but frequently disrupt spatial layouts and temporal continuity by disregarding positional relationships. In this work, we propose a novel encoding operator dubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding (\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal structure during visual token compression. PPE explicitly introduces the disentangled encoding of 3D positions in the token dimension, enabling each compressed token to encapsulate different positions from multiple original tokens. Furthermore, we show that PPE can effectively support cascade clustering -- a progressive token compression strategy that leads to better performance retention. PPE is a parameter-free and generic operator that can be seamlessly integrated into existing token merging methods without any adjustments. Applied to state-of-the-art token merging framework, PPE achieves consistent improvements of $2\\%\\sim5\\%$ across multiple vision-language benchmarks, including MMBench (general vision understanding), TextVQA (layout understanding) and VideoMME (temporal understanding). These results demonstrate that preserving positional cues is critical for efficient and effective MLLM reasoning. Our code is available at https://github.com/MouxiaoHuang/PPE.",
    "published": "2025-10-27T02:40:02Z",
    "updated": "2026-02-05T08:25:22Z",
    "link": "http://arxiv.org/pdf/2510.22936v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mouxiao Huang",
      "Borui Jiang",
      "Dehua Zheng",
      "Hailin Hu",
      "Kai Han",
      "Xinghao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05435v1",
    "title": "Stable Velocity: A Variance Perspective on Flow Matching",
    "summary": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
    "published": "2026-02-05T08:25:05Z",
    "updated": "2026-02-05T08:25:05Z",
    "link": "http://arxiv.org/pdf/2602.05435v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Donglin Yang",
      "Yongxing Zhang",
      "Xin Yu",
      "Liang Hou",
      "Xin Tao",
      "Pengfei Wan",
      "Xiaojuan Qi",
      "Renjie Liao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05434v1",
    "title": "LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects",
    "summary": "Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.",
    "published": "2026-02-05T08:24:38Z",
    "updated": "2026-02-05T08:24:38Z",
    "link": "http://arxiv.org/pdf/2602.05434v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sanghoon Jeon",
      "Gihyun Jung",
      "Suhyeon Ka",
      "Jae-Sang Hyun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05426v1",
    "title": "Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications",
    "summary": "Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.",
    "published": "2026-02-05T08:17:42Z",
    "updated": "2026-02-05T08:17:42Z",
    "link": "http://arxiv.org/pdf/2602.05426v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wahyu Rahmaniar",
      "Kenji Suzuki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05423v1",
    "title": "NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks",
    "summary": "In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.",
    "published": "2026-02-05T08:15:06Z",
    "updated": "2026-02-05T08:15:06Z",
    "link": "http://arxiv.org/pdf/2602.05423v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Pengcheng Chen",
      "Yue Hu",
      "Wenhao Li",
      "Nicole M Gunderson",
      "Andrew Feng",
      "Zhenglong Sun",
      "Peter Beerel",
      "Eric J Seibel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13812v3",
    "title": "Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning",
    "summary": "Existing point cloud representation learning methods primarily rely on data-driven strategies to extract geometric information from large amounts of scattered data. However, most methods focus solely on the spatial distribution features of point clouds while overlooking the relationship between local information and the whole structure, which limits the accuracy of point cloud representation. Local information reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the object's shape. In real-world, objects undergo deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the object's geometric features. Therefore, appropriately introducing a physics-driven mechanism to capture the topological relationships between local parts and the whole object can effectively mitigate for the limitations of data-driven point cloud methods in structural modeling, and enhance the generalization and interpretability of point cloud representations for downstream tasks such as understanding and recognition. Inspired by this, we incorporate a physics-driven mechanism into the data-driven method to learn fine-grained features in point clouds and model the structural relationship between local regions and the whole shape. Specifically, we design a dual-task encoder-decoder framework that combines the geometric modeling capability of data-driven implicit fields with physics-driven elastic deformation. Through the integration of physics-based loss functions, the framework is guided to predict localized deformation and explicitly capture the correspondence between local structural changes and whole shape variations.",
    "published": "2025-05-20T01:48:32Z",
    "updated": "2026-02-05T08:12:34Z",
    "link": "http://arxiv.org/pdf/2505.13812v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhongyu Chen",
      "Rong Zhao",
      "Xie Han",
      "Xindong Guo",
      "Song Wang",
      "Zherui Qiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05415v1",
    "title": "VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection",
    "summary": "Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.",
    "published": "2026-02-05T07:58:12Z",
    "updated": "2026-02-05T07:58:12Z",
    "link": "http://arxiv.org/pdf/2602.05415v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ningkang Peng",
      "Qianfeng Yu",
      "Yuhao Zhang",
      "Yafei Liu",
      "Xiaoqian Peng",
      "Peirong Ma",
      "Yi Chen",
      "Peiheng Li",
      "Yanhui Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05414v1",
    "title": "TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions",
    "summary": "Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.",
    "published": "2026-02-05T07:52:37Z",
    "updated": "2026-02-05T07:52:37Z",
    "link": "http://arxiv.org/pdf/2602.05414v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ngoc Doan-Minh Huynh",
      "Duong Nguyen-Ngoc Tran",
      "Long Hoang Pham",
      "Tai Huu-Phuong Tran",
      "Hyung-Joon Jeon",
      "Huy-Hung Nguyen",
      "Duong Khac Vu",
      "Hyung-Min Jeon",
      "Son Hong Phan",
      "Quoc Pham-Nam Ho",
      "Chi Dai Tran",
      "Trinh Le Ba Khanh",
      "Jae Wook Jeon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.10829v3",
    "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation",
    "summary": "Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks.",
    "published": "2025-04-15T03:12:01Z",
    "updated": "2026-02-05T07:47:50Z",
    "link": "http://arxiv.org/pdf/2504.10829v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hengyu Shi",
      "Junhao Su",
      "Tianyang Han",
      "Junfeng Luo",
      "Jialin Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05397v1",
    "title": "Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing",
    "summary": "Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., \"second-order moment\") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.",
    "published": "2026-02-05T07:28:54Z",
    "updated": "2026-02-05T07:28:54Z",
    "link": "http://arxiv.org/pdf/2602.05397v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuechen Yang",
      "Junlin Guo",
      "Ruining Deng",
      "Junchao Zhu",
      "Zhengyi Lu",
      "Chongyu Qu",
      "Yanfan Zhu",
      "Xingyi Guo",
      "Yu Wang",
      "Shilin Zhao",
      "Haichun Yang",
      "Yuankai Huo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.12636v2",
    "title": "Image inpainting for corrupted images by using the semi-super resolution GAN",
    "summary": "Image inpainting is a valuable technique for enhancing images that have been corrupted. The primary challenge in this research revolves around the extent of corruption in the input image that the deep learning model must restore. To address this challenge, we introduce a Generative Adversarial Network (GAN) for learning and replicating the missing pixels. Additionally, we have developed a distinct variant of the Super-Resolution GAN (SRGAN), which we refer to as the Semi-SRGAN (SSRGAN). Furthermore, we leveraged three diverse datasets to assess the robustness and accuracy of our proposed model. Our training process involves varying levels of pixel corruption to attain optimal accuracy and generate high-quality images.",
    "published": "2024-09-19T10:21:16Z",
    "updated": "2026-02-05T07:25:37Z",
    "link": "http://arxiv.org/pdf/2409.12636v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Mehrshad Momen-Tayefeh",
      "Mehrdad Momen-Tayefeh",
      "Amir Ali Ghafourian Ghahramani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.02103v3",
    "title": "MVGS: Multi-view Regulated Gaussian Splatting for Novel View Synthesis",
    "summary": "Recent works in volume rendering, \\textit{e.g.} NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.",
    "published": "2024-10-02T23:48:31Z",
    "updated": "2026-02-05T07:22:17Z",
    "link": "http://arxiv.org/pdf/2410.02103v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaobiao Du",
      "Yida Wang",
      "Xin Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05391v1",
    "title": "Dataset Distillation via Relative Distribution Matching and Cognitive Heritage",
    "summary": "Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.",
    "published": "2026-02-05T07:18:48Z",
    "updated": "2026-02-05T07:18:48Z",
    "link": "http://arxiv.org/pdf/2602.05391v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qianxin Xia",
      "Jiawei Du",
      "Yuhan Zhang",
      "Jielei Wang",
      "Guoming Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.00841v3",
    "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
    "summary": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.",
    "published": "2026-01-31T18:12:29Z",
    "updated": "2026-02-05T07:15:39Z",
    "link": "http://arxiv.org/pdf/2602.00841v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jintao Cheng",
      "Weibin Li",
      "Zhijian He",
      "Jin Wu",
      "Chi Man Vong",
      "Wei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05387v1",
    "title": "Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning",
    "summary": "MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.",
    "published": "2026-02-05T07:13:54Z",
    "updated": "2026-02-05T07:13:54Z",
    "link": "http://arxiv.org/pdf/2602.05387v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zolnamar Dorjsembe",
      "Hung-Yi Chen",
      "Furen Xiao",
      "Hsing-Kuo Pao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05384v1",
    "title": "Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting",
    "summary": "Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.",
    "published": "2026-02-05T07:09:57Z",
    "updated": "2026-02-05T07:09:57Z",
    "link": "http://arxiv.org/pdf/2602.05384v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Feng",
      "Wei Shi",
      "Ke Zhang",
      "Xiang Fei",
      "Lei Liao",
      "Dingkang Yang",
      "Yongkun Du",
      "Xuecheng Wu",
      "Jingqun Tang",
      "Yang Liu",
      "Hong Chen",
      "Can Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05382v1",
    "title": "VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs",
    "summary": "Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.",
    "published": "2026-02-05T07:07:27Z",
    "updated": "2026-02-05T07:07:27Z",
    "link": "http://arxiv.org/pdf/2602.05382v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Tina Khezresmaeilzadeh",
      "Jike Zhong",
      "Konstantinos Psounis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05380v1",
    "title": "SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback",
    "summary": "Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \\textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \\textbf{SAIL} (\\textbf{S}elf-\\textbf{A}mplified \\textbf{I}terative \\textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.",
    "published": "2026-02-05T06:58:38Z",
    "updated": "2026-02-05T06:58:38Z",
    "link": "http://arxiv.org/pdf/2602.05380v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoxuan He",
      "Siming Fu",
      "Wanli Li",
      "Zhiyuan Li",
      "Dacheng Yin",
      "Kang Rong",
      "Fengyun Rao",
      "Bo Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05375v1",
    "title": "Erase at the Core: Representation Unlearning for Machine Unlearning",
    "summary": "Many approximate machine unlearning methods demonstrate strong logit-level forgetting -- such as near-zero accuracy on the forget set -- yet continue to preserve substantial information within their internal feature representations. We refer to this discrepancy as superficial forgetting. Recent studies indicate that most existing unlearning approaches primarily alter the final classifier, leaving intermediate representations largely unchanged and highly similar to those of the original model. To address this limitation, we introduce the Erase at the Core (EC), a framework designed to enforce forgetting throughout the entire network hierarchy. EC integrates multi-layer contrastive unlearning on the forget set with retain set preservation through deeply supervised learning. Concretely, EC attaches auxiliary modules to intermediate layers and applies both contrastive unlearning and cross-entropy losses at each supervision point, with layer-wise weighted losses. Experimental results show that EC not only achieves effective logit-level forgetting, but also substantially reduces representational similarity to the original model across intermediate layers. Furthermore, EC is model-agnostic and can be incorporated as a plug-in module into existing unlearning methods, improving representation-level forgetting while maintaining performance on the retain set.",
    "published": "2026-02-05T06:54:44Z",
    "updated": "2026-02-05T06:54:44Z",
    "link": "http://arxiv.org/pdf/2602.05375v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Jaewon Lee",
      "Yongwoo Kim",
      "Donghyun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05362v1",
    "title": "Imagine a City: CityGenAgent for Procedural 3D City Generation",
    "summary": "The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.",
    "published": "2026-02-05T06:36:03Z",
    "updated": "2026-02-05T06:36:03Z",
    "link": "http://arxiv.org/pdf/2602.05362v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zishan Liu",
      "Zecong Tang",
      "RuoCheng Wu",
      "Xinzhe Zheng",
      "Jingyu Hu",
      "Ka-Hei Hui",
      "Haoran Xie",
      "Bo Dai",
      "Zhengzhe Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05360v1",
    "title": "Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection",
    "summary": "While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.",
    "published": "2026-02-05T06:32:33Z",
    "updated": "2026-02-05T06:32:33Z",
    "link": "http://arxiv.org/pdf/2602.05360v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ningkang Peng",
      "Xiaoqian Peng",
      "Yuhao Zhang",
      "Qianfeng Yu",
      "Feng Xing",
      "Peirong Ma",
      "Xichen Yang",
      "Yi Chen",
      "Tingyu Lu",
      "Yanhui Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05359v1",
    "title": "Multimodal Latent Reasoning via Hierarchical Visual Cues Injection",
    "summary": "The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a \"fast thinking\" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\\emph{HIVE}), a novel framework that instills deliberate, \"slow thinking\" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.",
    "published": "2026-02-05T06:31:12Z",
    "updated": "2026-02-05T06:31:12Z",
    "link": "http://arxiv.org/pdf/2602.05359v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yiming Zhang",
      "Qiangyu Yan",
      "Borui Jiang",
      "Kai Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05349v1",
    "title": "Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection",
    "summary": "Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.",
    "published": "2026-02-05T06:21:16Z",
    "updated": "2026-02-05T06:21:16Z",
    "link": "http://arxiv.org/pdf/2602.05349v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ningkang Peng",
      "JiuTao Zhou",
      "Yuhao Zhang",
      "Xiaoqian Peng",
      "Qianfeng Yu",
      "Linjing Qian",
      "Tingyu Lu",
      "Yi Chen",
      "Yanhui Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05339v1",
    "title": "Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation",
    "summary": "With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.",
    "published": "2026-02-05T06:05:24Z",
    "updated": "2026-02-05T06:05:24Z",
    "link": "http://arxiv.org/pdf/2602.05339v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yongwoo Kim",
      "Sungmin Cha",
      "Hyunsoo Kim",
      "Jaewon Lee",
      "Donghyun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.19254v4",
    "title": "Imperceptible Protection against Style Imitation from Diffusion Models",
    "summary": "Recent progress in diffusion models has profoundly enhanced the fidelity of image generation, but it has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we introduce a visually improved protection method while preserving its protection capability. To this end, we devise a perceptual map to highlight areas sensitive to human eyes, guided by instance-aware refinement, which refines the protection intensity accordingly. We also introduce a difficulty-aware protection by predicting how difficult the artwork is to protect and dynamically adjusting the intensity based on this. Lastly, we integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.",
    "published": "2024-03-28T09:21:00Z",
    "updated": "2026-02-05T05:55:23Z",
    "link": "http://arxiv.org/pdf/2403.19254v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Namhyuk Ahn",
      "Wonhyuk Ahn",
      "KiYoon Yoo",
      "Daesik Kim",
      "Seung-Hun Nam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.07253v4",
    "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
    "summary": "Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions and generate results with undesired properties or even harmful content. Inspired by the success and popularity of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.",
    "published": "2024-09-11T13:21:32Z",
    "updated": "2026-02-05T05:52:23Z",
    "link": "http://arxiv.org/pdf/2409.07253v4.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Buhua Liu",
      "Shitong Shao",
      "Bao Li",
      "Lichen Bai",
      "Zhiqiang Xu",
      "Haoyi Xiong",
      "James Kwok",
      "Sumi Helal",
      "Zeke Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05330v1",
    "title": "MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors",
    "summary": "Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.",
    "published": "2026-02-05T05:51:28Z",
    "updated": "2026-02-05T05:51:28Z",
    "link": "http://arxiv.org/pdf/2602.05330v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingdong Zhang",
      "Xiaohang Zhan",
      "Lingzhi Zhang",
      "Yizhou Wang",
      "Zhengming Yu",
      "Jionghao Wang",
      "Wenping Wang",
      "Xin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05321v1",
    "title": "Wid3R: Wide Field-of-View 3D Reconstruction via Camera Model Conditioning",
    "summary": "We present Wid3R, a feed-forward neural network for visual geometry reconstruction that supports wide field-of-view camera models. Prior methods typically assume that input images are rectified or captured with pinhole cameras, since both their architectures and training datasets are tailored to perspective images only. These assumptions limit their applicability in real-world scenarios that use fisheye or panoramic cameras and often require careful calibration and undistortion. In contrast, Wid3R is a generalizable multi-view 3D estimation method that can model wide field-of-view camera types. Our approach leverages a ray representation with spherical harmonics and a novel camera model token within the network, enabling distortion-aware 3D reconstruction. Furthermore, Wid3R is the first multi-view foundation model to support feed-forward 3D reconstruction directly from 360 imagery. It demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D.",
    "published": "2026-02-05T05:42:03Z",
    "updated": "2026-02-05T05:42:03Z",
    "link": "http://arxiv.org/pdf/2602.05321v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Adil Qureshi",
      "Somi Jeong",
      "Dinesh Manocha",
      "Suyong Yeon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01411v2",
    "title": "Human Body Restoration with One-Step Diffusion Model and A New Benchmark",
    "summary": "Human body restoration, as a specific application of image restoration, is widely applied in practice and plays a vital role across diverse fields. However, thorough research remains difficult, particularly due to the lack of benchmark datasets. In this study, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing object detection datasets and other unlabeled images to automatically crop and filter high-quality human images. Using this pipeline, we constructed a person-based restoration with sophisticated objects and natural activities (\\emph{PERSONA}) dataset, which includes training, validation, and test sets. The dataset significantly surpasses other human-related datasets in both quality and content richness. Finally, we propose \\emph{OSDHuman}, a novel one-step diffusion model for human body restoration. Specifically, we propose a high-fidelity image embedder (HFIE) as the prompt generator to better guide the model with low-quality human image information, effectively avoiding misleading prompts. Experimental results show that OSDHuman outperforms existing methods in both visual quality and quantitative metrics. The dataset and code will at https://github.com/gobunu/OSDHuman.",
    "published": "2025-02-03T14:48:40Z",
    "updated": "2026-02-05T05:29:27Z",
    "link": "http://arxiv.org/pdf/2502.01411v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jue Gong",
      "Jingkai Wang",
      "Zheng Chen",
      "Xing Liu",
      "Hong Gu",
      "Yulun Zhang",
      "Xiaokang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06033v1",
    "title": "Can vision language models learn intuitive physics from interaction?",
    "summary": "Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.",
    "published": "2026-02-05T18:59:20Z",
    "updated": "2026-02-05T18:59:20Z",
    "link": "http://arxiv.org/pdf/2602.06033v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Luca M. Schulze Buschoff",
      "Konstantinos Voudouris",
      "Can Demircan",
      "Eric Schulz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06030v1",
    "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
    "summary": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.",
    "published": "2026-02-05T18:59:01Z",
    "updated": "2026-02-05T18:59:01Z",
    "link": "http://arxiv.org/pdf/2602.06030v1.pdf",
    "category": [
      "cs.MA",
      "cs.LG"
    ],
    "authors": [
      "Kavana Venkatesh",
      "Yinhan He",
      "Jundong Li",
      "Jiaming Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06031v1",
    "title": "AP-OOD: Attention Pooling for Out-of-Distribution Detection",
    "summary": "Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.",
    "published": "2026-02-05T18:59:01Z",
    "updated": "2026-02-05T18:59:01Z",
    "link": "http://arxiv.org/pdf/2602.06031v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Claus Hofmann",
      "Christian Huber",
      "Bernhard Lehner",
      "Daniel Klotz",
      "Sepp Hochreiter",
      "Werner Zellinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06029v1",
    "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference",
    "summary": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.",
    "published": "2026-02-05T18:58:32Z",
    "updated": "2026-02-05T18:58:32Z",
    "link": "http://arxiv.org/pdf/2602.06029v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yingke Li",
      "Anjali Parashar",
      "Enlu Zhou",
      "Chuchu Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06021v1",
    "title": "Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold",
    "summary": "When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model's performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model's inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.",
    "published": "2026-02-05T18:55:03Z",
    "updated": "2026-02-05T18:55:03Z",
    "link": "http://arxiv.org/pdf/2602.06021v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.NA",
      "math.PR"
    ],
    "authors": [
      "Ye He",
      "Yitong Qiu",
      "Molei Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06020v1",
    "title": "Mechanisms of AI Protein Folding in ESMFold",
    "summary": "How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.",
    "published": "2026-02-05T18:54:54Z",
    "updated": "2026-02-05T18:54:54Z",
    "link": "http://arxiv.org/pdf/2602.06020v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.BM"
    ],
    "authors": [
      "Kevin Lu",
      "Jannik Brinkmann",
      "Stefan Huber",
      "Aaron Mueller",
      "Yonatan Belinkov",
      "David Bau",
      "Chris Wendler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05999v1",
    "title": "On Computation and Reinforcement Learning",
    "summary": "How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.",
    "published": "2026-02-05T18:45:57Z",
    "updated": "2026-02-05T18:45:57Z",
    "link": "http://arxiv.org/pdf/2602.05999v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Raj Ghugare",
      "Michał Bortkiewicz",
      "Alicja Ziarko",
      "Benjamin Eysenbach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08734v2",
    "title": "Transmuting prompts into weights",
    "summary": "A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt's influence can be mathematically mapped to token-dependent implicit weight updates (Dherin et. al, 2025), we derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector-and-matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.",
    "published": "2025-10-09T18:40:39Z",
    "updated": "2026-02-05T18:44:09Z",
    "link": "http://arxiv.org/pdf/2510.08734v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hanna Mazzawi",
      "Benoit Dherin",
      "Michael Munn",
      "Michael Wunder",
      "Javier Gonzalvo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05997v1",
    "title": "Causal Inference on Stopped Random Walks in Online Advertising",
    "summary": "We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.",
    "published": "2026-02-05T18:43:29Z",
    "updated": "2026-02-05T18:43:29Z",
    "link": "http://arxiv.org/pdf/2602.05997v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Jia Yuan Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05996v1",
    "title": "Orthogonal Self-Attention",
    "summary": "Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.",
    "published": "2026-02-05T18:42:57Z",
    "updated": "2026-02-05T18:42:57Z",
    "link": "http://arxiv.org/pdf/2602.05996v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Leo Zhang",
      "James Martens"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05988v1",
    "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
    "summary": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
    "published": "2026-02-05T18:38:53Z",
    "updated": "2026-02-05T18:38:53Z",
    "link": "http://arxiv.org/pdf/2602.05988v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Keith Ando Ogawa",
      "Bruno Lopes Yamamoto",
      "Lucas Lauton de Alcantara",
      "Lucas Pellicer",
      "Rosimeire Pereira Costa",
      "Edson Bollis",
      "Anna Helena Reali Costa",
      "Artur Jordao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05967v1",
    "title": "A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders",
    "summary": "Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.",
    "published": "2026-02-05T18:21:28Z",
    "updated": "2026-02-05T18:21:28Z",
    "link": "http://arxiv.org/pdf/2602.05967v1.pdf",
    "category": [
      "cs.LG",
      "eess.SY"
    ],
    "authors": [
      "Mohamad Amin Jamshidi",
      "Mehrbod Zarifi",
      "Zolfa Anvari",
      "Hamed Ghafarirad",
      "Mohammad Zareinejad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26000v2",
    "title": "Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access",
    "summary": "Asymmetric actor-critic methods are widely used in partially observable reinforcement learning, but typically assume full state observability to condition the critic during training, which is often unrealistic in practice. We introduce the informed asymmetric actor-critic framework, allowing the critic to be conditioned on arbitrary state-dependent privileged signals without requiring access to the full state. We show that any such privileged signal yields unbiased policy gradient estimates, substantially expanding the set of admissible privileged information. This raises the problem of selecting the most adequate privileged information in order to improve learning. For this purpose, we propose two novel informativeness criteria: a dependence-based test that can be applied prior to training, and a criterion based on improvements in value prediction accuracy that can be applied post-hoc. Empirical results on partially observable benchmark tasks and synthetic environments demonstrate that carefully selected privileged signals can match or outperform full-state asymmetric baselines while relying on strictly less state information.",
    "published": "2025-09-30T09:32:20Z",
    "updated": "2026-02-05T18:21:20Z",
    "link": "http://arxiv.org/pdf/2509.26000v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Daniel Ebi",
      "Gaspard Lambrechts",
      "Damien Ernst",
      "Klemens Böhm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05961v1",
    "title": "Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces",
    "summary": "Sampling from a distribution $p(x) \\propto e^{-\\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.",
    "published": "2026-02-05T18:16:57Z",
    "updated": "2026-02-05T18:16:57Z",
    "link": "http://arxiv.org/pdf/2602.05961v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Arran Carter",
      "Sanghyeok Choi",
      "Kirill Tamogashev",
      "Víctor Elvira",
      "Nikolay Malkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05950v1",
    "title": "Breaking Symmetry Bottlenecks in GNN Readouts",
    "summary": "Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.",
    "published": "2026-02-05T18:08:13Z",
    "updated": "2026-02-05T18:08:13Z",
    "link": "http://arxiv.org/pdf/2602.05950v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mouad Talhi",
      "Arne Wolf",
      "Anthea Monod"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05946v1",
    "title": "$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
    "summary": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.",
    "published": "2026-02-05T18:01:52Z",
    "updated": "2026-02-05T18:01:52Z",
    "link": "http://arxiv.org/pdf/2602.05946v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Rajdeep Haldar",
      "Lantao Mei",
      "Guang Lin",
      "Yue Xing",
      "Qifan Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05943v1",
    "title": "Orthogonal Model Merging",
    "summary": "Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.",
    "published": "2026-02-05T17:57:14Z",
    "updated": "2026-02-05T17:57:14Z",
    "link": "http://arxiv.org/pdf/2602.05943v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sihan Yang",
      "Kexuan Shi",
      "Weiyang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.08419v2",
    "title": "Energy Guided smoothness to improve Robustness in Graph Classification",
    "summary": "Graph Neural Networks (GNNs) are powerful at solving graph classification tasks, yet applied problems often contain noisy labels. In this work, we study GNN robustness to label noise, demonstrate GNN failure modes when models struggle to generalise on low-order graphs, low label coverage, or when a model is over-parameterized. We establish both empirical and theoretical links between GNN robustness and the reduction of the total Dirichlet Energy of learned node representations, which encapsulates the hypothesized GNN smoothness inductive bias. Finally, we introduce two training strategies to enhance GNN robustness: (1) by incorporating a novel inductive bias in the weight matrices through the removal of negative eigenvalues, connected to Dirichlet Energy minimization; (2) by extending to GNNs a loss penalty that promotes learned smoothness. Importantly, neither approach negatively impacts performance in noise-free settings, supporting our hypothesis that the source of GNNs robustness is their smoothness inductive bias.",
    "published": "2024-12-11T14:35:37Z",
    "updated": "2026-02-05T17:56:08Z",
    "link": "http://arxiv.org/pdf/2412.08419v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Farooq Ahmad Wani",
      "Maria Sofia Bucarelli",
      "Andrea Giuseppe Di Francesco",
      "Oleksandr Pryymak",
      "Fabrizio Silvestri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05936v1",
    "title": "Dimensionality Reduction on Riemannian Manifolds in Data Analysis",
    "summary": "In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.",
    "published": "2026-02-05T17:46:58Z",
    "updated": "2026-02-05T17:46:58Z",
    "link": "http://arxiv.org/pdf/2602.05936v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Alaa El Ichi",
      "Khalide Jbilou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05935v1",
    "title": "Tuning Out-of-Distribution (OOD) Detectors Without Given OOD Data",
    "summary": "Existing out-of-distribution (OOD) detectors are often tuned by a separate dataset deemed OOD with respect to the training distribution of a neural network (NN). OOD detectors process the activations of NN layers and score the output, where parameters of the detectors are determined by fitting to an in-distribution (training) set and the aforementioned dataset chosen adhocly. At detector training time, this adhoc dataset may not be available or difficult to obtain, and even when it's available, it may not be representative of actual OOD data, which is often ''unknown unknowns.\" Current benchmarks may specify some left-out set from test OOD sets. We show that there can be significant variance in performance of detectors based on the adhoc dataset chosen in current literature, and thus even if such a dataset can be collected, the performance of the detector may be highly dependent on the choice. In this paper, we introduce and formalize the often neglected problem of tuning OOD detectors without a given ``OOD'' dataset. To this end, we present strong baselines as an attempt to approach this problem. Furthermore, we propose a new generic approach to OOD detector tuning that does not require any extra data other than those used to train the NN. We show that our approach improves over baseline methods consistently across higher-parameter OOD detector families, while being comparable across lower-parameter families.",
    "published": "2026-02-05T17:46:40Z",
    "updated": "2026-02-05T17:46:40Z",
    "link": "http://arxiv.org/pdf/2602.05935v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sudeepta Mondal",
      "Xinyi Mary Xie",
      "Ruxiao Duan",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05933v1",
    "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
    "summary": "Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.",
    "published": "2026-02-05T17:44:28Z",
    "updated": "2026-02-05T17:44:28Z",
    "link": "http://arxiv.org/pdf/2602.05933v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhenghao Xu",
      "Qin Lu",
      "Changlong Yu",
      "Tuo Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04408v2",
    "title": "Separation-Utility Pareto Frontier: An Information-Theoretic Characterization",
    "summary": "We study the Pareto frontier (optimal trade-off) between utility and separation, a fairness criterion requiring predictive independence from sensitive attributes conditional on the true outcome. Through an information-theoretic lens, we prove a characterization of the utility-separation Pareto frontier, establish its concavity, and thereby prove the increasing marginal cost of separation in terms of utility. In addition, we characterize the conditions under which this trade-off becomes strict, providing a guide for trade-off selection in practice. Based on the theoretical characterization, we develop an empirical regularizer based on conditional mutual information (CMI) between predictions and sensitive attributes given the true outcome. The CMI regularizer is compatible with any deep model trained via gradient-based optimization and serves as a scalar monitor of residual separation violations, offering tractable guarantees during training. Finally, numerical experiments support our theoretical findings: across COMPAS, UCI Adult, UCI Bank, and CelebA, the proposed method substantially reduces separation violations while matching or exceeding the utility of established baseline methods. This study thus offers a provable, stable, and flexible approach to enforcing separation in deep learning.",
    "published": "2026-02-04T10:38:44Z",
    "updated": "2026-02-05T17:37:51Z",
    "link": "http://arxiv.org/pdf/2602.04408v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Shizhou Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05927v1",
    "title": "Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences",
    "summary": "Transformers underpin modern large language models (LLMs) and are commonly assumed to be behaviorally unstructured at random initialization, with all meaningful preferences emerging only through large-scale training. We challenge this assumption by showing that randomly initialized transformers already exhibit strong and systematic structural biases. In particular, untrained models display extreme token preferences: across random input sequences, certain tokens are predicted with probabilities orders of magnitude larger.\n  We provide a mechanistic explanation for this phenomenon by dissecting the transformer architecture at initialization. We show that extreme token preference arises from a contraction of token representations along a random seed-dependent direction. This contraction is driven by two interacting forces: (i) asymmetric nonlinear activations in MLP sublayers induce global (inter-sequence) representation concentration, and (ii) self-attention further amplifies this effect through local (intra-sequence) aggregation. Together, these mechanisms align hidden representations along a direction determined solely by the random initialization, producing highly non-uniform next-token predictions.\n  Beyond mechanistic insight, we demonstrate that these initialization-induced biases persist throughout training, forming a stable and intrinsic model identity. Leveraging this property, we introduce SeedPrint, a fingerprinting method that can reliably distinguish models that differ only in their random initialization, even after extensive training and under substantial distribution shift. Finally, we identify a fundamental positional discrepancy inherent to the attention mechanism's intra-sequence contraction that is causally linked to the attention-sink phenomenon. This discovery provides a principled explanation for the emergence of sinks and offers a pathway for their control.",
    "published": "2026-02-05T17:37:41Z",
    "updated": "2026-02-05T17:37:41Z",
    "link": "http://arxiv.org/pdf/2602.05927v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Siquan Li",
      "Yao Tong",
      "Haonan Wang",
      "Tianyang Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05910v1",
    "title": "Chunky Post-Training: Data Driven Failures of Generalization",
    "summary": "LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.",
    "published": "2026-02-05T17:25:51Z",
    "updated": "2026-02-05T17:25:51Z",
    "link": "http://arxiv.org/pdf/2602.05910v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Seoirse Murray",
      "Allison Qi",
      "Timothy Qian",
      "John Schulman",
      "Collin Burns",
      "Sara Price"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04661v2",
    "title": "Flexible inference for animal learning rules using neural networks",
    "summary": "Understanding how animals learn is a central challenge in neuroscience, with growing relevance to the development of animal- or human-aligned artificial intelligence. However, existing approaches tend to assume fixed parametric forms for the learning rule (e.g., Q-learning, policy gradient), which may not accurately describe the complex forms of learning employed by animals in realistic settings. Here we address this gap by developing a framework to infer learning rules directly from behavioral data collected during de novo task learning. We assume that animals follow a decision policy parameterized by a generalized linear model (GLM), and we model their learning rule -- the mapping from task covariates to per-trial weight updates -- using a deep neural network (DNN). This formulation allows flexible, data-driven inference of learning rules while maintaining an interpretable form of the decision policy itself. To capture more complex learning dynamics, we introduce a recurrent neural network (RNN) variant that relaxes the Markovian assumption that learning depends solely on covariates of the current trial, allowing for learning rules that integrate information over multiple trials. Simulations demonstrate that the framework can recover ground-truth learning rules. We applied our DNN and RNN-based methods to a large behavioral dataset from mice learning to perform a sensory decision-making task and found that they outperformed traditional RL learning rules at predicting the learning trajectories of held-out mice. The inferred learning rules exhibited reward-history-dependent learning dynamics, with larger updates following sequences of rewarded trials. Overall, these methods provide a flexible framework for inferring learning rules from behavioral data in de novo learning tasks, setting the stage for improved animal training protocols and the development of behavioral digital twins.",
    "published": "2025-09-04T21:20:11Z",
    "updated": "2026-02-05T17:21:21Z",
    "link": "http://arxiv.org/pdf/2509.04661v2.pdf",
    "category": [
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Yuhan Helena Liu",
      "Victor Geadah",
      "Jonathan Pillow"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05898v1",
    "title": "Universal approximation with signatures of non-geometric rough paths",
    "summary": "We establish a universal approximation theorem for signatures of rough paths that are not necessarily weakly geometric. By extending the path with time and its rough path bracket terms, we prove that linear functionals of the signature of the resulting rough paths approximate continuous functionals on rough path spaces uniformly on compact sets. Moreover, we construct the signature of a path extended by its pathwise quadratic variation terms based on general pathwise stochastic integration à la Föllmer, in particular, allowing for pathwise Itô, Stratonovich, and backward Itô integration. In a probabilistic setting, we obtain a universal approximation result for linear functionals of the signature of continuous semimartingales extended by the quadratic variation terms, defined via stochastic Itô integration. Numerical examples illustrate the use of signatures when the path is extended by time and quadratic variation in the context of model calibration and option pricing in mathematical finance.",
    "published": "2026-02-05T17:16:25Z",
    "updated": "2026-02-05T17:16:25Z",
    "link": "http://arxiv.org/pdf/2602.05898v1.pdf",
    "category": [
      "math.PR",
      "cs.LG",
      "q-fin.MF"
    ],
    "authors": [
      "Mihriban Ceylan",
      "Anna P. Kwossek",
      "David J. Prömel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05892v1",
    "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
    "summary": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.",
    "published": "2026-02-05T17:10:26Z",
    "updated": "2026-02-05T17:10:26Z",
    "link": "http://arxiv.org/pdf/2602.05892v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Han Li",
      "Letian Zhu",
      "Bohan Zhang",
      "Rili Feng",
      "Jiaming Wang",
      "Yue Pan",
      "Earl T. Barr",
      "Sarro Federica",
      "Zhaoyang Chu",
      "He Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21022v2",
    "title": "A Sketch-and-Project Analysis of Subsampled Natural Gradient Algorithms",
    "summary": "Subsampled natural gradient descent (SNG) has been used to enable high-precision scientific machine learning, but standard analyses based on stochastic preconditioning fail to provide insight into realistic small-sample settings. We overcome this limitation by instead analyzing SNG as a sketch-and-project method. Motivated by this lens, we discard the usual theoretical proxy which decouples gradients and preconditioners using two independent mini-batches, and we replace it with a new proxy based on squared volume sampling. Under this new proxy we show that the expectation of the SNG direction becomes equal to a preconditioned gradient descent step even in the presence of coupling, leading to (i) global convergence guarantees when using a single mini-batch of any size, and (ii) an explicit characterization of the convergence rate in terms of quantities related to the sketch-and-project structure. These findings in turn yield new insights into small-sample settings, for example by suggesting that the advantage of SNG over SGD is that it can more effectively exploit spectral decay in the model Jacobian. We also extend these ideas to explain a popular structured momentum scheme for SNG, known as SPRING, by showing that it arises naturally from accelerated sketch-and-project methods.",
    "published": "2025-08-28T17:24:59Z",
    "updated": "2026-02-05T17:09:24Z",
    "link": "http://arxiv.org/pdf/2508.21022v2.pdf",
    "category": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Gil Goldshlager",
      "Jiang Hu",
      "Lin Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02258v2",
    "title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization",
    "summary": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.",
    "published": "2026-02-02T16:03:16Z",
    "updated": "2026-02-05T17:05:45Z",
    "link": "http://arxiv.org/pdf/2602.02258v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Gaurav Bhatt",
      "Aditya Chinchure",
      "Jiawei Zhou",
      "Leonid Sigal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05887v1",
    "title": "Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting",
    "summary": "Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.",
    "published": "2026-02-05T17:05:02Z",
    "updated": "2026-02-05T17:05:02Z",
    "link": "http://arxiv.org/pdf/2602.05887v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Tianqi Shen",
      "Jinji Yang",
      "Junze He",
      "Kunhan Gao",
      "Ziye Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.03405v2",
    "title": "Enhancing Quantum Diffusion Models for Complex Image Generation",
    "summary": "Quantum generative models offer a novel approach to exploring high-dimensional Hilbert spaces but face significant challenges in scalability and expressibility when applied to multi-modal distributions. In this study, we explore a Hybrid Quantum-Classical U-Net architecture integrated with Adaptive Non-Local Observables (ANO) as a potential solution to these hurdles. By compressing classical data into a dense quantum latent space and utilizing trainable observables, our model aims to extract non-local features that complement classical processing. We also investigate the role of Skip Connections in preserving semantic information during the reverse diffusion process. Experimental results on the full MNIST dataset (digits 0-9) demonstrate that the proposed architecture is capable of generating structurally coherent and recognizable images for all digit classes. While hardware constraints still impose limitations on resolution, our findings suggest that hybrid architectures with adaptive measurements provide a feasible pathway for mitigating mode collapse and enhancing generative capabilities in the NISQ era.",
    "published": "2026-02-03T11:32:19Z",
    "updated": "2026-02-05T16:55:22Z",
    "link": "http://arxiv.org/pdf/2602.03405v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Jeongbin Jo",
      "Santanam Wishal",
      "Shah Md Khalil Ullah",
      "Shan Zeng",
      "Dikshant Dulal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05873v1",
    "title": "Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks",
    "summary": "Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.",
    "published": "2026-02-05T16:51:07Z",
    "updated": "2026-02-05T16:51:07Z",
    "link": "http://arxiv.org/pdf/2602.05873v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Minyoung Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01388v2",
    "title": "The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton's Laws in Financial Deep Reinforcement Learning (RL) Algorithms",
    "summary": "Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.",
    "published": "2026-02-01T18:48:33Z",
    "updated": "2026-02-05T16:48:41Z",
    "link": "http://arxiv.org/pdf/2602.01388v2.pdf",
    "category": [
      "cs.CE",
      "cs.LG"
    ],
    "authors": [
      "Trang Thoi",
      "Hung Tran",
      "Tram Thoi",
      "Huaiyang Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21004v2",
    "title": "Multi-Agent Inverted Transformer for Flight Trajectory Prediction",
    "summary": "Flight trajectory prediction for multiple aircraft is essential and provides critical insights into how aircraft navigate within current air traffic flows. However, predicting multi-agent flight trajectories is inherently challenging. One of the major difficulties is modeling both the individual aircraft behaviors over time and the complex interactions between flights. Generating explainable prediction outcomes is also a challenge. Therefore, we propose a Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture that predicts multi-agent flight trajectories. The proposed framework features two key attention modules: (i) masked multivariate attention, which captures spatio-temporal patterns of individual aircraft, and (ii) agent attention, which models the social patterns among multiple agents in complex air traffic scenes. We evaluated MAIFormer using a real-world automatic dependent surveillance-broadcast flight trajectory dataset from the terminal airspace of Incheon International Airport in South Korea. The experimental results show that MAIFormer achieves the best performance across multiple metrics and outperforms other methods. In addition, MAIFormer produces prediction outcomes that are interpretable from a human perspective, which improves both the transparency of the model and its practical utility in air traffic control.",
    "published": "2025-09-25T10:59:29Z",
    "updated": "2026-02-05T16:47:36Z",
    "link": "http://arxiv.org/pdf/2509.21004v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Seokbin Yoon",
      "Keumjin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05869v1",
    "title": "Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity",
    "summary": "We introduce Wedge Sampling, a new non-adaptive sampling scheme for low-rank tensor completion. We study recovery of an order-$k$ low-rank tensor of dimension $n \\times \\cdots \\times n$ from a subset of its entries. Unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. By directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.\n  Our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. The approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\\tilde{O}(n)$ uniformly sampled entries, substantially improving over the $\\tilde{O}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. Overall, our results suggest that the statistical-to-computational gap highlighted in Barak and Moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.",
    "published": "2026-02-05T16:47:13Z",
    "updated": "2026-02-05T16:47:13Z",
    "link": "http://arxiv.org/pdf/2602.05869v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.NA",
      "math.PR",
      "math.ST"
    ],
    "authors": [
      "Hengrui Luo",
      "Anna Ma",
      "Ludovic Stephan",
      "Yizhe Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05862v1",
    "title": "Distribution-free two-sample testing with blurred total variation distance",
    "summary": "Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.",
    "published": "2026-02-05T16:43:31Z",
    "updated": "2026-02-05T16:43:31Z",
    "link": "http://arxiv.org/pdf/2602.05862v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Rohan Hore",
      "Rina Foygel Barber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05861v1",
    "title": "CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs",
    "summary": "Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.",
    "published": "2026-02-05T16:42:51Z",
    "updated": "2026-02-05T16:42:51Z",
    "link": "http://arxiv.org/pdf/2602.05861v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Seyedmasoud Mousavi",
      "Ruomeng Xu",
      "Xiaojing Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13821v2",
    "title": "The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces",
    "summary": "Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability - adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a theoretically grounded approach to AI control for code generation tasks, though practical deployment requires addressing the high false positive rates observed in initial evaluations.",
    "published": "2025-12-15T19:05:37Z",
    "updated": "2026-02-05T16:40:19Z",
    "link": "http://arxiv.org/pdf/2512.13821v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Subramanyam Sahoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05855v1",
    "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion",
    "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.",
    "published": "2026-02-05T16:38:42Z",
    "updated": "2026-02-05T16:38:42Z",
    "link": "http://arxiv.org/pdf/2602.05855v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Dennis Bank",
      "Joost Cordes",
      "Thomas Seel",
      "Simon F. G. Ehlers"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05852v1",
    "title": "Exact Recovery in the Data Block Model",
    "summary": "Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.",
    "published": "2026-02-05T16:36:57Z",
    "updated": "2026-02-05T16:36:57Z",
    "link": "http://arxiv.org/pdf/2602.05852v1.pdf",
    "category": [
      "cs.LG",
      "cs.IT",
      "stat.ML"
    ],
    "authors": [
      "Amir R. Asadi",
      "Akbar Davoodi",
      "Ramin Javadi",
      "Farzad Parvaresh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05849v1",
    "title": "Visualizing the loss landscapes of physics-informed neural networks",
    "summary": "Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a \"loss landscape\" community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.",
    "published": "2026-02-05T16:35:51Z",
    "updated": "2026-02-05T16:35:51Z",
    "link": "http://arxiv.org/pdf/2602.05849v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Conor Rowan",
      "Finn Murphy-Blanchard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05846v1",
    "title": "Optimal scaling laws in learning hierarchical multi-index models",
    "summary": "In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.",
    "published": "2026-02-05T16:33:51Z",
    "updated": "2026-02-05T16:33:51Z",
    "link": "http://arxiv.org/pdf/2602.05846v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Leonardo Defilippis",
      "Florent Krzakala",
      "Bruno Loureiro",
      "Antoine Maillard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08916v2",
    "title": "A Representer Theorem for Hawkes Processes via Penalized Least Squares Minimization",
    "summary": "The representer theorem is a cornerstone of kernel methods, which aim to estimate latent functions in reproducing kernel Hilbert spaces (RKHSs) in a nonparametric manner. Its significance lies in converting inherently infinite-dimensional optimization problems into finite-dimensional ones over dual coefficients, thereby enabling practical and computationally tractable algorithms. In this paper, we address the problem of estimating the latent triggering kernels--functions that encode the interaction structure between events--for linear multivariate Hawkes processes based on observed event sequences within an RKHS framework. We show that, under the principle of penalized least squares minimization, a novel form of representer theorem emerges: a family of transformed kernels can be defined via a system of simultaneous integral equations, and the optimal estimator of each triggering kernel is expressed as a linear combination of these transformed kernels evaluated at the data points. Remarkably, the dual coefficients are all analytically fixed to unity, obviating the need to solve a costly optimization problem to obtain the dual coefficients. This leads to a highly efficient estimator capable of handling large-scale data more effectively than conventional nonparametric approaches. Empirical evaluations on synthetic datasets reveal that the proposed method attains competitive predictive accuracy while substantially improving computational efficiency over existing state-of-the-art kernel method-based estimators.",
    "published": "2025-10-10T02:00:56Z",
    "updated": "2026-02-05T16:24:50Z",
    "link": "http://arxiv.org/pdf/2510.08916v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Hideaki Kim",
      "Tomoharu Iwata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.09736v2",
    "title": "Data Heterogeneity and Forgotten Labels in Split Federated Learning",
    "summary": "In Split Federated Learning (SFL), the clients collaboratively train a model with the help of a server by splitting the model into two parts. Part-1 is trained locally at each client and aggregated by the aggregator at the end of each round. Part-2 is trained at a server that sequentially processes the intermediate activations received from each client. We study the phenomenon of catastrophic forgetting (CF) in SFL in the presence of data heterogeneity. In detail, due to the nature of SFL, local updates of part-1 may drift away from global optima, while part-2 is sensitive to the processing sequence, similar to forgetting in continual learning (CL). Specifically, we observe that the trained model performs better in classes (labels) seen at the end of the sequence. We investigate this phenomenon with emphasis on key aspects of SFL, such as the processing order at the server and the cut layer. Based on our findings, we propose Hydra, a novel mitigation method inspired by multi-head neural networks and adapted for the SFL setting. Extensive numerical evaluations show that Hydra outperforms baselines and methods from the literature.",
    "published": "2025-11-12T20:57:49Z",
    "updated": "2026-02-05T16:23:20Z",
    "link": "http://arxiv.org/pdf/2511.09736v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Joana Tirana",
      "Dimitra Tsigkari",
      "David Solans Noguero",
      "Nicolas Kourtellis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01607v2",
    "title": "Minimax optimal differentially private synthetic data for smooth queries",
    "summary": "Differentially private synthetic data enables the sharing and analysis of sensitive datasets while providing rigorous privacy guarantees for individual contributors. A central challenge is to achieve strong utility guarantees for meaningful downstream analysis. Many existing methods ensure uniform accuracy over broad query classes, such as all Lipschitz functions, but this level of generality often leads to suboptimal rates for statistics of practical interest. Since many common data analysis queries exhibit smoothness beyond what worst-case Lipschitz bounds capture, we ask whether exploiting this additional structure can yield improved utility.\n  We study the problem of generating $(\\varepsilon,δ)$-differentially private synthetic data from a dataset of size $n$ supported on the hypercube $[-1,1]^d$, with utility guarantees uniformly for all smooth queries having bounded derivatives up to order $k$. We propose a polynomial-time algorithm that achieves a minimax error rate of $n^{-\\min \\{1, \\frac{k}{d}\\}}$, up to a $\\log(n)$ factor. This characterization uncovers a phase transition at $k=d$. Our results generalize the Chebyshev moment matching framework of (Musco et al., 2025; Wang et al., 2016) and strictly improve the error rates for $k$-smooth queries established in (Wang et al., 2016). Moreover, we establish the first minimax lower bound for the utility of $(\\varepsilon,δ)$-differentially private synthetic data with respect to $k$-smooth queries, extending the Wasserstein lower bound for $\\varepsilon$-differential privacy in (Boedihardjo et al., 2024).",
    "published": "2026-02-02T03:54:11Z",
    "updated": "2026-02-05T16:22:04Z",
    "link": "http://arxiv.org/pdf/2602.01607v2.pdf",
    "category": [
      "math.ST",
      "cs.IT",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Rundong Ding",
      "Yiyun He",
      "Yizhe Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05833v1",
    "title": "Synthesizing Realistic Test Data without Breaking Privacy",
    "summary": "There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.",
    "published": "2026-02-05T16:22:01Z",
    "updated": "2026-02-05T16:22:01Z",
    "link": "http://arxiv.org/pdf/2602.05833v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Laura Plein",
      "Alexi Turcotte",
      "Arina Hallemans",
      "Andreas Zeller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24139v4",
    "title": "Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction",
    "summary": "While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \\(1-α\\pm δ\\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.",
    "published": "2025-12-30T11:02:35Z",
    "updated": "2026-02-05T16:11:55Z",
    "link": "http://arxiv.org/pdf/2512.24139v4.pdf",
    "category": [
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Qianyi Chen",
      "Bo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05817v1",
    "title": "Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows",
    "summary": "The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.",
    "published": "2026-02-05T16:08:24Z",
    "updated": "2026-02-05T16:08:24Z",
    "link": "http://arxiv.org/pdf/2602.05817v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "authors": [
      "Enrique Feito-Casares",
      "Francisco M. Melgarejo-Meseguer",
      "Elena Casiraghi",
      "Giorgio Valentini",
      "José-Luis Rojo-Álvarez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08588v3",
    "title": "Sample Complexity of Composite Quantum Hypothesis Testing",
    "summary": "This paper investigates symmetric composite binary quantum hypothesis testing (QHT), where the goal is to determine which of two uncertainty sets contains an unknown quantum state. While asymptotic error exponents for this problem are well-studied, the finite-sample regime remains poorly understood. We bridge this gap by characterizing the sample complexity -- the minimum number of state copies required to achieve a target error level. Specifically, we derive lower bounds that generalize the sample complexity of simple QHT and introduce new upper bounds for various uncertainty sets, including of both finite and infinite cardinalities. Notably, our upper and lower bounds match up to universal constants, providing a tight characterization of the sample complexity. Finally, we extend our analysis to the differentially private setting, establishing the sample complexity for privacy-preserving composite QHT.",
    "published": "2026-01-13T14:15:12Z",
    "updated": "2026-02-05T16:08:11Z",
    "link": "http://arxiv.org/pdf/2601.08588v3.pdf",
    "category": [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Jacob Paul Simpson",
      "Efstratios Palias",
      "Sharu Theresa Jose"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05813v1",
    "title": "Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers",
    "summary": "We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.\n  Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup",
    "published": "2026-02-05T16:06:19Z",
    "updated": "2026-02-05T16:06:19Z",
    "link": "http://arxiv.org/pdf/2602.05813v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Artem Riabinin",
      "Andrey Veprikov",
      "Arman Bolatov",
      "Martin Takáč",
      "Aleksandr Beznosikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05812v1",
    "title": "Principled Confidence Estimation for Deep Computed Tomography",
    "summary": "We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.",
    "published": "2026-02-05T16:04:19Z",
    "updated": "2026-02-05T16:04:19Z",
    "link": "http://arxiv.org/pdf/2602.05812v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Matteo Gätzner",
      "Johannes Kirschner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05810v1",
    "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents",
    "summary": "Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.",
    "published": "2026-02-05T16:03:56Z",
    "updated": "2026-02-05T16:03:56Z",
    "link": "http://arxiv.org/pdf/2602.05810v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Quan M. Tran",
      "Zhuo Huang",
      "Wenbin Zhang",
      "Bo Han",
      "Koji Yatani",
      "Masashi Sugiyama",
      "Tongliang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05799v1",
    "title": "Non-Stationary Inventory Control with Lead Times",
    "summary": "We study non-stationary single-item, periodic-review inventory control problems in which the demand distribution is unknown and may change over time. We analyze how demand non-stationarity affects learning performance across inventory models, including systems with demand backlogging or lost-sales, both with and without lead times. For each setting, we propose an adaptive online algorithm that optimizes over the class of base-stock policies and establish performance guarantees in terms of dynamic regret relative to the optimal base-stock policy at each time step. Our results reveal a sharp separation across inventory models. In backlogging systems and lost-sales models with zero lead time, we show that it is possible to adapt to demand changes without incurring additional performance loss in stationary environments, even without prior knowledge of the demand distributions or the number of demand shifts. In contrast, for lost-sales systems with positive lead times, we establish weaker guarantees that reflect fundamental limitations imposed by delayed replenishment in combination with censored feedback. Our algorithms leverage the convexity and one-sided feedback structure of inventory costs to enable counterfactual policy evaluation despite demand censoring. We complement the theoretical analysis with simulation results showing that our methods significantly outperform existing benchmarks.",
    "published": "2026-02-05T15:53:37Z",
    "updated": "2026-02-05T15:53:37Z",
    "link": "http://arxiv.org/pdf/2602.05799v1.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Nele H. Amiri",
      "Sean R. Sinclair",
      "Maximiliano Udenio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05798v1",
    "title": "Learning False Discovery Rate Control via Model-Based Neural Networks",
    "summary": "Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches.",
    "published": "2026-02-05T15:53:11Z",
    "updated": "2026-02-05T15:53:11Z",
    "link": "http://arxiv.org/pdf/2602.05798v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "authors": [
      "Arnau Vilella",
      "Jasin Machkour",
      "Michael Muma",
      "Daniel P. Palomar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05797v1",
    "title": "Classification Under Local Differential Privacy with Model Reversal and Model Averaging",
    "summary": "Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.",
    "published": "2026-02-05T15:52:34Z",
    "updated": "2026-02-05T15:52:34Z",
    "link": "http://arxiv.org/pdf/2602.05797v1.pdf",
    "category": [
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Caihong Qin",
      "Yang Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.03582v2",
    "title": "Optimization and Generation in Aerodynamics Inverse Design",
    "summary": "Inverse design with physics-based objectives is challenging because it couples high-dimensional geometry with expensive simulations, as exemplified by aerodynamic shape optimization for drag reduction. We revisit inverse design through two canonical solutions, the optimal design point and the optimal design distribution, and relate them to optimization and guided generation. Building on this view, we propose a new training loss for cost predictors and a density-gradient optimization method that improves objectives while preserving plausible shapes. We further unify existing training-free guided generation methods. To address their inability to approximate conditional covariance in high dimensions, we develop a time- and memory-efficient algorithm for approximate covariance estimation. Experiments on a controlled 2D study and high-fidelity 3D aerodynamic benchmarks (car and aircraft), validated by OpenFOAM simulations and miniature wind-tunnel tests with 3D-printed prototypes, demonstrate consistent gains in both optimization and guided generation. Additional offline RL results further support the generality of our approach.",
    "published": "2026-02-03T14:32:26Z",
    "updated": "2026-02-05T15:47:18Z",
    "link": "http://arxiv.org/pdf/2602.03582v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Huaguan Chen",
      "Ning Lin",
      "Luxi Chen",
      "Rui Zhang",
      "Wenbing Huang",
      "Chongxuan Li",
      "Hao Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05790v1",
    "title": "Price of universality in vector quantization is at most 0.11 bit",
    "summary": "Fast computation of a matrix product $W^\\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\\widehat W$ in place of true $W$ (\"weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as \"waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive.\n  Equivalently, our result shows existence of a net in $\\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms.",
    "published": "2026-02-05T15:46:53Z",
    "updated": "2026-02-05T15:46:53Z",
    "link": "http://arxiv.org/pdf/2602.05790v1.pdf",
    "category": [
      "cs.IT",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Alina Harbuzova",
      "Or Ordentlich",
      "Yury Polyanskiy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.16858v5",
    "title": "Improved Generalization Bounds for Transductive Learning by Transductive Local Complexity and Its Applications",
    "summary": "We introduce Transductive Local Complexity (TLC) to extend the classical Local Rademacher Complexity (LRC) to the transductive setting, incorporating substantial and novel components. Although LRC has been used to obtain sharp generalization bounds and minimax rates for inductive tasks such as classification and nonparametric regression, it has remained an open problem whether a localized Rademacher complexity framework can be effectively adapted to transductive learning to achieve sharp or nearly sharp bounds consistent with inductive results. We provide an affirmative answer via TLC. TLC is constructed by first deriving a new concentration inequality in Theorem 4.1 for the supremum of empirical processes capturing the gap between test and training losses, termed the test-train process, under uniform sampling without replacement, which leverages a novel combinatorial property of the test-train process and a new proof strategy applying the exponential Efron-Stein inequality twice. A subsequent peeling strategy applied to a new decomposition of the expectation of the test-train process and a new surrogate variance operator then yield excess risk bounds in the transductive setting that are nearly consistent with classical LRC-based inductive bounds up to a logarithmic gap. We further advance transductive learning through two applications: (1) for realizable transductive learning over binary-valued classes with finite VC dimension of $\\dVC$ and $u \\ge m \\ge \\dVC$, where $u$ and $m$ are the number of test features and training features, our Theorem 6.1 gives a nearly optimal bound $Θ(\\dVC \\log(me/\\dVC)/m)$ matching the minimax rate $Θ(\\dVC/m)$ up to $\\log m$, resolving a decade-old open question; and (2) Theorem 6.2 presents a sharper excess risk bound for transductive kernel learning compared to the current state-of-the-art.",
    "published": "2023-09-28T21:21:44Z",
    "updated": "2026-02-05T15:46:27Z",
    "link": "http://arxiv.org/pdf/2309.16858v5.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Yingzhen Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05786v1",
    "title": "Selecting Hyperparameters for Tree-Boosting",
    "summary": "Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.",
    "published": "2026-02-05T15:44:42Z",
    "updated": "2026-02-05T15:44:42Z",
    "link": "http://arxiv.org/pdf/2602.05786v1.pdf",
    "category": [
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "authors": [
      "Floris Jan Koster",
      "Fabio Sigrist"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.09643v2",
    "title": "A Policy Gradient-Based Sequence-to-Sequence Method for Time Series Prediction",
    "summary": "Sequence-to-sequence architectures built upon recurrent neural networks have become a standard choice for multi-step-ahead time series prediction. In these models, the decoder produces future values conditioned on contextual inputs, typically either actual historical observations (ground truth) or previously generated predictions. During training, feeding ground-truth values helps stabilize learning but creates a mismatch between training and inference conditions, known as exposure bias, since such true values are inaccessible during real-world deployment. On the other hand, using the model's own outputs as inputs at test time often causes errors to compound rapidly across prediction steps. To mitigate these limitations, we introduce a new training paradigm grounded in reinforcement learning: a policy gradient-based method to learn an adaptive input selection strategy for sequence-to-sequence prediction models. Auxiliary models first synthesize plausible input candidates for the decoder, and a trainable policy network optimized via policy gradients dynamically chooses the most beneficial inputs to maximize long-term prediction performance. Empirical evaluations on diverse time series datasets confirm that our approach enhances both accuracy and stability in multi-step forecasting compared to conventional methods.",
    "published": "2024-06-14T00:24:29Z",
    "updated": "2026-02-05T15:42:52Z",
    "link": "http://arxiv.org/pdf/2406.09643v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Qi Sima",
      "Xinze Zhang",
      "Yukun Bao",
      "Siyue Yang",
      "Liang Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05783v1",
    "title": "Distributional Reinforcement Learning with Diffusion Bridge Critics",
    "summary": "Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.",
    "published": "2026-02-05T15:40:14Z",
    "updated": "2026-02-05T15:40:14Z",
    "link": "http://arxiv.org/pdf/2602.05783v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shutong Ding",
      "Yimiao Zhou",
      "Ke Hu",
      "Mokai Pan",
      "Shan Zhong",
      "Yanwei Fu",
      "Jingya Wang",
      "Ye Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05779v1",
    "title": "How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs",
    "summary": "The intermediate layers of deep networks can be characterised as a Gaussian process, in particular the Edge-of-Chaos (EoC) initialisation strategy prescribes the limiting covariance matrix of the Gaussian process. Here we show that the under-utilised chosen variance of the Gaussian process is important in the training of deep networks with sparsity inducing activation, such as a shifted and clipped ReLU, $\\text{CReLU}_{τ,m}(x)=\\min(\\max(x-τ,0),m)$. Specifically, initialisations leading to larger fixed Gaussian process variances, allow for improved expressivity with activation sparsity as large as 90% in DNNs and CNNs, and generally improve the stability of the training process. Enabling full, or near full, accuracy at such high levels of sparsity in the hidden layers suggests a promising mechanism to reduce the energy consumption of machine learning models involving fully connected layers.",
    "published": "2026-02-05T15:38:37Z",
    "updated": "2026-02-05T15:38:37Z",
    "link": "http://arxiv.org/pdf/2602.05779v1.pdf",
    "category": [
      "cs.LG",
      "cs.IT"
    ],
    "authors": [
      "Emily Dent",
      "Jared Tanner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05776v1",
    "title": "Cross-Domain Offline Policy Adaptation via Selective Transition Correction",
    "summary": "It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.",
    "published": "2026-02-05T15:37:29Z",
    "updated": "2026-02-05T15:37:29Z",
    "link": "http://arxiv.org/pdf/2602.05776v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mengbei Yan",
      "Jiafei Lyu",
      "Shengjie Sun",
      "Zhongjian Qiao",
      "Jingwen Yang",
      "Zichuan Lin",
      "Deheng Ye",
      "Xiu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05767v1",
    "title": "PMT Waveform Simulation and Reconstruction with Conditional Diffusion Network",
    "summary": "Photomultiplier tubes (PMTs) are widely employed in particle and nuclear physics experiments. The accuracy of PMT waveform reconstruction directly impacts the detector's spatial and energy resolution. A key challenge arises when multiple photons arrive within a few nanoseconds, making it difficult to resolve individual photoelectrons (PEs). Although supervised deep learning methods have surpassed traditional methods in performance, their practical applicability is limited by the lack of ground-truth PE labels in real data. To address this issue, we propose an innovative weakly supervised waveform simulation and reconstruction approach based on a bidirectional conditional diffusion network framework. The method is fully data-driven and requires only raw waveforms and coarse estimates of PE information as input. It first employs a PE-conditioned diffusion model to simulate realistic waveforms from PE sequences, thereby learning the features of overlapping waveforms. Subsequently, these simulated waveforms are used to train a waveform-conditioned diffusion model to reconstruct the PE sequences from waveforms, reinforcing the learning of features of overlapping waveforms. Through iterative refinement between the two conditional diffusion processes, the model progressively improves reconstruction accuracy. Experimental results demonstrate that the proposed method achieves 99% of the normalized PE-number resolution averaged over 1-5 p.e. and 80% of the timing resolution attained by fully supervised learning.",
    "published": "2026-02-05T15:30:47Z",
    "updated": "2026-02-05T15:30:47Z",
    "link": "http://arxiv.org/pdf/2602.05767v1.pdf",
    "category": [
      "hep-ex",
      "cs.LG"
    ],
    "authors": [
      "Kainan Liu",
      "Jingyu Huang",
      "Guihong Huang",
      "Jianyi Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05749v1",
    "title": "How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning",
    "summary": "Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.",
    "published": "2026-02-05T15:16:04Z",
    "updated": "2026-02-05T15:16:04Z",
    "link": "http://arxiv.org/pdf/2602.05749v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kai Ming Ting",
      "Wei-Jie Xu",
      "Hang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13817v3",
    "title": "Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network",
    "summary": "6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.",
    "published": "2026-01-20T10:24:10Z",
    "updated": "2026-02-05T15:15:57Z",
    "link": "http://arxiv.org/pdf/2601.13817v3.pdf",
    "category": [
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Haitao Zhao",
      "Xiaoyu Tang",
      "Bo Xu",
      "Jinlong Sun",
      "Linghao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05742v1",
    "title": "Fast Rates for Nonstationary Weighted Risk Minimization",
    "summary": "Weighted empirical risk minimization is a common approach to prediction under distribution drift. This article studies its out-of-sample prediction error under nonstationarity. We provide a general decomposition of the excess risk into a learning term and an error term associated with distribution drift, and prove oracle inequalities for the learning error under mixing conditions. The learning bound holds uniformly over arbitrary weight classes and accounts for the effective sample size induced by the weight vector, the complexity of the weight and hypothesis classes, and potential data dependence. We illustrate the applicability and sharpness of our results in (auto-) regression problems with linear models, basis approximations, and neural networks, recovering minimax-optimal rates (up to logarithmic factors) when specialized to unweighted and stationary settings.",
    "published": "2026-02-05T15:10:07Z",
    "updated": "2026-02-05T15:10:07Z",
    "link": "http://arxiv.org/pdf/2602.05742v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Tobias Brock",
      "Thomas Nagler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02830v2",
    "title": "SC3D: Dynamic and Differentiable Causal Discovery for Temporal and Instantaneous Graphs",
    "summary": "Discovering causal structures from multivariate time series is a key problem because interactions span across multiple lags and possibly involve instantaneous dependencies. Additionally, the search space of the dynamic graphs is combinatorial in nature. In this study, we propose \\textit{Stable Causal Dynamic Differentiable Discovery (SC3D)}, a two-stage differentiable framework that jointly learns lag-specific adjacency matrices and, if present, an instantaneous directed acyclic graph (DAG). In Stage 1, SC3D performs edge preselection through node-wise prediction to obtain masks for lagged and instantaneous edges, whereas Stage 2 refines these masks by optimizing a likelihood with sparsity along with enforcing acyclicity on the instantaneous block. Numerical results across synthetic and benchmark dynamical systems demonstrate that SC3D achieves improved stability and more accurate recovery of both lagged and instantaneous causal structures compared to existing temporal baselines.",
    "published": "2026-02-02T21:32:18Z",
    "updated": "2026-02-05T15:06:56Z",
    "link": "http://arxiv.org/pdf/2602.02830v2.pdf",
    "category": [
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Sourajit Das",
      "Dibyajyoti Chakraborty",
      "Romit Maulik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05725v1",
    "title": "Muon in Associative Memory Learning: Training Dynamics and Scaling Laws",
    "summary": "Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.",
    "published": "2026-02-05T14:49:40Z",
    "updated": "2026-02-05T14:49:40Z",
    "link": "http://arxiv.org/pdf/2602.05725v1.pdf",
    "category": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Binghui Li",
      "Kaifei Wang",
      "Han Zhong",
      "Pinyan Lu",
      "Liwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25753v2",
    "title": "How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs",
    "summary": "Pretrained Transformers demonstrate remarkable in-context learning (ICL) capabilities, enabling them to adapt to new tasks from demonstrations without parameter updates. However, theoretical studies often rely on simplified architectures (e.g., omitting MLPs), plain data models (e.g., linear regression with isotropic inputs), and single-source training, limiting their relevance to realistic settings. In this work, we study ICL in pretrained Transformers with nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with heterogeneous input, task, and noise distributions. We analyze a model where the MLP comprises two layers, with the first layer trained via a single gradient step and the second layer fully optimized. Under high-dimensional asymptotics, we prove that such models are equivalent in ICL error to structured polynomial predictors, leveraging results from the theory of Gaussian universality and orthogonal polynomials. This equivalence reveals that nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear tasks, compared to linear baselines. It also enables a precise analysis of data mixing effects: we identify key properties of high-quality data sources (low noise, structured covariances) and show that feature learning emerges only when the task covariance exhibits sufficient structure. These results are validated empirically across various activation functions, model sizes, and data distributions. Finally, we experiment with a real-world scenario involving multilingual sentiment analysis where each language is treated as a different source. Our experimental results for this case exemplify how our findings extend to real-world cases. Overall, our work advances the theoretical foundations of ICL in Transformers and provides actionable insight into the role of architecture and data in ICL.",
    "published": "2025-10-29T17:51:57Z",
    "updated": "2026-02-05T14:46:08Z",
    "link": "http://arxiv.org/pdf/2510.25753v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Samet Demir",
      "Zafer Dogan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12161v2",
    "title": "Streaming Operator Inference for Model Reduction of Large-Scale Dynamical Systems",
    "summary": "Projection-based model reduction enables efficient simulation of complex dynamical systems by constructing low-dimensional surrogate models from high-dimensional data. The Operator Inference (OpInf) approach learns such reduced surrogate models through a two-step process: constructing a low-dimensional basis via Singular Value Decomposition (SVD) to compress the data, then solving a linear least-squares (LS) problem to infer reduced operators that govern the dynamics in this compressed space, all without access to the underlying code or full model operators, i.e., non-intrusively. Traditional OpInf operates as a batch learning method, where both the SVD and LS steps process all data simultaneously. This poses a barrier to deployment of the approach on large-scale applications where dataset sizes prevent the loading of all data into memory at once. Additionally, the traditional batch approach does not naturally allow model updates using new data acquired during online computation. To address these limitations, we propose Streaming OpInf, which learns reduced models from sequentially arriving data streams. Our approach employs incremental SVD for adaptive basis construction and recursive LS for streaming operator updates, eliminating the need to store complete data sets while enabling online model adaptation. The approach can flexibly combine different choices of streaming algorithms for numerical linear algebra: we systematically explore the impact of these choices both analytically and numerically to identify effective combinations for accurate reduced model learning. Numerical experiments on benchmark problems and a large-scale turbulent channel flow demonstrate that Streaming OpInf achieves accuracy comparable to batch OpInf while reducing memory requirements by over 99% and enabling dimension reductions exceeding 31,000x, resulting in orders-of-magnitude faster predictions.",
    "published": "2026-01-17T20:46:47Z",
    "updated": "2026-02-05T14:45:48Z",
    "link": "http://arxiv.org/pdf/2601.12161v2.pdf",
    "category": [
      "math.NA",
      "cs.LG",
      "math.DS",
      "physics.comp-ph"
    ],
    "authors": [
      "Tomoki Koike",
      "Prakash Mohan",
      "Marc T. Henry de Frahan",
      "Julie Bessac",
      "Elizabeth Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.22186v2",
    "title": "Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems",
    "summary": "Thompson sampling (TS) is a Bayesian randomized exploration strategy that samples options (e.g., system parameters or control laws) from the current posterior and then applies the selected option that is optimal for a task, thereby balancing exploration and exploitation; this makes TS effective for active learning-based controller design. However, TS relies on finite parametric representations, which limits its applicability to more general spaces, which are more commonly encountered in control system design. To address this issue, this work proposes a parameterization method for control law learning using reproducing kernel Hilbert spaces and designs a data-driven active learning control approach. Specifically, the proposed method treats the control law as an element in a function space, allowing the design of control laws without imposing restrictions on the system structure or the form of the controller. A TS framework is proposed in this work to reduce control costs through online exploration and exploitation, and the convergence guarantees are further provided for the learning process. Theoretical analysis shows that the proposed method learns the relationship between control laws and closed-loop performance metrics at an exponential rate, and the upper bound of control regret is also derived. Furthermore, the closed-loop stability of the proposed learning framework is analyzed. Numerical experiments on controlling unknown nonlinear systems validate the effectiveness of the proposed method.",
    "published": "2025-06-27T12:49:43Z",
    "updated": "2026-02-05T14:44:35Z",
    "link": "http://arxiv.org/pdf/2506.22186v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kaikai Zheng",
      "Dawei Shi",
      "Yang Shi",
      "Long Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05713v1",
    "title": "Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions",
    "summary": "Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a \"fairness cost\" term $δ_t = \\sqrt{\\mathrm{KL}(w^t \\| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.",
    "published": "2026-02-05T14:38:32Z",
    "updated": "2026-02-05T14:38:32Z",
    "link": "http://arxiv.org/pdf/2602.05713v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Amir Asiaee",
      "Kaveh Aryan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05707v1",
    "title": "Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift",
    "summary": "Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent \"fairness helps accuracy\" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.",
    "published": "2026-02-05T14:32:05Z",
    "updated": "2026-02-05T14:32:05Z",
    "link": "http://arxiv.org/pdf/2602.05707v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Amir Asiaee",
      "Kaveh Aryan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05704v1",
    "title": "Limitations of SGD for Multi-Index Models Beyond Statistical Queries",
    "summary": "Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.",
    "published": "2026-02-05T14:29:10Z",
    "updated": "2026-02-05T14:29:10Z",
    "link": "http://arxiv.org/pdf/2602.05704v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Daniel Barzilai",
      "Ohad Shamir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05702v1",
    "title": "Broken neural scaling laws in materials science",
    "summary": "In materials science, data are scarce and expensive to generate, whether computationally or experimentally. Therefore, it is crucial to identify how model performance scales with dataset size and model capacity to distinguish between data- and model-limited regimes. Neural scaling laws provide a framework for quantifying this behavior and guide the design of materials datasets and machine learning architectures. Here, we investigate neural scaling laws for a paradigmatic materials science task: predicting the dielectric function of metals, a high-dimensional response that governs how solids interact with light. Using over 200,000 dielectric functions from high-throughput ab initio calculations, we study two multi-objective graph neural networks trained to predict the frequency-dependent complex interband dielectric function and the Drude frequency. We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law that rapidly saturates.",
    "published": "2026-02-05T14:27:08Z",
    "updated": "2026-02-05T14:27:08Z",
    "link": "http://arxiv.org/pdf/2602.05702v1.pdf",
    "category": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "authors": [
      "Max Großmann",
      "Malte Grunert",
      "Erich Runge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05693v1",
    "title": "FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning",
    "summary": "Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.",
    "published": "2026-02-05T14:19:21Z",
    "updated": "2026-02-05T14:19:21Z",
    "link": "http://arxiv.org/pdf/2602.05693v1.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Arno Geimer",
      "Beltran Fiz Pontiveros",
      "Radu State"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05690v1",
    "title": "Almost Asymptotically Optimal Active Clustering Through Pairwise Observations",
    "summary": "We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.",
    "published": "2026-02-05T14:16:47Z",
    "updated": "2026-02-05T14:16:47Z",
    "link": "http://arxiv.org/pdf/2602.05690v1.pdf",
    "category": [
      "cs.LG",
      "cs.IT"
    ],
    "authors": [
      "Rachel S. Y. Teo",
      "P. N. Karthik",
      "Ramya Korlakai Vinayak",
      "Vincent Y. F. Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17667v2",
    "title": "Entropic Risk-Aware Monte Carlo Tree Search",
    "summary": "We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving risk-aware Markov decision processes (MDPs) with entropic risk measure (ERM) objectives. We provide a non-asymptotic analysis of our proposed algorithm, showing that the algorithm: (i) is correct in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys polynomial regret concentration. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.",
    "published": "2026-01-25T03:07:31Z",
    "updated": "2026-02-05T14:15:55Z",
    "link": "http://arxiv.org/pdf/2601.17667v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Pedro P. Santos",
      "Jacopo Silvestrin",
      "Alberto Sardinha",
      "Francisco S. Melo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05679v1",
    "title": "Perception-Based Beliefs for POMDPs with Visual Observations",
    "summary": "Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.",
    "published": "2026-02-05T14:01:39Z",
    "updated": "2026-02-05T14:01:39Z",
    "link": "http://arxiv.org/pdf/2602.05679v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Miriam Schäfers",
      "Merlijn Krale",
      "Thiago D. Simão",
      "Nils Jansen",
      "Maximilian Weininger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.07061v3",
    "title": "Local EGOP for Continuous Index Learning",
    "summary": "We introduce the setting of continuous index learning, in which a function of many variables varies only along a small number of directions at each point. For efficient estimation, it is beneficial for a learning algorithm to adapt, near each point $x$, to the subspace that captures the local variability of the function $f$. We pose this task as kernel adaptation along a manifold with noise, and introduce Local EGOP learning, a recursive algorithm that utilizes the Expected Gradient Outer Product (EGOP) quadratic form as both a metric and inverse-covariance of our target distribution. We prove that Local EGOP learning adapts to the regularity of the function of interest, showing that under a supervised noisy manifold hypothesis, intrinsic dimensional learning rates are achieved for arbitrarily high-dimensional noise. Empirically, we compare our algorithm to the feature learning capabilities of deep learning. Additionally, we demonstrate improved regression quality compared to two-layer neural networks in the continuous single-index setting.",
    "published": "2026-01-11T21:03:02Z",
    "updated": "2026-02-05T14:01:17Z",
    "link": "http://arxiv.org/pdf/2601.07061v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Alex Kokot",
      "Anand Hemmady",
      "Vydhourie Thiyageswaran",
      "Marina Meila"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13762v2",
    "title": "Progressive multi-fidelity learning with neural networks for physical system predictions",
    "summary": "Highly accurate datasets from numerical or physical experiments are often expensive and time-consuming to acquire, posing a significant challenge for applications that require precise evaluations, potentially across multiple scenarios and in real-time. Even building sufficiently accurate surrogate models can be extremely challenging with limited high-fidelity data. Conversely, less expensive, low-fidelity data can be computed more easily and encompass a broader range of scenarios. By leveraging multi-fidelity information, prediction capabilities of surrogates can be improved. However, in practical situations, data may be different in types, come from sources of different modalities, and not be concurrently available, further complicating the modeling process. To address these challenges, we introduce a progressive multi-fidelity surrogate model. This model can sequentially incorporate diverse data types using tailored encoders. Multi-fidelity regression from the encoded inputs to the target quantities of interest is then performed using neural networks. Input information progressively flows from lower to higher fidelity levels through two sets of connections: concatenations among all the encoded inputs, and additive connections among the final outputs. This dual connection system enables the model to exploit correlations among different datasets while ensuring that each level makes an additive correction to the previous level without altering it. This approach prevents performance degradation as new input data are integrated into the model and automatically adapts predictions based on the available inputs. We demonstrate the effectiveness of the approach on numerical benchmarks and a real-world case study, showing that it reliably integrates multi-modal data and provides accurate predictions, maintaining performance when generalizing across time and parameter variations.",
    "published": "2025-10-15T17:10:47Z",
    "updated": "2026-02-05T13:59:56Z",
    "link": "http://arxiv.org/pdf/2510.13762v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Paolo Conti",
      "Mengwu Guo",
      "Attilio Frangi",
      "Andrea Manzoni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05667v1",
    "title": "Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection",
    "summary": "Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS",
    "published": "2026-02-05T13:50:39Z",
    "updated": "2026-02-05T13:50:39Z",
    "link": "http://arxiv.org/pdf/2602.05667v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ling Zhan",
      "Zhen Li",
      "Junjie Huang",
      "Tao Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05657v1",
    "title": "Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization",
    "summary": "The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \\in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\\log(t)}$, where $β_p = \\frac{4(p-1)}{3p-2}$ for $p \\in (1,2)$ and $e^{-t/\\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \\in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.",
    "published": "2026-02-05T13:41:13Z",
    "updated": "2026-02-05T13:41:13Z",
    "link": "http://arxiv.org/pdf/2602.05657v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Aleksandar Armacki",
      "Dragana Bajović",
      "Dušan Jakovetić",
      "Soummya Kar",
      "Ali H. Sayed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.14865v4",
    "title": "Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning",
    "summary": "We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.",
    "published": "2024-12-19T14:00:03Z",
    "updated": "2026-02-05T13:36:27Z",
    "link": "http://arxiv.org/pdf/2412.14865v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Anthony Kobanda",
      "Rémy Portelas",
      "Odalric-Ambrym Maillard",
      "Ludovic Denoyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19842v2",
    "title": "Symplectic convolutional neural networks",
    "summary": "We propose a new symplectic convolutional neural network (CNN) architecture by leveraging symplectic neural networks, proper symplectic decomposition, and tensor techniques. Specifically, we first introduce a mathematically equivalent form of the convolution layer and then, using symplectic neural networks, we demonstrate a way to parameterize the layers of the CNN to ensure that the convolution layer remains symplectic. To construct a complete autoencoder, we introduce a symplectic pooling layer. We demonstrate the performance of the proposed neural network on three examples: the wave equation, the nonlinear Schrödinger (NLS) equation, and the sine-Gordon equation. The numerical results indicate that the symplectic CNN outperforms the linear symplectic autoencoder obtained via proper symplectic decomposition.",
    "published": "2025-08-27T12:55:24Z",
    "updated": "2026-02-05T13:34:31Z",
    "link": "http://arxiv.org/pdf/2508.19842v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Süleyman Yıldız",
      "Konrad Janik",
      "Peter Benner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05649v1",
    "title": "End-to-End Compression for Tabular Foundation Models",
    "summary": "The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.",
    "published": "2026-02-05T13:33:58Z",
    "updated": "2026-02-05T13:33:58Z",
    "link": "http://arxiv.org/pdf/2602.05649v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Guri Zabërgja",
      "Rafiq Kamel",
      "Arlind Kadra",
      "Christian M. M. Frey",
      "Josif Grabocka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05646v1",
    "title": "Empowering Time Series Analysis with Large-Scale Multimodal Pretraining",
    "summary": "While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.",
    "published": "2026-02-05T13:26:35Z",
    "updated": "2026-02-05T13:26:35Z",
    "link": "http://arxiv.org/pdf/2602.05646v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Peng Chen",
      "Siyuan Wang",
      "Shiyan Hu",
      "Xingjian Wu",
      "Yang Shu",
      "Zhongwen Rao",
      "Meng Wang",
      "Yijie Li",
      "Bin Yang",
      "Chenjuan Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06141v3",
    "title": "High-probability Convergence Guarantees of Decentralized SGD",
    "summary": "Convergence in high-probability (HP) has attracted increasing interest, due to implying exponentially decaying tail bounds and strong guarantees for individual runs of an algorithm. While many works study HP guarantees in centralized settings, much less is understood in the decentralized setup, where existing works require strong assumptions, like uniformly bounded gradients, or asymptotically vanishing noise. This results in a significant gap between the assumptions used to establish convergence in the HP and the mean-squared error (MSE) sense, and is also contrary to centralized settings, where it is known that $\\mathtt{SGD}$ converges in HP under the same conditions on the cost function as needed for MSE convergence. Motivated by these observations, we study the HP convergence of Decentralized $\\mathtt{SGD}$ ($\\mathtt{DSGD}$) in the presence of light-tailed noise, providing several strong results. First, we show that $\\mathtt{DSGD}$ converges in HP under the same conditions on the cost as in the MSE sense, removing the restrictive assumptions used in prior works. Second, our sharp analysis yields order-optimal rates for both non-convex and strongly convex costs. Third, we establish a linear speed-up in the number of users, leading to matching, or strictly better transient times than those obtained from MSE results, further underlining the tightness of our analysis. To the best of our knowledge, this is the first work that shows $\\mathtt{DSGD}$ achieves a linear speed-up in the HP sense. Our relaxed assumptions and sharp rates stem from several technical results of independent interest, including a result on the variance-reduction effect of decentralized methods in the HP sense, as well as a novel bound on the MGF of strongly convex costs, which is of interest even in centralized settings. Finally, we provide experiments that validate our theory.",
    "published": "2025-10-07T17:15:08Z",
    "updated": "2026-02-05T13:26:07Z",
    "link": "http://arxiv.org/pdf/2510.06141v3.pdf",
    "category": [
      "cs.LG",
      "cs.MA",
      "math.OC"
    ],
    "authors": [
      "Aleksandar Armacki",
      "Ali H. Sayed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05644v1",
    "title": "UAV Trajectory Optimization via Improved Noisy Deep Q-Network",
    "summary": "This paper proposes an Improved Noisy Deep Q-Network (Noisy DQN) to enhance the exploration and stability of Unmanned Aerial Vehicle (UAV) when applying deep reinforcement learning in simulated environments. This method enhances the exploration ability by combining the residual NoisyLinear layer with an adaptive noise scheduling mechanism, while improving training stability through smooth loss and soft target network updates. Experiments show that the proposed model achieves faster convergence and up to $+40$ higher rewards compared to standard DQN and quickly reach to the minimum number of steps required for the task 28 in the 15 * 15 grid navigation environment set up. The results show that our comprehensive improvements to the network structure of NoisyNet, exploration control, and training stability contribute to enhancing the efficiency and reliability of deep Q-learning.",
    "published": "2026-02-05T13:23:47Z",
    "updated": "2026-02-05T13:23:47Z",
    "link": "http://arxiv.org/pdf/2602.05644v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG"
    ],
    "authors": [
      "Zhang Hengyu",
      "Maryam Cheraghy",
      "Liu Wei",
      "Armin Farhadi",
      "Meysam Soltanpour",
      "Zhong Zhuoqing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05639v1",
    "title": "Joint Embedding Variational Bayes",
    "summary": "We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.",
    "published": "2026-02-05T13:18:53Z",
    "updated": "2026-02-05T13:18:53Z",
    "link": "http://arxiv.org/pdf/2602.05639v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Amin Oji",
      "Paul Fieguth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05635v1",
    "title": "Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias",
    "summary": "Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.",
    "published": "2026-02-05T13:14:01Z",
    "updated": "2026-02-05T13:14:01Z",
    "link": "http://arxiv.org/pdf/2602.05635v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ojasva Nema",
      "Kaustubh Sharma",
      "Aditya Chauhan",
      "Parikshit Pareek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04660v2",
    "title": "An Attention-based Feature Memory Design for Energy-Efficient Continual Learning",
    "summary": "Tabular data streams are increasingly prevalent in real-time decision-making across healthcare, finance, and the Internet of Things, often generated and processed on resource-constrained edge and mobile devices. Continual learning (CL) enables models to learn sequentially from such streams while retaining previously acquired knowledge. While recent CL advances have made significant progress in mitigating catastrophic forgetting, the energy and memory efficiency of CL for tabular data streams remains largely unexplored. To address this gap, we propose AttenMLP, which integrates attention-based feature replay with context retrieval and sliding buffer updates within a minibatch training framework for streaming tabular learning.\n  We evaluate AttenMLP against state-of-the-art (SOTA) tabular models on real-world concept drift benchmarks with temporal distribution shifts. Experimental results show that AttenMLP achieves accuracy comparable to strong baselines without replay, while substantially reducing energy consumption through tunable design choices. In particular, with the proposed attention-based feature memory design, AttenMLP costs a 0.062 decrease in final accuracy under the incremental concept drift dataset, while reducing energy usage up to 33.3\\% compared to TabPFNv2. Under the abrupt concept drift dataset, AttenMLP reduces 1.47\\% energy consumption compared to TabR, at the cost of a 0.038 decrease in final accuracy. Although ranking third in global efficiency, AttenMLP demonstrates energy-accuracy trade-offs across both abrupt and incremental concept drift scenarios compared to SOTA tabular models.",
    "published": "2025-10-06T10:05:44Z",
    "updated": "2026-02-05T13:13:25Z",
    "link": "http://arxiv.org/pdf/2510.04660v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuandou Wang",
      "Filip Gunnarsson",
      "Rihan Hai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21818v2",
    "title": "Sharpness-Aware Minimization Can Hallucinate Minimizers",
    "summary": "Sharpness-Aware Minimization (SAM) is widely used to seek flatter minima -- often linked to better generalization. In its standard implementation, SAM updates the current iterate using the loss gradient evaluated at a point perturbed by distance $ρ$ along the normalized gradient direction. We show that, for some choices of $ρ$, SAM can stall at points where this shifted (perturbed-point) gradient vanishes despite a nonzero original gradient, and therefore, they are not stationary points of the original loss. We call these points hallucinated minimizers, prove their existence under simple nonconvex landscape conditions (e.g., the presence of a local minimizer and a local maximizer), and establish sufficient conditions for local convergence of the SAM iterates to them. We corroborate this failure mode in neural network training and observe that it aligns with SAM's performance degradation often seen at large $ρ$. Finally, as a practical safeguard, we find that a short initial SGD warm-start before enabling SAM mitigates this failure mode and reduces sensitivity to the choice of $ρ$.",
    "published": "2025-09-26T03:26:07Z",
    "updated": "2026-02-05T13:11:45Z",
    "link": "http://arxiv.org/pdf/2509.21818v2.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Chanwoong Park",
      "Uijeong Jang",
      "Ernest K. Ryu",
      "Insoon Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.04560v2",
    "title": "GAMformer: Bridging Tabular Foundation Models and Interpretable Machine Learning",
    "summary": "While interpretability is crucial for machine learning applications in safety-critical domains and for regulatory compliance, existing tabular foundation models like TabPFN lack transparency. Generalized Additive Models (GAMs) provide the needed interpretability through their additive structure, but traditional GAM methods rely on iterative learning algorithms (such as splines, boosted trees, or neural networks) that are fundamentally incompatible with the in-context learning paradigm of foundation models. In this paper, we introduce GAMformer, the first tabular foundation model for GAMs that bridges the gap between the power of foundation models and the interpretability requirements of critical real-world applications. GAMformer estimates GAM shape functions in a single forward pass using in-context learning, representing a significant departure from conventional iterative approaches. Building on previous research on tabular foundation models, we train GAMformer exclusively on synthetically generated tables to prevent data leakage. Our experiments demonstrate that GAMformer performs comparably to other leading GAMs across various classification benchmarks.",
    "published": "2024-10-06T17:28:20Z",
    "updated": "2026-02-05T12:56:21Z",
    "link": "http://arxiv.org/pdf/2410.04560v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Andreas Mueller",
      "Julien Siems",
      "Harsha Nori",
      "David Salinas",
      "Arber Zela",
      "Rich Caruana",
      "Frank Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04795v2",
    "title": "Maximum-Volume Nonnegative Matrix Factorization",
    "summary": "Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.",
    "published": "2026-02-04T17:43:25Z",
    "updated": "2026-02-05T12:53:57Z",
    "link": "http://arxiv.org/pdf/2602.04795v2.pdf",
    "category": [
      "cs.LG",
      "eess.SP",
      "math.NA",
      "stat.ML"
    ],
    "authors": [
      "Olivier Vu Thanh",
      "Nicolas Gillis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05600v1",
    "title": "On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature",
    "summary": "Stochastic Gradient Descent (SGD) introduces anisotropic noise that is correlated with the local curvature of the loss landscape, thereby biasing optimization toward flat minima. Prior work often assumes an equivalence between the Fisher Information Matrix and the Hessian for negative log-likelihood losses, leading to the claim that the SGD noise covariance $\\mathbf{C}$ is proportional to the Hessian $\\mathbf{H}$. We show that this assumption holds only under restrictive conditions that are typically violated in deep neural networks. Using the recently discovered Activity--Weight Duality, we find a more general relationship agnostic to the specific loss formulation, showing that $\\mathbf{C} \\propto \\mathbb{E}_p[\\mathbf{h}_p^2]$, where $\\mathbf{h}_p$ denotes the per-sample Hessian with $\\mathbf{H} = \\mathbb{E}_p[\\mathbf{h}_p]$. As a consequence, $\\mathbf{C}$ and $\\mathbf{H}$ commute approximately rather than coincide exactly, and their diagonal elements follow an approximate power-law relation $C_{ii} \\propto H_{ii}^γ$ with a theoretically bounded exponent $1 \\leq γ\\leq 2$, determined by per-sample Hessian spectra. Experiments across datasets, architectures, and loss functions validate these bounds, providing a unified characterization of the noise-curvature relationship in deep learning.",
    "published": "2026-02-05T12:35:13Z",
    "updated": "2026-02-05T12:35:13Z",
    "link": "http://arxiv.org/pdf/2602.05600v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yikuan Zhang",
      "Ning Yang",
      "Yuhai Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05591v1",
    "title": "Efficient Algorithms for Robust Markov Decision Processes with $s$-Rectangular Ambiguity Sets",
    "summary": "Robust Markov decision processes (MDPs) have attracted significant interest due to their ability to protect MDPs from poor out-of-sample performance in the presence of ambiguity. In contrast to classical MDPs, which account for stochasticity by modeling the dynamics through a stochastic process with a known transition kernel, a robust MDP additionally accounts for ambiguity by optimizing against the most adverse transition kernel from an ambiguity set constructed via historical data. In this paper, we develop a unified solution framework for a broad class of robust MDPs with $s$-rectangular ambiguity sets, where the most adverse transition probabilities are considered independently for each state. Using our algorithms, we show that $s$-rectangular robust MDPs with $1$- and $2$-norm as well as $φ$-divergence ambiguity sets can be solved several orders of magnitude faster than with state-of-the-art commercial solvers, and often only a logarithmic factor slower than classical MDPs. We demonstrate the favorable scaling properties of our algorithms on a range of synthetically generated as well as standard benchmark instances.",
    "published": "2026-02-05T12:20:26Z",
    "updated": "2026-02-05T12:20:26Z",
    "link": "http://arxiv.org/pdf/2602.05591v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Chin Pang Ho",
      "Marek Petrik",
      "Wolfram Wiesemann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04653v2",
    "title": "Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates",
    "summary": "Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.",
    "published": "2026-02-04T15:28:53Z",
    "updated": "2026-02-05T11:59:44Z",
    "link": "http://arxiv.org/pdf/2602.04653v2.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Ariel Fogel",
      "Omer Hofman",
      "Eilon Cohen",
      "Roman Vainshtein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05576v1",
    "title": "OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph",
    "summary": "Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.",
    "published": "2026-02-05T11:59:40Z",
    "updated": "2026-02-05T11:59:40Z",
    "link": "http://arxiv.org/pdf/2602.05576v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chenxi Wan",
      "Xunkai Li",
      "Yilong Zuo",
      "Haokun Deng",
      "Sihan Li",
      "Bowen Fan",
      "Hongchao Qin",
      "Ronghua Li",
      "Guoren Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05571v1",
    "title": "EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking",
    "summary": "Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\\%, a +3.8 pp improvement over the prior state of the art (74.2\\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/",
    "published": "2026-02-05T11:52:06Z",
    "updated": "2026-02-05T11:52:06Z",
    "link": "http://arxiv.org/pdf/2602.05571v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Rishabh Bhattacharya",
      "Naresh Manwani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24779v4",
    "title": "Are Your Generated Instances Truly Useful? GenBench-MILP: A Benchmark Suite for MILP Instance Generation",
    "summary": "The proliferation of machine learning-based methods for Mixed-Integer Linear Programming (MILP) instance generation has surged, driven by the need for diverse training datasets. However, a critical question remains: Are these generated instances truly useful and realistic? Current evaluation protocols often rely on superficial structural metrics or simple solvability checks, which frequently fail to capture the true computational complexity of real-world problems. To bridge this gap, we introduce GenBench-MILP, a comprehensive benchmark suite designed for the standardized and objective evaluation of MILP generators. Our framework assesses instance quality across four key dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream tasks. A distinctive innovation of GenBench-MILP is the analysis of solver-internal features -- including root node gaps, heuristic success rates, and cut plane usage. By treating the solver's dynamic behavior as an expert assessment, we reveal nuanced computational discrepancies that static graph features miss. Our experiments on instance generative models demonstrate that instances with high structural similarity scores can still exhibit drastically divergent solver interactions and difficulty levels. By providing this multifaceted evaluation toolkit, GenBench-MILP aims to facilitate rigorous comparisons and guide the development of high-fidelity instance generators.",
    "published": "2025-05-30T16:42:15Z",
    "updated": "2026-02-05T11:41:59Z",
    "link": "http://arxiv.org/pdf/2505.24779v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yidong Luo",
      "Chenguang Wang",
      "Dong Li",
      "Tianshu Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05567v1",
    "title": "MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks",
    "summary": "Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.",
    "published": "2026-02-05T11:39:49Z",
    "updated": "2026-02-05T11:39:49Z",
    "link": "http://arxiv.org/pdf/2602.05567v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Long D. Nguyen",
      "Binh P. Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17001v2",
    "title": "Should Bias be Eliminated? A General Framework to Use Bias for OOD Generalization",
    "summary": "Most approaches to out-of-distribution (OOD) generalization learn domain-invariant representations by discarding contextual bias. In this paper, we raise a critical question: Should bias be eliminated? If not, is there a general way to leverage bias for better OOD generalization? To answer these questions, we first provide a theoretical analysis that characterizes the circumstances in which biased features contribute positively. Although theoretical results show that bias may sometimes play a positive role, leveraging it effectively is non-trivial, since its harmful and beneficial components are often entangled. Recent advances have sought to refine the prediction of bias by presuming reliable predictions from invariant features. However, such assumptions may be too strong in the real world, especially when the target also shifts from training to testing domains. Motivated by this challenge, we introduce a framework to leverage bias in a more general scenario. Specifically, we employ a generative model to capture the data generation process and identify the underlying bias factors, which are then used to construct a bias-aware predictor. Since the bias-aware predictor may shift across environments, we first estimate the environment state to train predictors under different environments, combining them as a mixture of domain experts for the final prediction. Then, we build a general invariant predictor, which can be invariant under label shift to guide the adaptation of the bias-aware predictor. Evaluations on synthetic data and standard domain generalization benchmarks demonstrate that our method consistently outperforms both invariance only baselines, recent bias utilization approaches and advanced baselines, yielding improved robustness and adaptability.",
    "published": "2025-07-22T20:17:48Z",
    "updated": "2026-02-05T11:17:24Z",
    "link": "http://arxiv.org/pdf/2507.17001v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yan Li",
      "Yunlong Deng",
      "Zijian Li",
      "Anpeng Wu",
      "Zeyu Tang",
      "Kun Zhang",
      "Guangyi Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05549v1",
    "title": "Logical Guidance for the Exact Composition of Diffusion Models",
    "summary": "We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.\n  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.\n  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.\n  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.\n  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.\n  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.\n  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.\n  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.",
    "published": "2026-02-05T11:10:06Z",
    "updated": "2026-02-05T11:10:06Z",
    "link": "http://arxiv.org/pdf/2602.05549v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Francesco Alesiani",
      "Jonathan Warrell",
      "Tanja Bien",
      "Henrik Christiansen",
      "Matheus Ferraz",
      "Mathias Niepert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05541v1",
    "title": "Reducing the Complexity of Matrix Multiplication to $O(N^2log_2N)$ by an Asymptotically Optimal Quantum Algorithm",
    "summary": "Matrix multiplication is a fundamental classical computing operation whose efficiency becomes a major challenge at scale, especially for machine learning applications. Quantum computing, with its inherent parallelism and exponential storage capacity, offers a potential solution to these limitations. This work presents a quantum kernel-based matrix multiplication algorithm (QKMM) that achieves an asymptotically optimal computational complexity of $ O(N^2 \\log_2 N) $, outperforming the classical optimal complexity of $ O(N^{2.371552}) $, where $N$ denotes the matrix dimension. Through noiseless and noisy quantum simulation experiments, we demonstrate that the proposed algorithm not only exhibits superior theoretical efficiency but also shows practical advantages in runtime performance and stability.",
    "published": "2026-02-05T10:58:52Z",
    "updated": "2026-02-05T10:58:52Z",
    "link": "http://arxiv.org/pdf/2602.05541v1.pdf",
    "category": [
      "quant-ph",
      "cs.CC",
      "cs.LG"
    ],
    "authors": [
      "Jiaqi Yao",
      "Ding Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.23177v3",
    "title": "MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics",
    "summary": "We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.\n  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.",
    "published": "2026-01-30T17:02:47Z",
    "updated": "2026-02-05T10:55:13Z",
    "link": "http://arxiv.org/pdf/2601.23177v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mikel M. Iparraguirre",
      "Iciar Alfaro",
      "David Gonzalez",
      "Elias Cueto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05535v1",
    "title": "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification",
    "summary": "Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.",
    "published": "2026-02-05T10:51:39Z",
    "updated": "2026-02-05T10:51:39Z",
    "link": "http://arxiv.org/pdf/2602.05535v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Tao Huang",
      "Rui Wang",
      "Xiaofei Liu",
      "Yi Qin",
      "Li Duan",
      "Liping Jing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05531v1",
    "title": "Solving Stochastic Variational Inequalities without the Bounded Variance Assumption",
    "summary": "We analyze algorithms for solving stochastic variational inequalities (VI) without the bounded variance or bounded domain assumptions, where our main focus is min-max optimization with possibly unbounded constraint sets. We focus on two classes of problems: monotone VIs; and structured nonmonotone VIs that admit a solution to the weak Minty VI. The latter assumption allows us to solve structured nonconvex-nonconcave min-max problems. For both classes of VIs, to make the expected residual norm less than $\\varepsilon$, we show an oracle complexity of $\\widetilde{O}(\\varepsilon^{-4})$, which is the best-known for constrained VIs. In our setting, this complexity had been obtained with the bounded variance assumption in the literature, which is not even satisfied for bilinear min-max problems with an unbounded domain. We obtain this complexity for stochastic oracles whose variance can grow as fast as the squared norm of the optimization variable.",
    "published": "2026-02-05T10:44:04Z",
    "updated": "2026-02-05T10:44:04Z",
    "link": "http://arxiv.org/pdf/2602.05531v1.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Ahmet Alacaoglu",
      "Jun-Hyun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.04897v3",
    "title": "From Link Prediction to Forecasting: Addressing Challenges in Batch-based Temporal Graph Learning",
    "summary": "Dynamic link prediction is an important problem considered in many recent works that propose approaches for learning temporal edge patterns. To assess their efficacy, models are evaluated on continuous-time and discrete-time temporal graph datasets, typically using a traditional batch-oriented evaluation setup. However, as we show in this work, a batch-oriented evaluation is often unsuitable and can cause several issues. Grouping edges into fixed-sized batches regardless of their occurrence time leads to information loss or leakage, depending on the temporal granularity of the data. Furthermore, fixed-size batches create time windows with different durations, resulting in an inconsistent dynamic link prediction task. In this work, we empirically show how traditional batch-based evaluation leads to skewed model performance and hinders the fair comparison of methods. We mitigate this problem by reformulating dynamic link prediction as a link forecasting task that better accounts for temporal information present in the data.",
    "published": "2024-06-07T12:45:12Z",
    "updated": "2026-02-05T10:21:15Z",
    "link": "http://arxiv.org/pdf/2406.04897v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Moritz Lampert",
      "Christopher Blöcker",
      "Ingo Scholtes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01177v2",
    "title": "Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning",
    "summary": "We present a unified information-theoretic framework elucidating the interplay between stability, privacy, and the generalization performance of quantum learning algorithms. We establish a bound on the expected generalization error in terms of quantum mutual information and derive a probabilistic upper bound that generalizes the classical result by Esposito et al. (2021). Complementing these findings, we provide a lower bound on the expected true loss relative to the expected empirical loss. Additionally, we demonstrate that $(\\varepsilon, δ)$-quantum differentially private learning algorithms are stable, thereby ensuring strong generalization guarantees. Finally, we extend our analysis to dishonest learning algorithms, introducing Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy when the learning algorithm is oblivious to specific dataset instances.",
    "published": "2026-02-01T12:03:07Z",
    "updated": "2026-02-05T10:06:53Z",
    "link": "http://arxiv.org/pdf/2602.01177v2.pdf",
    "category": [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ],
    "authors": [
      "Ayanava Dasgupta",
      "Naqueeb Ahmad Warsi",
      "Masahito Hayashi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08262v2",
    "title": "GenIAS: Generator for Instantiating Anomalies in time Series",
    "summary": "Synthetic anomaly injection is a recent and promising approach for time series anomaly detection (TSAD), but existing methods rely on ad hoc, hand-crafted strategies applied to raw time series that fail to capture diverse and complex anomalous patterns, particularly in multivariate settings. We propose a synthetic anomaly generation method named Generator for Instantiating Anomalies in Time Series (GenIAS), which generates realistic and diverse anomalies via a novel learnable perturbation in the latent space of a variational autoencoder. This enables abnormal patterns to be injected across different temporal segments at varying scales based on variational reparameterization. To generate anomalies that align with normal patterns while remaining distinguishable, we introduce a learning strategy that jointly learns the perturbation scale and compact latent representations via a tunable prior, which improves the distinguishability of generated anomalies, as supported by our theoretical analysis. Extensive experiments show that GenIAS produces more diverse and realistic anomalies, and that detection models trained with these anomalies outperform 17 baseline methods on 9 popular TSAD benchmarks.",
    "published": "2025-02-12T10:10:04Z",
    "updated": "2026-02-05T10:00:30Z",
    "link": "http://arxiv.org/pdf/2502.08262v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zahra Zamanzadeh Darban",
      "Qizhou Wang",
      "Geoffrey I. Webb",
      "Shirui Pan",
      "Charu C. Aggarwal",
      "Mahsa Salehi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21292v2",
    "title": "Additive Models Explained: A Computational Complexity Approach",
    "summary": "Generalized Additive Models (GAMs) are commonly considered *interpretable* within the ML community, as their structure makes the relationship between inputs and outputs relatively understandable. Therefore, it may seem natural to hypothesize that obtaining meaningful explanations for GAMs could be performed efficiently and would not be computationally infeasible. In this work, we challenge this hypothesis by analyzing the *computational complexity* of generating different explanations for various forms of GAMs across multiple contexts. Our analysis reveals a surprisingly diverse landscape of both positive and negative complexity outcomes. Particularly, under standard complexity assumptions such as P!=NP, we establish several key findings: (1) in stark contrast to many other common ML models, the complexity of generating explanations for GAMs is heavily influenced by the structure of the input space; (2) the complexity of explaining GAMs varies significantly with the types of component models used - but interestingly, these differences only emerge under specific input domain settings; (3) significant complexity distinctions appear for obtaining explanations in regression tasks versus classification tasks in GAMs; and (4) expressing complex models like neural networks additively (e.g., as neural additive models) can make them easier to explain, though interestingly, this benefit appears only for certain explanation methods and input domains. Collectively, these results shed light on the feasibility of computing diverse explanations for GAMs, offering a rigorous theoretical picture of the conditions under which such computations are possible or provably hard.",
    "published": "2025-10-24T09:40:30Z",
    "updated": "2026-02-05T09:58:34Z",
    "link": "http://arxiv.org/pdf/2510.21292v2.pdf",
    "category": [
      "cs.LG",
      "cs.CC"
    ],
    "authors": [
      "Shahaf Bassan",
      "Michal Moshkovitz",
      "Guy Katz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05489v1",
    "title": "Convergence Rate of the Last Iterate of Stochastic Proximal Algorithms",
    "summary": "We analyze two classical algorithms for solving additively composite convex optimization problems where the objective is the sum of a smooth term and a nonsmooth regularizer: proximal stochastic gradient method for a single regularizer; and the randomized incremental proximal method, which uses the proximal operator of a randomly selected function when the regularizer is given as the sum of many nonsmooth functions. We focus on relaxing the bounded variance assumption that is common, yet stringent, for getting last iterate convergence rates. We prove the $\\widetilde{O}(1/\\sqrt{T})$ rate of convergence for the last iterate of both algorithms under componentwise convexity and smoothness, which is optimal up to log terms. Our results apply directly to graph-guided regularizers that arise in multi-task and federated learning, where the regularizer decomposes as a sum over edges of a collaboration graph.",
    "published": "2026-02-05T09:50:06Z",
    "updated": "2026-02-05T09:50:06Z",
    "link": "http://arxiv.org/pdf/2602.05489v1.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Kevin Kurian Thomas Vaidyan",
      "Michael P. Friedlander",
      "Ahmet Alacaoglu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.14980v3",
    "title": "Softly Constrained Denoisers for Diffusion Models",
    "summary": "Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem when the constraint is misspecified, which is a common issue in scientific applications where constraint formulation is challenging. We propose to integrate guidance-inspired adjustments to the denoiser, instead of the loss or sampling loop. This achieves a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, while maintaining enough flexibility to deviate from it in case of misspecification with observed data.",
    "published": "2025-12-17T00:35:45Z",
    "updated": "2026-02-05T09:09:23Z",
    "link": "http://arxiv.org/pdf/2512.14980v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Victor M. Yeom-Song",
      "Severi Rissanen",
      "Arno Solin",
      "Samuel Kaski",
      "Mingfei Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05459v1",
    "title": "When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL",
    "summary": "Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\\approx$ 20\\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.",
    "published": "2026-02-05T09:08:17Z",
    "updated": "2026-02-05T09:08:17Z",
    "link": "http://arxiv.org/pdf/2602.05459v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jan Malte Töpperwien",
      "Aditya Mohan",
      "Marius Lindauer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19969v3",
    "title": "Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models",
    "summary": "Achieving differential privacy (DP) guarantees in fully decentralized machine learning is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. We present a framework for DP analysis of decentralized gossip-based averaging algorithms with additive node-level noise, from arbitrary views of nodes in a graph. We present an analytical framework based on a linear systems formulation that accurately characterizes privacy leakage between nodes. Our main contribution is showing that the DP guarantees are those of a Gaussian mechanism, where the growth of the squared sensitivity is asymptotically $O(T)$, where $T$ is the number of training rounds, similarly as in the case of central aggregation. As an application of the sensitivity analysis, we show that the excess risk of decentralized private learning for strongly convex losses is asymptotically similar as in centralized private learning.",
    "published": "2025-05-26T13:31:43Z",
    "updated": "2026-02-05T09:02:12Z",
    "link": "http://arxiv.org/pdf/2505.19969v3.pdf",
    "category": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "authors": [
      "Antti Koskela",
      "Tejas Kulkarni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23071v2",
    "title": "Rethinking Multi-Modal Learning from Gradient Uncertainty",
    "summary": "Multi-Modal Learning (MML) integrates information from diverse modalities to improve predictive accuracy. While existing optimization strategies have made significant strides by mitigating gradient direction conflicts, we revisit MML from a gradient-based perspective to explore further improvements. Empirically, we observe an interesting phenomenon: performance fluctuations can persist in both conflict and non-conflict settings. Based on this, we argue that: beyond gradient direction, the intrinsic reliability of gradients acts as a decisive factor in optimization, necessitating the explicit modeling of gradient uncertainty. Guided by this insight, we propose Bayesian-Oriented Gradient Calibration for MML (BOGC-MML). Our approach explicitly models gradients as probability distributions to capture uncertainty, interpreting their precision as evidence within the framework of subjective logic and evidence theory. By subsequently aggregating these signals using a reduced Dempster's combination rule, BOGC-MML adaptively weights gradients based on their reliability to generate a calibrated update. Extensive experiments demonstrate the effectiveness and advantages of the proposed method.",
    "published": "2025-05-29T04:23:22Z",
    "updated": "2026-02-05T08:56:52Z",
    "link": "http://arxiv.org/pdf/2505.23071v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Peizheng Guo",
      "Jingyao Wang",
      "Wenwen Qiang",
      "Jiahuan Zhou",
      "Changwen Zheng",
      "Gang Hua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05448v1",
    "title": "BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs",
    "summary": "Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\\times$ fewer than pairwise methods at near-identical quality.",
    "published": "2026-02-05T08:41:00Z",
    "updated": "2026-02-05T08:41:00Z",
    "link": "http://arxiv.org/pdf/2602.05448v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sheshansh Agrawal",
      "Thien Hang Nguyen",
      "Douwe Kiela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.20383v2",
    "title": "Hierarchical Time Series Forecasting with Robust Reconciliation",
    "summary": "This paper focuses on forecasting hierarchical time-series data, where each higher-level observation equals the sum of its corresponding lower-level time series. In such contexts, the forecast values should be coherent, meaning that the forecast value of each parent series exactly matches the sum of the forecast values of its child series. Existing hierarchical forecasting methods typically generate base forecasts independently for each series and then apply a reconciliation procedure to adjust them so that the resulting forecast values are coherent across the hierarchy. These methods generally derive an optimal reconciliation, using a covariance matrix of the forecast error. In practice, however, the true covariance matrix is unknown and has to be estimated from finite samples in advance. This gap between the true and estimated covariance matrix may degrade forecast performance. To address this issue, we propose a robust optimization framework for hierarchical reconciliation that accounts for uncertainty in the estimated covariance matrix. We first introduce an uncertainty set for the estimated covariance matrix and formulate a reconciliation problem that minimizes the worst-case average of weighted squared residuals over this uncertainty set. We show that our problem can be cast as a semidefinite optimization problem. Numerical experiments demonstrate that the proposed robust reconciliation method achieved better forecast performance than existing hierarchical forecasting methods, which indicates the effectiveness of integrating uncertainty into the reconciliation process.",
    "published": "2025-10-23T09:30:53Z",
    "updated": "2026-02-05T08:33:38Z",
    "link": "http://arxiv.org/pdf/2510.20383v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shuhei Aikawa",
      "Aru Suzuki",
      "Kei Yoshitake",
      "Kanata Teshigawara",
      "Akira Iwabuchi",
      "Ken Kobayashi",
      "Kazuhide Nakata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06505v2",
    "title": "On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data",
    "summary": "The generative adversarial network (GAN) aims to approximate an unknown distribution via a parameterized neural network (NN). While GANs have been widely applied in reinforcement and semi-supervised learning as well as computer vision tasks, selecting their parameters often needs an exhaustive search, and only a few selection methods have been proven to be theoretically optimal. One of the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on optimal parameters for population WGAN is limited to the linear-quadratic-Gaussian (LQG) setting, where the generator NN is linear, and the data is Gaussian. In this paper, we focus on the characterization of optimal solutions of population WGAN beyond the LQG setting. As a basic result, closed-form optimal parameters for one-dimensional WGAN are derived when the NN has non-linear activation functions, and the data is non-Gaussian. For high-dimensional data, we adopt the sliced Wasserstein framework and show that the linear generator can be asymptotically optimal. Moreover, the original sliced WGAN only constrains the projected data marginal instead of the whole one in classical WGAN, and thus, we propose another new unprojected sliced WGAN and identify its asymptotic optimality. Empirical studies show that compared to the celebrated r-principal component analysis (r-PCA) solution, which has cubic complexity to the data dimension, our generator for sliced WGAN can achieve better performance with only linear complexity.",
    "published": "2025-09-08T10:10:37Z",
    "updated": "2026-02-05T08:25:42Z",
    "link": "http://arxiv.org/pdf/2509.06505v2.pdf",
    "category": [
      "cs.LG",
      "cs.IT",
      "stat.ML"
    ],
    "authors": [
      "Yu-Jui Huang",
      "Hsin-Hua Shen",
      "Yu-Chih Huang",
      "Wan-Yi Lin",
      "Shih-Chun Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05410v1",
    "title": "Robust Federated Learning via Byzantine Filtering over Encrypted Updates",
    "summary": "Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.",
    "published": "2026-02-05T07:46:19Z",
    "updated": "2026-02-05T07:46:19Z",
    "link": "http://arxiv.org/pdf/2602.05410v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Adda Akram Bendoukha",
      "Aymen Boudguiga",
      "Nesrine Kaaniche",
      "Renaud Sirdey",
      "Didem Demirag",
      "Sébastien Gambs"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05389v1",
    "title": "A Decomposition-based State Space Model for Multivariate Time-Series Forecasting",
    "summary": "Multivariate time series (MTS) forecasting is crucial for decision-making in domains such as weather, energy, and finance. It remains challenging because real-world sequences intertwine slow trends, multi-rate seasonalities, and irregular residuals. Existing methods often rely on rigid, hand-crafted decompositions or generic end-to-end architectures that entangle components and underuse structure shared across variables. To address these limitations, we propose DecompSSM, an end-to-end decomposition framework using three parallel deep state space model branches to capture trend, seasonal, and residual components. The model features adaptive temporal scales via an input-dependent predictor, a refinement module for shared cross-variable context, and an auxiliary loss that enforces reconstruction and orthogonality. Across standard benchmarks (ECL, Weather, ETTm2, and PEMS04), DecompSSM outperformed strong baselines, indicating the effectiveness of combining component-wise deep state space models and global context refinement.",
    "published": "2026-02-05T07:17:08Z",
    "updated": "2026-02-05T07:17:08Z",
    "link": "http://arxiv.org/pdf/2602.05389v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shunya Nagashima",
      "Shuntaro Suzuki",
      "Shuitsu Koyama",
      "Shinnosuke Hirano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05379v1",
    "title": "Variance Reduction Based Experience Replay for Policy Optimization",
    "summary": "Effective reinforcement learning (RL) for complex stochastic systems requires leveraging historical data collected in previous iterations to accelerate policy optimization. Classical experience replay treats all past observations uniformly and fails to account for their varying contributions to learning. To overcome this limitation, we propose Variance Reduction Experience Replay (VRER), a principled framework that selectively reuses informative samples to reduce variance in policy gradient estimation. VRER is algorithm-agnostic and integrates seamlessly with existing policy optimization methods, forming the basis of our sample-efficient off-policy algorithm, Policy Gradient with VRER (PG-VRER). Motivated by the lack of rigorous theoretical analysis of experience replay, we develop a novel framework that explicitly captures dependencies introduced by Markovian dynamics and behavior-policy interactions. Using this framework, we establish finite-time convergence guarantees for PG-VRER and reveal a fundamental bias-variance trade-off: reusing older experience increases bias but simultaneously reduces gradient variance. Extensive empirical experiments demonstrate that VRER consistently accelerates policy learning and improves performance over state-of-the-art policy optimization algorithms.",
    "published": "2026-02-05T06:58:28Z",
    "updated": "2026-02-05T06:58:28Z",
    "link": "http://arxiv.org/pdf/2602.05379v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Hua Zheng",
      "Wei Xie",
      "M. Ben Feng",
      "Keilung Choy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05371v1",
    "title": "Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting",
    "summary": "Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.",
    "published": "2026-02-05T06:49:01Z",
    "updated": "2026-02-05T06:49:01Z",
    "link": "http://arxiv.org/pdf/2602.05371v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hongyi Li",
      "Han Lin",
      "Jun Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.21200v2",
    "title": "Provably Reliable Classifier Guidance via Cross-Entropy Control",
    "summary": "Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of the log-probability predicted by a probabilistic classifier. In practice, this classifier is usually obtained by minimizing an empirical loss function. While existing statistical theory guarantees good generalization performance when the sample size is sufficiently large, it remains unclear whether such training yields an effective guidance mechanism.\n  We study this question in the context of cross-entropy loss, which is widely used for classifier training. Under mild smoothness assumptions on the classifier, we show that controlling the cross-entropy at each diffusion model step is sufficient to control the corresponding guidance error. In particular, probabilistic classifiers achieving conditional KL divergence $\\varepsilon^2$ induce guidance vectors with mean squared error $\\widetilde O(d \\varepsilon )$, up to constant and logarithmic factors. Our result yields an upper bound on the sampling error of classifier-guided diffusion models and bears resemblance to a reverse log-Sobolev--type inequality. To the best of our knowledge, this is the first result that quantitatively links classifier training to guidance alignment in diffusion models, providing both a theoretical explanation for the empirical success of classifier guidance, and principled guidelines for selecting classifiers that induce effective guidance.",
    "published": "2026-01-29T02:59:04Z",
    "updated": "2026-02-05T06:45:59Z",
    "link": "http://arxiv.org/pdf/2601.21200v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Sharan Sahu",
      "Arisina Banerjee",
      "Yuchen Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2310.09488v2",
    "title": "ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning",
    "summary": "Long-term time series forecasting (LTSF) is important for various domains but is confronted by challenges in handling the complex temporal-contextual relationships. As multivariate input models underperforming some recent univariate counterparts, we posit that the issue lies in the inefficiency of existing multivariate LTSF Transformers to model series-wise relationships: the characteristic differences between series are often captured incorrectly. To address this, we introduce ARM: a multivariate temporal-contextual adaptive learning method, which is an enhanced architecture specifically designed for multivariate LTSF modelling. ARM employs Adaptive Univariate Effect Learning (AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local Smoothing (MKLS), to better handle individual series temporal patterns and correctly learn inter-series dependencies. ARM demonstrates superior performance on multiple benchmarks without significantly increasing computational costs compared to vanilla Transformer, thereby advancing the state-of-the-art in LTSF. ARM is also generally applicable to other LTSF architecture beyond vanilla Transformer.",
    "published": "2023-10-14T04:37:38Z",
    "updated": "2026-02-05T06:38:48Z",
    "link": "http://arxiv.org/pdf/2310.09488v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Shihao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05358v1",
    "title": "Bayesian Neighborhood Adaptation for Graph Neural Networks",
    "summary": "The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.",
    "published": "2026-02-05T06:29:38Z",
    "updated": "2026-02-05T06:29:38Z",
    "link": "http://arxiv.org/pdf/2602.05358v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Paribesh Regmi",
      "Rui Li",
      "Kishan K C"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05352v1",
    "title": "Smoothness Errors in Dynamics Models and How to Avoid Them",
    "summary": "Modern neural networks have shown promise for solving partial differential equations over surfaces, often by discretizing the surface as a mesh and learning with a mesh-aware graph neural network. However, graph neural networks suffer from oversmoothing, where a node's features become increasingly similar to those of its neighbors. Unitary graph convolutions, which are mathematically constrained to preserve smoothness, have been proposed to address this issue. Despite this, in many physical systems, such as diffusion processes, smoothness naturally increases and unitarity may be overconstraining. In this paper, we systematically study the smoothing effects of different GNNs for dynamics modeling and prove that unitary convolutions hurt performance for such tasks. We propose relaxed unitary convolutions that balance smoothness preservation with the natural smoothing required for physical systems. We also generalize unitary and relaxed unitary convolutions from graphs to meshes. In experiments on PDEs such as the heat and wave equations over complex meshes and on weather forecasting, we find that our method outperforms several strong baselines, including mesh-aware transformers and equivariant neural networks.",
    "published": "2026-02-05T06:23:25Z",
    "updated": "2026-02-05T06:23:25Z",
    "link": "http://arxiv.org/pdf/2602.05352v1.pdf",
    "category": [
      "cs.LG",
      "math.SG"
    ],
    "authors": [
      "Edward Berman",
      "Luisa Li",
      "Jung Yeon Park",
      "Robin Walters"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05340v1",
    "title": "Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach",
    "summary": "We consider the sequential experimental design problem in the predict-then-optimize paradigm. In this paradigm, the outputs of the prediction model are used as coefficient vectors in a downstream linear optimization problem. Traditional sequential experimental design aims to control the input variables (features) so that the improvement in prediction accuracy from each experimental outcome (label) is maximized. However, in the predict-then-optimize setting, performance is ultimately evaluated based on the decision loss induced by the downstream optimization, rather than by prediction error. This mismatch between prediction accuracy and decision loss renders traditional decision-blind designs inefficient. To address this issue, we propose a directional-based metric to quantify predictive uncertainty. This metric does not require solving an optimization oracle and is therefore computationally tractable. We show that the resulting sequential design criterion enjoys strong consistency and convergence guarantees. Under a broad class of distributions, we demonstrate that our directional uncertainty-based design attains an earlier stopping time than decision-blind designs. This advantage is further supported by real-world experiments on an LLM job allocation problem.",
    "published": "2026-02-05T06:06:07Z",
    "updated": "2026-02-05T06:06:07Z",
    "link": "http://arxiv.org/pdf/2602.05340v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Beichen Wan",
      "Mo Liu",
      "Paul Grigas",
      "Zuo-Jun Max Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05333v1",
    "title": "Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis",
    "summary": "This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.",
    "published": "2026-02-05T05:57:54Z",
    "updated": "2026-02-05T05:57:54Z",
    "link": "http://arxiv.org/pdf/2602.05333v1.pdf",
    "category": [
      "cs.LG",
      "cs.IT"
    ],
    "authors": [
      "Kosuke Sugiyama",
      "Masato Uchida"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21799v4",
    "title": "PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective",
    "summary": "The ever-growing scale of deep learning models and training data underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing \"matrix-aware\" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.",
    "published": "2025-05-27T22:11:21Z",
    "updated": "2026-02-05T05:37:29Z",
    "link": "http://arxiv.org/pdf/2505.21799v4.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Tim Tsz-Kit Lau",
      "Qi Long",
      "Weijie Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05319v1",
    "title": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
    "summary": "Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.",
    "published": "2026-02-05T05:37:14Z",
    "updated": "2026-02-05T05:37:14Z",
    "link": "http://arxiv.org/pdf/2602.05319v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yinan Huang",
      "Hans Hao-Hsun Hsu",
      "Junran Wang",
      "Bo Dai",
      "Pan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05492v1",
    "title": "Monte Carlo Rendering to Diffusion Curves with Differential BEM",
    "summary": "We present a method for generating vector graphics, in the form of diffusion curves, directly from noisy samples produced by a Monte Carlo renderer. While generating raster images from 3D geometry via Monte Carlo raytracing is commonplace, there is no corresponding practical approach for robustly and directly extracting editable vector images with shading information from 3D geometry. To fill this gap, we formulate the problem as a stochastic optimization problem over the space of geometries and colors of diffusion curve handles, and solve it with the Levenberg-Marquardt algorithm. At the core of our method is a novel differential boundary element method (BEM) framework that reconstructs colors from diffusion curve handles and computes gradients with respect to their parameters, requiring the expensive matrix factorization only once at the beginning of the optimization. Unlike triangulation-based techniques that require a clean domain decomposition, our method is robust to geometrically challenging scenarios, such as intersecting diffusion curves, and to color noise in the target image, enabling the direct use of noisy Monte Carlo samples without requiring a converged, error-free input image. We demonstrate the robustness and broad applicability of our approach across several test cases. Finally, we highlight several open questions raised by our work, which spans both theory and applications.",
    "published": "2026-02-05T09:54:11Z",
    "updated": "2026-02-05T09:54:11Z",
    "link": "http://arxiv.org/pdf/2602.05492v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Ryusuke Sugimoto",
      "Christopher Batty",
      "Siddhartha Chaudhuri",
      "Iliyan Georgiev",
      "Toshiya Hachisuka",
      "Kevin Wampler",
      "Michal Lukáč"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05335v1",
    "title": "Boxplots and quartile plots for grouped and periodic angular data",
    "summary": "Angular observations, or observations lying on the unit circle, arise in many disciplines and require special care in their description, analysis, interpretation and visualization. We provide methods to construct concentric circular boxplot displays of distributions of groups of angular data. The use of concentric boxplots brings challenges of visual perception, so we set the boxwidths to be inversely proportional to the square root of their distance from the centre. A perception survey supports this scaled boxwidth choice. For a large number of groups, we propose circular quartile plots. A three-dimensional toroidal display is also implemented for periodic angular distributions. We illustrate our methods on datasets in (1) psychology, to display motor resonance under different conditions, (2) genomics, to understand the distribution of peak phases for ancillary clock genes, and (3) meteorology and wind turbine power generation, to study the changing and periodic distribution of wind direction over the course of a year.",
    "published": "2026-02-05T05:58:15Z",
    "updated": "2026-02-05T05:58:15Z",
    "link": "http://arxiv.org/pdf/2602.05335v1.pdf",
    "category": [
      "stat.ME",
      "cs.GR",
      "stat.AP"
    ],
    "authors": [
      "Joshua D. Berlinski",
      "Fan Dai",
      "Ranjan Maitra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.06001v1",
    "title": "Visuo-Tactile World Models",
    "summary": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.",
    "published": "2026-02-05T18:46:33Z",
    "updated": "2026-02-05T18:46:33Z",
    "link": "http://arxiv.org/pdf/2602.06001v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Carolina Higuera",
      "Sergio Arnaud",
      "Byron Boots",
      "Mustafa Mukadam",
      "Francois Robert Hogan",
      "Franziska Meier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05948v1",
    "title": "Location-Aware Dispersion on Anonymous Graphs",
    "summary": "The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\\sf{col}(v) \\in C = \\{c_1, \\dots, c_t\\}, t\\leq n$. A set $R = \\{r_1, \\dots, r_k\\}$ of $k \\leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\\mathsf{col}(r_i) \\in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\\leq n$ without knowing $k,n$.\n  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.",
    "published": "2026-02-05T18:02:24Z",
    "updated": "2026-02-05T18:02:24Z",
    "link": "http://arxiv.org/pdf/2602.05948v1.pdf",
    "category": [
      "cs.DC",
      "cs.DS",
      "cs.MA",
      "cs.RO"
    ],
    "authors": [
      " Himani",
      "Supantha Pandit",
      "Gokarna Sharma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19462v2",
    "title": "Physical Human-Robot Interaction: A Critical Review of Safety Constraints",
    "summary": "This paper aims to provide a clear and rigorous understanding of commonly recognized safety constraints in physical human-robot interaction, particularly regarding ISO/TS 15066. We investigate the derivation of these constraints, critically examine the underlying assumptions, and evaluate their practical implications for system-level safety and performance in industrially relevant scenarios. Key design parameters within safety-critical control architectures are identified, and numerical examples are provided to quantify performance degradation arising from typical approximations and design decisions in manufacturing environments. Within this analysis, the fundamental role of energy in safety assessment is emphasized, providing focused insights into energy-based safety methodologies for collaborative industrial robot systems.",
    "published": "2026-01-27T10:45:50Z",
    "updated": "2026-02-05T17:48:00Z",
    "link": "http://arxiv.org/pdf/2601.19462v2.pdf",
    "category": [
      "eess.SY",
      "cs.RO"
    ],
    "authors": [
      "Riccardo Zanella",
      "Federico Califano",
      "Stefano Stramigioli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05922v1",
    "title": "From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits",
    "summary": "Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.",
    "published": "2026-02-05T17:34:49Z",
    "updated": "2026-02-05T17:34:49Z",
    "link": "http://arxiv.org/pdf/2602.05922v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Aziz Mohamed Mili",
      "Louis Catar",
      "Paul Gérard",
      "Ilyass Tabiai",
      "David St-Onge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05895v1",
    "title": "Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools",
    "summary": "This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.",
    "published": "2026-02-05T17:14:06Z",
    "updated": "2026-02-05T17:14:06Z",
    "link": "http://arxiv.org/pdf/2602.05895v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Qi Li",
      "Karsten Berns"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12084v2",
    "title": "Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation",
    "summary": "Mobile robots are increasingly deployed in unstructured environments where obstacles and objects are movable. Navigation in such environments is known as interactive navigation, where task completion requires not only avoiding obstacles but also strategic interactions with movable objects. Non-prehensile interactive navigation focuses on non-grasping interaction strategies, such as pushing, rather than relying on prehensile manipulation. Despite a growing body of research in this field, most solutions are evaluated using case-specific setups, limiting reproducibility and cross-comparison. In this paper, we present Bench-NPIN, the first comprehensive benchmark for non-prehensile interactive navigation. Bench-NPIN includes multiple components: 1) a comprehensive range of simulated environments for non-prehensile interactive navigation tasks, including navigating a maze with movable obstacles, autonomous ship navigation in icy waters, box delivery, and area clearing, each with varying levels of complexity; 2) a set of evaluation metrics that capture unique aspects of interactive navigation, such as efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-NPIN to evaluate example implementations of established baselines across environments. Bench-NPIN is an open-source Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
    "published": "2025-05-17T16:54:18Z",
    "updated": "2026-02-05T16:31:05Z",
    "link": "http://arxiv.org/pdf/2505.12084v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ninghan Zhong",
      "Steven Caro",
      "Avraiem Iskandar",
      "Megnath Ramesh",
      "Stephen L. Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05791v1",
    "title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion",
    "summary": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.",
    "published": "2026-02-05T15:48:15Z",
    "updated": "2026-02-05T15:48:15Z",
    "link": "http://arxiv.org/pdf/2602.05791v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yufei Xue",
      "YunFeng Lin",
      "Wentao Dong",
      "Yang Tang",
      "Jingbo Wang",
      "Jiangmiao Pang",
      "Ming Zhou",
      "Minghuan Liu",
      "Weinan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.17399v2",
    "title": "Enriching physical-virtual interaction in AR gaming by tracking identical objects via an egocentric partial observation frame",
    "summary": "Augmented reality (AR) games, particularly those designed for head-mounted displays, have grown increasingly prevalent. However, most existing systems depend on pre-scanned, static environments and rely heavily on continuous tracking or marker-based solutions, which limit adaptability in dynamic physical spaces. This is particularly problematic for AR headsets and glasses, which typically follow the user's head movement and cannot maintain a fixed, stationary view of the scene. Moreover, continuous scene observation is neither power-efficient nor practical for wearable devices, given their limited battery and processing capabilities. A persistent challenge arises when multiple identical objects are present in the environment-standard object tracking pipelines often fail to maintain consistent identities without uninterrupted observation or external sensors. These limitations hinder fluid physical-virtual interactions, especially in dynamic or occluded scenes where continuous tracking is infeasible. To address this, we introduce a novel optimization-based framework for re-identifying identical objects in AR scenes using only one partial egocentric observation frame captured by a headset. We formulate the problem as a label assignment task solved via integer programming, augmented with a Voronoi diagram-based pruning strategy to improve computational efficiency. This method reduces computation time by 50% while preserving 91% accuracy in simulated experiments. Moreover, we evaluated our approach in quantitative synthetic and quantitative real-world experiments. We also conducted three qualitative real-world experiments to demonstrate the practical utility and generalizability for enabling dynamic, markerless object interaction in AR environments. Our video demo is available at https://youtu.be/RwptEfLtW1U.",
    "published": "2025-02-24T18:28:22Z",
    "updated": "2026-02-05T15:39:59Z",
    "link": "http://arxiv.org/pdf/2502.17399v2.pdf",
    "category": [
      "cs.HC",
      "cs.RO"
    ],
    "authors": [
      "Liuchuan Yu",
      "Ching-I Huang",
      "Hsueh-Cheng Wang",
      "Lap-Fai Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05760v1",
    "title": "Task-Oriented Robot-Human Handovers on Legged Manipulators",
    "summary": "Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.",
    "published": "2026-02-05T15:28:04Z",
    "updated": "2026-02-05T15:28:04Z",
    "link": "http://arxiv.org/pdf/2602.05760v1.pdf",
    "category": [
      "cs.RO",
      "cs.HC"
    ],
    "authors": [
      "Andreea Tulbure",
      "Carmen Scheidemann",
      "Elias Steiner",
      "Marco Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04746v2",
    "title": "Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics",
    "summary": "In robotics, the concept of \"dull, dirty, and dangerous\" (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on \"dull,\" \"dirty,\" and \"dangerous\" work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.",
    "published": "2026-02-04T16:48:06Z",
    "updated": "2026-02-05T15:26:42Z",
    "link": "http://arxiv.org/pdf/2602.04746v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nozomi Nakajima",
      "Pedro Reynolds-Cuéllar",
      "Caitrin Lynch",
      "Kate Darling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05683v1",
    "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking",
    "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.",
    "published": "2026-02-05T14:09:09Z",
    "updated": "2026-02-05T14:09:09Z",
    "link": "http://arxiv.org/pdf/2602.05683v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Chuwei Wang",
      "Eduardo Sebastián",
      "Amanda Prorok",
      "Anastasia Bizyaeva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18639v3",
    "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
    "summary": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency.",
    "published": "2026-01-26T16:11:05Z",
    "updated": "2026-02-05T13:10:26Z",
    "link": "http://arxiv.org/pdf/2601.18639v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ojasva Mishra",
      "Xiaolong Wu",
      "Min Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05608v1",
    "title": "HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments",
    "summary": "Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.",
    "published": "2026-02-05T12:46:37Z",
    "updated": "2026-02-05T12:46:37Z",
    "link": "http://arxiv.org/pdf/2602.05608v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yufei Zhu",
      "Shih-Min Yang",
      "Martin Magnusson",
      "Allan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05596v1",
    "title": "TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards",
    "summary": "With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.",
    "published": "2026-02-05T12:30:49Z",
    "updated": "2026-02-05T12:30:49Z",
    "link": "http://arxiv.org/pdf/2602.05596v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hokyun Lee",
      "Woo-Jeong Baek",
      "Junhyeok Cha",
      "Jaeheung Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05516v1",
    "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments",
    "summary": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.",
    "published": "2026-02-05T10:16:05Z",
    "updated": "2026-02-05T10:16:05Z",
    "link": "http://arxiv.org/pdf/2602.05516v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Runxiao Liu",
      "Pengda Mao",
      "Xiangli Le",
      "Shuang Gu",
      "Yapeng Chen",
      "Quan Quan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.23059v3",
    "title": "FilMBot: A High-Speed Soft Parallel Robotic Micromanipulator",
    "summary": "Soft robotic manipulators are generally slow despite their great adaptability, resilience, and compliance. This limitation also extends to current soft robotic micromanipulators. Here, we introduce FilMBot, a 3-DOF film-based, electromagnetically actuated, soft kinematic robotic micromanipulator achieving speeds up to 2117 °/s and 2456 °/s in α and \\{beta} angular motions, with corresponding linear velocities of 1.61 m/s and 1.92 m/s using a 4-cm needle end-effector, 0.54 m/s along the Z axis, and 1.57 m/s during Z-axis morph switching. The robot can reach ~1.50 m/s in path-following tasks, with an operational bandwidth below ~30 Hz, and remains responsive at 50 Hz. It demonstrates high precision (~6.3 μm, or ~0.05% of its workspace) in path-following tasks, with precision remaining largely stable across frequencies. The novel combination of the low-stiffness soft kinematic film structure and strong electromagnetic actuation in FilMBot opens new avenues for soft robotics. Furthermore, its simple construction and inexpensive, readily accessible components could broaden the application of micromanipulators beyond current academic and professional users.",
    "published": "2024-10-30T14:33:22Z",
    "updated": "2026-02-05T09:54:13Z",
    "link": "http://arxiv.org/pdf/2410.23059v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiangkun Yu",
      "Houari Bettahar",
      "Hakan Kandemir",
      "Quan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.05415v2",
    "title": "Do Robots Really Need Anthropomorphic Hands? -- A Comparison of Human and Robotic Hands",
    "summary": "Human manipulation skills represent a pinnacle of their voluntary motor functions, requiring the coordination of many degrees of freedom and processing of high-dimensional sensor input to achieve such a high level of dexterity. Thus, we attempt to answer whether the human hand, with its associated biomechanical properties, sensors, and control mechanisms, is an ideal that we should strive for in robotics-do we really need anthropomorphic robotic hands? This survey can help practitioners to make the trade-off between hand complexity and potential manipulation skills. We provide an overview of the human hand, a comparison of commercially available robotic and prosthetic hands, and a systematic review of hand mechanisms and skills that they are capable of. This leads to follow-up questions. What is the minimum requirement for mechanisms and sensors to implement most skills that a robot needs? What is missing to reach human-level dexterity? Can we improve upon human dexterity? Although complex five-fingered hands are often used as the ultimate goal for robotic manipulators, they are not necessary for all tasks. We found that wrist flexibility and finger abduction/adduction are often more important for manipulation capabilities. Increasing the number of fingers, actuators, or degrees of freedom is not always necessary. Three fingers often are a good compromise between simplicity and dexterity. Non-anthropomorphic hand designs with two opposing pairs of fingers or human hands with six fingers can further increase dexterity, suggesting that the human hand is not the optimum. Consequently, we argue for function-based rather than form-based biomimicry.",
    "published": "2025-08-07T14:07:51Z",
    "updated": "2026-02-05T09:49:35Z",
    "link": "http://arxiv.org/pdf/2508.05415v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Alexander Fabisch",
      "Wadhah Zai El Amri",
      "Chandandeep Singh",
      "Nicolás Navarro-Guerrero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.04516v2",
    "title": "TACO: Temporal Consensus Optimization for Continual Neural Mapping",
    "summary": "Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.",
    "published": "2026-02-04T13:07:08Z",
    "updated": "2026-02-05T09:31:12Z",
    "link": "http://arxiv.org/pdf/2602.04516v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xunlan Zhou",
      "Hongrui Zhao",
      "Negar Mehr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01789v3",
    "title": "RFS: Reinforcement Learning with Residual Flow Steering for Dexterous Manipulation",
    "summary": "Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors. We propose Residual Flow Steering(RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy. We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning in both simulation and real-world settings when adapting pretrained base policies. Project website:https://weirdlabuw.github.io/rfs.",
    "published": "2026-02-02T08:11:57Z",
    "updated": "2026-02-05T09:22:39Z",
    "link": "http://arxiv.org/pdf/2602.01789v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Entong Su",
      "Tyler Westenbroek",
      "Anusha Nagabandi",
      "Abhishek Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05468v1",
    "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation",
    "summary": "Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.",
    "published": "2026-02-05T09:16:06Z",
    "updated": "2026-02-05T09:16:06Z",
    "link": "http://arxiv.org/pdf/2602.05468v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Pranav Ponnivalavan",
      "Satoshi Funabashi",
      "Alexander Schmitz",
      "Tetsuya Ogata",
      "Shigeki Sugano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.14422v3",
    "title": "A Sliced Learning Framework for Online Disturbance Identification in Quadrotor SO(3) Attitude Control",
    "summary": "This paper introduces a dimension-decomposed geometric learning framework called Sliced Learning for disturbance identification in quadrotor geometric attitude control. Instead of conventional learning-from-states, this framework adopts a learning-from-error strategy by using the Lie-algebraic error representation as the input feature, enabling axis-wise space decomposition (``slicing\") while preserving the SO(3) structure. This is highly consistent with the geometric mechanism of cognitive control observed in neuroscience, where neural systems organize adaptive representations within structured subspaces to enable cognitive flexibility and efficiency. Based on this framework, we develop a lightweight and structurally interpretable Sliced Adaptive-Neuro Mapping (SANM) module. The high-dimensional mapping for online identification is axially ``sliced\" into multiple low-dimensional submappings (``slices\"), implemented by shallow neural networks and adaptive laws. These neural networks and adaptive laws are updated online via Lyapunov-based adaptation within their respective shared subspaces. To enhance interpretability, we prove exponential convergence despite time-varying disturbances and inertia uncertainties. To our knowledge, Sliced Learning is among the first frameworks to demonstrate lightweight online neural adaptation at 400 Hz on resource-constrained microcontroller units (MCUs), such as STM32, with real-world experimental validation.",
    "published": "2025-08-20T04:41:42Z",
    "updated": "2026-02-05T07:37:55Z",
    "link": "http://arxiv.org/pdf/2508.14422v3.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "math.OC"
    ],
    "authors": [
      "Tianhua Gao",
      "Masashi Izumita",
      "Kohji Tomita",
      "Akiya Kamimura"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05325v1",
    "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
    "summary": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently \"painted\" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.",
    "published": "2026-02-05T05:45:12Z",
    "updated": "2026-02-05T05:45:12Z",
    "link": "http://arxiv.org/pdf/2602.05325v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiacheng Fan",
      "Zhiyue Zhao",
      "Yiqian Zhang",
      "Chao Chen",
      "Peide Wang",
      "Hengdi Zhang",
      "Zhengxue Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05324v1",
    "title": "A Data Driven Structural Decomposition of Dynamic Games via Best Response Maps",
    "summary": "Dynamic games are powerful tools to model multi-agent decision-making, yet computing Nash (generalized Nash) equilibria remains a central challenge in such settings. Complexity arises from tightly coupled optimality conditions, nested optimization structures, and poor numerical conditioning. Existing game-theoretic solvers address these challenges by directly solving the joint game, typically requiring explicit modeling of all agents' objective functions and constraints, while learning-based approaches often decouple interaction through prediction or policy approximation, sacrificing equilibrium consistency. This paper introduces a conceptually novel formulation for dynamic games by restructuring the equilibrium computation. Rather than solving a fully coupled game or decoupling agents through prediction or policy approximation, a data-driven structural reduction of the game is proposed that removes nested optimization layers and derivative coupling by embedding an offline-compiled best-response map as a feasibility constraint. Under standard regularity conditions, when the best-response operator is exact, any converged solution of the reduced problem corresponds to a local open-loop Nash (GNE) equilibrium of the original game; with a learned surrogate, the solution is approximately equilibrium-consistent up to the best-response approximation error. The proposed formulation is supported by mathematical proofs, accompanying a large-scale Monte Carlo study in a two-player open-loop dynamic game motivated by the autonomous racing problem. Comparisons are made against state-of-the-art joint game solvers, and results are reported on solution quality, computational cost, and constraint satisfaction.",
    "published": "2026-02-05T05:44:53Z",
    "updated": "2026-02-05T05:44:53Z",
    "link": "http://arxiv.org/pdf/2602.05324v1.pdf",
    "category": [
      "cs.GT",
      "cs.MA",
      "cs.RO",
      "eess.SY",
      "math.OC"
    ],
    "authors": [
      "Mahdis Rabbani",
      "Navid Mojahed",
      "Shima Nazari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05310v1",
    "title": "Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework",
    "summary": "Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.",
    "published": "2026-02-05T05:05:03Z",
    "updated": "2026-02-05T05:05:03Z",
    "link": "http://arxiv.org/pdf/2602.05310v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jipeng Kong",
      "Xinzhe Liu",
      "Yuhang Lin",
      "Jinrui Han",
      "Sören Schwertfeger",
      "Chenjia Bai",
      "Xuelong Li"
    ]
  }
]