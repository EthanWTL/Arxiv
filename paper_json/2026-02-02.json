[
  {
    "id": "http://arxiv.org/abs/2602.02067v1",
    "title": "Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data",
    "summary": "Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.",
    "published": "2026-02-02T13:07:52Z",
    "updated": "2026-02-02T13:07:52Z",
    "link": "http://arxiv.org/pdf/2602.02067v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Nikola Cenikj",
      "Özgün Turgut",
      "Alexander Müller",
      "Alexander Steger",
      "Jan Kehrer",
      "Marcus Brugger",
      "Daniel Rueckert",
      "Eimo Martens",
      "Philip Müller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02063v1",
    "title": "See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers",
    "summary": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.",
    "published": "2026-02-02T13:03:48Z",
    "updated": "2026-02-02T13:03:48Z",
    "link": "http://arxiv.org/pdf/2602.02063v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Ding Xia",
      "Xinyue Gui",
      "Mark Colley",
      "Fan Gao",
      "Zhongyi Zhou",
      "Dongyuan Li",
      "Renhe Jiang",
      "Takeo Igarashi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02060v1",
    "title": "FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance",
    "summary": "Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.",
    "published": "2026-02-02T13:00:57Z",
    "updated": "2026-02-02T13:00:57Z",
    "link": "http://arxiv.org/pdf/2602.02060v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hyunsuk Chung",
      "Caren Han",
      "Yerin Choi",
      "Seungyeon Ji",
      "Jinwoo Kim",
      "Eun-Jung Holden",
      "Kyungreem Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02055v1",
    "title": "FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification",
    "summary": "In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.",
    "published": "2026-02-02T12:57:09Z",
    "updated": "2026-02-02T12:57:09Z",
    "link": "http://arxiv.org/pdf/2602.02055v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nan Qiao",
      "Sheng Yue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02051v1",
    "title": "SIDiffAgent: Self-Improving Diffusion Agent",
    "summary": "Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse\" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \\modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.",
    "published": "2026-02-02T12:53:21Z",
    "updated": "2026-02-02T12:53:21Z",
    "link": "http://arxiv.org/pdf/2602.02051v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shivank Garg",
      "Ayush Singh",
      "Gaurav Kumar Nayak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02050v1",
    "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents",
    "summary": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.",
    "published": "2026-02-02T12:52:14Z",
    "updated": "2026-02-02T12:52:14Z",
    "link": "http://arxiv.org/pdf/2602.02050v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Zeping Li",
      "Hongru Wang",
      "Yiwen Zhao",
      "Guanhua Chen",
      "Yixia Li",
      "Keyang Chen",
      "Yixin Cao",
      "Guangnan Ye",
      "Hongfeng Chai",
      "Mengdi Wang",
      "Zhenfei Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04217v3",
    "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering",
    "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision-language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically adapt training-based strategies such as gradient ascent or preference optimization, but these methods are computationally expensive, irreversible, and often distort retained knowledge. In this work, we propose MLLMEraser, an input-aware, training-free framework for test-time unlearning. Our approach leverages activation steering to enable dynamic knowledge erasure without parameter updates. Specifically, we construct a multimodal erasure direction by contrasting adversarially perturbed, knowledge-recall image-text pairs with knowledge-erasure counterparts, capturing both textual and visual discrepancies. To prevent unnecessary interference, we further design an input-aware steering mechanism that adaptively determines when and how the erasure direction should be applied, preserving utility on retained knowledge while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms state-of-the-art MLLM unlearning baselines, achieving stronger forgetting performance with lower computational cost and minimal utility degradation.",
    "published": "2025-10-05T14:20:17Z",
    "updated": "2026-02-02T12:41:26Z",
    "link": "http://arxiv.org/pdf/2510.04217v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Chenlu Ding",
      "Jiancan Wu",
      "Leheng Sheng",
      "Fan Zhang",
      "Yancheng Yuan",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02043v1",
    "title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models",
    "summary": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).",
    "published": "2026-02-02T12:39:39Z",
    "updated": "2026-02-02T12:39:39Z",
    "link": "http://arxiv.org/pdf/2602.02043v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Cristian Sbrolli",
      "Matteo Matteucci",
      "Toshihiko Yamasaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2311.18547v2",
    "title": "Real-Time Vibration-Based Bearing Fault Diagnosis Under Time-Varying Speed Conditions",
    "summary": "Detection of rolling-element bearing faults is crucial for implementing proactive maintenance strategies and for minimizing the economic and operational consequences of unexpected failures. However, many existing techniques are developed and tested under strictly controlled conditions, limiting their adaptability to the diverse and dynamic settings encountered in practical applications. This paper presents an efficient real-time convolutional neural network (CNN) for diagnosing multiple bearing faults under various noise levels and time-varying rotational speeds. Additionally, we propose a novel Fisher-based spectral separability analysis (SSA) method to elucidate the effectiveness of the designed CNN model. We conducted experiments on both healthy bearings and bearings afflicted with inner race, outer race, and roller ball faults. The experimental results show the superiority of our model over the current state-of-the-art approach in three folds: it achieves substantial accuracy gains of up to 15.8%, it is robust to noise with high performance across various signal-to-noise ratios, and it runs in real-time with processing durations five times less than acquisition. Additionally, by using the proposed SSA technique, we offer insights into the model's performance and underscore its effectiveness in tackling real-world challenges.",
    "published": "2023-11-30T13:30:00Z",
    "updated": "2026-02-02T12:39:06Z",
    "link": "http://arxiv.org/pdf/2311.18547v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "authors": [
      "Tuomas Jalonen",
      "Mohammad Al-Sa'd",
      "Serkan Kiranyaz",
      "Moncef Gabbouj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18739v3",
    "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
    "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.",
    "published": "2026-01-26T18:01:46Z",
    "updated": "2026-02-02T12:37:43Z",
    "link": "http://arxiv.org/pdf/2601.18739v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ignacio Antequera-Sánchez",
      "Juan Luis Suárez-Díaz",
      "Rosana Montes",
      "Francisco Herrera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02039v1",
    "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
    "summary": "The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.",
    "published": "2026-02-02T12:36:57Z",
    "updated": "2026-02-02T12:36:57Z",
    "link": "http://arxiv.org/pdf/2602.02039v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Wei Liu",
      "Peijie Yu",
      "Michele Orini",
      "Yali Du",
      "Yulan He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20941v3",
    "title": "Multivariate Standardized Residuals for Conformal Prediction",
    "summary": "While split conformal prediction guarantees marginal coverage, approaching the stronger property of conditional coverage is essential for reliable uncertainty quantification. Naive conformal scores, however, suffer from poor conditional coverage in heteroskedastic settings. In univariate regression, this is commonly addressed by normalizing nonconformity scores using estimated local score variance. In this work, we propose a natural extension of this normalization to the multivariate setting, effectively whitening the residuals to decouple output correlations and standardize local variance. We demonstrate that using the Mahalanobis distance induced by a learned local covariance as a nonconformity score provides a closed-form, computationally efficient mechanism for capturing inter-output correlations and heteroskedasticity, avoiding the expensive sampling required by previous methods based on cumulative distribution functions. This structure unlocks several practical extensions, including the handling of missing output values, the refinement of conformal sets when partial information is revealed, and the construction of valid conformal sets for transformations of the output. Finally, we provide extensive empirical evidence on both synthetic and real-world datasets showing that our approach yields conformal sets that significantly improve upon the conditional coverage of existing multivariate baselines.",
    "published": "2025-07-28T15:55:29Z",
    "updated": "2026-02-02T12:32:40Z",
    "link": "http://arxiv.org/pdf/2507.20941v3.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME",
      "stat.OT"
    ],
    "authors": [
      "Sacha Braun",
      "Eugène Berta",
      "Michael I. Jordan",
      "Francis Bach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02035v1",
    "title": "Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization",
    "summary": "Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.",
    "published": "2026-02-02T12:32:28Z",
    "updated": "2026-02-02T12:32:28Z",
    "link": "http://arxiv.org/pdf/2602.02035v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Ahmad Farooq",
      "Kamran Iqbal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02034v1",
    "title": "Constrained Process Maps for Multi-Agent Generative AI Workflows",
    "summary": "Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.",
    "published": "2026-02-02T12:32:11Z",
    "updated": "2026-02-02T12:32:11Z",
    "link": "http://arxiv.org/pdf/2602.02034v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ananya Joshi",
      "Michael Rudow"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02033v1",
    "title": "One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation",
    "summary": "Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \\textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.",
    "published": "2026-02-02T12:30:53Z",
    "updated": "2026-02-02T12:30:53Z",
    "link": "http://arxiv.org/pdf/2602.02033v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shuo Lu",
      "Haohan Wang",
      "Wei Feng",
      "Weizhen Wang",
      "Shen Zhang",
      "Yaoyu Li",
      "Ao Ma",
      "Zheng Zhang",
      "Jingjing Lv",
      "Junjie Shen",
      "Ching Law",
      "Bing Zhan",
      "Yuan Xu",
      "Huizai Yao",
      "Yongcan Yu",
      "Chenyang Si",
      "Jian Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02029v1",
    "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation",
    "summary": "Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.",
    "published": "2026-02-02T12:26:27Z",
    "updated": "2026-02-02T12:26:27Z",
    "link": "http://arxiv.org/pdf/2602.02029v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Zhongyuan Lyu",
      "Shuoyu Hu",
      "Lujie Liu",
      "Hongxia Yang",
      "Ming LI"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02028v1",
    "title": "Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories",
    "summary": "Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.",
    "published": "2026-02-02T12:22:51Z",
    "updated": "2026-02-02T12:22:51Z",
    "link": "http://arxiv.org/pdf/2602.02028v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ya Gao",
      "Kalle Kujanpää",
      "Pekka Marttinen",
      "Harri Valpola",
      "Alexander Ilin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02027v1",
    "title": "Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron",
    "summary": "The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.",
    "published": "2026-02-02T12:21:54Z",
    "updated": "2026-02-02T12:21:54Z",
    "link": "http://arxiv.org/pdf/2602.02027v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sicheng Shen",
      "Mingyang Lv",
      "Han Shen",
      "Jialin Wu",
      "Binghao Wang",
      "Zhou Yang",
      "Guobin Shen",
      "Dongcheng Zhao",
      "Feifei Zhao",
      "Yi Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24795v2",
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. Despite their remarkable performance, foundational VLAs are hindered by the prohibitive computational and data demands inherent to their large-scale architectures. While a surge of recent research has focused on enhancing VLA efficiency, the field lacks a unified framework to consolidate these disparate advancements. To bridge this gap, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire model-training-data pipeline. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/.",
    "published": "2025-10-27T17:57:33Z",
    "updated": "2026-02-02T12:16:44Z",
    "link": "http://arxiv.org/pdf/2510.24795v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Zhaoshu Yu",
      "Bo Wang",
      "Pengpeng Zeng",
      "Haonan Zhang",
      "Ji Zhang",
      "Zheng Wang",
      "Lianli Gao",
      "Jingkuan Song",
      "Nicu Sebe",
      "Heng Tao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02018v1",
    "title": "Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction",
    "summary": "Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.",
    "published": "2026-02-02T12:15:50Z",
    "updated": "2026-02-02T12:15:50Z",
    "link": "http://arxiv.org/pdf/2602.02018v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Enes Altinisik",
      "Masoomali Fatehkia",
      "Fatih Deniz",
      "Nadir Durrani",
      "Majd Hawasly",
      "Mohammad Raza",
      "Husrev Taha Sencar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02014v1",
    "title": "Rethinking Genomic Modeling Through Optical Character Recognition",
    "summary": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.",
    "published": "2026-02-02T12:12:00Z",
    "updated": "2026-02-02T12:12:00Z",
    "link": "http://arxiv.org/pdf/2602.02014v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Hongxin Xiang",
      "Pengsen Ma",
      "Yunkang Cao",
      "Di Yu",
      "Haowen Chen",
      "Xinyu Yang",
      "Xiangxiang Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02007v1",
    "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation",
    "summary": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.",
    "published": "2026-02-02T12:04:58Z",
    "updated": "2026-02-02T12:04:58Z",
    "link": "http://arxiv.org/pdf/2602.02007v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zhanghao Hu",
      "Qinglin Zhu",
      "Hanqi Yan",
      "Yulan He",
      "Lin Gui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02004v1",
    "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
    "summary": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain.",
    "published": "2026-02-02T12:03:56Z",
    "updated": "2026-02-02T12:03:56Z",
    "link": "http://arxiv.org/pdf/2602.02004v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Gongli Xi",
      "Kun Wang",
      "Zeming Gao",
      "Huahui Yi",
      "Haolang Lu",
      "Ye Tian",
      "Wendong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.19875v2",
    "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention",
    "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \\log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model's next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99\\% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96\\% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at https://anonymous.4open.science/r/stream-03B8/.",
    "published": "2025-10-22T09:42:29Z",
    "updated": "2026-02-02T12:03:14Z",
    "link": "http://arxiv.org/pdf/2510.19875v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "J Rosser",
      "José Luis Redondo García",
      "Gustavo Penha",
      "Konstantina Palla",
      "Hugues Bouchard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02001v1",
    "title": "Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs",
    "summary": "Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.",
    "published": "2026-02-02T12:02:21Z",
    "updated": "2026-02-02T12:02:21Z",
    "link": "http://arxiv.org/pdf/2602.02001v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yoonjun Cho",
      "Dongjae Jeon",
      "Soeun Kim",
      "Moongyu Jeon",
      "Albert No"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02000v1",
    "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
    "summary": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/",
    "published": "2026-02-02T11:58:26Z",
    "updated": "2026-02-02T11:58:26Z",
    "link": "http://arxiv.org/pdf/2602.02000v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Bing He",
      "Jingnan Gao",
      "Yunuo Chen",
      "Ning Cao",
      "Gang Chen",
      "Zhengxue Cheng",
      "Li Song",
      "Wenjun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01997v1",
    "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
    "summary": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.",
    "published": "2026-02-02T11:57:22Z",
    "updated": "2026-02-02T11:57:22Z",
    "link": "http://arxiv.org/pdf/2602.01997v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Safal Shrestha",
      "Anubhav Shrestha",
      "Aadim Nepal",
      "Minwu Kim",
      "Keith Ross"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.09473v2",
    "title": "SimMerge: Learning to Select Merge Operators from Similarity Signals",
    "summary": "Model merging combines multiple models into a single model with aggregated capabilities, making it a powerful tool for large language model (LLM) development. However, scaling model merging is challenging: performance depends on the choice of merge operator, model subset, and merge order, often requiring expensive merge-and-evaluate searches. In this work, we introduce SimMerge, a predictive merge-selection method that identifies high-performing merges using inexpensive, task-agnostic similarity signals between models. Given a small set of unlabeled probes, SimMerge extracts functional and structural features to predict the performance of candidate two-way merges, enabling merge operator, order and model subset selection without iterative evaluation. We show that SimMerge consistently outperforms the best fixed merge operator across 7B-parameter LLMs and generalizes to multi-way merges and 111B-parameter LLMs without retraining. We further introduce a bandit variant that supports adding new tasks and operators online. Our results suggest that learning how to merge enables scalable model composition when checkpoint catalogs are large and evaluation budgets are limited.",
    "published": "2026-01-14T13:30:00Z",
    "updated": "2026-02-02T11:56:59Z",
    "link": "http://arxiv.org/pdf/2601.09473v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Oliver Bolton",
      " Aakanksha",
      "Arash Ahmadian",
      "Sara Hooker",
      "Marzieh Fadaee",
      "Beyza Ermis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01995v1",
    "title": "Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs",
    "summary": "Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.",
    "published": "2026-02-02T11:56:36Z",
    "updated": "2026-02-02T11:56:36Z",
    "link": "http://arxiv.org/pdf/2602.01995v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jeongmoon Won",
      "Seungwon Kook",
      "Yohan Jo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01996v1",
    "title": "Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations",
    "summary": "Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.",
    "published": "2026-02-02T11:56:36Z",
    "updated": "2026-02-02T11:56:36Z",
    "link": "http://arxiv.org/pdf/2602.01996v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.MS"
    ],
    "authors": [
      "Theologos Anthimopoulos",
      "Milad Kokhazadeh",
      "Vasilios Kelefouras",
      "Benjamin Himpel",
      "Georgios Keramidas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14226v4",
    "title": "Code-Mixed Phonetic Perturbations for Red-Teaming LLMs",
    "summary": "Large language models (LLMs) continue to be demonstrably unsafe despite sophisticated safety alignment techniques and multilingual red-teaming. However, recent red-teaming work has focused on incremental gains in attack success over identifying underlying architectural vulnerabilities in models. In this work, we present \\textbf{CMP-RT}, a novel red-teaming probe that combines code-mixing with phonetic perturbations (CMP), exposing a tokenizer-level safety vulnerability in transformers. Combining realistic elements from digital communication such as code-mixing and textese, CMP-RT preserves phonetics while perturbing safety-critical tokens, allowing harmful prompts to bypass alignment mechanisms while maintaining high prompt interpretability, exposing a gap between pre-training and safety alignment. Our results demonstrate robustness against standard defenses, attack scalability, and generalization of the vulnerability across modalities and to SOTA models like Gemini-3-Pro, establishing CMP-RT as a major threat model and highlighting tokenization as an under-examined vulnerability in current safety pipelines.",
    "published": "2025-05-20T11:35:25Z",
    "updated": "2026-02-02T11:56:18Z",
    "link": "http://arxiv.org/pdf/2505.14226v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Darpan Aswal",
      "Siddharth D Jaiswal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23234v5",
    "title": "p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
    "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. The code is available at https://github.com/ryttry/p-less .",
    "published": "2025-09-27T10:33:41Z",
    "updated": "2026-02-02T11:54:54Z",
    "link": "http://arxiv.org/pdf/2509.23234v5.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Runyan Tan",
      "Shuang Wu",
      "Phillip Howard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01992v1",
    "title": "Emergent Analogical Reasoning in Transformers",
    "summary": "Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.",
    "published": "2026-02-02T11:49:36Z",
    "updated": "2026-02-02T11:49:36Z",
    "link": "http://arxiv.org/pdf/2602.01992v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Gouki Minegishi",
      "Jingyuan Feng",
      "Hiroki Furuta",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.16886v3",
    "title": "Your Latent Reasoning is Secretly Policy Improvement Operator",
    "summary": "Recently, small models with latent recursion have obtained promising results on complex reasoning tasks. These results are typically explained by the theory that such recursion increases a networks depth, allowing it to compactly emulate the capacity of larger models. However, the performance of recursively added layers remains behind the capabilities of one pass models with the same feed forward depth. This means that in the looped version, not every recursive step effectively contributes to depth. This raises the question: when and why does latent reasoning improve performance, and when does it result in dead compute? In our work, we analyze the algorithms that latent reasoning provides answer to this question. We show that latent reasoning can be formalized as a classifier free guidance and policy improvement algorithm. Building on these insights, we propose to use a training schemes from reinforcement learning and diffusion methods for latent reasoning models. Using the Tiny Recursive Model as our testbed, we show that with our modifications we can avoid dead compute steps and reduce the total number of forward passes by 18x while maintaining performance. Broadly speaking, we show how a policy improvement perspective on recursive steps can explain model behavior and provide insights for further improvements.",
    "published": "2025-11-21T01:54:23Z",
    "updated": "2026-02-02T11:47:43Z",
    "link": "http://arxiv.org/pdf/2511.16886v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Arip Asadulaev",
      "Rayan Banerjee",
      "Fakhri Karray",
      "Martin Takac"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01990v1",
    "title": "SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning",
    "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.",
    "published": "2026-02-02T11:47:06Z",
    "updated": "2026-02-02T11:47:06Z",
    "link": "http://arxiv.org/pdf/2602.01990v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhen-Hao Xie",
      "Jun-Tao Tang",
      "Yu-Cheng Shi",
      "Han-Jia Ye",
      "De-Chuan Zhan",
      "Da-Wei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.02741v2",
    "title": "DeepGB-TB: A Risk-Balanced Cross-Attention Gradient-Boosted Convolutional Network for Rapid, Interpretable Tuberculosis Screening",
    "summary": "Large-scale tuberculosis (TB) screening is limited by the high cost and operational complexity of traditional diagnostics, creating a need for artificial-intelligence solutions. We propose DeepGB-TB, a non-invasive system that instantly assigns TB risk scores using only cough audio and basic demographic data. The model couples a lightweight one-dimensional convolutional neural network for audio processing with a gradient-boosted decision tree for tabular features. Its principal innovation is a Cross-Modal Bidirectional Cross-Attention module (CM-BCA) that iteratively exchanges salient cues between modalities, emulating the way clinicians integrate symptoms and risk factors. To meet the clinical priority of minimizing missed cases, we design a Tuberculosis Risk-Balanced Loss (TRBL) that places stronger penalties on false-negative predictions, thereby reducing high-risk misclassifications. DeepGB-TB is evaluated on a diverse dataset of 1,105 patients collected across seven countries, achieving an AUROC of 0.903 and an F1-score of 0.851, representing a new state of the art. Its computational efficiency enables real-time, offline inference directly on common mobile devices, making it ideal for low-resource settings. Importantly, the system produces clinically validated explanations that promote trust and adoption by frontline health workers. By coupling AI innovation with public-health requirements for speed, affordability, and reliability, DeepGB-TB offers a tool for advancing global TB control.",
    "published": "2025-08-02T14:11:07Z",
    "updated": "2026-02-02T11:44:43Z",
    "link": "http://arxiv.org/pdf/2508.02741v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Zhixiang Lu",
      "Yulong Li",
      "Feilong Tang",
      "Zhengyong Jiang",
      "Chong Li",
      "Mian Zhou",
      "Tenglong Li",
      "Jionglong Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01983v1",
    "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
    "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\\uparrow$ and +23.04%$\\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
    "published": "2026-02-02T11:37:45Z",
    "updated": "2026-02-02T11:37:45Z",
    "link": "http://arxiv.org/pdf/2602.01983v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xintian Shen",
      "Jiawei Chen",
      "Lihao Zheng",
      "Hao Ma",
      "Tao Wei",
      "Kun Zhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01976v1",
    "title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning",
    "summary": "General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.",
    "published": "2026-02-02T11:32:56Z",
    "updated": "2026-02-02T11:32:56Z",
    "link": "http://arxiv.org/pdf/2602.01976v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Hongwei Yan",
      "Guanglong Sun",
      "Kanglei Zhou",
      "Qian Li",
      "Liyuan Wang",
      "Yi Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.05825v2",
    "title": "Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI",
    "summary": "Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.",
    "published": "2026-01-09T14:59:25Z",
    "updated": "2026-02-02T11:31:27Z",
    "link": "http://arxiv.org/pdf/2601.05825v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Lucija Mihić Zidar",
      "Philipp Wicke",
      "Praneel Bhatia",
      "Rosa Lutz",
      "Marius Klug",
      "Thorsten O. Zander"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.19673v2",
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a unified policy, overlooking their internal mechanisms. In this paper, we decompose the LLM-based policy into Internal Layer Policies and Internal Modular Policies via Transformer's residual stream. Our entropy analysis on internal policy reveals distinct patterns: (1) universally, policies evolve from high-entropy exploration in early layers to deterministic refinement in top layers; and (2) Qwen exhibits a progressive, human-like reasoning structure, contrasting with the abrupt final-layer convergence in Llama. Furthermore, we discover that optimizing internal layers induces feature refinement, forcing lower layers to capture high-level reasoning representations early. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that reconstructs the LLM's reasoning foundation from the bottom up by optimizing internal layers in early stages. Extensive experiments on complex reasoning benchmarks demonstrate the effectiveness of BuPO. Our code is available at https://github.com/Trae1ounG/BuPO.",
    "published": "2025-12-22T18:51:48Z",
    "updated": "2026-02-02T11:29:09Z",
    "link": "http://arxiv.org/pdf/2512.19673v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yuqiao Tan",
      "Minzheng Wang",
      "Shizhu He",
      "Huanxuan Liao",
      "Chengfeng Zhao",
      "Qiunan Lu",
      "Tian Liang",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01975v1",
    "title": "IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs",
    "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.",
    "published": "2026-02-02T11:28:56Z",
    "updated": "2026-02-02T11:28:56Z",
    "link": "http://arxiv.org/pdf/2602.01975v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Meng Li",
      "Peisong Wang",
      "Yuantian Shao",
      "Qinghao Hu",
      "Hongjian Fang",
      "Yifan Zhang",
      "Zhihui Wei",
      "Jian Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01973v1",
    "title": "Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated",
    "summary": "Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.",
    "published": "2026-02-02T11:26:37Z",
    "updated": "2026-02-02T11:26:37Z",
    "link": "http://arxiv.org/pdf/2602.01973v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Muli Yang",
      "Gabriel James Goenawan",
      "Henan Wang",
      "Huaiyuan Qin",
      "Chenghao Xu",
      "Yanhua Yang",
      "Fen Fang",
      "Ying Sun",
      "Joo-Hwee Lim",
      "Hongyuan Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20576v2",
    "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
    "summary": "Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms the existing performative RL algorithms aiming for stability.",
    "published": "2025-12-23T18:20:06Z",
    "updated": "2026-02-02T11:24:53Z",
    "link": "http://arxiv.org/pdf/2512.20576v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Debabrota Basu",
      "Udvas Das",
      "Brahim Driss",
      "Uddalak Mukherjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01970v1",
    "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
    "summary": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.",
    "published": "2026-02-02T11:24:36Z",
    "updated": "2026-02-02T11:24:36Z",
    "link": "http://arxiv.org/pdf/2602.01970v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yun Qu",
      "Qi Wang",
      "Yixiu Mao",
      "Heming Zou",
      "Yuhang Jiang",
      "Weijie Liu",
      "Clive Bai",
      "Kai Yang",
      "Yangkun Chen",
      "Saiyong Yang",
      "Xiangyang Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.16600v3",
    "title": "You Only Forward Once: An Efficient Compositional Judging Paradigm",
    "summary": "Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis -- where subsequent judgments are conditioned on previous ones -- and further benefits from post-hoc CoT.",
    "published": "2025-11-20T17:55:21Z",
    "updated": "2026-02-02T11:23:30Z",
    "link": "http://arxiv.org/pdf/2511.16600v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tianlong Zhang",
      "Hongwei Xue",
      "Shilin Yan",
      "Di Wu",
      "Chen Xu",
      "Guannan Zhang",
      "Yunyun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.13363v3",
    "title": "FPBoost: Fully Parametric Gradient Boosting for Survival Analysis",
    "summary": "Survival analysis is a statistical framework for modeling time-to-event data. It plays a pivotal role in medicine, reliability engineering, and social science research, where understanding event dynamics even with few data samples is critical. Recent advancements in machine learning, particularly those employing neural networks and decision trees, have introduced sophisticated algorithms for survival modeling. However, many of these methods rely on restrictive assumptions about the underlying event-time distribution, such as proportional hazard, time discretization, or accelerated failure time. In this study, we propose FPBoost, a survival model that combines a weighted sum of fully parametric hazard functions with gradient boosting. Distribution parameters are estimated with decision trees trained by maximizing the full survival likelihood. We show how FPBoost is a universal approximator of hazard functions, offering full event-time modeling flexibility while maintaining interpretability through the use of well-established parametric distributions. We evaluate concordance and calibration of FPBoost across multiple benchmark datasets, showcasing its robustness and versatility as a new tool for survival estimation.",
    "published": "2024-09-20T09:57:17Z",
    "updated": "2026-02-02T11:17:40Z",
    "link": "http://arxiv.org/pdf/2409.13363v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alberto Archetti",
      "Eugenio Lomurno",
      "Diego Piccinotti",
      "Matteo Matteucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01967v1",
    "title": "Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition",
    "summary": "Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.",
    "published": "2026-02-02T11:16:34Z",
    "updated": "2026-02-02T11:16:34Z",
    "link": "http://arxiv.org/pdf/2602.01967v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Wonjun Lee",
      "Hyounghun Kim",
      "Gary Geunbae Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01965v1",
    "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
    "summary": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.",
    "published": "2026-02-02T11:13:38Z",
    "updated": "2026-02-02T11:13:38Z",
    "link": "http://arxiv.org/pdf/2602.01965v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Kwun Hang Lau",
      "Fangyuan Zhang",
      "Boyu Ruan",
      "Yingli Zhou",
      "Qintian Guo",
      "Ruiyuan Zhang",
      "Xiaofang Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01962v1",
    "title": "Zero-Shot Off-Policy Learning",
    "summary": "Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.",
    "published": "2026-02-02T11:06:31Z",
    "updated": "2026-02-02T11:06:31Z",
    "link": "http://arxiv.org/pdf/2602.01962v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Arip Asadulaev",
      "Maksim Bobrin",
      "Salem Lahlou",
      "Dmitry Dylov",
      "Fakhri Karray",
      "Martin Takac"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01956v1",
    "title": "Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation",
    "summary": "Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.",
    "published": "2026-02-02T11:03:37Z",
    "updated": "2026-02-02T11:03:37Z",
    "link": "http://arxiv.org/pdf/2602.01956v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Seonghyeon Park",
      "Jewon Yeom",
      "Jaewon Sok",
      "Jeongjae Park",
      "Heejun Kim",
      "Taesup Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18352v2",
    "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning",
    "summary": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.",
    "published": "2026-01-26T10:58:52Z",
    "updated": "2026-02-02T10:53:40Z",
    "link": "http://arxiv.org/pdf/2601.18352v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Manjie Xu",
      "Isabella Yin",
      "Xinyi Tu",
      "Chi Zhang",
      "Yixin Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01942v1",
    "title": "Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework",
    "summary": "AI is moving from domain-specific autonomy in closed, predictable settings to large-language-model-driven agents that plan and act in open, cross-organizational environments. As a result, the cybersecurity risk landscape is changing in fundamental ways. Agentic AI systems can plan, act, collaborate, and persist over time, functioning as participants in complex socio-technical ecosystems rather than as isolated software components. Although recent work has strengthened defenses against model and pipeline level vulnerabilities such as prompt injection, data poisoning, and tool misuse, these system centric approaches may fail to capture risks that arise from autonomy, interaction, and emergent behavior. This article introduces the 4C Framework for multi-agent AI security, inspired by societal governance. It organizes agentic risks across four interdependent dimensions: Core (system, infrastructure, and environmental integrity), Connection (communication, coordination, and trust), Cognition (belief, goal, and reasoning integrity), and Compliance (ethical, legal, and institutional governance). By shifting AI security from a narrow focus on system-centric protection to the broader preservation of behavioral integrity and intent, the framework complements existing AI security strategies and offers a principled foundation for building agentic AI systems that are trustworthy, governable, and aligned with human values.",
    "published": "2026-02-02T10:45:16Z",
    "updated": "2026-02-02T10:45:16Z",
    "link": "http://arxiv.org/pdf/2602.01942v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Alsharif Abuadbba",
      "Nazatul Sultan",
      "Surya Nepal",
      "Sanjay Jha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01939v1",
    "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
    "summary": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.",
    "published": "2026-02-02T10:43:46Z",
    "updated": "2026-02-02T10:43:46Z",
    "link": "http://arxiv.org/pdf/2602.01939v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yuxin He",
      "Ruihao Zhang",
      "Tianao Shen",
      "Cheng Liu",
      "Qiang Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20137v4",
    "title": "ePC: Fast and Deep Predictive Coding for Digital Hardware",
    "summary": "Predictive Coding (PC) offers a brain-inspired alternative to backpropagation for neural network training, described as a physical system minimizing its internal energy. However, in practice, PC is predominantly digitally simulated, requiring excessive amounts of compute while struggling to scale to deeper architectures. This paper reformulates PC to overcome this hardware-algorithm mismatch. First, we uncover how the canonical state-based formulation of PC (sPC) is, by design, deeply inefficient in digital simulation, inevitably resulting in exponential signal decay that stalls the entire minimization process. Then, to overcome this fundamental limitation, we introduce error-based PC (ePC), a novel reparameterization of PC which does not suffer from signal decay. Though no longer biologically plausible, ePC numerically computes exact PC weights gradients and runs orders of magnitude faster than sPC. Experiments across multiple architectures and datasets demonstrate that ePC matches backpropagation's performance even for deeper models where sPC struggles. Besides practical improvements, our work provides theoretical insight into PC dynamics and establishes a foundation for scaling PC-based learning to deeper architectures on digital hardware and beyond.",
    "published": "2025-05-26T15:39:16Z",
    "updated": "2026-02-02T10:42:02Z",
    "link": "http://arxiv.org/pdf/2505.20137v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Cédric Goemaere",
      "Gaspard Oliviers",
      "Rafal Bogacz",
      "Thomas Demeester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01937v1",
    "title": "T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation",
    "summary": "Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.",
    "published": "2026-02-02T10:40:27Z",
    "updated": "2026-02-02T10:40:27Z",
    "link": "http://arxiv.org/pdf/2602.01937v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Suhan Guo",
      "Bingxu Wang",
      "Shaodan Zhang",
      "Furao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01936v1",
    "title": "PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting",
    "summary": "Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.",
    "published": "2026-02-02T10:40:07Z",
    "updated": "2026-02-02T10:40:07Z",
    "link": "http://arxiv.org/pdf/2602.01936v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Abdul Joseph Fofanah",
      "Lian Wen",
      "David Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01935v1",
    "title": "COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation",
    "summary": "Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.",
    "published": "2026-02-02T10:37:05Z",
    "updated": "2026-02-02T10:37:05Z",
    "link": "http://arxiv.org/pdf/2602.01935v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "authors": [
      "Annabelle Sujun Tang",
      "Christopher Priebe",
      "Lianhui Qin",
      "Hadi Esmaeilzadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.02463v2",
    "title": "Extending RLVR to Open-Ended Tasks via Verifiable Multiple-Choice Reformulation",
    "summary": "Reinforcement Learning with Verifiable Rewards(RLVR) has demonstrated great potential in enhancing the reasoning capabilities of large language models (LLMs). However, its success has thus far been largely confined to the mathematical and programming domains with clear and automatically checkable outcomes. Reinforcement learning on open-ended tasks (e.g., creative writing and subjective Q&A) continues to rely on reward models due to the absence of verifiable solutions. This raises a key question: how can we extend RLVR to strengthen reasoning in open-ended tasks regardless of the absence of the unambiguous ground truth? To overcome this challenge, we introduce Verifiable Multiple-Choice Reformulation for Reinforcement Learning from Verifiable Rewards (VMR-RLVR), a novel training strategy that restructures open-ended data into verifiable multiple-choice formats, enabling effective training even in the absence of explicit ground truth. Experimental results on multiple benchmarks validate the effectiveness of our method in improving LLM performance on open-ended tasks. Notably, across seven open-ended benchmarks, our VMR-RLVR training delivers an average gain of 3.29 points over the RL with reward model.",
    "published": "2025-11-04T10:45:52Z",
    "updated": "2026-02-02T10:35:44Z",
    "link": "http://arxiv.org/pdf/2511.02463v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Mengyu Zhang",
      "Siyu Ding",
      "Weichong Yin",
      "Yu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01933v1",
    "title": "Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling",
    "summary": "Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.",
    "published": "2026-02-02T10:35:42Z",
    "updated": "2026-02-02T10:35:42Z",
    "link": "http://arxiv.org/pdf/2602.01933v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Fabrice Boissier",
      "Monica Sen",
      "Irina Rychkova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01920v1",
    "title": "PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks",
    "summary": "Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \\textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\\%) and balanced accuracy (up to +8.3\\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \\texttt{https://github.com/afofanah/PIMPC-GNN}.",
    "published": "2026-02-02T10:21:58Z",
    "updated": "2026-02-02T10:21:58Z",
    "link": "http://arxiv.org/pdf/2602.01920v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Abdul Joseph Fofanah",
      "Lian Wen",
      "David Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01915v1",
    "title": "VLM-Guided Experience Replay",
    "summary": "Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/",
    "published": "2026-02-02T10:19:59Z",
    "updated": "2026-02-02T10:19:59Z",
    "link": "http://arxiv.org/pdf/2602.01915v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Elad Sharony",
      "Tom Jurgenson",
      "Orr Krupnik",
      "Dotan Di Castro",
      "Shie Mannor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01912v1",
    "title": "Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration",
    "summary": "Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.",
    "published": "2026-02-02T10:18:31Z",
    "updated": "2026-02-02T10:18:31Z",
    "link": "http://arxiv.org/pdf/2602.01912v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "q-fin.RM"
    ],
    "authors": [
      "Du-Yi Wang",
      "Guo Liang",
      "Kun Zhang",
      "Qianwen Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.12769v4",
    "title": "How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation of LLM Hallucination",
    "summary": "In the age of misinformation, hallucination - the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses - represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common in realistic settings than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering (LFQA). To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to translate-train a detection model. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build open-domain QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. Our analysis shows that LLMs, in absolute terms, hallucinate more tokens in high-resource languages due to longer responses, but that the actual hallucination rates (i.e., normalized for length) seems uncorrelated with the sizes of languages' digital footprints. We also find that smaller LLMs hallucinate more, and significantly, LLMs with broader language support display higher hallucination rates.",
    "published": "2025-02-18T11:32:43Z",
    "updated": "2026-02-02T10:18:10Z",
    "link": "http://arxiv.org/pdf/2502.12769v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Saad Obaid ul Islam",
      "Anne Lauscher",
      "Goran Glavaš"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01910v1",
    "title": "DomusFM: A Foundation Model for Smart-Home Sensor Data",
    "summary": "Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.",
    "published": "2026-02-02T10:16:34Z",
    "updated": "2026-02-02T10:16:34Z",
    "link": "http://arxiv.org/pdf/2602.01910v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Michele Fiori",
      "Gabriele Civitarese",
      "Flora D. Salim",
      "Claudio Bettini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01906v1",
    "title": "DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification",
    "summary": "Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.",
    "published": "2026-02-02T10:12:18Z",
    "updated": "2026-02-02T10:12:18Z",
    "link": "http://arxiv.org/pdf/2602.01906v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Farhan Ullah",
      "Irfan Ullah",
      "Khalil Khan",
      "Giovanni Pau",
      "JaKeoung Koo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01905v1",
    "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization",
    "summary": "Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.",
    "published": "2026-02-02T10:12:17Z",
    "updated": "2026-02-02T10:12:17Z",
    "link": "http://arxiv.org/pdf/2602.01905v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Theodore Zhengde Zhao",
      "Sid Kiblawi",
      "Jianwei Yang",
      "Naoto Usuyama",
      "Reuben Tan",
      "Noel C Codella",
      "Tristan Naumann",
      "Hoifung Poon",
      "Mu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00049v2",
    "title": "AI-Based Stroke Rehabilitation Domiciliary Assessment System with ST_GCN Attention",
    "summary": "Effective stroke recovery requires continuous rehabilitation integrated with daily living. To support this need, we propose a home-based rehabilitation exercise and feedback system. The system consists of (1) hardware setup with RGB-D camera and wearable sensors to capture stroke movements, (2) a mobile application for exercise guidance, and (3) an AI server for assessment and feedback. When a stroke user exercises following the application guidance, the system records skeleton sequences, which are then assessed by the deep learning model, RAST-G@ (Rehabilitation Assessment Spatio-Temporal Graph ATtention). The model employs a spatio-temporal graph convolutional network to extract skeletal features and integrates transformer-based temporal attention to figure out action quality. For system implementation, we constructed the NRC dataset, include 10 upper-limb activities of daily living (ADL) and 5 range-of-motion (ROM) collected from stroke and non-disabled participants, with Score annotations provided by licensed physiotherapists. Results on the KIMORE and NRC datasets show that RAST-G@ improves over baseline in terms of MAD, RMSE, and MAPE. Furthermore, the system provides user feedback that combines patient-centered assessment and monitoring. The results demonstrate that the proposed system offers a scalable approach for quantitative and consistent domiciliary rehabilitation assessment.",
    "published": "2025-09-27T16:45:56Z",
    "updated": "2026-02-02T10:09:26Z",
    "link": "http://arxiv.org/pdf/2510.00049v2.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Suhyeon Lim",
      "Ye-eun Kim",
      "Andrew J. Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01893v1",
    "title": "Geometric Analysis of Token Selection in Multi-Head Attention",
    "summary": "We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.",
    "published": "2026-02-02T10:04:40Z",
    "updated": "2026-02-02T10:04:40Z",
    "link": "http://arxiv.org/pdf/2602.01893v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Timur Mudarisov",
      "Mikhal Burtsev",
      "Tatiana Petrova",
      "Radu State"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.07809v2",
    "title": "Conformal mapping based Physics-informed neural networks for designing neutral inclusions",
    "summary": "We address the neutral inclusion problem with imperfect boundary conditions, focusing on designing interface functions for inclusions of arbitrary shapes. Traditional Physics-Informed Neural Networks (PINNs) struggle with this inverse problem, leading to the development of Conformal Mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs), which integrate geometric function theory with PINNs. CoCo-PINNs effectively solve forward-inverse problems by modeling the interface function through neural network training, which yields a neutral inclusion effect. This approach enhances the performance of PINNs in terms of credibility, consistency, and stability.",
    "published": "2025-01-14T03:20:17Z",
    "updated": "2026-02-02T10:01:54Z",
    "link": "http://arxiv.org/pdf/2501.07809v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.AP"
    ],
    "authors": [
      "Daehee Cho",
      "Hyeonmin Yun",
      "Jaeyong Lee",
      "Mikyoung Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01885v1",
    "title": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support",
    "summary": "Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.",
    "published": "2026-02-02T09:58:26Z",
    "updated": "2026-02-02T09:58:26Z",
    "link": "http://arxiv.org/pdf/2602.01885v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tiantian Chen",
      "Jiaqi Lu",
      "Ying Shen",
      "Lin Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01884v1",
    "title": "Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models",
    "summary": "Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.",
    "published": "2026-02-02T09:58:24Z",
    "updated": "2026-02-02T09:58:24Z",
    "link": "http://arxiv.org/pdf/2602.01884v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shidong Yang",
      "Tongwen Huang",
      "Hao Wen",
      "Yong Wang",
      "Li Chen",
      "Xiangxiang Chu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.05711v2",
    "title": "Automated Archival Descriptions with Federated Intelligence of LLMs",
    "summary": "Enforcing archival standards requires specialized expertise, and manually creating metadata descriptions for archival materials is a tedious and error-prone task. This work aims at exploring the potential of agentic AI and large language models (LLMs) in addressing the challenges of implementing a standardized archival description process. To this end, we introduce an agentic AI-driven system for automated generation of high-quality metadata descriptions of archival materials. We develop a federated optimization approach that unites the intelligence of multiple LLMs to construct optimal archival metadata. We also suggest methods to overcome the challenges associated with using LLMs for consistent metadata generation. To evaluate the feasibility and effectiveness of our techniques, we conducted extensive experiments using a real-world dataset of archival materials, which covers a variety of document types and formats. The evaluation results demonstrate the feasibility of our techniques and highlight the superior performance of the federated optimization approach compared to single-model solutions in metadata quality and reliability.",
    "published": "2025-04-08T06:11:05Z",
    "updated": "2026-02-02T09:43:35Z",
    "link": "http://arxiv.org/pdf/2504.05711v2.pdf",
    "category": [
      "cs.AI",
      "cs.DL",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Jinghua Groppe",
      "Andreas Marquet",
      "Annabel Walz",
      "Sven Groppe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01869v1",
    "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents",
    "summary": "LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.",
    "published": "2026-02-02T09:43:12Z",
    "updated": "2026-02-02T09:43:12Z",
    "link": "http://arxiv.org/pdf/2602.01869v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Qirui Mi",
      "Zhijian Ma",
      "Mengyue Yang",
      "Haoxuan Li",
      "Yisen Wang",
      "Haifeng Zhang",
      "Jun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01865v1",
    "title": "GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm",
    "summary": "Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized.",
    "published": "2026-02-02T09:38:03Z",
    "updated": "2026-02-02T09:38:03Z",
    "link": "http://arxiv.org/pdf/2602.01865v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Shaopeng Chen",
      "Chuyue Xie",
      "Huimin Ren",
      "Shaozong Zhang",
      "Han Zhang",
      "Ruobing Cheng",
      "Zhiqiang Cao",
      "Zehao Ju",
      "Gao Yu",
      "Jie Ding",
      "Xiaodong Chen",
      "Xuewu Jiao",
      "Shuanglong Li",
      "Liu Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08018v3",
    "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
    "summary": "The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.",
    "published": "2025-05-18T07:04:53Z",
    "updated": "2026-02-02T09:35:31Z",
    "link": "http://arxiv.org/pdf/2506.08018v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Fei Li",
      "Song Liu",
      "Weiguo Wu",
      "Shiqiang Nie",
      "Jinyu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.15969v2",
    "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems",
    "summary": "Reformulating nonlinear optimization problems into solver-ready linear optimization problems is often necessary for practical applications, but the process is often manual and requires domain expertise. We propose LinearizeLLM, an agent-based LLM framework that produces solver-ready linear reformulations of nonlinear optimization problems. Agents first detect the nonlinearity pattern (e.g., bilinear products) and apply nonlinearity pattern-aware reformulation techniques, selecting the most suitable linearization technique. We benchmark on 40 instances: 27 derived from ComplexOR by injecting exactly-linearizable operators, and 13 automatically generated instances with deeply nested nonlinearities. LinearizeLLM achieves 73\\% mean end-to-end overall success (OSR) across nonlinearity depths (8.3x higher than a one-shot LLM baseline; 4.3x higher than Pyomo). The results suggest that a set of pattern-specialized agents can automate linearization, supporting natural-language-based modeling of nonlinear optimization.",
    "published": "2025-10-12T16:43:21Z",
    "updated": "2026-02-02T09:32:11Z",
    "link": "http://arxiv.org/pdf/2510.15969v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Paul-Niklas Ken Kandora",
      "Simon Caspar Zeller",
      "Aaron Jeremias Elsing",
      "Elena Kuss",
      "Steffen Rebennack"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01858v1",
    "title": "SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures",
    "summary": "Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.",
    "published": "2026-02-02T09:30:43Z",
    "updated": "2026-02-02T09:30:43Z",
    "link": "http://arxiv.org/pdf/2602.01858v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Liangtao Lin",
      "Zhaomeng Zhu",
      "Tianwei Zhang",
      "Yonggang Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01855v1",
    "title": "Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG",
    "summary": "Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\\pm$ 2.98% to 96.9% $\\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.",
    "published": "2026-02-02T09:28:27Z",
    "updated": "2026-02-02T09:28:27Z",
    "link": "http://arxiv.org/pdf/2602.01855v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Blagoj Hristov",
      "Hristijan Gjoreski",
      "Vesna Ojleska Latkoska",
      "Gorjan Nadzinski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09198v2",
    "title": "SACO: Sequence-Aware Constrained Optimization Framework for Coupon Distribution in E-commerce",
    "summary": "Coupon distribution is a critical marketing strategy used by online platforms to boost revenue and enhance user engagement. Regrettably, existing coupon distribution strategies fall far short of effectively leveraging the complex sequential interactions between platforms and users. This critical oversight, despite the abundance of e-commerce log data, has precipitated a performance plateau. In this paper, we focus on the scene that the platforms make sequential coupon distribution decision multiple times for various users, with each user interacting with the platform repeatedly. Based on this scenario, we propose a novel marketing framework, named \\textbf{S}equence-\\textbf{A}ware \\textbf{C}onstrained \\textbf{O}ptimization (SACO) framework, to directly devise coupon distribution policy for long-term revenue boosting. SACO framework enables optimized online decision-making in a variety of real-world marketing scenarios. It achieves this by seamlessly integrating three key characteristics, general scenarios, sequential modeling with more comprehensive historical data, and efficient iterative updates within a unified framework. Furthermore, empirical results on real-world industrial dataset, alongside public and synthetic datasets demonstrate the superiority of our framework.",
    "published": "2025-08-08T13:03:17Z",
    "updated": "2026-02-02T09:25:16Z",
    "link": "http://arxiv.org/pdf/2508.09198v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Li Kong",
      "Bingzhe Wang",
      "Zhou Chen",
      "Suhan Hu",
      "Yuchao Ma",
      "Qi Qi",
      "Suoyuan Song",
      "Bicheng Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.09200v3",
    "title": "A.X K1 Technical Report",
    "summary": "We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a multi-stage data processing pipeline. Designed to bridge the gap between reasoning capability and inference efficiency, A.X K1 supports explicitly controllable reasoning to facilitate scalable deployment across diverse real-world scenarios. We propose a simple yet effective Think-Fusion training recipe, enabling user-controlled switching between thinking and non-thinking modes within a single unified model. Extensive evaluations demonstrate that A.X K1 achieves performance competitive with leading open-source models, while establishing a distinctive advantage in Korean-language benchmarks.",
    "published": "2026-01-14T06:11:17Z",
    "updated": "2026-02-02T09:25:11Z",
    "link": "http://arxiv.org/pdf/2601.09200v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sung Jun Cheon",
      "Jaekyung Cho",
      "Seongho Choi",
      "Hyunjun Eun",
      "Seokhwan Jo",
      "Jaehyun Jun",
      "Minsoo Kang",
      "Jin Kim",
      "Jiwon Kim",
      "Minsang Kim",
      "Sungwan Kim",
      "Seungsik Kim",
      "Tae Yoon Kim",
      "Youngrang Kim",
      "Hyeongmun Lee",
      "Sangyeol Lee",
      "Sungeun Lee",
      "Youngsoon Lee",
      "Yujin Lee",
      "Seongmin Ok",
      "Chanyong Park",
      "Hyewoong Park",
      "Junyoung Park",
      "Hyunho Yang",
      "Subin Yi",
      "Soohyun Bae",
      "Dhammiko Arya",
      "Yongseok Choi",
      "Sangho Choi",
      "Dongyeon Cho",
      "Seungmo Cho",
      "Gyoungeun Han",
      "Yong-jin Han",
      "Seokyoung Hong",
      "Hyeon Hwang",
      "Wonbeom Jang",
      "Minjeong Ju",
      "Wonjin Jung",
      "Keummin Ka",
      "Sungil Kang",
      "Dongnam Kim",
      "Joonghoon Kim",
      "Jonghwi Kim",
      "SaeRom Kim",
      "Sangjin Kim",
      "Seongwon Kim",
      "Youngjin Kim",
      "Seojin Lee",
      "Sunwoo Lee",
      "Taehoon Lee",
      "Chanwoo Park",
      "Sohee Park",
      "Sooyeon Park",
      "Yohan Ra",
      "Sereimony Sek",
      "Seungyeon Seo",
      "Gun Song",
      "Sanghoon Woo",
      "Janghan Yoon",
      "Sungbin Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01848v1",
    "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems",
    "summary": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.",
    "published": "2026-02-02T09:20:59Z",
    "updated": "2026-02-02T09:20:59Z",
    "link": "http://arxiv.org/pdf/2602.01848v1.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Salaheddin Alzu'bi",
      "Baran Nama",
      "Arda Kaz",
      "Anushri Eswaran",
      "Weiyuan Chen",
      "Sarvesh Khetan",
      "Rishab Bala",
      "Tu Vu",
      "Sewoong Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01844v1",
    "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions",
    "summary": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\\footnote{As in this example.",
    "published": "2026-02-02T09:16:16Z",
    "updated": "2026-02-02T09:16:16Z",
    "link": "http://arxiv.org/pdf/2602.01844v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yuliang Zhan",
      "Jian Li",
      "Wenbing Huang",
      "Wenbing Huang",
      "Yang Liu",
      "Hao Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.04164v2",
    "title": "Clinical Data Goes MEDS? Let's OWL make sense of it",
    "summary": "The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to represent MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We evaluate the proposed approach on two datasets: a synthetic clinical cohort describing care pathways for ruptured intracranial aneurysms, and a real-world subset of MIMIC-IV. To assess semantic consistency, we performed a SHACL validation against the resulting knowledge graphs. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.",
    "published": "2026-01-07T18:25:02Z",
    "updated": "2026-02-02T09:13:19Z",
    "link": "http://arxiv.org/pdf/2601.04164v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alberto Marfoglia",
      "Jong Ho Jhee",
      "Adrien Coulet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01839v1",
    "title": "DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis",
    "summary": "Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.\n  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.",
    "published": "2026-02-02T09:10:09Z",
    "updated": "2026-02-02T09:10:09Z",
    "link": "http://arxiv.org/pdf/2602.01839v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "authors": [
      "Ru Zhang",
      "Xunkai Li",
      "Yaxin Deng",
      "Sicheng Liu",
      "Daohan Su",
      "Qiangqiang Dai",
      "Hongchao Qin",
      "Rong-Hua Li",
      "Guoren Wang",
      "Jia Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01832v1",
    "title": "Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs",
    "summary": "Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.",
    "published": "2026-02-02T09:06:11Z",
    "updated": "2026-02-02T09:06:11Z",
    "link": "http://arxiv.org/pdf/2602.01832v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Rui Wang",
      "Yaoguang Cao",
      "Yuyi Chen",
      "Jianyi Xu",
      "Zhuoyang Li",
      "Jiachen Shang",
      "Shichun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22102v2",
    "title": "Reinforcement Learning for Durable Algorithmic Recourse",
    "summary": "Algorithmic recourse seeks to provide individuals with actionable recommendations that increase their chances of receiving favorable outcomes from automated decision systems (e.g., loan approvals). While prior research has emphasized robustness to model updates, considerably less attention has been given to the temporal dynamics of recourse--particularly in competitive, resource-constrained settings where recommendations shape future applicant pools. In this work, we present a novel time-aware framework for algorithmic recourse, explicitly modeling how candidate populations adapt in response to recommendations. Additionally, we introduce a novel reinforcement learning (RL)-based recourse algorithm that captures the evolving dynamics of the environment to generate recommendations that are both feasible and valid. We design our recommendations to be durable, supporting validity over a predefined time horizon T. This durability allows individuals to confidently reapply after taking time to implement the suggested changes. Through extensive experiments in complex simulation environments, we show that our approach substantially outperforms existing baselines, offering a superior balance between feasibility and long-term validity. Together, these results underscore the importance of incorporating temporal and behavioral dynamics into the design of practical recourse systems.",
    "published": "2025-09-26T09:24:12Z",
    "updated": "2026-02-02T09:01:26Z",
    "link": "http://arxiv.org/pdf/2509.22102v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Marina Ceccon",
      "Alessandro Fabris",
      "Goran Radanović",
      "Asia J. Biega",
      "Gian Antonio Susto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01826v1",
    "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
    "summary": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to \"training inference mismatch stemming\" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.",
    "published": "2026-02-02T09:00:53Z",
    "updated": "2026-02-02T09:00:53Z",
    "link": "http://arxiv.org/pdf/2602.01826v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yaxiang Zhang",
      "Yingru Li",
      "Jiacai Liu",
      "Jiawei Xu",
      "Ziniu Li",
      "Qian Liu",
      "Haoyuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01815v1",
    "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
    "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
    "published": "2026-02-02T08:47:36Z",
    "updated": "2026-02-02T08:47:36Z",
    "link": "http://arxiv.org/pdf/2602.01815v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yunhui Jang",
      "Seonghyun Park",
      "Jaehyung Kim",
      "Sungsoo Ahn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12946v4",
    "title": "AI-generated data contamination erodes pathological variability and diagnostic reliability",
    "summary": "Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination remain unexplored. Here, we show that in the absence of mandatory human verification, this self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability. By analysing more than 800,000 synthetic data points across clinical text generation, vision-language reporting, and medical image synthesis, we find that models progressively converge toward generic phenotypes regardless of the model architecture. Specifically, rare but critical findings, including pneumothorax and effusions, vanish from the synthetic content generated by AI models, while demographic representations skew heavily toward middle-aged male phenotypes. Crucially, this degradation is masked by false diagnostic confidence; models continue to issue reassuring reports while failing to detect life-threatening pathology, with false reassurance rates tripling to 40%. Blinded physician evaluation confirms that this decoupling of confidence and accuracy renders AI-generated documentation clinically useless after just two generations. We systematically evaluate three mitigation strategies, finding that while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity. Ultimately, our results suggest that without policy-mandated human oversight, the deployment of generative AI threatens to degrade the very healthcare data ecosystems it relies upon.",
    "published": "2026-01-19T10:54:03Z",
    "updated": "2026-02-02T08:40:18Z",
    "link": "http://arxiv.org/pdf/2601.12946v4.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Hongyu He",
      "Shaowen Xiang",
      "Ye Zhang",
      "Yingtao Zhu",
      "Jin Zhang",
      "Hao Deng",
      "Emily Alsentzer",
      "Yun Liu",
      "Qingyu Chen",
      "Kun-Hsing Yu",
      "Andrew Marshall",
      "Tingting Chen",
      "Srinivas Anumasa",
      "Daniel Ebner",
      "Dean Ho",
      "Kee Yuan Ngiam",
      "Ching-Yu Cheng",
      "Dianbo Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.22099v2",
    "title": "Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs",
    "summary": "Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.",
    "published": "2025-11-27T04:40:56Z",
    "updated": "2026-02-02T08:35:00Z",
    "link": "http://arxiv.org/pdf/2511.22099v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Daniel Agyei Asante",
      "Md Mokarram Chowdhury",
      "Yang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01801v1",
    "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
    "summary": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.",
    "published": "2026-02-02T08:31:21Z",
    "updated": "2026-02-02T08:31:21Z",
    "link": "http://arxiv.org/pdf/2602.01801v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Dvir Samuel",
      "Issar Tzachor",
      "Matan Levy",
      "Micahel Green",
      "Gal Chechik",
      "Rami Ben-Ari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08373v3",
    "title": "Draft-based Approximate Inference for LLMs",
    "summary": "Optimizing inference for long-context large language models (LLMs) is increasingly important due to the quadratic compute and linear memory cost of Transformers. Existing approximate inference methods, including key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on coarse predictions of token or KV pair importance. We unify and extend recent work by introducing a framework for approximate LLM inference that leverages small draft models to more accurately predict token and KV pair importance. We provide novel theoretical and empirical analyses justifying lookahead-based importance estimation techniques. Within this framework, we present: (i) SpecKV, the first method to use lookahead with a small draft model to enable precise KV cache dropping; (ii) SpecPC, which leverages draft model attention activations to identify and discard less important prompt tokens; and (iii) SpecKV-PC, a cascaded compression strategy combining both techniques. Extensive experiments on long-context benchmarks demonstrate that our methods consistently achieve higher accuracy than existing baselines while retaining the same efficiency gains in memory usage, latency, and throughput.",
    "published": "2025-06-10T02:37:46Z",
    "updated": "2026-02-02T08:30:58Z",
    "link": "http://arxiv.org/pdf/2506.08373v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Kevin Galim",
      "Ethan Ewer",
      "Wonjun Kang",
      "Minjae Lee",
      "Hyung Il Koo",
      "Kangwook Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01797v1",
    "title": "ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing",
    "summary": "Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.",
    "published": "2026-02-02T08:27:58Z",
    "updated": "2026-02-02T08:27:58Z",
    "link": "http://arxiv.org/pdf/2602.01797v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hanlin Zhou",
      "Huah Yong Chan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01795v1",
    "title": "RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse",
    "summary": "Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general utility via the \"alignment tax\", while detection-based filtering incurs prohibitive latency and memory costs. To bridge this gap, we propose RedVisor, a unified framework that synthesizes the explainability of detection systems with the seamless integration of prevention strategies. To the best of our knowledge, RedVisor is the first approach to leverage fine-grained reasoning paths to simultaneously detect attacks and guide the model's safe response. We implement this via a lightweight, removable adapter positioned atop the frozen backbone. This adapter serves a dual function: it first generates an explainable analysis that precisely localizes the injection and articulates the threat, which then explicitly conditions the model to reject the malicious command. Uniquely, the adapter is active only during this reasoning phase and is effectively muted during the subsequent response generation. This architecture yields two distinct advantages: (1) it mathematically preserves the backbone's original utility on benign inputs; and (2) it enables a novel KV Cache Reuse strategy, eliminating the redundant prefill computation inherent to decoupled pipelines. We further pioneer the integration of this defense into the vLLM serving engine with custom kernels. Experiments demonstrate that RedVisor outperforms state-of-the-art defenses in detection accuracy and throughput while incurring negligible utility loss.",
    "published": "2026-02-02T08:26:51Z",
    "updated": "2026-02-02T08:26:51Z",
    "link": "http://arxiv.org/pdf/2602.01795v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mingrui Liu",
      "Sixiao Zhang",
      "Cheng Long",
      "Kwok-Yan Lam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01410v5",
    "title": "Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems",
    "summary": "Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics can mask this operational asymmetry. We introduce an operator-legible evaluation framework -- Under-Prediction Rate (UPR), tail Reserve$_{99.5}^{\\%}$ requirements, and explicit inflation diagnostics (Bias$_{24h}$/OPR) -- to quantify one-sided reliability risk beyond MAPE.\n  Using this framework, we evaluate state space models (Mamba variants) and strong baselines on a weather-aligned California Independent System Operator (CAISO) dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 regional transmission areas) under a rolling-origin walk-forward backtest. We develop and evaluate thermal-lag-aligned weather fusion strategies for these architectures.\n  Our results demonstrate that standard accuracy metrics are insufficient proxies for operational safety: models with comparable MAPE can imply materially different tail reserve requirements (Reserve$_{99.5}^{\\%}$). We show that explicit weather integration narrows error distributions, reducing the impact of temperature-driven demand spikes. Furthermore, while probabilistic calibration reduces large-error events, it can induce systematic schedule inflation. We introduce Bias/OPR-constrained objectives to enable auditable trade-offs between minimizing tail risk and preventing trivial over-forecasting.",
    "published": "2026-01-04T07:30:50Z",
    "updated": "2026-02-02T08:23:49Z",
    "link": "http://arxiv.org/pdf/2601.01410v5.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sunki Hong",
      "Jisoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11367v2",
    "title": "Sparse Autoencoder Features for Classifications and Transferability",
    "summary": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: https://github.com/shan23chen/MOSAIC.",
    "published": "2025-02-17T02:30:45Z",
    "updated": "2026-02-02T08:18:48Z",
    "link": "http://arxiv.org/pdf/2502.11367v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jack Gallifant",
      "Shan Chen",
      "Kuleen Sasse",
      "Hugo Aerts",
      "Thomas Hartvigsen",
      "Danielle S. Bitterman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.15658v2",
    "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
    "summary": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.\n  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
    "published": "2025-11-19T17:45:02Z",
    "updated": "2026-02-02T08:03:41Z",
    "link": "http://arxiv.org/pdf/2511.15658v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Naomi Simumba",
      "Nils Lehmann",
      "Paolo Fraccaro",
      "Hamed Alemohammad",
      "Geeth De Mel",
      "Salman Khan",
      "Manil Maskey",
      "Nicolas Longepe",
      "Xiao Xiang Zhu",
      "Hannah Kerner",
      "Juan Bernabe-Moreno",
      "Alexandre Lacoste"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01779v1",
    "title": "LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning",
    "summary": "Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.",
    "published": "2026-02-02T08:02:25Z",
    "updated": "2026-02-02T08:02:25Z",
    "link": "http://arxiv.org/pdf/2602.01779v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Rui Hua",
      "Yu Wei",
      "Zixin Shu",
      "Kai Chang",
      "Dengying Yan",
      "Jianan Xia",
      "Zeyu Liu",
      "Hui Zhu",
      "Shujie Song",
      "Mingzhong Xiao",
      "Xiaodong Li",
      "Dongmei Jia",
      "Zhuye Gao",
      "Yanyan Meng",
      "Naixuan Zhao",
      "Yu Fu",
      "Haibin Yu",
      "Benman Yu",
      "Yuanyuan Chen",
      "Fei Dong",
      "Zhizhou Meng",
      "Pengcheng Yang",
      "Songxue Zhao",
      "Lijuan Pei",
      "Yunhui Hu",
      "Kan Ding",
      "Jiayuan Duan",
      "Wenmao Yin",
      "Yang Gu",
      "Runshun Zhang",
      "Qiang Zhu",
      "Jian Yu",
      "Jiansheng Li",
      "Baoyan Liu",
      "Wenjia Wang",
      "Xuezhong Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.15098v3",
    "title": "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm With Cuckoo Filter",
    "summary": "Although retrieval-augmented generation(RAG) significantly improves generation quality by retrieving external knowledge bases and integrating generated content, it faces computational efficiency bottlenecks, particularly in knowledge retrieval tasks involving hierarchical structures for Tree-RAG. This paper proposes a Tree-RAG acceleration method based on the improved Cuckoo Filter, which optimizes entity localization during the retrieval process to achieve significant performance improvements. Tree-RAG effectively organizes entities through the introduction of a hierarchical tree structure, while the Cuckoo Filter serves as an efficient data structure that supports rapid membership queries and dynamic updates. The experiment results demonstrate that our method is much faster than naive Tree-RAG while maintaining high levels of generative quality. When the number of trees is large, our method is hundreds of times faster than naive Tree-RAG. Our work is available at https://github.com/TUPYP7180/CFT-RAG-2025.",
    "published": "2025-01-25T06:09:02Z",
    "updated": "2026-02-02T08:01:28Z",
    "link": "http://arxiv.org/pdf/2501.15098v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zihang Li",
      "Yangdong Ruan",
      "Wenjun Liu",
      "Zhengyang Wang",
      "Tong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01777v1",
    "title": "Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions",
    "summary": "Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.",
    "published": "2026-02-02T08:01:13Z",
    "updated": "2026-02-02T08:01:13Z",
    "link": "http://arxiv.org/pdf/2602.01777v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML"
    ],
    "authors": [
      "M. Arashi",
      "M. Amintoosi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01775v1",
    "title": "Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction",
    "summary": "Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.",
    "published": "2026-02-02T08:00:36Z",
    "updated": "2026-02-02T08:00:36Z",
    "link": "http://arxiv.org/pdf/2602.01775v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yucheng Wu",
      "Yuekui Yang",
      "Hongzheng Li",
      "Anan Liu",
      "Jian Xiao",
      "Junjie Zhai",
      "Huan Yu",
      "Shaoping Ma",
      "Leye Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17307v5",
    "title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning",
    "summary": "Chain-of-thought (CoT) enhances the problem-solving ability of large language models (LLMs) but incurs substantial inference cost due to long autoregressive trajectories. Existing acceleration strategies either shorten traces via early stopping or compression, or adopt speculative decoding with a smaller model. However, speculative decoding provides limited gains when model agreement is low and rigidly enforces token-level consistency, overlooking the observation that some smaller models, when correct, produce significantly more concise reasoning traces that could reduce inference length. We introduce R-Stitch, a training-free hybrid decoding framework that leverages token-level entropy as an uncertainty proxy to delegate computation between a small language model (SLM) and an LLM. Our analysis shows that high-entropy tokens are more likely to induce errors, motivating an entropy-guided routing strategy that lets the SLM efficiently handle low-entropy tokens while delegating uncertain ones to the LLM, thereby avoiding full rollbacks and preserving answer quality. We further extend this design with R-Stitch$^{+}$, which learns an adaptive routing policy to adjust the token budget dynamically beyond fixed thresholds. By jointly reducing per-token decoding complexity and the number of generated tokens, our method achieves substantial acceleration with negligible accuracy loss. Concretely, it attains peak speedups of 3.00$\\times$ on DeepSeek-R1-Distill-Qwen-7B, 3.85$\\times$ on 14B, and 4.10$\\times$ on QWQ-32B while maintaining accuracy comparable to full LLM decoding. Moreover, it naturally enables adaptive efficiency--accuracy trade-offs that can be tailored to diverse computational budgets without retraining.",
    "published": "2025-07-23T08:14:36Z",
    "updated": "2026-02-02T07:57:06Z",
    "link": "http://arxiv.org/pdf/2507.17307v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zhuokun Chen",
      "Zeren Chen",
      "Jiahao He",
      "Lu Sheng",
      "Mingkui Tan",
      "Jianfei Cai",
      "Bohan Zhuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01772v1",
    "title": "DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics",
    "summary": "Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.",
    "published": "2026-02-02T07:55:24Z",
    "updated": "2026-02-02T07:55:24Z",
    "link": "http://arxiv.org/pdf/2602.01772v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "authors": [
      "Yucheng Liao",
      "Han Wen",
      "Weinan E",
      "Weijie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01771v1",
    "title": "<SOG_k>: One LLM Token for Explicit Graph Structural Understanding",
    "summary": "Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.",
    "published": "2026-02-02T07:55:09Z",
    "updated": "2026-02-02T07:55:09Z",
    "link": "http://arxiv.org/pdf/2602.01771v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.NI"
    ],
    "authors": [
      "Jingyao Wu",
      "Bin Lu",
      "Zijun Di",
      "Xiaoying Gan",
      "Meng Jin",
      "Luoyi Fu",
      "Xinbing Wang",
      "Chenghu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19771v3",
    "title": "Frictional Q-Learning",
    "summary": "Off-policy reinforcement learning suffers from extrapolation errors when a learned policy selects actions that are weakly supported in the replay buffer. In this study, we address this issue by drawing an analogy to static friction in classical mechanics. From this perspective, the replay buffer is represented as a smooth, low-dimensional action manifold, where the support directions correspond to the tangential component, while the normal component captures the dominant first-order extrapolation error. This decomposition reveals an intrinsic anisotropy in value sensitivity that naturally induces a stability condition analogous to a friction threshold. To mitigate deviations toward unsupported actions, we propose Frictional Q-Learning, an off-policy algorithm that encodes supported actions as tangent directions using a contrastive variational autoencoder. We further show that an orthonormal basis of the orthogonal complement corresponds to normal components under mild local isometry assumptions. Empirical results on standard continuous-control benchmarks demonstrate robust, stable performance compared with existing baselines.",
    "published": "2025-09-24T05:42:38Z",
    "updated": "2026-02-02T07:54:27Z",
    "link": "http://arxiv.org/pdf/2509.19771v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hyunwoo Kim",
      "Hyo Kyung Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01769v1",
    "title": "IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination",
    "summary": "Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.\n  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.",
    "published": "2026-02-02T07:51:57Z",
    "updated": "2026-02-02T07:51:57Z",
    "link": "http://arxiv.org/pdf/2602.01769v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yuanshuai Li",
      "Yuping Yan",
      "Jirui Han",
      "Fei Ming",
      "Lingjuan Lv",
      "Yaochu Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01766v1",
    "title": "CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling",
    "summary": "The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/",
    "published": "2026-02-02T07:49:44Z",
    "updated": "2026-02-02T07:49:44Z",
    "link": "http://arxiv.org/pdf/2602.01766v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Runsong Zhao",
      "Shilei Liu",
      "Jiwei Tang",
      "Langming Liu",
      "Haibin Chen",
      "Weidong Zhang",
      "Yujin Yuan",
      "Tong Xiao",
      "Jingbo Zhu",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01765v1",
    "title": "Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency",
    "summary": "Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.\n  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.\n  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\\%$ with negligible additional overhead, and invalidates an average of $98.5\\%$ of triggered samples with only a mild degradation in generation quality.",
    "published": "2026-02-02T07:48:44Z",
    "updated": "2026-02-02T07:48:44Z",
    "link": "http://arxiv.org/pdf/2602.01765v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Bingzheng Wang",
      "Xiaoyan Gu",
      "Hongbo Xu",
      "Hongcheng Li",
      "Zimo Yu",
      "Jiang Zhou",
      "Weiping Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01763v1",
    "title": "A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention",
    "summary": "Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.",
    "published": "2026-02-02T07:47:21Z",
    "updated": "2026-02-02T07:47:21Z",
    "link": "http://arxiv.org/pdf/2602.01763v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CC"
    ],
    "authors": [
      "Xiaowei Ye",
      "Xiaoyu He",
      "Chao Liao",
      "Chen Wu",
      "Pinyan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15386v3",
    "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection",
    "summary": "Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. Previous works improved the capability of hallucination detection by measuring uncertainty. But they can not explain the provenance behind why hallucinations occur, particularly in identifying which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as a total score. Experiments show that it achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and it is capable of producing token-level uncertainty scores as explanations of hallucination.",
    "published": "2025-05-21T11:23:05Z",
    "updated": "2026-02-02T07:47:17Z",
    "link": "http://arxiv.org/pdf/2505.15386v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yiming Huang",
      "Junyan Zhang",
      "Zihao Wang",
      "Biquan Bie",
      "Yunzhong Qiu",
      "Xuming Hu",
      "Yi R. Fung",
      "Xinlei He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01762v1",
    "title": "PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models",
    "summary": "Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.\n  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.",
    "published": "2026-02-02T07:46:03Z",
    "updated": "2026-02-02T07:46:03Z",
    "link": "http://arxiv.org/pdf/2602.01762v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Xuliang Wang",
      "Yuetao Chen",
      "Maochan Zhen",
      "Fang Liu",
      "Xinzhou Zheng",
      "Xingwu Liu",
      "Hong Xu",
      "Ming Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2309.16146v3",
    "title": "T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems",
    "summary": "To address the interpretability challenge in machine learning (ML) systems, counterfactual explanations (CEs) have emerged as a promising solution. CEs are unique as they provide workable suggestions to users, instead of explaining why a certain outcome was predicted. The application of CEs encounters two main challenges: general user preferences and variable ML systems. On one hand, user preferences for specific values can vary depending on the task and scenario. On the other hand, the ML systems for verification may change while the CEs are performed. Thus, user preferences tend to be general rather than specific, and CEs need to be adaptable to variable ML models while maintaining robustness even as these models change. Facing these challenges, we propose general user preferences based on insights from psychology and behavioral science, and add the challenge of non-static ML systems as one preference. Moreover, we introduce a novel method, \\uline{T}ree-based \\uline{C}onditions \\uline{O}ptional \\uline{L}inks (T-COL) for generating CEs adaptable to general user preferences. Moreover, we employ T-COL to enhance the robustness of CEs with specific conditions, making CEs robust even when the ML models are replaced. To assess subjectivity preferences, we define LLM-based autonomous agents to simulate users and align them with real users. Experiments show that T-COL outperforms all baselines in adapting to general user preferences.",
    "published": "2023-09-28T03:51:49Z",
    "updated": "2026-02-02T07:44:17Z",
    "link": "http://arxiv.org/pdf/2309.16146v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ming Wang",
      "Daling Wang",
      "Wenfang Wu",
      "Shi Feng",
      "Yifei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.20352v2",
    "title": "AMA: Adaptive Memory via Multi-Agent Collaboration",
    "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.",
    "published": "2026-01-28T08:09:49Z",
    "updated": "2026-02-02T07:41:51Z",
    "link": "http://arxiv.org/pdf/2601.20352v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Weiquan Huang",
      "Zixuan Wang",
      "Hehai Lin",
      "Sudong Wang",
      "Bo Xu",
      "Qian Li",
      "Beier Zhu",
      "Linyi Yang",
      "Chengwei Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22083v2",
    "title": "Latent Adversarial Regularization for Offline Preference Optimization",
    "summary": "Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.",
    "published": "2026-01-29T18:21:57Z",
    "updated": "2026-02-02T07:41:21Z",
    "link": "http://arxiv.org/pdf/2601.22083v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Enyi Jiang",
      "Yibo Jacky Zhang",
      "Yinglun Xu",
      "Andreas Haupt",
      "Nancy Amato",
      "Sanmi Koyejo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01750v1",
    "title": "Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.",
    "published": "2026-02-02T07:34:57Z",
    "updated": "2026-02-02T07:34:57Z",
    "link": "http://arxiv.org/pdf/2602.01750v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mohammad Beigi",
      "Ming Jin",
      "Junshan Zhang",
      "Qifan Wang",
      "Lifu Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01749v1",
    "title": "Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives",
    "summary": "Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \\times$ increase in the number of discovered modes.",
    "published": "2026-02-02T07:34:30Z",
    "updated": "2026-02-02T07:34:30Z",
    "link": "http://arxiv.org/pdf/2602.01749v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Lin Chen",
      "Samuel Drapeau",
      "Fanghao Shao",
      "Xuekai Zhu",
      "Bo Xue",
      "Yunchong Song",
      "Mathieu Laurière",
      "Zhouhan Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01746v1",
    "title": "Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment",
    "summary": "Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.",
    "published": "2026-02-02T07:27:44Z",
    "updated": "2026-02-02T07:27:44Z",
    "link": "http://arxiv.org/pdf/2602.01746v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hongyi Peng",
      "Han Yu",
      "Xiaoxiao Li",
      "Qiang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01745v1",
    "title": "Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning",
    "summary": "Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.",
    "published": "2026-02-02T07:27:19Z",
    "updated": "2026-02-02T07:27:19Z",
    "link": "http://arxiv.org/pdf/2602.01745v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wenhao Yu",
      "Shaohang Wei",
      "Jiahong Liu",
      "Yifan Li",
      "Minda Hu",
      "Aiwei Liu",
      "Hao Zhang",
      "Irwin King"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01744v1",
    "title": "Softmax Linear Attention: Reclaiming Global Competition",
    "summary": "While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \\emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \\textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.",
    "published": "2026-02-02T07:25:03Z",
    "updated": "2026-02-02T07:25:03Z",
    "link": "http://arxiv.org/pdf/2602.01744v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mingwei Xu",
      "Xuan Lin",
      "Xinnan Guo",
      "Wanqing Xu",
      "Wanyun Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.15204v2",
    "title": "Physics-Based Benchmarking Metrics for Multimodal Synthetic Images",
    "summary": "Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.",
    "published": "2025-11-19T07:52:20Z",
    "updated": "2026-02-02T07:21:32Z",
    "link": "http://arxiv.org/pdf/2511.15204v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kishor Datta Gupta",
      "Marufa Kamal",
      "Md. Mahfuzur Rahman",
      "Fahad Rahman",
      "Mohd Ariful Haque",
      "Sunzida Siddique"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01740v1",
    "title": "MACD: Model-Aware Contrastive Decoding via Counterfactual Data",
    "summary": "Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.",
    "published": "2026-02-02T07:21:02Z",
    "updated": "2026-02-02T07:21:02Z",
    "link": "http://arxiv.org/pdf/2602.01740v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Qixin Xiao",
      "Kun Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23973v2",
    "title": "A Community-Aware Framework for Influence Maximization with Explicit Accounting for Inter-Community Influence",
    "summary": "Influence Maximization (IM) seeks to identify a small set of seed nodes in a social network to maximize expected information spread under a diffusion model. While community-based approaches improve scalability by exploiting modular structure, they typically assume independence between communities, overlooking inter-community influence$\\unicode{x2014}$a limitation that reduces effectiveness in real-world networks. We introduce Community-IM++, a scalable framework that explicitly models cross-community diffusion through a principled heuristic based on community-based diffusion degree (CDD) and a progressive budgeting strategy. The algorithm partitions the network, computes CDD to prioritize bridging nodes, and allocates seeds adaptively across communities using lazy evaluation to minimize redundant computations. Experiments on large real-world social networks under different edge weight models show that Community-IM++ achieves near-greedy influence spread at up to 100 times lower runtime, while outperforming Community-IM and degree heuristics across budgets and structural conditions. These results demonstrate the practicality of Community-IM++ for large-scale applications such as viral marketing, misinformation control, and public health campaigns, where efficiency and cross-community reach are critical.",
    "published": "2025-12-30T04:05:21Z",
    "updated": "2026-02-02T07:16:51Z",
    "link": "http://arxiv.org/pdf/2512.23973v2.pdf",
    "category": [
      "cs.SI",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Eliot W. Robson",
      "Abhishek K. Umrawal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19653v2",
    "title": "Token-Importance Guided Direct Preference Optimization",
    "summary": "Aligning Large Language Models (LLMs) with human preferences is crucial for safe and effective AI interactions. While popular methods like Direct Preference Optimization (DPO) have simplified alignment, they remain sensitive to data noise and overlook the differential importance of individual tokens. Existing token-level approaches often rely on probability prediction or simplistic weighting schemes to obtain token importance, which still cannot fully address these issues. To solve this problem, we propose the Token-Importance Guided Direct Preference Optimization (TI-DPO), a framework that achieves fine-grained semantic control through two synergistic innovations. First, we propose a novel hybrid weighting mechanism that combines gradient attribution with a Gaussian prior, ensuring both the accuracy and robustness of token importance scores. Second, we employ a triplet loss to provide structured guidance for the optimization, explicitly guiding model outputs to approach preferred responses and diverge from non-preferred ones. Experimental results show that TI-DPO achieves higher accuracy and stronger generative diversity, providing more stable and computationally efficient solutions compared with DPO and other RLHF methods.",
    "published": "2025-05-26T08:11:24Z",
    "updated": "2026-02-02T07:09:20Z",
    "link": "http://arxiv.org/pdf/2505.19653v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ning Yang",
      "Hai Lin",
      "Yibo Liu",
      "Baoliang Tian",
      "Guoqing Liu",
      "Haijun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01725v1",
    "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
    "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
    "published": "2026-02-02T07:04:06Z",
    "updated": "2026-02-02T07:04:06Z",
    "link": "http://arxiv.org/pdf/2602.01725v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yurun Chen",
      "Zeyi Liao",
      "Ping Yin",
      "Taotao Xie",
      "Keting Yin",
      "Shengyu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16664v3",
    "title": "HyBattNet: Hybrid Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries",
    "summary": "Accurate prediction of the Remaining Useful Life (RUL) is essential for enabling timely maintenance of lithium-ion batteries, impacting the operational efficiency of electric applications that rely on them. This paper proposes a RUL prediction approach that leverages data from recent charge-discharge cycles to estimate the number of remaining usable cycles. The approach introduces both a novel signal preprocessing pipeline and a deep learning prediction model. In the signal preprocessing pipeline, a derived capacity feature is computed using interpolated current and capacity signals. Alongside original capacity, voltage and current, these features are denoised and enhanced using statistical metrics and a delta-based method to capture differences between the current and previous cycles. In the prediction model, the processed features are then fed into a hybrid deep learning architecture composed of 1D Convolutional Neural Networks (CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential Equation-based LSTM (ODE-LSTM) blocks. The ODE-LSTM architecture employs ordinary differential equations to integrate continuous dynamics into sequence-to-sequence modeling, thereby combining continuous and discrete temporal representations, while the A-LSTM incorporates an attention mechanism to capture local temporal dependencies. The model is further evaluated using transfer learning across different learning strategies and target data partitioning scenarios. Results indicate that the model maintains robust performance, even when fine-tuned on limited target data. Experimental results on two publicly available LFP/graphite lithium-ion battery datasets demonstrate that the proposed method outperforms a baseline deep learning approach and machine learning techniques, achieving an RMSE of 101.59, highlighting its potential for real-world RUL prediction applications.",
    "published": "2025-05-22T13:28:18Z",
    "updated": "2026-02-02T07:02:23Z",
    "link": "http://arxiv.org/pdf/2505.16664v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Khoa Tran",
      "Tri Le",
      "Bao Huynh",
      "Hung-Cuong Trinh",
      "Vy-Rin Nguyen",
      "T. Nguyen-Thoi",
      "Vin Nguyen-Thai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01717v1",
    "title": "BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition",
    "summary": "Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.",
    "published": "2026-02-02T06:56:27Z",
    "updated": "2026-02-02T06:56:27Z",
    "link": "http://arxiv.org/pdf/2602.01717v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hyunsik Kim",
      "Haeri Kim",
      "Munhak Lee",
      "Kyungmin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11891v3",
    "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents",
    "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline benchmarks evaluate the agents through single-path trajectories, which stands in contrast to the inherently multi-solution characteristics of GUI tasks. Additionally, both types of benchmarks fail to assess whether mobile agents can handle noise or engage in proactive interactions due to a lack of noisy apps or overly full instructions during the evaluation process. To address these limitations, we use a slot-based instruction generation method to construct a more realistic and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a common task split, with offline multi-path evaluation to assess the agent's ability to obtain step rewards during task execution. It contains a noisy split based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to formulate a real noisy environment. Furthermore, an ambiguous instruction split with preset Q\\&A interactions is released to evaluate the agent's proactive interaction capabilities. We conduct evaluations on these splits using the single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2, as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.",
    "published": "2025-05-17T07:58:34Z",
    "updated": "2026-02-02T06:55:10Z",
    "link": "http://arxiv.org/pdf/2505.11891v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Weikai Xu",
      "Zhizheng Jiang",
      "Yuxuan Liu",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan",
      "Yuanchun Li",
      "Yunxin Liu",
      "Bin Wang",
      "Bo An"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22709v2",
    "title": "Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs",
    "summary": "Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.",
    "published": "2026-01-30T08:30:52Z",
    "updated": "2026-02-02T06:39:48Z",
    "link": "http://arxiv.org/pdf/2601.22709v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yanlong Chen",
      "Amirhossein Habibian",
      "Luca Benini",
      "Yawei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01711v1",
    "title": "Optimizing Prompts for Large Language Models: A Causal Approach",
    "summary": "Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.",
    "published": "2026-02-02T06:37:11Z",
    "updated": "2026-02-02T06:37:11Z",
    "link": "http://arxiv.org/pdf/2602.01711v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wei Chen",
      "Yanbin Fang",
      "Shuran Fu",
      "Fasheng Xu",
      "Xuan Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01710v1",
    "title": "Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis",
    "summary": "Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.",
    "published": "2026-02-02T06:36:06Z",
    "updated": "2026-02-02T06:36:06Z",
    "link": "http://arxiv.org/pdf/2602.01710v1.pdf",
    "category": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "authors": [
      "Salma Zahran",
      "Zhou Ao",
      "Zhengyang Zhang",
      "Chen Chi",
      "Chenchen Yuan",
      "Yanming Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01708v1",
    "title": "Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory",
    "summary": "Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \\textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.",
    "published": "2026-02-02T06:33:18Z",
    "updated": "2026-02-02T06:33:18Z",
    "link": "http://arxiv.org/pdf/2602.01708v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.GT"
    ],
    "authors": [
      "Langyuan Cui",
      "Chun Kai Ling",
      "Hwee Tou Ng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.24047v3",
    "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
    "summary": "As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.",
    "published": "2025-03-31T13:11:28Z",
    "updated": "2026-02-02T06:26:53Z",
    "link": "http://arxiv.org/pdf/2503.24047v3.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Shuo Ren",
      "Can Xie",
      "Pu Jian",
      "Zhenjiang Ren",
      "Chunlin Leng",
      "Jiajun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01705v1",
    "title": "Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner",
    "summary": "Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.",
    "published": "2026-02-02T06:26:31Z",
    "updated": "2026-02-02T06:26:31Z",
    "link": "http://arxiv.org/pdf/2602.01705v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haoqiang Kang",
      "Yizhe Zhang",
      "Nikki Lijing Kuang",
      "Yi-An Ma",
      "Lianhui Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06199v2",
    "title": "FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation",
    "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision, language, and video understanding tasks, scaling them to long-form speech remains a critical bottleneck due to the explosive growth of input tokens. Existing speech-language models typically project high-frame-rate acoustic features directly into the LLM input space, rendering long-context processing computationally prohibitive as audio duration increases. In this paper, we present FastSLM, a token-efficient architecture designed to overcome this scalability limit through extreme temporal compression. At its core is the Hierarchical Frame Querying Transformer (HFQ-Former), which progressively distills local acoustic details into compact, semantically rich representations across multiple temporal scales. This hierarchical abstraction reduces the speech representation rate to just 1.67 tokens per second, achieving a 93 percent reduction in tokens compared to standard frame-level adapters, while preserving the critical context required for complex reasoning. Experimental results demonstrate that FastSLM achieves competitive performance with state-of-the-art models on long-form benchmarks, despite operating with significantly lower FLOPs and parameter counts. Our findings establish that extreme token compression is a viable pathway to making real-time, long-context speech understanding feasible for LLMs, even under strict computational constraints. The source code and model checkpoints are available at https://anonymous.4open.science/r/FastSLM-8BD3",
    "published": "2026-01-08T07:46:03Z",
    "updated": "2026-02-02T06:22:57Z",
    "link": "http://arxiv.org/pdf/2601.06199v2.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Junseok Lee",
      "Sangyong Lee",
      "Chang-Jae Chun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01701v1",
    "title": "Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems",
    "summary": "With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.\n  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some \"all-in-one\" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.\n  This paper introduces Meta Engine, a novel \"query system on query systems\", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.",
    "published": "2026-02-02T06:16:04Z",
    "updated": "2026-02-02T06:16:04Z",
    "link": "http://arxiv.org/pdf/2602.01701v1.pdf",
    "category": [
      "cs.DB",
      "cs.AI"
    ],
    "authors": [
      "Ruyu Li",
      "Tinghui Zhang",
      "Haodi Ma",
      "Daisy Zhe Wang",
      "Yifan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01699v1",
    "title": "Mitigating loss of control in advanced AI systems through instrumental goal trajectories",
    "summary": "Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.",
    "published": "2026-02-02T06:13:21Z",
    "updated": "2026-02-02T06:13:21Z",
    "link": "http://arxiv.org/pdf/2602.01699v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Willem Fourie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01696v1",
    "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection",
    "summary": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.",
    "published": "2026-02-02T06:11:33Z",
    "updated": "2026-02-02T06:11:33Z",
    "link": "http://arxiv.org/pdf/2602.01696v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jiaming Cui",
      "Shuai Zhou",
      "Wenqiang Li",
      "Ruifeng Qin",
      "Feng Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01695v1",
    "title": "Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning",
    "summary": "Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.",
    "published": "2026-02-02T06:08:35Z",
    "updated": "2026-02-02T06:08:35Z",
    "link": "http://arxiv.org/pdf/2602.01695v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yadong Wang",
      "Haodong Chen",
      "Yu Tian",
      "Chuanxing Geng",
      "Dong Liang",
      "Xiang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01689v1",
    "title": "What LLMs Think When You Don't Tell Them What to Think About?",
    "summary": "Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.",
    "published": "2026-02-02T06:06:06Z",
    "updated": "2026-02-02T06:06:06Z",
    "link": "http://arxiv.org/pdf/2602.01689v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yongchan Kwon",
      "James Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01687v1",
    "title": "Counting Hypothesis: Potential Mechanism of In-Context Learning",
    "summary": "In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.",
    "published": "2026-02-02T05:57:33Z",
    "updated": "2026-02-02T05:57:33Z",
    "link": "http://arxiv.org/pdf/2602.01687v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jung H. Lee",
      "Sujith Vijayan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01685v1",
    "title": "Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment",
    "summary": "Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.",
    "published": "2026-02-02T05:56:16Z",
    "updated": "2026-02-02T05:56:16Z",
    "link": "http://arxiv.org/pdf/2602.01685v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Byeonghu Na",
      "Hyungho Na",
      "Yeongmin Kim",
      "Suhyeon Jo",
      "HeeSun Bae",
      "Mina Kang",
      "Il-Chul Moon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24157v2",
    "title": "Experience-based Knowledge Correction for Robust Planning in Minecraft",
    "summary": "Large Language Model (LLM)-based planning has advanced embodied agents in long-horizon environments such as Minecraft, where acquiring latent knowledge of goal (or item) dependencies and feasible actions is critical. However, LLMs often begin with flawed priors and fail to correct them through prompting, even with feedback. We present XENON (eXpErience-based kNOwledge correctioN), an agent that algorithmically revises knowledge from experience, enabling robustness to flawed priors and sparse binary feedback. XENON integrates two mechanisms: Adaptive Dependency Graph, which corrects item dependencies using past successes, and Failure-aware Action Memory, which corrects action knowledge using past failures. Together, these components allow XENON to acquire complex dependencies despite limited guidance. Experiments across multiple Minecraft benchmarks show that XENON outperforms prior agents in both knowledge learning and long-horizon planning. Remarkably, with only a 7B open-weight LLM, XENON surpasses agents that rely on much larger proprietary models. Code available at https://sjlee-me.github.io/XENON",
    "published": "2025-05-30T03:01:44Z",
    "updated": "2026-02-02T05:55:44Z",
    "link": "http://arxiv.org/pdf/2505.24157v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Seungjoon Lee",
      "Suhwan Kim",
      "Minhyeon Oh",
      "Youngsik Yoon",
      "Jungseul Ok"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08688v2",
    "title": "STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision",
    "summary": "Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks.",
    "published": "2025-08-12T07:27:50Z",
    "updated": "2026-02-02T05:52:55Z",
    "link": "http://arxiv.org/pdf/2508.08688v2.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Chen Li",
      "Han Zhang",
      "Zhantao Yang",
      "Fangyi Chen",
      "Zihan Wang",
      "Anudeepsekhar Bolimera",
      "Marios Savvides"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01684v1",
    "title": "The Strategic Foresight of LLMs: Evidence from a Fully Prospective Venture Tournament",
    "summary": "Can artificial intelligence outperform humans at strategic foresight -- the capacity to form accurate judgments about uncertain, high-stakes outcomes before they unfold? We address this question through a fully prospective prediction tournament using live Kickstarter crowdfunding projects. Thirty U.S.-based technology ventures, launched after the training cutoffs of all models studied, were evaluated while fundraising remained in progress and outcomes were unknown. A diverse suite of frontier and open-weight large language models (LLMs) completed 870 pairwise comparisons, producing complete rankings of predicted fundraising success. We benchmarked these forecasts against 346 experienced managers recruited via Prolific and three MBA-trained investors working under monitored conditions. The results are striking: human evaluators achieved rank correlations with actual outcomes between 0.04 and 0.45, while several frontier LLMs exceeded 0.60, with the best (Gemini 2.5 Pro) reaching 0.74 -- correctly ordering nearly four of every five venture pairs. These differences persist across multiple performance metrics and robustness checks. Neither wisdom-of-the-crowd ensembles nor human-AI hybrid teams outperformed the best standalone model.",
    "published": "2026-02-02T05:52:16Z",
    "updated": "2026-02-02T05:52:16Z",
    "link": "http://arxiv.org/pdf/2602.01684v1.pdf",
    "category": [
      "econ.GN",
      "cs.AI"
    ],
    "authors": [
      "Felipe A. Csaszar",
      "Aticus Peterson",
      "Daniel Wilde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01683v1",
    "title": "FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding",
    "summary": "Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical \"gist\"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.",
    "published": "2026-02-02T05:52:11Z",
    "updated": "2026-02-02T05:52:11Z",
    "link": "http://arxiv.org/pdf/2602.01683v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kangcong Li",
      "Peng Ye",
      "Lin Zhang",
      "Chao Wang",
      "Huafeng Qin",
      "Tao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01679v1",
    "title": "Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications",
    "summary": "The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.",
    "published": "2026-02-02T05:46:31Z",
    "updated": "2026-02-02T05:46:31Z",
    "link": "http://arxiv.org/pdf/2602.01679v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Raghavasimhan Sankaranarayanan",
      "Paul Stuart",
      "Nicholas Ahn",
      "Arno Sungarian",
      "Yash Chitalia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01675v1",
    "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
    "summary": "As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \\textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose \\textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.",
    "published": "2026-02-02T05:43:08Z",
    "updated": "2026-02-02T05:43:08Z",
    "link": "http://arxiv.org/pdf/2602.01675v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuanzhe Shen",
      "Zisu Huang",
      "Zhengyuan Wang",
      "Muzhao Tian",
      "Zhengkang Guo",
      "Chenyang Zhang",
      "Shuaiyu Zhou",
      "Zengjie Hu",
      "Dailin Li",
      "Jingwen Xu",
      "Kaimin Wang",
      "Wenhao Liu",
      "Tianlong Li",
      "Fengpeng Yue",
      "Feng Hong",
      "Cao Liu",
      "Ke Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01673v1",
    "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss",
    "summary": "Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.",
    "published": "2026-02-02T05:41:42Z",
    "updated": "2026-02-02T05:41:42Z",
    "link": "http://arxiv.org/pdf/2602.01673v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Enguang Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01671v1",
    "title": "AI-Assisted Adaptive Rendering for High-Frequency Security Telemetry in Web Interfaces",
    "summary": "Modern cybersecurity platforms must process and display high-frequency telemetry such as network logs, endpoint events, alerts, and policy changes in real time. Traditional rendering techniques based on static pagination or fixed polling intervals fail under volume conditions exceeding hundreds of thousands of events per second, leading to UI freezes, dropped frames, or stale data. This paper presents an AI-assisted adaptive rendering framework that dynamically regulates visual update frequency, prioritizes semantically relevant events, and selectively aggregates lower-priority data using behavior-driven heuristics and lightweight on-device machine learning models. Experimental validation demonstrates a 45-60 percent reduction in rendering overhead while maintaining analyst perception of real-time responsiveness.",
    "published": "2026-02-02T05:40:21Z",
    "updated": "2026-02-02T05:40:21Z",
    "link": "http://arxiv.org/pdf/2602.01671v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Mona Rajhans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.23001v2",
    "title": "Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs",
    "summary": "Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.",
    "published": "2026-01-30T14:07:25Z",
    "updated": "2026-02-02T05:40:17Z",
    "link": "http://arxiv.org/pdf/2601.23001v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Afrozah Nadeem",
      " Agrima",
      "Mehwish Nasim",
      "Usman Naseem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01668v1",
    "title": "ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting",
    "summary": "Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba",
    "published": "2026-02-02T05:38:21Z",
    "updated": "2026-02-02T05:38:21Z",
    "link": "http://arxiv.org/pdf/2602.01668v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qianyang Li",
      "Xingjun Zhang",
      "Shaoxun Wang",
      "Jia Wei",
      "Yueqi Xing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01665v1",
    "title": "TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning",
    "summary": "The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.",
    "published": "2026-02-02T05:34:38Z",
    "updated": "2026-02-02T05:34:38Z",
    "link": "http://arxiv.org/pdf/2602.01665v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Hayeong Lee",
      "JunHyeok Oh",
      "Byung-Jun Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01664v1",
    "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
    "summary": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.",
    "published": "2026-02-02T05:30:42Z",
    "updated": "2026-02-02T05:30:42Z",
    "link": "http://arxiv.org/pdf/2602.01664v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mingda Zhang",
      "Haoran Luo",
      "Tiesunlong Shen",
      "Qika Lin",
      "Xiaoying Tang",
      "Rui Mao",
      "Erik Cambria"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01660v1",
    "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
    "summary": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.",
    "published": "2026-02-02T05:28:26Z",
    "updated": "2026-02-02T05:28:26Z",
    "link": "http://arxiv.org/pdf/2602.01660v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zhongyuan Peng",
      "Caijun Xu",
      "Changyi Xiao",
      "Shibo Hong",
      "Eli Zhang",
      "Stephen Huang",
      "Yixin Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01658v1",
    "title": "Efficient Adversarial Attacks on High-dimensional Offline Bandits",
    "summary": "Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...",
    "published": "2026-02-02T05:24:31Z",
    "updated": "2026-02-02T05:24:31Z",
    "link": "http://arxiv.org/pdf/2602.01658v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Seyed Mohammad Hadi Hosseini",
      "Amir Najafi",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01655v1",
    "title": "ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development",
    "summary": "Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.",
    "published": "2026-02-02T05:17:23Z",
    "updated": "2026-02-02T05:17:23Z",
    "link": "http://arxiv.org/pdf/2602.01655v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Pengrui Lu",
      "Shiqi Zhang",
      "Yunzhong Hou",
      "Lyumanshan Ye",
      "Chaoyi Huang",
      "Zixi Chen",
      "Ji Zeng",
      "Hantao Jiang",
      "Pengfei Liu",
      "Yiwei Wang",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01651v1",
    "title": "On the Spatiotemporal Dynamics of Generalization in Neural Networks",
    "summary": "Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.",
    "published": "2026-02-02T05:11:48Z",
    "updated": "2026-02-02T05:11:48Z",
    "link": "http://arxiv.org/pdf/2602.01651v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zichao Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01649v1",
    "title": "Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning",
    "summary": "Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \\textbf{C}ontribution-\\textbf{a}ware token \\textbf{Co}mpression algorithm for \\textbf{VID}eo understanding (\\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.",
    "published": "2026-02-02T05:09:48Z",
    "updated": "2026-02-02T05:09:48Z",
    "link": "http://arxiv.org/pdf/2602.01649v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yinchao Ma",
      "Qiang Zhou",
      "Zhibin Wang",
      "Xianing Chen",
      "Hanqing Yang",
      "Jun Song",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01644v1",
    "title": "From Perception to Action: Spatial AI Agents and World Models",
    "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.",
    "published": "2026-02-02T05:00:55Z",
    "updated": "2026-02-02T05:00:55Z",
    "link": "http://arxiv.org/pdf/2602.01644v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MA",
      "cs.RO"
    ],
    "authors": [
      "Gloria Felicia",
      "Nolan Bryant",
      "Handi Putra",
      "Ayaan Gazali",
      "Eliel Lobo",
      "Esteban Rojas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01643v1",
    "title": "De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion",
    "summary": "Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.",
    "published": "2026-02-02T05:00:00Z",
    "updated": "2026-02-02T05:00:00Z",
    "link": "http://arxiv.org/pdf/2602.01643v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xichen Sun",
      "Wentao Wei",
      "Jiahua Rao",
      "Jiancong Xie",
      "Yuedong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02053v1",
    "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
    "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.",
    "published": "2026-02-02T12:55:29Z",
    "updated": "2026-02-02T12:55:29Z",
    "link": "http://arxiv.org/pdf/2602.02053v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Pengyu Wang",
      "Benfeng Xu",
      "Licheng Zhang",
      "Shaohan Wang",
      "Mingxuan Du",
      "Chiwei Zhu",
      "Zhendong Mao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02047v1",
    "title": "Dissecting Outlier Dynamics in LLM NVFP4 Pretraining",
    "summary": "Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with \"post-QK\" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.",
    "published": "2026-02-02T12:50:27Z",
    "updated": "2026-02-02T12:50:27Z",
    "link": "http://arxiv.org/pdf/2602.02047v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Peijie Dong",
      "Ruibo Fan",
      "Yuechen Tao",
      "Di Mou",
      "Wenhu Hu",
      "Zhenheng Tang",
      "Yinghao Yu",
      "Jiamang Wang",
      "Wenbo Su",
      "Guodong Yang",
      "Liping Zhang",
      "Xiaowen Chu",
      "Baochun Li",
      "Bo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15048v3",
    "title": "MaiBERT: A Pre-training Corpus and Language Model for Low-Resourced Maithili Language",
    "summary": "Natural Language Understanding (NLU) for low-resource languages remains a major challenge in NLP due to the scarcity of high-quality data and language-specific models. Maithili, despite being spoken by millions, lacks adequate computational resources, limiting its inclusion in digital and AI-driven applications. To address this gap, we introducemaiBERT, a BERT-based language model pre-trained specifically for Maithili using the Masked Language Modeling (MLM) technique. Our model is trained on a newly constructed Maithili corpus and evaluated through a news classification task. In our experiments, maiBERT achieved an accuracy of 87.02%, outperforming existing regional models like NepBERTa and HindiBERT, with a 0.13% overall accuracy gain and 5-7% improvement across various classes. We have open-sourced maiBERT on Hugging Face enabling further fine-tuning for downstream tasks such as sentiment analysis and Named Entity Recognition (NER).",
    "published": "2025-09-18T15:11:18Z",
    "updated": "2026-02-02T12:45:15Z",
    "link": "http://arxiv.org/pdf/2509.15048v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sumit Yadav",
      "Raju Kumar Yadav",
      "Utsav Maskey",
      "Gautam Siddharth Kashyap",
      "Ganesh Gautam",
      "Usman Naseem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.16063v2",
    "title": "Free Access to World News: Reconstructing Full-Text Articles from GDELT",
    "summary": "News data have become essential resources across various disciplines. Still, access to full-text news corpora remains challenging due to high costs and the limited availability of free alternatives. This paper presents a novel Python package (gdeltnews) that reconstructs full-text newspaper articles at near-zero cost by leveraging the Global Database of Events, Language, and Tone (GDELT) Web News NGrams 3.0 dataset. Our method merges overlapping n-grams extracted from global online news to rebuild complete articles. We validate the approach on a benchmark set of 2211 articles from major U.S. news outlets, achieving up to 95% text similarity against original articles based on Levenshtein and SequenceMatcher metrics. Our tool facilitates economic forecasting, computational social science, information science, and natural language processing applications by enabling free and large-scale access to full-text news data.",
    "published": "2025-04-22T17:40:42Z",
    "updated": "2026-02-02T12:45:11Z",
    "link": "http://arxiv.org/pdf/2504.16063v2.pdf",
    "category": [
      "cs.CL",
      "cs.DB",
      "cs.IR"
    ],
    "authors": [
      "A. Fronzetti Colladon",
      "R. Vestrelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16123v5",
    "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning",
    "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting framework that embeds domain-specific expert financial reasoning blueprints to guide large language models' behaviors. We identify three main prompting styles in financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured CoT (free-form reasoning), and (3) structured CoT (with explicitly structured reasoning steps). Prior work has mainly focused on the first two, while structured CoT remains underexplored and lacks domain expertise incorporation. Therefore, we evaluate all three prompting approaches across ten CFA-style financial domains and introduce FinCoT as the first structured finance-specific prompting approach incorporating blueprints from domain experts. FinCoT improves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to 80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%, while reducing output length by up to 8.9x and 1.16x compared to structured CoT methods, respectively. We find that FinCoT proves most effective for models lacking financial post-training. Our findings show that FinCoT does not only improve performance and reduce inference costs but also yields more interpretable and expert-aligned reasoning traces.",
    "published": "2025-06-19T08:18:55Z",
    "updated": "2026-02-02T12:35:50Z",
    "link": "http://arxiv.org/pdf/2506.16123v5.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Natapong Nitarach",
      "Warit Sirichotedumrong",
      "Panop Pitchayarthorn",
      "Pittawat Taveekitworachai",
      "Potsawee Manakul",
      "Kunat Pipatanakul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02010v1",
    "title": "NEAT: Neuron-Based Early Exit for Large Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) often suffer from \\emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \\textbf{NEAT}, a \\textbf{N}euron-based \\textbf{E}arly re\\textbf{A}soning exi\\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\\% to 28\\% when averaged over the four benchmarks, while maintaining accuracy.",
    "published": "2026-02-02T12:09:59Z",
    "updated": "2026-02-02T12:09:59Z",
    "link": "http://arxiv.org/pdf/2602.02010v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kang Liu",
      "Yongkang Liu",
      "Xiaocui Yang",
      "Peidong Wang",
      "Wen Zhang",
      "Shi Feng",
      "Yifei Zhang",
      "Daling Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01999v1",
    "title": "From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs",
    "summary": "R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.",
    "published": "2026-02-02T11:58:24Z",
    "updated": "2026-02-02T11:58:24Z",
    "link": "http://arxiv.org/pdf/2602.01999v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yanrui Du",
      "Yibo Gao",
      "Sendong Zhao",
      "Jiayun Li",
      "Haochun Wang",
      "Qika Lin",
      "Kai He",
      "Bing Qin",
      "Mengling Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07890v2",
    "title": "Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects",
    "summary": "Research on cross-dialectal transfer from a standard to a non-standard dialect variety has typically focused on text data. However, dialects are primarily spoken, and non-standard spellings cause issues in text processing. We compare standard-to-dialect transfer in three settings: text models, speech models, and cascaded systems where speech first gets automatically transcribed and then further processed by a text model. We focus on German dialects in the context of written and spoken intent classification -- releasing the first dialectal audio intent classification dataset -- with supporting experiments on topic classification. The speech-only setup provides the best results on the dialect data while the text-only setup works best on the standard data. While the cascaded systems lag behind the text-only models for German, they perform relatively well on the dialectal data if the transcription system generates normalized, standard-like output.",
    "published": "2025-10-09T07:43:08Z",
    "updated": "2026-02-02T11:52:54Z",
    "link": "http://arxiv.org/pdf/2510.07890v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Verena Blaschke",
      "Miriam Winkler",
      "Barbara Plank"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01982v1",
    "title": "S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs",
    "summary": "Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.",
    "published": "2026-02-02T11:37:36Z",
    "updated": "2026-02-02T11:37:36Z",
    "link": "http://arxiv.org/pdf/2602.01982v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yanrui Du",
      "Sendong Zhao",
      "Yibo Gao",
      "Danyang Zhao",
      "Qika Lin",
      "Ming Ma",
      "Jiayun Li",
      "Yi Jiang",
      "Kai He",
      "Qianyi Xu",
      "Bing Qin",
      "Mengling Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01977v1",
    "title": "Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing",
    "summary": "Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.",
    "published": "2026-02-02T11:33:25Z",
    "updated": "2026-02-02T11:33:25Z",
    "link": "http://arxiv.org/pdf/2602.01977v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shuainan Liu",
      "Xuanang Chen",
      "Ben He",
      "Le Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01969v1",
    "title": "Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models",
    "summary": "Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.",
    "published": "2026-02-02T11:22:43Z",
    "updated": "2026-02-02T11:22:43Z",
    "link": "http://arxiv.org/pdf/2602.01969v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Bin Cao",
      "Huixian Lu",
      "Chenwen Ma",
      "Ting Wang",
      "Ruizhe Li",
      "Jing Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15755v2",
    "title": "Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs",
    "summary": "Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.",
    "published": "2026-01-22T08:45:55Z",
    "updated": "2026-02-02T10:44:36Z",
    "link": "http://arxiv.org/pdf/2601.15755v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tristan Williams",
      "Franziska Weeber",
      "Sebastian Padó",
      "Alan Akbik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.20312v2",
    "title": "SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger",
    "summary": "Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.",
    "published": "2026-01-28T07:04:30Z",
    "updated": "2026-02-02T10:36:52Z",
    "link": "http://arxiv.org/pdf/2601.20312v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kaiyuan Chen",
      "Guangmin Zheng",
      "Jin Wang",
      "Xiaobing Zhou",
      "Xuejie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01919v1",
    "title": "From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted \"Vibe Coding\"",
    "summary": "The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.",
    "published": "2026-02-02T10:21:34Z",
    "updated": "2026-02-02T10:21:34Z",
    "link": "http://arxiv.org/pdf/2602.01919v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hend Al-Khalifa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01917v1",
    "title": "GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs",
    "summary": "Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \\textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \\textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \\textbf{GuideWeb Agent} achieves \\textbf{30.79\\%} accuracy in guide target element prediction, while obtaining BLEU scores of \\textbf{44.94} for intent generation and \\textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.",
    "published": "2026-02-02T10:21:03Z",
    "updated": "2026-02-02T10:21:03Z",
    "link": "http://arxiv.org/pdf/2602.01917v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Chengguang Gan",
      "Yoshihiro Tsujii",
      "Yunhao Liang",
      "Tatsunori Mori",
      "Shiwen Ni",
      "Hiroki Itoh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.17279v5",
    "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement",
    "summary": "The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear as how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM) encoder, where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised method is learnt to align the LLM-based text embeddings with the Conditional Semantic Textual Similarity (C-STS) task. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings by improving the isotropy of the embedding space. Moreover, our supervised projection method significantly improves the performance of LLM-based embeddings despite requiring a small number of embedding dimensions.",
    "published": "2025-03-21T16:27:12Z",
    "updated": "2026-02-02T09:51:47Z",
    "link": "http://arxiv.org/pdf/2503.17279v5.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Gaifan Zhang",
      "Yi Zhou",
      "Danushka Bollegala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16929v3",
    "title": "Dimensional Collapse in Transformer Attention Outputs: A Challenge for Sparse Dictionary Learning",
    "summary": "Transformer architectures, and their attention mechanisms in particular, form the foundation of modern large language models. While transformer models are widely believed to operate in high-dimensional hidden spaces, we show that attention outputs are in fact confined to a surprisingly low-dimensional subspace, with an effective dimensionality of only about $60\\%$ of the full space. In contrast, MLP outputs and residual streams remain much closer to full-rank, exhibiting effective ranks around $90\\%$. This striking dimensional discrepancy is consistently observed across diverse model families and datasets, and is strongly shaped by the attention output projection matrix. Critically, we find this low-rank structure as a key factor of the prevalent dead feature problem in sparse dictionary learning, where it creates a mismatch between randomly initialized features and the intrinsic geometry of the activation space. Building on this insight, we propose a subspace-constrained training method for sparse autoencoders (SAEs), initializing feature directions into the active subspace of activations. Our approach reduces dead features from 87\\% to below 1\\% in Attention Output SAEs with 1M features, and can further extend to other sparse dictionary learning methods. Our findings provide both new insights into the geometry of attention and practical tools for improving sparse dictionary learning in large language models.",
    "published": "2025-08-23T07:27:00Z",
    "updated": "2026-02-02T09:46:23Z",
    "link": "http://arxiv.org/pdf/2508.16929v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Junxuan Wang",
      "Xuyang Ge",
      "Wentao Shu",
      "Zhengfu He",
      "Xipeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01875v1",
    "title": "PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning",
    "summary": "Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of \"low-probability truth\" and \"high-probability falsehood\". Recent approaches, such as teaching models to say \"I don't know\" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \\textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is \"\\textbf{debiasing then learning}.\" It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making \"room\" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.",
    "published": "2026-02-02T09:46:05Z",
    "updated": "2026-02-02T09:46:05Z",
    "link": "http://arxiv.org/pdf/2602.01875v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Langming Liu",
      "Kangtao Lv",
      "Haibin Chen",
      "Weidong Zhang",
      "Yejing Wang",
      "Shilei Liu",
      "Xin Tong",
      "Yujin Yuan",
      "Yongwei Wang",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.05488v2",
    "title": "MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards",
    "summary": "Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.",
    "published": "2026-01-09T02:44:37Z",
    "updated": "2026-02-02T09:29:13Z",
    "link": "http://arxiv.org/pdf/2601.05488v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhiyu Shen",
      "Ziming Wu",
      "Fuming Lai",
      "Shaobing Lian",
      "Yanghui Rao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22929v2",
    "title": "Semantic Leakage from Image Embeddings",
    "summary": "Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1",
    "published": "2026-01-30T12:46:41Z",
    "updated": "2026-02-02T09:20:53Z",
    "link": "http://arxiv.org/pdf/2601.22929v2.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Yiyi Chen",
      "Qiongkai Xu",
      "Desmond Elliott",
      "Qiongxiu Li",
      "Johannes Bjerva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.09701v4",
    "title": "Reassessing Active Learning Adoption in Contemporary NLP: A Community Survey",
    "summary": "Supervised learning relies on data annotation which usually is time-consuming and therefore expensive. A longstanding strategy to reduce annotation costs is active learning, an iterative process, in which a human annotates only data instances deemed informative by a model. Research in active learning has made considerable progress, especially with the rise of large language models (LLMs). However, we still know little about how these remarkable advances have translated into real-world applications, or contributed to removing key barriers to active learning adoption. To fill in this gap, we conduct an online survey in the NLP community to collect previously intangible insights on current implementation practices, common obstacles in application, and future prospects in active learning. We also reassess the perceived relevance of data annotation and active learning as fundamental assumptions. Our findings show that data annotation is expected to remain important and active learning to stay relevant while benefiting from LLMs. Consistent with a community survey from over 15 years ago, three key challenges yet persist -- setup complexity, uncertain cost reduction, and tooling -- for which we propose alleviation strategies. We publish an anonymized version of the dataset.",
    "published": "2025-03-12T18:00:04Z",
    "updated": "2026-02-02T09:14:57Z",
    "link": "http://arxiv.org/pdf/2503.09701v4.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Julia Romberg",
      "Christopher Schröder",
      "Julius Gonsior",
      "Katrin Tomanek",
      "Fredrik Olsson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.02426v3",
    "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding",
    "summary": "As social media and the World Wide Web become hubs for information dissemination, effectively organizing and understanding the vast amounts of dynamically evolving Web content is crucial. Knowledge graphs (KGs) provide a powerful framework for structuring this information. However, the rapid emergence of new hot topics, user relationships, and events in social media renders traditional static knowledge graph embedding (KGE) models rapidly outdated. Continual Knowledge Graph Embedding (CKGE) aims to address this issue, but existing methods commonly suffer from catastrophic forgetting, whereby older, but still valuable, information is lost when learning new knowledge (such as new memes or trending events). This means the model cannot effectively learn the evolution of the data. We propose a novel CKGE framework, BAKE. Unlike existing methods, BAKE formulates CKGE as a sequential Bayesian inference problem and utilizes the Bayesian posterior update principle as a natural continual learning strategy. This principle is insensitive to data order and provides theoretical guarantees to preserve prior knowledge as much as possible. Specifically, we treat each batch of new data as a Bayesian update to the model's prior. By maintaining the posterior distribution, the model effectively preserves earlier knowledge even as it evolves over multiple snapshots. Furthermore, to constrain the evolution of knowledge across snapshots, we introduce a continual clustering method that maintains the compact cluster structure of entity embeddings through a regularization term, ensuring semantic consistency while allowing controlled adaptation to new knowledge. We conduct extensive experiments on multiple CKGE benchmarks, which demonstrate that BAKE achieves the top performance in the vast majority of cases compared to existing approaches.",
    "published": "2025-08-04T13:46:33Z",
    "updated": "2026-02-02T09:12:58Z",
    "link": "http://arxiv.org/pdf/2508.02426v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Linyu Li",
      "Zhi Jin",
      "Yuanpeng He",
      "Dongming Jin",
      "Yichi Zhang",
      "Haoran Duan",
      "Xuan Zhang",
      "Zhengwei Tao",
      "Nyima Tash"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01840v1",
    "title": "Read As Human: Compressing Context via Parallelizable Close Reading and Skimming",
    "summary": "Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).",
    "published": "2026-02-02T09:10:56Z",
    "updated": "2026-02-02T09:10:56Z",
    "link": "http://arxiv.org/pdf/2602.01840v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jiwei Tang",
      "Shilei Liu",
      "Zhicheng Zhang",
      "Qingsong Lv",
      "Runsong Zhao",
      "Tingwei Lu",
      "Langming Liu",
      "Haibin Chen",
      "Yujin Yuan",
      "Hai-Tao Zheng",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01838v1",
    "title": "AXE: Low-Cost Cross-Domain Web Structured Information Extraction",
    "summary": "Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized \"pruning\" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.",
    "published": "2026-02-02T09:09:35Z",
    "updated": "2026-02-02T09:09:35Z",
    "link": "http://arxiv.org/pdf/2602.01838v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Abdelrahman Mansour",
      "Khaled W. Alshaer",
      "Moataz Elsaban"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.16553v3",
    "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models",
    "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due to their massive general knowledge and remarkable inference capacities. Current studies on LLMs have shown immense potential in applying LLMs to model individual mobility prediction problems. However, most LLM-based mobility prediction models only train on specific datasets or use single well-designed prompts, leading to difficulty in adapting to different cities and users with diverse contexts. To fill these gaps, this paper proposes a unified fine-tuning framework to train a foundational open source LLM-based mobility prediction model. We conducted extensive experiments on six real-world mobility datasets to validate the proposed model. The results showed that the proposed model achieved the best performance in prediction accuracy and transferability over state-of-the-art models based on deep learning and LLMs.",
    "published": "2025-03-19T15:08:37Z",
    "updated": "2026-02-02T09:08:26Z",
    "link": "http://arxiv.org/pdf/2503.16553v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhenlin Qin",
      "Leizhen Wang",
      "Francisco Camara Pereira",
      "Zhenliang Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01461v3",
    "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
    "summary": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.",
    "published": "2026-01-04T10:08:53Z",
    "updated": "2026-02-02T08:51:02Z",
    "link": "http://arxiv.org/pdf/2601.01461v3.pdf",
    "category": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Yuxiang Mei",
      "Dongxing Xu",
      "Jiaen Liang",
      "Yanhua Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23361v2",
    "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
    "summary": "Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience-enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves a Pass@1 resolution rate of 73.0% on SWE-Bench Verified using the state-of-the-art LLM Claude 4 Sonnet, significantly outperforming prior results under other agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.",
    "published": "2025-07-31T09:13:42Z",
    "updated": "2026-02-02T08:45:09Z",
    "link": "http://arxiv.org/pdf/2507.23361v2.pdf",
    "category": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Silin Chen",
      "Shaoxin Lin",
      "Yuling Shi",
      "Heng Lian",
      "Xiaodong Gu",
      "Longfei Yun",
      "Dong Chen",
      "Lin Cao",
      "Jiyang Liu",
      "Nu Xia",
      "Qianxiang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01807v1",
    "title": "Sentence Curve Language Models",
    "summary": "Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.",
    "published": "2026-02-02T08:40:53Z",
    "updated": "2026-02-02T08:40:53Z",
    "link": "http://arxiv.org/pdf/2602.01807v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "DongNyeong Heo",
      "Heelyoul Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01785v1",
    "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.",
    "published": "2026-02-02T08:10:21Z",
    "updated": "2026-02-02T08:10:21Z",
    "link": "http://arxiv.org/pdf/2602.01785v1.pdf",
    "category": [
      "cs.CL",
      "cs.SE"
    ],
    "authors": [
      "Yuling Shi",
      "Chaoxiang Xie",
      "Zhensu Sun",
      "Yeheng Chen",
      "Chenxu Zhang",
      "Longfei Yun",
      "Chengcheng Wan",
      "Hongyu Zhang",
      "David Lo",
      "Xiaodong Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01778v1",
    "title": "Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model",
    "summary": "The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.",
    "published": "2026-02-02T08:01:57Z",
    "updated": "2026-02-02T08:01:57Z",
    "link": "http://arxiv.org/pdf/2602.01778v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kangtao Lv",
      "Jiwei Tang",
      "Langming Liu",
      "Haibin Chen",
      "Weidong Zhang",
      "Shilei Liu",
      "Yongwei Wang",
      "Yujin Yuan",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01757v1",
    "title": "Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings",
    "summary": "The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.",
    "published": "2026-02-02T07:42:18Z",
    "updated": "2026-02-02T07:42:18Z",
    "link": "http://arxiv.org/pdf/2602.01757v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Doohyun Kim",
      "Donghwa Kang",
      "Kyungjae Lee",
      "Hyeongboo Baek",
      "Brent Byunghoon Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01752v1",
    "title": "WorldCup Sampling for Multi-bit LLM Watermarking",
    "summary": "As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.",
    "published": "2026-02-02T07:36:38Z",
    "updated": "2026-02-02T07:36:38Z",
    "link": "http://arxiv.org/pdf/2602.01752v1.pdf",
    "category": [
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Yidan Wang",
      "Yubing Ren",
      "Yanan Cao",
      "Li Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01747v1",
    "title": "Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training",
    "summary": "Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.",
    "published": "2026-02-02T07:29:15Z",
    "updated": "2026-02-02T07:29:15Z",
    "link": "http://arxiv.org/pdf/2602.01747v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Hongseok Choi",
      "Serynn Kim",
      "Wencke Liermann",
      "Jin Seong",
      "Jin-Xia Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07610v5",
    "title": "SpatialViz-Bench: A Cognitively-Grounded Benchmark for Diagnosing Spatial Visualization in MLLMs",
    "summary": "Humans can imagine and manipulate visual images mentally, a capability known as spatial visualization. While many multi-modal benchmarks assess reasoning on visible visual information, the ability to infer unseen relationships through spatial visualization remains insufficiently evaluated as a spatial skill. This reliance on publicly sourced problems from IQ tests or math competitions risks data contamination and compromises assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 programmatically generated problems, a scalable framework that allows for expansion to ensure fair and continuously reliable evaluations. Our evaluation of 27 Multi-modal Large Language Models (MLLMs) reveals wide performance variations, demonstrates the benchmark's strong discriminative power, and uncovers counter-intuitive findings: Chain-of-Thought (CoT) prompting paradoxically degrades accuracy on open-source models. Through statistical and qualitative analysis of error types, SpatialViz-Bench demonstrates that state-of-the-art MLLMs exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark data and evaluation code are publicly available.",
    "published": "2025-07-10T10:27:20Z",
    "updated": "2026-02-02T07:28:58Z",
    "link": "http://arxiv.org/pdf/2507.07610v5.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.HC"
    ],
    "authors": [
      "Siting Wang",
      "Minnan Pei",
      "Luoyang Sun",
      "Cheng Deng",
      "Kun Shao",
      "Zheng Tian",
      "Haifeng Zhang",
      "Jun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01719v1",
    "title": "COMI: Coarse-to-fine Context Compression via Marginal Information Gain",
    "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.",
    "published": "2026-02-02T06:57:22Z",
    "updated": "2026-02-02T06:57:22Z",
    "link": "http://arxiv.org/pdf/2602.01719v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jiwei Tang",
      "Shilei Liu",
      "Zhicheng Zhang",
      "Yujin Yuan",
      "Libin Zheng",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01716v1",
    "title": "Mechanistic Indicators of Steering Effectiveness in Large Language Models",
    "summary": "Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.",
    "published": "2026-02-02T06:56:22Z",
    "updated": "2026-02-02T06:56:22Z",
    "link": "http://arxiv.org/pdf/2602.01716v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Mehdi Jafari",
      "Hao Xue",
      "Flora Salim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01714v1",
    "title": "MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark",
    "summary": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.",
    "published": "2026-02-02T06:52:20Z",
    "updated": "2026-02-02T06:52:20Z",
    "link": "http://arxiv.org/pdf/2602.01714v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Mouath Abu-Daoud",
      "Leen Kharouf",
      "Omar El Hajj",
      "Dana El Samad",
      "Mariam Al-Omari",
      "Jihad Mallat",
      "Khaled Saleh",
      "Nizar Habash",
      "Farah E. Shamout"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01709v1",
    "title": "ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation",
    "summary": "Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \\emph{\\name}, \\emph{\\underline{A}gentic \\underline{R}isk-Aware \\underline{T}est-Time Scaling via \\underline{I}terative \\underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \\emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.",
    "published": "2026-02-02T06:33:22Z",
    "updated": "2026-02-02T06:33:22Z",
    "link": "http://arxiv.org/pdf/2602.01709v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xingshan Zeng",
      "Lingzhi Wang",
      "Weiwen Liu",
      "Liangyou Li",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.00920v3",
    "title": "Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios",
    "summary": "Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current RM evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering \"How accurate is the RM's preference perception for given samples?\", it employs scientific auditing to answer: \"Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?\". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.",
    "published": "2025-11-30T14:54:12Z",
    "updated": "2026-02-02T06:23:08Z",
    "link": "http://arxiv.org/pdf/2512.00920v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jianxiang Zang",
      "Yongda Wei",
      "Ruxue Bai",
      "Shiyu Jiang",
      "Nijia Mo",
      "Binhong Li",
      "Qiang Sun",
      "Hui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01703v1",
    "title": "$\\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality",
    "summary": "While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.",
    "published": "2026-02-02T06:19:27Z",
    "updated": "2026-02-02T06:19:27Z",
    "link": "http://arxiv.org/pdf/2602.01703v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Pengyu Li",
      "Lingling Zhang",
      "Zhitao Gao",
      "Yanrui Wu",
      "Yuxuan Dong",
      "Huan Liu",
      "Bifan Wei",
      "Jun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01698v1",
    "title": "Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.",
    "published": "2026-02-02T06:12:33Z",
    "updated": "2026-02-02T06:12:33Z",
    "link": "http://arxiv.org/pdf/2602.01698v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Wenhui Tan",
      "Fiorenzo Parascandolo",
      "Enver Sangineto",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Qian Cao",
      "Rita Cucchiara",
      "Ruihua Song",
      "Jian Luan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17310v2",
    "title": "PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding",
    "summary": "While Large Language Models (LLMs) demonstrate strong performance across domains, their long-context capabilities are limited by transient neural activations causing information decay and unstructured feed-forward network (FFN) weights leading to semantic fragmentation. Inspired by the brain's working memory and cortical modularity, we propose PaceLLM, featuring two innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal cortex (PFC) neurons' persistent firing by introducing an activation-level memory bank to dynamically retrieve, reuse, and update critical FFN states, addressing contextual decay; and (2) Cortical Expert (CE) Clustering that emulates task-adaptive neural specialization to reorganize FFN weights into semantic modules, establishing cross-token dependencies and mitigating fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement on LongBench's Multi-document QA and 12.5-17.5% performance gains on Infinite-Bench tasks, while extending measurable context length to 200K tokens in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM optimization and is complementary to other works. Besides, it can be generalized to any model and enhance their long-context performance and interpretability without structural overhauls.",
    "published": "2025-06-18T09:17:06Z",
    "updated": "2026-02-02T06:05:34Z",
    "link": "http://arxiv.org/pdf/2506.17310v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.CL",
      "cs.NE"
    ],
    "authors": [
      "Kangcong Li",
      "Peng Ye",
      "Chongjun Tu",
      "Lin Zhang",
      "Chunfeng Song",
      "Jiamin Wu",
      "Tao Yang",
      "Qihao Zheng",
      "Tao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01672v1",
    "title": "Scaling Search-Augmented LLM Reasoning via Adaptive Information Control",
    "summary": "Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.",
    "published": "2026-02-02T05:40:38Z",
    "updated": "2026-02-02T05:40:38Z",
    "link": "http://arxiv.org/pdf/2602.01672v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Siheng Xiong",
      "Oguzhan Gungordu",
      "Blair Johnson",
      "James C. Kerce",
      "Faramarz Fekri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.19110v3",
    "title": "APE-Bench: Evaluating Automated Proof Engineering for Formal Math Libraries",
    "summary": "While frontier formal mathematics systems now routinely develop repository-scale proof engineering artifacts requiring multi-file coordination and semantic correctness beyond compilation, existing evaluation benchmarks remain focused on isolated theorem proving. We introduce Automated Proof Engineering (APE), the first systematic framework for evaluating repository-scale proof engineering through dual verification that validates both syntactic compilation and semantic requirement satisfaction in pinned library environments. We present a complete infrastructure comprising APE-Bench, which automatically extracts proof engineering tasks from real library commit histories, and APE-Harness, a unified execution framework based on task contract abstraction. This contract-based design enables standardized evaluation across diverse formal mathematics tasks and fair systematic comparison of different agent implementations (including our APE-Agent reference scaffold alongside Claude Code and Codex CLI) on identical task specifications. We demonstrate the framework's effectiveness through comprehensive evaluation. All code and benchmark dataset are released as open-source at https://github.com/xinhjBrant/APE-Bench.",
    "published": "2025-04-27T05:04:02Z",
    "updated": "2026-02-02T05:16:28Z",
    "link": "http://arxiv.org/pdf/2504.19110v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Huajian Xin",
      "Luming Li",
      "Xiaoran Jin",
      "Jacques Fleuriot",
      "Wenda Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01654v1",
    "title": "Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models",
    "summary": "Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.",
    "published": "2026-02-02T05:14:42Z",
    "updated": "2026-02-02T05:14:42Z",
    "link": "http://arxiv.org/pdf/2602.01654v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jiaqian Li",
      "Yanshu Li",
      "Kuan-Hao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05980v2",
    "title": "Lost in Localization: Building RabakBench with Human-in-the-Loop Validation to Measure Multilingual Safety Gaps",
    "summary": "Large language models (LLMs) often fail to maintain safety in low-resource language varieties, such as code-mixed vernaculars and regional dialects. We introduce RabakBench, a multilingual safety benchmark and scalable pipeline localized to Singapore's unique linguistic landscape, covering Singlish, Chinese, Malay, and Tamil. We construct the benchmark through a three-stage pipeline: (1) Generate: augmenting real-world unsafe web content via LLM-driven red teaming; (2) Label: applying semi-automated multi-label annotation using majority-voted LLM labelers; and (3) Translate: performing high-fidelity, toxicity-preserving translation. The resulting dataset contains over 5,000 examples across six fine-grained safety categories. Despite using LLMs for scalability, our framework maintains rigorous human oversight, achieving 0.70-0.80 inter-annotator agreement. Evaluations of 13 state-of-the-art guardrails reveal significant performance degradation, underscoring the need for localized evaluation. RabakBench provides a reproducible framework for building safety benchmarks in underserved communities.",
    "published": "2025-07-08T13:37:25Z",
    "updated": "2026-02-02T05:02:34Z",
    "link": "http://arxiv.org/pdf/2507.05980v2.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Gabriel Chua",
      "Leanne Tan",
      "Ziyu Ge",
      "Roy Ka-Wei Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17779v3",
    "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding",
    "summary": "Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.",
    "published": "2025-05-23T11:48:48Z",
    "updated": "2026-02-02T13:10:09Z",
    "link": "http://arxiv.org/pdf/2505.17779v3.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Anjie Le",
      "Henan Liu",
      "Yue Wang",
      "Zhenyu Liu",
      "Rongkun Zhu",
      "Taohan Weng",
      "Jinze Yu",
      "Boyang Wang",
      "Yalun Wu",
      "Kaiwen Yan",
      "Quanlin Sun",
      "Meirui Jiang",
      "Jialun Pei",
      "Siya Liu",
      "Haoyun Zheng",
      "Zhoujun Li",
      "Alison Noble",
      "Jacques Souquet",
      "Xiaoqing Guo",
      "Manxi Lin",
      "Hongcheng Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.01817v2",
    "title": "SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art",
    "summary": "The ability to connect visual patterns with the processes that form them represents one of the deepest forms of visual understanding. Textures of clouds and waves, the growth of cities and forests, or the formation of materials and landscapes are all examples of patterns emerging from underlying mechanisms. We present the SciTextures dataset, a large-scale collection of textures and visual patterns from all domains of science, tech, and art, along with the models and code that generate these images. Covering over 1,270 different models and 100,000 images of patterns and textures from physics, chemistry, biology, sociology, technology, mathematics, and art, this dataset offers a way to explore the deep connection between the visual patterns that shape our world and the mechanisms that produce them. Built through an agentic AI pipeline that autonomously collects, implements, and standardizes scientific and generative models. This AI pipeline is also used to autonomously invent and implement novel methods for generating visual patterns and textures. SciTextures enables systematic evaluation of vision language models (VLM's) ability to link visual patterns to the models and code that generate them, and to identify different patterns that emerge from the same underlying process. We also test VLMs ability to infer and recreate the mechanisms behind visual patterns by providing a natural image of a real-world phenomenon and asking the AI to identify and code a model of the process that formed it, then run this code to generate a simulated image that is compared to the reference image. These benchmarks reveal that VLM's can understand and simulate physical systems beyond visual patterns at multiple levels of abstraction. The dataset and code are available at: https://zenodo.org/records/17485502",
    "published": "2025-11-03T18:22:11Z",
    "updated": "2026-02-02T13:00:09Z",
    "link": "http://arxiv.org/pdf/2511.01817v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sagi Eppel",
      "Alona Strugatski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10942v2",
    "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
    "summary": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
    "published": "2025-12-11T18:59:22Z",
    "updated": "2026-02-02T12:38:20Z",
    "link": "http://arxiv.org/pdf/2512.10942v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Delong Chen",
      "Mustafa Shukor",
      "Theo Moutakanni",
      "Willy Chung",
      "Jade Yu",
      "Tejaswi Kasarla",
      "Yejin Bang",
      "Allen Bolourchi",
      "Yann LeCun",
      "Pascale Fung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.20650v2",
    "title": "OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks",
    "summary": "Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.",
    "published": "2026-01-28T14:35:23Z",
    "updated": "2026-02-02T12:28:52Z",
    "link": "http://arxiv.org/pdf/2601.20650v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jing Wu",
      "Daphne Barretto",
      "Yiye Chen",
      "Nicholas Gydé",
      "Yanan Jian",
      "Yuhang He",
      "Vibhav Vineet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02002v1",
    "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
    "summary": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream",
    "published": "2026-02-02T12:02:27Z",
    "updated": "2026-02-02T12:02:27Z",
    "link": "http://arxiv.org/pdf/2602.02002v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Guosheng Zhao",
      "Yaozeng Wang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Tingdong Yu",
      "Guan Huang",
      "Yongchen Zai",
      "Ji Jiao",
      "Changliang Xue",
      "Xiaole Wang",
      "Zhen Yang",
      "Futang Zhu",
      "Xingang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22161v2",
    "title": "Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset",
    "summary": "We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9% to 65.56% (+3.66pp), while frequency-domain features for EEG achieved 67.62% (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached 75.30%, exceeding the paper's ViViT result (74.5%) through domain-specific pretraining, and vision delta features achieved 72.68% (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.",
    "published": "2026-01-07T18:22:01Z",
    "updated": "2026-02-02T11:50:11Z",
    "link": "http://arxiv.org/pdf/2601.22161v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Anmol Guragain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01991v1",
    "title": "Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models",
    "summary": "Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.",
    "published": "2026-02-02T11:47:48Z",
    "updated": "2026-02-02T11:47:48Z",
    "link": "http://arxiv.org/pdf/2602.01991v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pablo Domingo-Gregorio",
      "Javier Ruiz-Hidalgo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01984v1",
    "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling",
    "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.",
    "published": "2026-02-02T11:38:01Z",
    "updated": "2026-02-02T11:38:01Z",
    "link": "http://arxiv.org/pdf/2602.01984v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Minyoung Lee",
      "Yeji Park",
      "Dongjun Hwang",
      "Yejin Kim",
      "Seong Joon Oh",
      "Junsuk Choe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01954v1",
    "title": "Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images",
    "summary": "Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.",
    "published": "2026-02-02T11:03:01Z",
    "updated": "2026-02-02T11:03:01Z",
    "link": "http://arxiv.org/pdf/2602.01954v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuai Yang",
      "Ziyue Huang",
      "Jiaxin Chen",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01951v1",
    "title": "Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network",
    "summary": "Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.",
    "published": "2026-02-02T11:00:07Z",
    "updated": "2026-02-02T11:00:07Z",
    "link": "http://arxiv.org/pdf/2602.01951v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuyang Wu",
      "Yifu Qiu",
      "Ines P. Nearchou",
      "Sandrine Prost",
      "Jonathan A Fallowfield",
      "Hakan Bilen",
      "Timothy J Kendall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01949v1",
    "title": "Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity",
    "summary": "Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.",
    "published": "2026-02-02T10:59:20Z",
    "updated": "2026-02-02T10:59:20Z",
    "link": "http://arxiv.org/pdf/2602.01949v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Leonardo Stoppani",
      "Davide Bacciu",
      "Shahab Mokarizadeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10028v2",
    "title": "3D Dynamics-Aware Manipulation: Endowing Manipulation Policies with 3D Foresight",
    "summary": "The incorporation of world modeling into manipulation policy learning has pushed the boundary of manipulation performance. However, existing efforts simply model the 2D visual dynamics, which is insufficient for robust manipulation when target tasks involve prominent depth-wise movement. To address this, we present a 3D dynamics-aware manipulation framework that seamlessly integrates 3D world modeling and policy learning. Three self-supervised learning tasks (current depth estimation, future RGB-D prediction, 3D flow prediction) are introduced within our framework, which complement each other and endow the policy model with 3D foresight. Extensive experiments on simulation and the real world show that 3D foresight can greatly boost the performance of manipulation policies without sacrificing inference speed. Code is available at https://github.com/Stardust-hyx/3D-Foresight.",
    "published": "2025-02-14T09:13:57Z",
    "updated": "2026-02-02T10:52:49Z",
    "link": "http://arxiv.org/pdf/2502.10028v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Yuxin He",
      "Ruihao Zhang",
      "Xianzu Wu",
      "Zhiyuan Zhang",
      "Cheng Ding",
      "Qiang Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01930v1",
    "title": "LIEREx: Language-Image Embeddings for Robotic Exploration",
    "summary": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.",
    "published": "2026-02-02T10:30:50Z",
    "updated": "2026-02-02T10:30:50Z",
    "link": "http://arxiv.org/pdf/2602.01930v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Felix Igelbrink",
      "Lennart Niecksch",
      "Marian Renz",
      "Martin Günther",
      "Martin Atzmueller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15748v2",
    "title": "Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields",
    "summary": "Because of the variabilities of real-world image structures under the natural image transformations that arise when observing similar objects or spatio-temporal events under different viewing conditions, the receptive field responses computed in the earliest layers of the visual hierarchy may be strongly influenced by such geometric image transformations. One way of handling this variability is by basing the vision system on covariant receptive field families, which expand the receptive field shapes over the degrees of freedom in the image transformations.\n  This paper addresses the problem of deriving relationships between spatial and spatio-temporal receptive field responses obtained for different values of the shape parameters in the resulting multi-parameter families of receptive fields. For this purpose, we derive both (i) infinitesimal relationships, roughly corresponding to a combination of notions from semi-groups and Lie groups, as well as (ii) macroscopic cascade smoothing properties, which describe how receptive field responses at coarser spatial and temporal scales can be computed by applying smaller support incremental filters to the output from corresponding receptive fields at finer spatial and temporal scales, structurally related to the notion of Lie algebras, although with directional preferences.\n  The presented results provide (i) a deeper understanding of the relationships between spatial and spatio-temporal receptive field responses for different values of the filter parameters, which can be used for both (ii) designing more efficient schemes for computing receptive field responses over populations of multi-parameter families of receptive fields, as well as (iii)~formulating idealized theoretical models of the computations of simple cells in biological vision.",
    "published": "2025-09-19T08:23:44Z",
    "updated": "2026-02-02T10:29:16Z",
    "link": "http://arxiv.org/pdf/2509.15748v2.pdf",
    "category": [
      "cs.CV",
      "q-bio.NC"
    ],
    "authors": [
      "Tony Lindeberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08321v2",
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding and Visual Text Editing",
    "summary": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.",
    "published": "2026-01-13T08:18:49Z",
    "updated": "2026-02-02T10:21:29Z",
    "link": "http://arxiv.org/pdf/2601.08321v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lichen Ma",
      "Xiaolong Fu",
      "Gaojing Zhou",
      "Zipeng Guo",
      "Ting Zhu",
      "Yichun Liu",
      "Yu Shi",
      "Jason Li",
      "Junshi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23599v4",
    "title": "DA-Occ: Direction-Aware 2D Convolution for Efficient and Geometry-Preserving 3D Occupancy Prediction in Autonomous Driving",
    "summary": "Efficient and high-accuracy 3D occupancy prediction is vital for the performance of autonomous driving systems. However, existing methods struggle to balance precision and efficiency: high-accuracy approaches are often hindered by heavy computational overhead, leading to slow inference speeds, while others leverage pure bird's-eye-view (BEV) representations to gain speed at the cost of losing vertical spatial cues and compromising geometric integrity. To overcome these limitations, we build on the efficient Lift-Splat-Shoot (LSS) paradigm and propose a pure 2D framework, DA-Occ, for 3D occupancy prediction that preserves fine-grained geometry. Standard LSS-based methods lift 2D features into 3D space solely based on depth scores, making it difficult to fully capture vertical structure. To improve upon this, DA-Occ augments depth-based lifting with a complementary height-score projection that explicitly encodes vertical geometric information. We further employ direction-aware convolution to extract geometric features along both vertical and horizontal orientations, effectively balancing accuracy and computational efficiency. On the Occ3D-nuScenes, the proposed method achieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively balancing accuracy and efficiency. In simulations on edge devices, the inference speed reaches 14.8 FPS, further demonstrating the method's applicability for real-time deployment in resource-constrained environments.",
    "published": "2025-07-31T14:39:31Z",
    "updated": "2026-02-02T10:11:04Z",
    "link": "http://arxiv.org/pdf/2507.23599v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuchen Zhou",
      "Yan Luo",
      "Xiaogang Wang",
      "Xingjian Gu",
      "Mingzhou Lu",
      "Xiangbo Shu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01901v1",
    "title": "Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model",
    "summary": "Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.",
    "published": "2026-02-02T10:08:00Z",
    "updated": "2026-02-02T10:08:00Z",
    "link": "http://arxiv.org/pdf/2602.01901v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiedong Zhuang",
      "Lu Lu",
      "Ming Dai",
      "Rui Hu",
      "Jian Chen",
      "Qiang Liu",
      "Haoji Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01899v1",
    "title": "Multi-Task Learning for Robot Perception with Imbalanced Data",
    "summary": "Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.",
    "published": "2026-02-02T10:05:59Z",
    "updated": "2026-02-02T10:05:59Z",
    "link": "http://arxiv.org/pdf/2602.01899v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Ozgur Erkent"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22221v2",
    "title": "Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models",
    "summary": "Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the model's reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation.",
    "published": "2025-09-26T11:34:42Z",
    "updated": "2026-02-02T10:01:02Z",
    "link": "http://arxiv.org/pdf/2509.22221v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaqi Liu",
      "Lang Sun",
      "Ronghao Fu",
      "Bo Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.24410v2",
    "title": "GenTrack2: An Improved Hybrid Approach for Visual Multi-Object Tracking",
    "summary": "This paper proposes a visual multi-object tracking method that jointly employs stochastic and deterministic mechanisms to ensure identifier consistency for unknown and time-varying target numbers under nonlinear dynamics. A stochastic particle filter addresses nonlinear dynamics and non-Gaussian noise, with support from particle swarm optimization (PSO) to guide particles toward state distribution modes and mitigate divergence through proposed fitness measures incorporating motion consistency, appearance similarity, and social-interaction cues with neighboring targets. Deterministic association further enforces identifier consistency via a proposed cost matrix incorporating spatial consistency between particles and current detections, detection confidences, and track penalties. Subsequently, a novel scheme is proposed for the smooth updating of target states while preserving their identities, particularly for weak tracks during interactions with other targets and prolonged occlusions. Moreover, velocity regression over past states provides trend-seed velocities, enhancing particle sampling and state updates. The proposed tracker is designed to operate flexibly for both pre-recorded videos and camera live streams, where future frames are unavailable. Experimental results confirm superior performance compared to state-of-the-art trackers. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack2",
    "published": "2025-10-28T13:22:24Z",
    "updated": "2026-02-02T09:56:39Z",
    "link": "http://arxiv.org/pdf/2510.24410v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Toan Van Nguyen",
      "Rasmus G. K. Christiansen",
      "Dirk Kraft",
      "Leon Bodenhagen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01881v1",
    "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
    "summary": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.",
    "published": "2026-02-02T09:53:45Z",
    "updated": "2026-02-02T09:53:45Z",
    "link": "http://arxiv.org/pdf/2602.01881v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ye Chen",
      "Yupeng Zhu",
      "Xiongzhen Zhang",
      "Zhewen Wan",
      "Yingzhe Li",
      "Wenjun Zhang",
      "Bingbing Ni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18151v2",
    "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems",
    "summary": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.",
    "published": "2025-11-22T18:42:04Z",
    "updated": "2026-02-02T09:53:38Z",
    "link": "http://arxiv.org/pdf/2511.18151v2.pdf",
    "category": [
      "cs.DC",
      "cs.AR",
      "cs.CV",
      "cs.LG",
      "cs.NI"
    ],
    "authors": [
      "Rajat Bhattacharjya",
      "Sing-Yao Wu",
      "Hyunwoo Oh",
      "Chaewon Nam",
      "Suyeon Koo",
      "Mohsen Imani",
      "Elaheh Bozorgzadeh",
      "Nikil Dutt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.10701v2",
    "title": "Diffusion-based Layer-wise Semantic Reconstruction for Unsupervised Out-of-Distribution Detection",
    "summary": "Unsupervised out-of-distribution (OOD) detection aims to identify out-of-domain data by learning only from unlabeled In-Distribution (ID) training samples, which is crucial for developing a safe real-world machine learning system. Current reconstruction-based methods provide a good alternative approach by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space. However, such generative methods face a key dilemma: improving the reconstruction power of the generative model while keeping a compact representation of the ID data. To address this issue, we propose the diffusion-based layer-wise semantic reconstruction approach for unsupervised OOD detection. The innovation of our approach is that we leverage the diffusion model's intrinsic data reconstruction ability to distinguish ID samples from OOD samples in the latent feature space. Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy. By distorting the extracted features with Gaussian noise and applying the diffusion model for feature reconstruction, the separation of ID and OOD samples is implemented according to the reconstruction errors. Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed. Code is available at <https://github.com/xbyym/DLSR>.",
    "published": "2024-11-16T04:54:07Z",
    "updated": "2026-02-02T09:43:02Z",
    "link": "http://arxiv.org/pdf/2411.10701v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Ying Yang",
      "De Cheng",
      "Chaowei Fang",
      "Yubiao Wang",
      "Changzhe Jiao",
      "Lechao Cheng",
      "Nannan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01864v1",
    "title": "Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling",
    "summary": "Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a \"Trust but Verify\" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.",
    "published": "2026-02-02T09:34:57Z",
    "updated": "2026-02-02T09:34:57Z",
    "link": "http://arxiv.org/pdf/2602.01864v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuan Wang",
      "Yuhao Wan",
      "Siming Zheng",
      "Bo Li",
      "Qibin Hou",
      "Peng-Tao Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01854v1",
    "title": "Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection",
    "summary": "In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.",
    "published": "2026-02-02T09:28:16Z",
    "updated": "2026-02-02T09:28:16Z",
    "link": "http://arxiv.org/pdf/2602.01854v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "A S M Sharifuzzaman Sagar",
      "Mohammed Bennamoun",
      "Farid Boussaid",
      "Naeha Sharif",
      "Lian Xu",
      "Shaaban Sahmoud",
      "Ali Kishk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01851v1",
    "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
    "summary": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.",
    "published": "2026-02-02T09:24:45Z",
    "updated": "2026-02-02T09:24:45Z",
    "link": "http://arxiv.org/pdf/2602.01851v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Huanyu Zhang",
      "Xuehai Bai",
      "Chengzu Li",
      "Chen Liang",
      "Haochen Tian",
      "Haodong Li",
      "Ruichuan An",
      "Yifan Zhang",
      "Anna Korhonen",
      "Zhang Zhang",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01850v1",
    "title": "WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?",
    "summary": "IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.",
    "published": "2026-02-02T09:22:35Z",
    "updated": "2026-02-02T09:22:35Z",
    "link": "http://arxiv.org/pdf/2602.01850v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pei Li",
      "Jiaxi Yin",
      "Lei Ouyang",
      "Shihan Pan",
      "Ge Wang",
      "Han Ding",
      "Fei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22674v2",
    "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
    "summary": "Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
    "published": "2026-01-30T07:45:48Z",
    "updated": "2026-02-02T09:21:10Z",
    "link": "http://arxiv.org/pdf/2601.22674v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hanxun Yu",
      "Wentong Li",
      "Xuan Qu",
      "Song Wang",
      "Junbo Chen",
      "Jianke Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.09606v2",
    "title": "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting",
    "summary": "Given that visual foundation models (VFMs) are trained on extensive datasets but often limited to 2D images, a natural question arises: how well do they understand the 3D world? With the differences in architecture and training protocols (i.e., objectives, proxy tasks), a unified framework to fairly and comprehensively probe their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness, and require 3D data as ground-truth, which limits the scale and diversity of their evaluation set. To address these issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM features extracted from unposed images. This allows us to probe 3D awareness for geometry and texture via novel view synthesis, without requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry ($\\boldsymbol{x}$, $α$, $Σ$) and texture ($\\boldsymbol{c}$) - enables separate analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis. Code and data are available at https://fanegg.github.io/Feat2GS/.",
    "published": "2024-12-12T18:59:28Z",
    "updated": "2026-02-02T09:20:50Z",
    "link": "http://arxiv.org/pdf/2412.09606v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yue Chen",
      "Xingyu Chen",
      "Anpei Chen",
      "Gerard Pons-Moll",
      "Yuliang Xiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01843v1",
    "title": "SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection",
    "summary": "Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.",
    "published": "2026-02-02T09:15:29Z",
    "updated": "2026-02-02T09:15:29Z",
    "link": "http://arxiv.org/pdf/2602.01843v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qian Xu",
      "Xi Li",
      "Fei Gao",
      "Jie Guo",
      "Haojuan Yuan",
      "Shuaipeng Fan",
      "Mingjin Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01836v1",
    "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery",
    "summary": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.",
    "published": "2026-02-02T09:09:07Z",
    "updated": "2026-02-02T09:09:07Z",
    "link": "http://arxiv.org/pdf/2602.01836v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yin Wu",
      "Daniel Slieter",
      "Carl Esselborn",
      "Ahmed Abouelazm",
      "Tsung Yuan Tseng",
      "J. Marius Zöllner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04714v3",
    "title": "Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction",
    "summary": "3D Semantic Scene Graph Prediction aims to detect objects and their semantic relationships in 3D scenes, and has emerged as a crucial technology for robotics and AR/VR applications. While previous research has addressed dataset limitations and explored various approaches including Open-Vocabulary settings, they frequently fail to optimize the representational capacity of object and relationship features, showing excessive reliance on Graph Neural Networks despite insufficient discriminative capability. In this work, we demonstrate through extensive analysis that the quality of object features plays a critical role in determining overall scene graph accuracy. To address this challenge, we design a highly discriminative object feature encoder and employ a contrastive pretraining strategy that decouples object representation learning from the scene graph prediction. This design not only enhances object classification accuracy but also yields direct improvements in relationship prediction. Notably, when plugging in our pretrained encoder into existing frameworks, we observe substantial performance improvements across all evaluation metrics. Additionally, whereas existing approaches have not fully exploited the integration of relationship information, we effectively combine both geometric and semantic features to achieve superior relationship prediction. Comprehensive experiments on the 3DSSG dataset demonstrate that our approach significantly outperforms previous state-of-the-art methods. Our code is publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.",
    "published": "2025-10-06T11:33:09Z",
    "updated": "2026-02-02T08:57:36Z",
    "link": "http://arxiv.org/pdf/2510.04714v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "KunHo Heo",
      "GiHyun Kim",
      "SuYeon Kim",
      "MyeongAh Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01816v1",
    "title": "Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies",
    "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.",
    "published": "2026-02-02T08:48:03Z",
    "updated": "2026-02-02T08:48:03Z",
    "link": "http://arxiv.org/pdf/2602.01816v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenjin Hou",
      "Wei Liu",
      "Han Hu",
      "Xiaoxiao Sun",
      "Serena Yeung-Levy",
      "Hehe Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.14552v2",
    "title": "From Slices to Structures: Unsupervised 3D Reconstruction of Female Pelvic Anatomy from Freehand Transvaginal Ultrasound",
    "summary": "Volumetric ultrasound has the potential to significantly improve diagnostic accuracy and clinical decision-making, yet its widespread adoption remains limited by dependence on specialized hardware and restrictive acquisition protocols. In this work, we present a novel unsupervised framework for reconstructing 3D anatomical structures from freehand 2D transvaginal ultrasound sweeps, without requiring external tracking or learned pose estimators. Our method, TVGS, adapts the principles of Gaussian Splatting to the domain of ultrasound, introducing a slice-aware, differentiable rasterizer tailored to the unique physics and geometry of ultrasound imaging. We model anatomy as a collection of anisotropic 3D Gaussians and optimize their parameters directly from image-level supervision. To ensure robustness against irregular probe motion, we introduce a joint optimization scheme that refines slice poses alongside anatomical structure. The result is a compact, flexible, and memory-efficient volumetric representation that captures anatomical detail with high spatial fidelity. This work demonstrates that accurate 3D reconstruction from 2D ultrasound images can be achieved through purely computational means, offering a scalable alternative to conventional 3D systems and enabling new opportunities for AI-assisted analysis and diagnosis.",
    "published": "2025-08-20T09:09:06Z",
    "updated": "2026-02-02T08:47:49Z",
    "link": "http://arxiv.org/pdf/2508.14552v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Max Krähenmann",
      "Sergio Tascon-Morales",
      "Fabian Laumer",
      "Julia E. Vogt",
      "Ece Ozkan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01814v1",
    "title": "GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation",
    "summary": "Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.",
    "published": "2026-02-02T08:47:33Z",
    "updated": "2026-02-02T08:47:33Z",
    "link": "http://arxiv.org/pdf/2602.01814v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiao Liang",
      "Yunzhu Zhang",
      "Linchao Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01812v1",
    "title": "LDRNet: Large Deformation Registration Model for Chest CT Registration",
    "summary": "Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.",
    "published": "2026-02-02T08:44:53Z",
    "updated": "2026-02-02T08:44:53Z",
    "link": "http://arxiv.org/pdf/2602.01812v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cheng Wang",
      "Qiyu Gao",
      "Fandong Zhang",
      "Shu Zhang",
      "Yizhou Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19103v2",
    "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
    "summary": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).",
    "published": "2026-01-27T02:10:34Z",
    "updated": "2026-02-02T08:40:13Z",
    "link": "http://arxiv.org/pdf/2601.19103v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Linshan Wu",
      "Jiaxin Zhuang",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01805v1",
    "title": "FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing",
    "summary": "Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.",
    "published": "2026-02-02T08:37:00Z",
    "updated": "2026-02-02T08:37:00Z",
    "link": "http://arxiv.org/pdf/2602.01805v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Menglin Han",
      "Zhangkai Ni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15475v2",
    "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events",
    "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.",
    "published": "2026-01-21T21:25:58Z",
    "updated": "2026-02-02T08:35:26Z",
    "link": "http://arxiv.org/pdf/2601.15475v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yunshan Qi",
      "Lin Zhu",
      "Nan Bao",
      "Yifan Zhao",
      "Jia Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01799v1",
    "title": "Spatio-Temporal Transformers for Long-Term NDVI Forecasting",
    "summary": "Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.",
    "published": "2026-02-02T08:29:45Z",
    "updated": "2026-02-02T08:29:45Z",
    "link": "http://arxiv.org/pdf/2602.01799v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Ido Faran",
      "Nathan S. Netanyahu",
      "Maxim Shoshany"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.09777v5",
    "title": "EgoFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Self-Driving",
    "summary": "Current End-to-End Autonomous Driving (E2E-AD) methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized with a fully differentiable framework in a planning-oriented manner, existing end-to-end driving systems lacking ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, due to rasterized scene representation learning and redundant information transmission. In this paper, we propose an ego-centric fully sparse paradigm, named EgoFSD, for end-to-end self-driving. Specifically, EgoFSD consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. In addition, position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thereby enhancing the training stability and convergence speed. Extensive experiments are conducted on nuScenes and Bench2Drive datasets, which significantly reduces the average L2 error by 59% and collision rate by 92% than UniAD while achieves 6.9x faster running efficiency.",
    "published": "2024-09-15T15:55:24Z",
    "updated": "2026-02-02T08:17:02Z",
    "link": "http://arxiv.org/pdf/2409.09777v5.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Haisheng Su",
      "Wei Wu",
      "Zhenjie Yang",
      "Isabel Guan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01783v1",
    "title": "Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation",
    "summary": "Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.",
    "published": "2026-02-02T08:09:05Z",
    "updated": "2026-02-02T08:09:05Z",
    "link": "http://arxiv.org/pdf/2602.01783v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dibyayan Patra",
      "Pasindu Ranasinghe",
      "Bikram Banerjee",
      "Simit Raval"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01780v1",
    "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models",
    "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.",
    "published": "2026-02-02T08:04:25Z",
    "updated": "2026-02-02T08:04:25Z",
    "link": "http://arxiv.org/pdf/2602.01780v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Shicheng Yin",
      "Kaixuan Yin",
      "Weixing Chen",
      "Yang Liu",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13745v2",
    "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy",
    "summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
    "published": "2025-10-15T16:52:07Z",
    "updated": "2026-02-02T08:01:40Z",
    "link": "http://arxiv.org/pdf/2510.13745v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianshuo Xu",
      "Kai Wang",
      "Zhifei Chen",
      "Leyi Wu",
      "Tianshui Wen",
      "Fei Chao",
      "Ying-Cong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23478v3",
    "title": "UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception",
    "summary": "Recent cooperative perception datasets have played a crucial role in advancing smart mobility applications by enabling information exchange between intelligent agents, helping to overcome challenges such as occlusions and improving overall scene understanding. While some existing real-world datasets incorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions, they are typically limited to a single intersection or a single vehicle. A comprehensive perception dataset featuring multiple connected vehicles and infrastructure sensors across several intersections remains unavailable, limiting the benchmarking of algorithms in diverse traffic environments. Consequently, overfitting can occur, and models may demonstrate misleadingly high performance due to similar intersection layouts and traffic participant behavior. To address this gap, we introduce UrbanIng-V2X, the first large-scale, multi-modal dataset supporting cooperative perception involving vehicles and infrastructure sensors deployed across three urban intersections in Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and spatially calibrated sensor sequences, each lasting 20 seconds. All sequences contain recordings from one of three intersections, involving two vehicles and up to three infrastructure-mounted sensor poles operating in coordinated scenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGB cameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12 infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with 3D bounding boxes spanning 13 object classes, resulting in approximately 712k annotated instances across the dataset. We provide comprehensive evaluations using state-of-the-art cooperative perception methods and publicly release the codebase, dataset, HD map, and a digital twin of the complete data collection environment.",
    "published": "2025-10-27T16:12:12Z",
    "updated": "2026-02-02T07:58:08Z",
    "link": "http://arxiv.org/pdf/2510.23478v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Karthikeyan Chandra Sekaran",
      "Markus Geisler",
      "Dominik Rößle",
      "Adithya Mohan",
      "Daniel Cremers",
      "Wolfgang Utschick",
      "Michael Botsch",
      "Werner Huber",
      "Torsten Schön"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01764v1",
    "title": "GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data",
    "summary": "The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.",
    "published": "2026-02-02T07:48:03Z",
    "updated": "2026-02-02T07:48:03Z",
    "link": "http://arxiv.org/pdf/2602.01764v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dennis Basile",
      "Dennis Sprute",
      "Helene Dörksen",
      "Holger Flatt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01760v1",
    "title": "MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement",
    "summary": "This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.",
    "published": "2026-02-02T07:43:29Z",
    "updated": "2026-02-02T07:43:29Z",
    "link": "http://arxiv.org/pdf/2602.01760v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Zhang",
      "Yanping Zha",
      "Zizhuo Li",
      "Meiqi Gong",
      "Jiayi Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01756v1",
    "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
    "summary": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.",
    "published": "2026-02-02T07:42:13Z",
    "updated": "2026-02-02T07:42:13Z",
    "link": "http://arxiv.org/pdf/2602.01756v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jun He",
      "Junyan Ye",
      "Zilong Huang",
      "Dongzhi Jiang",
      "Chenjue Zhang",
      "Leqi Zhu",
      "Renrui Zhang",
      "Xiang Zhang",
      "Weijia Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01754v1",
    "title": "Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration",
    "summary": "Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.",
    "published": "2026-02-02T07:39:37Z",
    "updated": "2026-02-02T07:39:37Z",
    "link": "http://arxiv.org/pdf/2602.01754v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Gustavo P. C. P. da Luz",
      "Alvaro M. Aspilcueta Narvaez",
      "Tiago Godoi Bannwart",
      "Gabriel Massuyoshi Sato",
      "Luis Fernando Gomez Gonzalez",
      "Juliana Freitag Borin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01753v1",
    "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings",
    "summary": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.",
    "published": "2026-02-02T07:38:45Z",
    "updated": "2026-02-02T07:38:45Z",
    "link": "http://arxiv.org/pdf/2602.01753v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shenghao Fu",
      "Yukun Su",
      "Fengyun Rao",
      "Jing Lyu",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22861v2",
    "title": "Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction",
    "summary": "Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.\n  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.\n  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.",
    "published": "2026-01-30T11:38:56Z",
    "updated": "2026-02-02T07:24:42Z",
    "link": "http://arxiv.org/pdf/2601.22861v2.pdf",
    "category": [
      "cs.CV",
      "cs.CY",
      "cs.ET",
      "cs.GR"
    ],
    "authors": [
      "Refael Sheffer",
      "Chen Pinchover",
      "Haim Zisman",
      "Dror Ozeri",
      "Roee Litman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01741v1",
    "title": "Tail-Aware Post-Training Quantization for 3D Geometry Models",
    "summary": "The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.",
    "published": "2026-02-02T07:21:15Z",
    "updated": "2026-02-02T07:21:15Z",
    "link": "http://arxiv.org/pdf/2602.01741v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sicheng Pan",
      "Chen Tang",
      "Shuzhao Xie",
      "Ke Yang",
      "Weixiang Zhang",
      "Jiawei Li",
      "Bin Chen",
      "Shu-Tao Xia",
      "Zhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01738v1",
    "title": "Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models",
    "summary": "While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.",
    "published": "2026-02-02T07:20:02Z",
    "updated": "2026-02-02T07:20:02Z",
    "link": "http://arxiv.org/pdf/2602.01738v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yue Zhou",
      "Xinan He",
      "Kaiqing Lin",
      "Bing Fan",
      "Feng Ding",
      "Bin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.21408v2",
    "title": "MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations",
    "summary": "With the rapid advancement of video generation models such as Veo and Wan, the visual quality of synthetic content has reached a level where macro-level semantic errors and temporal inconsistencies are no longer prominent. However, this does not imply that the distinction between real and cutting-edge high-fidelity fake is untraceable. We argue that AI-generated videos are essentially products of a manifold-fitting process rather than a physical recording. Consequently, the pixel composition logic of consecutive adjacent frames residual in AI videos exhibits a structured and homogenous characteristic. We term this phenomenon `Manifold Projection Fluctuations' (MPF). Driven by this insight, we propose a hierarchical dual-path framework that operates as a sequential filtering process. The first, the Static Manifold Deviation Branch, leverages the refined perceptual boundaries of Large-Scale Vision Foundation Models (VFMs) to capture residual spatial anomalies or physical violations that deviate from the natural real-world manifold (off-manifold). For the remaining high-fidelity videos that successfully reside on-manifold and evade spatial detection, we introduce the Micro-Temporal Fluctuation Branch as a secondary, fine-grained filter. By analyzing the structured MPF that persists even in visually perfect sequences, our framework ensures that forgeries are exposed regardless of whether they manifest as global real-world manifold deviations or subtle computational fingerprints.",
    "published": "2026-01-29T08:44:56Z",
    "updated": "2026-02-02T07:13:51Z",
    "link": "http://arxiv.org/pdf/2601.21408v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinan He",
      "Kaiqing Lin",
      "Yue Zhou",
      "Jiaming Zhong",
      "Wei Ye",
      "Wenhui Yi",
      "Bing Fan",
      "Feng Ding",
      "Haodong Li",
      "Bo Cao",
      "Bin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01724v1",
    "title": "DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation",
    "summary": "In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.",
    "published": "2026-02-02T07:03:07Z",
    "updated": "2026-02-02T07:03:07Z",
    "link": "http://arxiv.org/pdf/2602.01724v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tushar Anand",
      "Maheswar Bora",
      "Antitza Dantcheva",
      "Abhijit Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01723v1",
    "title": "FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization",
    "summary": "Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.",
    "published": "2026-02-02T07:00:42Z",
    "updated": "2026-02-02T07:00:42Z",
    "link": "http://arxiv.org/pdf/2602.01723v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yikun Ma",
      "Yiqing Li",
      "Jingwen Ye",
      "Zhongkai Wu",
      "Weidong Zhang",
      "Lin Gao",
      "Zhi Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14376v4",
    "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
    "summary": "Recent progress in text-to-image (T2I) generative models has led to significant improvements in generating high-quality images aligned with text prompts. However, these models still struggle with prompts involving multiple objects, often resulting in object neglect or object mixing. Through extensive studies, we identify four problematic scenarios, Similar Shapes, Similar Textures, Dissimilar Background Biases, and Many Objects, where inter-object relationships frequently lead to such failures. Motivated by two key observations about CLIP embeddings, we propose DOS (Directional Object Separation), a method that modifies three types of CLIP text embeddings before passing them into text-to-image models. Experimental results show that DOS consistently improves the success rate of multi-object image generation and reduces object mixing. In human evaluations, DOS significantly outperforms four competing methods, receiving 26.24%-43.04% more votes across four benchmarks. These results highlight DOS as a practical and effective solution for improving multi-object image generation.",
    "published": "2025-10-16T07:17:23Z",
    "updated": "2026-02-02T06:40:44Z",
    "link": "http://arxiv.org/pdf/2510.14376v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongnam Byun",
      "Jungwon Park",
      "Jungmin Ko",
      "Changin Choi",
      "Wonjong Rhee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.11051v2",
    "title": "NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion",
    "summary": "Low-Rank Adaptation (LoRA) fusion enables the composition of learned subject and style representations for controllable generation without retraining. However, existing methods rely on weight-based merging within a shared adaptation space, where independently trained LoRAs interfere and degrade fidelity. We show that this interference is fundamentally geometric: content and style LoRAs occupy overlapping, non-orthogonal low-rank subspaces, making weight-based fusion inherently flawed. Analyzing LoRA internal structure, we find that generative behavior is dominated by a few principal directions that must be preserved during fusion. Based on this insight, we reformulate LoRA fusion as a null-space projection problem and propose Null Space Projection LoRA (NP-LoRA), a projection-based framework that enforces subspace separation by construction. NP-LoRA extracts principal style directions via singular value decomposition (SVD) and projects the subject LoRA into the orthogonal complement of the style subspace, preventing interference. We further introduce a soft projection mechanism that provides continuous control over the trade-off between subject fidelity and style preservation. Experiments show that NP-LoRA consistently outperforms strong baselines and generalizes well across pretrained LoRA pairs without retraining.",
    "published": "2025-11-14T08:06:01Z",
    "updated": "2026-02-02T06:22:03Z",
    "link": "http://arxiv.org/pdf/2511.11051v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chuheng Chen",
      "Xiaofei Zhou",
      "Geyuan Zhang",
      "Yong Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01681v1",
    "title": "Hyperspectral Image Fusion with Spectral-Band and Fusion-Scale Agnosticism",
    "summary": "Current deep learning models for Multispectral and Hyperspectral Image Fusion (MS/HS fusion) are typically designed for fixed spectral bands and spatial scales, which limits their transferability across diverse sensors. To address this, we propose SSA, a universal framework for MS/HS fusion with spectral-band and fusion-scale agnosticism. Specifically, we introduce Matryoshka Kernel (MK), a novel operator that enables a single model to adapt to arbitrary numbers of spectral channels. Meanwhile, we build SSA upon an Implicit Neural Representation (INR) backbone that models the HS signal as a continuous function, enabling reconstruction at arbitrary spatial resolutions. Together, these two forms of agnosticism enable a single MS/HS fusion model that generalizes effectively to unseen sensors and spatial scales. Extensive experiments demonstrate that our single model achieves state-of-the-art performance while generalizing well to unseen sensors and scales, paving the way toward future HS foundation models.",
    "published": "2026-02-02T05:48:53Z",
    "updated": "2026-02-02T05:48:53Z",
    "link": "http://arxiv.org/pdf/2602.01681v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Yu-Jie Liang",
      "Zihan Cao",
      "Liang-Jian Deng",
      "Yang Yang",
      "Malu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01677v1",
    "title": "SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking",
    "summary": "Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.",
    "published": "2026-02-02T05:44:59Z",
    "updated": "2026-02-02T05:44:59Z",
    "link": "http://arxiv.org/pdf/2602.01677v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yinchao Ma",
      "Dengqing Yang",
      "Zhangyu He",
      "Wenfei Yang",
      "Tianzhu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01674v1",
    "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
    "summary": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.",
    "published": "2026-02-02T05:42:40Z",
    "updated": "2026-02-02T05:42:40Z",
    "link": "http://arxiv.org/pdf/2602.01674v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Hail Song",
      "Boram Yoon",
      "Seokhwan Yang",
      "Seoyoung Kang",
      "Hyunjeong Kim",
      "Henning Metzmacher",
      "Woontack Woo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20295v5",
    "title": "FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis",
    "summary": "Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: https://github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.",
    "published": "2025-09-24T16:28:15Z",
    "updated": "2026-02-02T05:41:48Z",
    "link": "http://arxiv.org/pdf/2509.20295v5.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xichen Xu",
      "Yanshu Wang",
      "Jinbao Wang",
      "Xiaoning Lei",
      "Guoyang Xie",
      "Guannan Jiang",
      "Zhichao Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01666v1",
    "title": "Moonworks Lunara Aesthetic II: An Image Variation Dataset",
    "summary": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.",
    "published": "2026-02-02T05:37:28Z",
    "updated": "2026-02-02T05:37:28Z",
    "link": "http://arxiv.org/pdf/2602.01666v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yan Wang",
      "Partho Hassan",
      "Samiha Sadeka",
      "Nada Soliman",
      "M M Sayeef Abdullah",
      "Sabit Hassan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01661v1",
    "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
    "summary": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.",
    "published": "2026-02-02T05:28:58Z",
    "updated": "2026-02-02T05:28:58Z",
    "link": "http://arxiv.org/pdf/2602.01661v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xingyu Miao",
      "Junting Dong",
      "Qin Zhao",
      "Yuhang Yang",
      "Junhao Chen",
      "Yang Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02061v1",
    "title": "Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits",
    "summary": "Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit\" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit\" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\\widetilde{\\mathcal{O}}(\\sqrt{t})$ for routing and a queue length regret of $\\widetilde{\\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.",
    "published": "2026-02-02T13:01:41Z",
    "updated": "2026-02-02T13:01:41Z",
    "link": "http://arxiv.org/pdf/2602.02061v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Seoungbin Bae",
      "Junyoung Son",
      "Dabeen Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02056v1",
    "title": "Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks",
    "summary": "Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.",
    "published": "2026-02-02T12:57:15Z",
    "updated": "2026-02-02T12:57:15Z",
    "link": "http://arxiv.org/pdf/2602.02056v1.pdf",
    "category": [
      "cs.AR",
      "cs.LG",
      "eess.SY",
      "stat.ML"
    ],
    "authors": [
      "Duc Hoang",
      "Aarush Gupta",
      "Philip Harris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02045v1",
    "title": "On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems",
    "summary": "Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \\emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \\emph{robust diffusion posterior sampling}, which is provably \\emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.",
    "published": "2026-02-02T12:47:15Z",
    "updated": "2026-02-02T12:47:15Z",
    "link": "http://arxiv.org/pdf/2602.02045v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yiming Yang",
      "Xiaoyuan Cheng",
      "Yi He",
      "Kaiyu Li",
      "Wenxuan Yuan",
      "Zhuo Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02044v1",
    "title": "Twinning Complex Networked Systems: Data-Driven Calibration of the mABCD Synthetic Graph Generator",
    "summary": "The increasing availability of relational data has contributed to a growing reliance on network-based representations of complex systems. Over time, these models have evolved to capture more nuanced properties, such as the heterogeneity of relationships, leading to the concept of multilayer networks. However, the analysis and evaluation of methods for these structures is often hindered by the limited availability of large-scale empirical data. As a result, graph generators are commonly used as a workaround, albeit at the cost of introducing systematic biases. In this paper, we address the inverse-generator problem by inferring the configuration parameters of a multilayer network generator, mABCD, from a real-world system. Our goal is to identify parameter settings that enable the generator to produce synthetic networks that act as digital twins of the original structure. We propose a method for estimating matching configurations and for quantifying the associated error. Our results demonstrate that this task is non-trivial, as strong interdependencies between configuration parameters weaken independent estimation and instead favour a joint-prediction approach.",
    "published": "2026-02-02T12:40:19Z",
    "updated": "2026-02-02T12:40:19Z",
    "link": "http://arxiv.org/pdf/2602.02044v1.pdf",
    "category": [
      "cs.SI",
      "cs.LG"
    ],
    "authors": [
      "Piotr Bródka",
      "Michał Czuba",
      "Bogumił Kamiński",
      "Łukasz Kraiński",
      "Katarzyna Musial",
      "Paweł Prałat",
      "Mateusz Stolarski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02025v1",
    "title": "Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data",
    "summary": "Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.",
    "published": "2026-02-02T12:21:24Z",
    "updated": "2026-02-02T12:21:24Z",
    "link": "http://arxiv.org/pdf/2602.02025v1.pdf",
    "category": [
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Serafeim Papadias",
      "Kostas Patroumpas",
      "Dimitrios Skoutas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02024v1",
    "title": "Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation",
    "summary": "A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing.",
    "published": "2026-02-02T12:20:07Z",
    "updated": "2026-02-02T12:20:07Z",
    "link": "http://arxiv.org/pdf/2602.02024v1.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Clémence Réda",
      "Tomas Rigaux",
      "Hiba Bederina",
      "Koh Takeuchi",
      "Hisashi Kashima",
      "Jill-Jênn Vie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02020v1",
    "title": "Scale-covariant spiking wavelets",
    "summary": "We establish a theoretical connection between wavelet transforms and spiking neural networks through scale-space theory. We rely on the scale-covariant guarantees in the leaky integrate-and-fire neurons to implement discrete mother wavelets that approximate continuous wavelets. A reconstruction experiment demonstrates the feasibility of the approach and warrants further analysis to mitigate current approximation errors. Our work suggests a novel spiking signal representation that could enable more energy-efficient signal processing algorithms.",
    "published": "2026-02-02T12:16:44Z",
    "updated": "2026-02-02T12:16:44Z",
    "link": "http://arxiv.org/pdf/2602.02020v1.pdf",
    "category": [
      "cs.NE",
      "cs.LG"
    ],
    "authors": [
      "Jens Egholm Pedersen",
      "Tony Lindeberg",
      "Peter Gerstoft"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02016v1",
    "title": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers",
    "summary": "Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \\method (for \\textbf{D}istributed \\textbf{A}ccelerated \\textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.",
    "published": "2026-02-02T12:14:45Z",
    "updated": "2026-02-02T12:14:45Z",
    "link": "http://arxiv.org/pdf/2602.02016v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ionut-Vlad Modoranu",
      "Philip Zmushko",
      "Erik Schultheis",
      "Mher Safaryan",
      "Dan Alistarh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02015v1",
    "title": "Robust Domain Generalization under Divergent Marginal and Conditional Distributions",
    "summary": "Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.",
    "published": "2026-02-02T12:13:41Z",
    "updated": "2026-02-02T12:13:41Z",
    "link": "http://arxiv.org/pdf/2602.02015v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jewon Yeom",
      "Kyubyung Chae",
      "Hyunggyu Lim",
      "Yoonna Oh",
      "Dongyoon Yang",
      "Taesup Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.06501v2",
    "title": "Individual Regret in Cooperative Stochastic Multi-Armed Bandits",
    "summary": "We study the regret in stochastic Multi-Armed Bandits (MAB) with multiple agents that communicate over an arbitrary connected communication graph. We analyzed a variant of Cooperative Successive Elimination algorithm, $\\coopse$, and show an individual regret bound of ${O}(\\mathcal{R} / m + A^2 + A \\sqrt{\\log T})$ and a nearly matching lower bound. Here $A$ is the number of actions, $T$ the time horizon, $m$ the number of agents, and $\\mathcal{R} = \\sum_{Δ_i > 0}\\log(T)/Δ_i$ is the optimal single agent regret, where $Δ_i$ is the sub-optimality gap of action $i$. Our work is the first to show an individual regret bound in cooperative stochastic MAB that is independent of the graph's diameter.\n  When considering communication networks there are additional considerations beyond regret, such as message size and number of communication rounds. First, we show that our regret bound holds even if we restrict the messages to be of logarithmic size. Second, for logarithmic number of communication rounds, we obtain a regret bound of ${O}(\\mathcal{R} / m+A \\log T)$.",
    "published": "2024-11-10T15:54:23Z",
    "updated": "2026-02-02T12:12:12Z",
    "link": "http://arxiv.org/pdf/2411.06501v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Idan Barnea",
      "Tal Lancewicki",
      "Yishay Mansour"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13024v3",
    "title": "When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic Models with Missing Values",
    "summary": "Predicting with missing inputs challenges even parametric models, as parameter estimation alone is insufficient for prediction on incomplete data. While several works study prediction in linear models, we focus on logistic models, where optimal predictors lack closed-form expressions. We prove that a Pattern-by-Pattern strategy (PbP), which learns one logistic model per missingness pattern, accurately approximates Bayes probabilities under a Gaussian Pattern Mixture Model (GPMM). Crucially, this result holds across standard missing data scenarios (MCAR and MAR) and, notably, in Missing Not at Random (MNAR) settings where standard methods often fail. Empirically, we compare PbP against imputation and EM methods across classification, probability estimation, calibration, and inference. Our analysis provides a comprehensive view of logistic regression with missing values. It reveals that mean imputation can be used as baseline for low sample sizes and PbP for large sample sizes, as both methods are fast to train and may have good performances in some settings. The best performances are achieved by non-linear multiple iterative imputation techniques that include the response label (Random Forest MICE with response), which are more computationally expensive.",
    "published": "2025-07-17T11:52:27Z",
    "updated": "2026-02-02T12:12:05Z",
    "link": "http://arxiv.org/pdf/2507.13024v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Christophe Muller",
      "Erwan Scornet",
      "Julie Josse"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02013v1",
    "title": "SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation",
    "summary": "We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.",
    "published": "2026-02-02T12:10:31Z",
    "updated": "2026-02-02T12:10:31Z",
    "link": "http://arxiv.org/pdf/2602.02013v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Xiaoyi Jiang",
      "Andreas Nienkötter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02009v1",
    "title": "Logic-Guided Vector Fields for Constrained Generative Modeling",
    "summary": "Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.",
    "published": "2026-02-02T12:09:42Z",
    "updated": "2026-02-02T12:09:42Z",
    "link": "http://arxiv.org/pdf/2602.02009v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ali Baheri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02005v1",
    "title": "Position: The Need for Ultrafast Training",
    "summary": "Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.",
    "published": "2026-02-02T12:04:11Z",
    "updated": "2026-02-02T12:04:11Z",
    "link": "http://arxiv.org/pdf/2602.02005v1.pdf",
    "category": [
      "cs.AR",
      "cs.LG",
      "eess.SY",
      "hep-ex",
      "quant-ph"
    ],
    "authors": [
      "Duc Hoang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.06329v3",
    "title": "Joint Bayesian Parameter and Model Order Estimation for Low-Rank Probability Mass Tensors",
    "summary": "Obtaining a reliable estimate of the joint probability mass function (PMF) of a set of random variables from observed data is a significant objective in statistical signal processing and machine learning. Modelling the joint PMF as a tensor that admits a low-rank canonical polyadic decomposition (CPD) has enabled the development of efficient PMF estimation algorithms. However, these algorithms require the rank (model order) of the tensor to be specified beforehand. In real-world applications, the true rank is unknown. Therefore, an appropriate rank is usually selected from a candidate set either by observing validation errors or by computing various likelihood-based information criteria, a procedure that could be costly in terms of computational time or hardware resources, or could result in mismatched models which affect the model accuracy. This paper presents a novel Bayesian framework for estimating the low-rank components of a joint PMF tensor and simultaneously inferring its rank from the observed data. We specify a Bayesian PMF estimation model and employ appropriate prior distributions for the model parameters, allowing the rank to be inferred without cross-validation.We then derive a deterministic solution based on variational inference (VI) to approximate the posterior distributions of various model parameters. Numerical experiments involving both synthetic data and real classification and item recommendation data illustrate the advantages of our VI-based method in terms of estimation accuracy, automatic rank detection, and computational efficiency.",
    "published": "2024-10-08T20:07:49Z",
    "updated": "2026-02-02T11:56:58Z",
    "link": "http://arxiv.org/pdf/2410.06329v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Joseph K. Chege",
      "Arie Yeredor",
      "Martin Haardt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02754v3",
    "title": "Safely Learning Controlled Stochastic Dynamics",
    "summary": "We address the problem of safely learning controlled stochastic dynamics from discrete-time trajectory observations, ensuring system trajectories remain within predefined safe regions during both training and deployment. Safety-critical constraints of this kind are crucial in applications such as autonomous robotics, finance, and biomedicine. We introduce a method that ensures safe exploration and efficient estimation of system dynamics by iteratively expanding an initial known safe control set using kernel-based confidence bounds. After training, the learned model enables predictions of the system's dynamics and permits safety verification of any given control. Our approach requires only mild smoothness assumptions and access to an initial safe control set, enabling broad applicability to complex real-world systems. We provide theoretical guarantees for safety and derive adaptive learning rates that improve with increasing Sobolev regularity of the true dynamics. Experimental evaluations demonstrate the practical effectiveness of our method in terms of safety, estimation accuracy, and computational efficiency.",
    "published": "2025-06-03T11:17:07Z",
    "updated": "2026-02-02T11:47:36Z",
    "link": "http://arxiv.org/pdf/2506.02754v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Luc Brogat-Motte",
      "Alessandro Rudi",
      "Riccardo Bonalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01988v1",
    "title": "Stochastic Interpolants in Hilbert Spaces",
    "summary": "Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.",
    "published": "2026-02-02T11:44:34Z",
    "updated": "2026-02-02T11:44:34Z",
    "link": "http://arxiv.org/pdf/2602.01988v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "James Boran Yu",
      "RuiKang OuYang",
      "Julien Horwood",
      "José Miguel Hernández-Lobato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01978v1",
    "title": "SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays",
    "summary": "Neuromorphic hardware implementations of Spiking Neural Networks (SNNs) promise energy-efficient, low-latency AI through sparse, event-driven computation. Yet, training SNNs under fine temporal discretization remains a major challenge, hindering both low-latency responsiveness and the mapping of software-trained SNNs to efficient hardware. In current approaches, spiking neurons are modeled as self-recurrent units, embedded into recurrent networks to maintain state over time, and trained with BPTT or RTRL variants based on surrogate gradients. These methods scale poorly with temporal resolution, while online approximations often exhibit instability for long sequences and tend to fail at capturing temporal patterns precisely. To address these limitations, we develop spiking neurons with internal recursive memory structures that we combine with sigma-delta spike-coding. We show that this SpikingGamma model supports direct error backpropagation without surrogate gradients, can learn fine temporal patterns with minimal spiking in an online manner, and scale feedforward SNNs to complex tasks and benchmarks with competitive accuracy, all while being insensitive to the temporal resolution of the model. Our approach offers both an alternative to current recurrent SNNs trained with surrogate gradients, and a direct route for mapping SNNs to neuromorphic hardware.",
    "published": "2026-02-02T11:35:16Z",
    "updated": "2026-02-02T11:35:16Z",
    "link": "http://arxiv.org/pdf/2602.01978v1.pdf",
    "category": [
      "cs.NE",
      "cs.LG"
    ],
    "authors": [
      "Roel Koopman",
      "Sebastian Otte",
      "Sander Bohté"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.13483v5",
    "title": "Robust Amortized Bayesian Inference with Self-Consistency Losses on Unlabeled Data",
    "summary": "Amortized Bayesian inference (ABI) with neural networks can solve probabilistic inverse problems orders of magnitude faster than classical methods. However, ABI is not yet sufficiently robust for widespread and safe application. When performing inference on observations outside the scope of the simulated training data, posterior approximations are likely to become highly biased, which cannot be corrected by additional simulations due to the bad pre-asymptotic behavior of current neural posterior estimators. In this paper, we propose a semi-supervised approach that enables training not only on labeled simulated data generated from the model, but also on \\textit{unlabeled} data originating from any source, including real data. To achieve this, we leverage Bayesian self-consistency properties that can be transformed into strictly proper losses that do not require knowledge of ground-truth parameters. We test our approach on several real-world case studies, including applications to high-dimensional time-series and image data. Our results show that semi-supervised learning with unlabeled data drastically improves the robustness of ABI in the out-of-simulation regime. Notably, inference remains accurate even when evaluated on observations far away from the labeled and unlabeled data seen during training.",
    "published": "2025-01-23T08:57:02Z",
    "updated": "2026-02-02T11:22:38Z",
    "link": "http://arxiv.org/pdf/2501.13483v5.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Aayush Mishra",
      "Daniel Habermann",
      "Marvin Schmitt",
      "Stefan T. Radev",
      "Paul-Christian Bürkner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01966v1",
    "title": "Self-Consolidation for Self-Evolving Agents",
    "summary": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.",
    "published": "2026-02-02T11:16:07Z",
    "updated": "2026-02-02T11:16:07Z",
    "link": "http://arxiv.org/pdf/2602.01966v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hongzhuo Yu",
      "Fei Zhu",
      "Guo-Sen Xie",
      "Ling Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01960v1",
    "title": "Grounding Generated Videos in Feasible Plans via World Models",
    "summary": "Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.",
    "published": "2026-02-02T11:04:47Z",
    "updated": "2026-02-02T11:04:47Z",
    "link": "http://arxiv.org/pdf/2602.01960v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Christos Ziakas",
      "Amir Bar",
      "Alessandra Russo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01953v1",
    "title": "Deep Multivariate Models with Parametric Conditionals",
    "summary": "We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.",
    "published": "2026-02-02T11:01:48Z",
    "updated": "2026-02-02T11:01:48Z",
    "link": "http://arxiv.org/pdf/2602.01953v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Dmitrij Schlesinger",
      "Boris Flach",
      "Alexander Shekhovtsov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01941v1",
    "title": "FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates",
    "summary": "Autoregressive learning of time-stepping operators offers an effective approach to data-driven PDE simulation on grids. For conservation laws, however, long-horizon rollouts are often destabilized when learned updates violate global conservation and, in many applications, additional state bounds such as nonnegative mass and densities or concentrations constrained to [0,1]. Enforcing these coupled constraints via direct next-state regression remains difficult. We introduce a framework for learning conservative transport operators on regular grids, inspired by lattice Boltzmann-style discrete-velocity transport representations. Instead of predicting the next state, the model outputs local transport operators that update cells through neighborhood exchanges, guaranteeing discrete conservation by construction. For bounded quantities, we parameterize transport within a capacity-constrained feasible set, enforcing bounds structurally rather than by post-hoc clipping. We validate FluxNet on 1D convection-diffusion, 2D shallow water equations, 1D traffic flow, and 2D spinodal decomposition. Experiments on shallow-water equations and traffic flow show improved rollout stability and physical consistency over strong baselines. On phase-field spinodal decomposition, the method enables large time-steps with long-range transport, accelerating simulation while preserving microstructure evolution in both pointwise and statistical measures.",
    "published": "2026-02-02T10:44:10Z",
    "updated": "2026-02-02T10:44:10Z",
    "link": "http://arxiv.org/pdf/2602.01941v1.pdf",
    "category": [
      "cond-mat.mtrl-sci",
      "cs.CE",
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Zishuo Lan",
      "Junjie Li",
      "Lei Wang",
      "Jincheng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07976v3",
    "title": "EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations",
    "summary": "Multiscale problems are ubiquitous in physics. Numerical simulations of such problems by solving partial differential equations (PDEs) at high resolution are computationally too expensive for many-query scenarios, such as uncertainty quantification, remeshing applications, and topology optimization. This limitation has motivated the development of data-driven surrogate models, where microscale computations are substituted by black-box mappings between macroscale quantities. While these approaches offer significant speedups, they typically struggle to incorporate microscale physical constraints, such as the balance of linear momentum. In this contribution, we propose the Equilibrium Neural Operator (EquiNO), a physics-informed PDE surrogate in which equilibrium is hard-enforced by construction. EquiNO achieves this by projecting the solution onto a set of divergence-free basis functions obtained via proper orthogonal decomposition (POD), thereby ensuring satisfaction of equilibrium without relying on penalty terms or multi-objective loss functions. We compare EquiNO with variational physics-informed neural and operator networks that enforce physical constraints only weakly through the loss function, as well as with purely data-driven operator-learning baselines. Our framework, applicable to multiscale FE$^{\\,2}$ computations, introduces a finite element-operator learning (FE-OL) approach that integrates the finite element (FE) method with operator learning (OL). We apply the proposed methodology to quasi-static problems in solid mechanics and demonstrate that FE-OL yields accurate solutions even when trained on restricted datasets. The results show that EquiNO achieves speedup factors exceeding 8000-fold compared to traditional methods and offers a robust and physically consistent alternative to existing data-driven surrogate models.",
    "published": "2025-03-27T08:42:13Z",
    "updated": "2026-02-02T10:42:32Z",
    "link": "http://arxiv.org/pdf/2504.07976v3.pdf",
    "category": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "authors": [
      "Hamidreza Eivazi",
      "Jendrik-Alexander Tröger",
      "Stefan Wittek",
      "Stefan Hartmann",
      "Andreas Rausch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01928v1",
    "title": "Privacy Amplification by Missing Data",
    "summary": "Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.",
    "published": "2026-02-02T10:28:41Z",
    "updated": "2026-02-02T10:28:41Z",
    "link": "http://arxiv.org/pdf/2602.01928v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Simon Roburin",
      "Rafaël Pinot",
      "Erwan Scornet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01924v1",
    "title": "Bayesian Integration of Nonlinear Incomplete Clinical Data",
    "summary": "Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.",
    "published": "2026-02-02T10:23:53Z",
    "updated": "2026-02-02T10:23:53Z",
    "link": "http://arxiv.org/pdf/2602.01924v1.pdf",
    "category": [
      "cs.LG",
      "cs.CY"
    ],
    "authors": [
      "Lucía González-Zamorano",
      "Nuria Balbás-Esteban",
      "Vanessa Gómez-Verdejo",
      "Albert Belenguer-Llorens",
      "Carlos Sevilla-Salcedo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01922v1",
    "title": "Embedding Learning on Multiplex Networks for Link Prediction",
    "summary": "Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.",
    "published": "2026-02-02T10:23:10Z",
    "updated": "2026-02-02T10:23:10Z",
    "link": "http://arxiv.org/pdf/2602.01922v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Orell Trautmann",
      "Olaf Wolkenhauer",
      "Clémence Réda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01914v1",
    "title": "Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs",
    "summary": "Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.",
    "published": "2026-02-02T10:19:52Z",
    "updated": "2026-02-02T10:19:52Z",
    "link": "http://arxiv.org/pdf/2602.01914v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Wenbo Pan",
      "Zhichao Liu",
      "Xianlong Wang",
      "Haining Yu",
      "Xiaohua Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01909v1",
    "title": "Propagating the prior from far to near offset: A self-supervised diffusion framework for progressively recovering near-offsets of towed-streamer data",
    "summary": "In marine towed-streamer seismic acquisition, the nearest hydrophone is often two hundred meter away from the source resulting in missing near-offset traces, which degrades critical processing workflows such as surface-related multiple elimination, velocity analysis, and full-waveform inversion. Existing reconstruction methods, like transform-domain interpolation, often produce kinematic inconsistencies and amplitude distortions, while supervised deep learning approaches require complete ground-truth near-offset data that are unavailable in realistic acquisition scenarios. To address these limitations, we propose a self-supervised diffusion-based framework that reconstructs missing near-offset traces without requiring near-offset reference data. Our method leverages overlapping patch extraction with single-trace shifts from the available far-offset section to train a conditional diffusion model, which learns offset-dependent statistical patterns governing event curvature, amplitude variation, and wavelet characteristics. At inference, we perform trace-by-trace recursive extrapolation from the nearest recorded offset toward zero offset, progressively propagating learned prior information from far to near offsets. The generative formulation further provides uncertainty estimates via ensemble sampling, quantifying prediction confidence where validation data are absent. Controlled validation experiments on synthetic and field datasets show substantial performance gains over conventional parabolic Radon transform baselines. Operational deployment on actual near-offset gaps demonstrates practical viability where ground-truth validation is impossible. Notably, the reconstructed waveforms preserve realistic amplitude-versus-offset trends despite training exclusively on far-offset observations, and uncertainty maps accurately identify challenging extrapolation regions.",
    "published": "2026-02-02T10:13:18Z",
    "updated": "2026-02-02T10:13:18Z",
    "link": "http://arxiv.org/pdf/2602.01909v1.pdf",
    "category": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "authors": [
      "Shijun Cheng",
      "Tariq Alkhalifah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01582v3",
    "title": "Bayes optimal learning of attention-indexed models",
    "summary": "We introduce the attention-indexed model (AIM), a theoretical framework for analyzing learning in deep attention layers. Inspired by multi-index models, AIM captures how token-level outputs emerge from layered bilinear interactions over high-dimensional embeddings. Unlike prior tractable attention models, AIM allows full-width key and query matrices, aligning more closely with practical transformers. Using tools from statistical mechanics and random matrix theory, we derive closed-form predictions for Bayes-optimal generalization error and identify sharp phase transitions as a function of sample complexity, model width, and sequence length. We propose a matching approximate message passing algorithm and show that gradient descent can reach optimal performance. AIM offers a solvable playground for understanding learning in self-attention layers, that are key components of modern architectures.",
    "published": "2025-06-02T12:11:26Z",
    "updated": "2026-02-02T10:09:54Z",
    "link": "http://arxiv.org/pdf/2506.01582v3.pdf",
    "category": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.IT",
      "stat.ML"
    ],
    "authors": [
      "Fabrizio Boncoraglio",
      "Emanuele Troiani",
      "Vittorio Erba",
      "Lenka Zdeborová"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01903v1",
    "title": "Data- and Variance-dependent Regret Bounds for Online Tabular MDPs",
    "summary": "This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.",
    "published": "2026-02-02T10:09:29Z",
    "updated": "2026-02-02T10:09:29Z",
    "link": "http://arxiv.org/pdf/2602.01903v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Mingyi Li",
      "Taira Tsuchiya",
      "Kenji Yamanishi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01898v1",
    "title": "Observation-dependent Bayesian active learning via input-warped Gaussian processes",
    "summary": "Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.",
    "published": "2026-02-02T10:05:56Z",
    "updated": "2026-02-02T10:05:56Z",
    "link": "http://arxiv.org/pdf/2602.01898v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Sanna Jarl",
      "Maria Bånkestad",
      "Jonathan J. S. Scragg",
      "Jens Sjölund"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01897v1",
    "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
    "summary": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \\emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \\emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \\emph{Code is available at} \\texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.",
    "published": "2026-02-02T10:05:54Z",
    "updated": "2026-02-02T10:05:54Z",
    "link": "http://arxiv.org/pdf/2602.01897v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sungheon Jeong",
      "Sanggeon Yun",
      "Ryozo Masukawa",
      "Wenjun Haung",
      "Hanning Chen",
      "Mohsen Imani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.13526v3",
    "title": "MOMA: Masked Orthogonal Matrix Alignment for Zero-Additional-Parameter Model Merging",
    "summary": "Model merging offers a scalable alternative to multi-task learning but often yields suboptimal performance on classification tasks. We attribute this degradation to a geometric misalignment between the merged encoder and static task-specific classifier heads. Existing methods typically rely on auxiliary parameters to enforce strict representation alignment. We challenge this approach by revealing that the misalignment is predominantly an orthogonal transformation, rendering such strict alignment unnecessary. Leveraging this insight, we propose MOMA (Masked Orthogonal Matrix Alignment), which rectifies the misalignment by jointly optimizing a global multi-task vector mask and task-specific orthogonal transformations. Crucially, MOMA absorbs corresponding new parameters directly into the existing model weights, achieving performance comparable to state-of-the-art baselines with zero additional parameters and zero added inference cost.",
    "published": "2024-12-18T05:53:15Z",
    "updated": "2026-02-02T10:03:31Z",
    "link": "http://arxiv.org/pdf/2412.13526v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Fanshuang Kong",
      "Richong Zhang",
      "Zhijie Nie",
      "Hang Zhou",
      "Ziqiao Wang",
      "Qiang Sun",
      "Chunming Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06178v2",
    "title": "Reusing Trajectories in Policy Gradients Enables Fast Convergence",
    "summary": "Policy gradient (PG) methods are a class of effective reinforcement learning algorithms, particularly when dealing with continuous control problems. They rely on fresh on-policy data, making them sample-inefficient and requiring $O(ε^{-2})$ trajectories to reach an $ε$-approximate stationary point. A common strategy to improve efficiency is to reuse information from past iterations, such as previous gradients or trajectories, leading to off-policy PG methods. While gradient reuse has received substantial attention, leading to improved rates up to $O(ε^{-3/2})$, the reuse of past trajectories, although intuitive, remains largely unexplored from a theoretical perspective. In this work, we provide the first rigorous theoretical evidence that reusing past off-policy trajectories can significantly accelerate PG convergence. We propose RT-PG (Reusing Trajectories - Policy Gradient), a novel algorithm that leverages a power mean-corrected multiple importance weighting estimator to effectively combine on-policy and off-policy data coming from the most recent $ω$ iterations. Through a novel analysis, we prove that RT-PG achieves a sample complexity of $\\widetilde{O}(ε^{-2}ω^{-1})$. When reusing all available past trajectories, this leads to a rate of $\\widetilde{O}(ε^{-1})$, the best known one in the literature for PG methods. We further validate our approach empirically, demonstrating its effectiveness against baselines with state-of-the-art rates.",
    "published": "2025-06-06T15:42:15Z",
    "updated": "2026-02-02T10:02:14Z",
    "link": "http://arxiv.org/pdf/2506.06178v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Alessandro Montenegro",
      "Federico Mansutti",
      "Marco Mussi",
      "Matteo Papini",
      "Alberto Maria Metelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24161v3",
    "title": "Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control",
    "summary": "Spiking Neural Networks (SNNs) offer low-latency and energy-efficient decision making on neuromorphic hardware, making them attractive for Reinforcement Learning (RL) in resource-constrained edge devices. However, most RL algorithms for continuous control are designed for Artificial Neural Networks (ANNs), particularly the target network soft update mechanism, which conflicts with the discrete and non-differentiable dynamics of spiking neurons. We show that this mismatch destabilizes SNN training and degrades performance. To bridge the gap between discrete SNNs and continuous-control algorithms, we propose a novel proxy target framework. The proxy network introduces continuous and differentiable dynamics that enable smooth target updates, stabilizing the learning process. Since the proxy operates only during training, the deployed SNN remains fully energy-efficient with no additional inference overhead. Extensive experiments on continuous control benchmarks demonstrate that our framework consistently improves stability and achieves up to $32\\%$ higher performance across various spiking neuron models. Notably, to the best of our knowledge, this is the first approach that enables SNNs with simple Leaky Integrate and Fire (LIF) neurons to surpass their ANN counterparts in continuous control. This work highlights the importance of SNN-tailored RL algorithms and paves the way for neuromorphic agents that combine high performance with low power consumption. Code is available at https://github.com/xuzijie32/Proxy-Target.",
    "published": "2025-05-30T03:08:03Z",
    "updated": "2026-02-02T09:52:11Z",
    "link": "http://arxiv.org/pdf/2505.24161v3.pdf",
    "category": [
      "cs.NE",
      "cs.LG"
    ],
    "authors": [
      "Zijie Xu",
      "Tong Bu",
      "Zecheng Hao",
      "Jianhao Ding",
      "Zhaofei Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01877v1",
    "title": "Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal",
    "summary": "Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.",
    "published": "2026-02-02T09:49:51Z",
    "updated": "2026-02-02T09:49:51Z",
    "link": "http://arxiv.org/pdf/2602.01877v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Zichun Wang",
      "Gar Goei Loke",
      "Ruiting Zuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01872v1",
    "title": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training",
    "summary": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We prove the corrected estimator is asymptotically unbiased under standard support and boundedness assumptions, and we derive a batch-level variant for compatibility with common deep-learning packages that minimizes mean-squared deviation from the ideal node-level correction. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4 times faster on average (up to 13 times) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.",
    "published": "2026-02-02T09:44:12Z",
    "updated": "2026-02-02T09:44:12Z",
    "link": "http://arxiv.org/pdf/2602.01872v1.pdf",
    "category": [
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Chongyang Xu",
      "Christoph Siebenbrunner",
      "Laurent Bindschaedler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01863v1",
    "title": "Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality",
    "summary": "Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \\sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.",
    "published": "2026-02-02T09:34:17Z",
    "updated": "2026-02-02T09:34:17Z",
    "link": "http://arxiv.org/pdf/2602.01863v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Ryotaro Kawata",
      "Taiji Suzuki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01861v1",
    "title": "RIR-Former: Coordinate-Guided Transformer for Continuous Reconstruction of Room Impulse Responses",
    "summary": "Room impulse responses (RIRs) are essential for many acoustic signal processing tasks, yet measuring them densely across space is often impractical. In this work, we propose RIR-Former, a grid-free, one-step feed-forward model for RIR reconstruction. By introducing a sinusoidal encoding module into a transformer backbone, our method effectively incorporates microphone position information, enabling interpolation at arbitrary array locations. Furthermore, a segmented multi-branch decoder is designed to separately handle early reflections and late reverberation, improving reconstruction across the entire RIR. Experiments on diverse simulated acoustic environments demonstrate that RIR-Former consistently outperforms state-of-the-art baselines in terms of normalized mean square error (NMSE) and cosine distance (CD), under varying missing rates and array configurations. These results highlight the potential of our approach for practical deployment and motivate future work on scaling from randomly spaced linear arrays to complex array geometries, dynamic acoustic scenes, and real-world environments.",
    "published": "2026-02-02T09:33:54Z",
    "updated": "2026-02-02T09:33:54Z",
    "link": "http://arxiv.org/pdf/2602.01861v1.pdf",
    "category": [
      "eess.AS",
      "cs.LG"
    ],
    "authors": [
      "Shaoheng Xu",
      "Chunyi Sun",
      " Jihui",
      " Zhang",
      "Prasanga N. Samarasinghe",
      "Thushara D. Abhayapala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22944v2",
    "title": "Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization",
    "summary": "Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.",
    "published": "2026-01-30T13:03:04Z",
    "updated": "2026-02-02T09:28:56Z",
    "link": "http://arxiv.org/pdf/2601.22944v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuanchao Wang",
      "Zhao-Rong Lai",
      "Tianqi Zhong",
      "Fengnan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01853v1",
    "title": "Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning",
    "summary": "A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.",
    "published": "2026-02-02T09:27:51Z",
    "updated": "2026-02-02T09:27:51Z",
    "link": "http://arxiv.org/pdf/2602.01853v1.pdf",
    "category": [
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "authors": [
      "Xiangkun Wu",
      "Qianglin Wen",
      "Yingying Zhang",
      "Hongtu Zhu",
      "Ting Li",
      "Chengchun Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01852v1",
    "title": "FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization",
    "summary": "Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.",
    "published": "2026-02-02T09:25:33Z",
    "updated": "2026-02-02T09:25:33Z",
    "link": "http://arxiv.org/pdf/2602.01852v1.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Zeyan Wang",
      "Zhengmao Liu",
      "Yongxin Cai",
      "Chi Li",
      "Xiaoying Tang",
      "Jingchao Chen",
      "Zibin Pan",
      "Jing Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01849v1",
    "title": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models",
    "summary": "This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.",
    "published": "2026-02-02T09:21:45Z",
    "updated": "2026-02-02T09:21:45Z",
    "link": "http://arxiv.org/pdf/2602.01849v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ziwei Luo",
      "Ziqi Jin",
      "Lei Wang",
      "Lidong Bing",
      "Thomas B. Schön"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.13144v2",
    "title": "Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching",
    "summary": "Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.",
    "published": "2025-11-17T08:55:22Z",
    "updated": "2026-02-02T09:20:14Z",
    "link": "http://arxiv.org/pdf/2511.13144v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jiacheng Cheng",
      "Xu Zhang",
      "Guanghui Qiu",
      "Yifang Zhang",
      "Yinchuan Li",
      "Kaiyuan Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01845v1",
    "title": "No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation",
    "summary": "Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \\textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference",
    "published": "2026-02-02T09:17:09Z",
    "updated": "2026-02-02T09:17:09Z",
    "link": "http://arxiv.org/pdf/2602.01845v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.QM"
    ],
    "authors": [
      "Furkan Eris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01842v1",
    "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
    "summary": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
    "published": "2026-02-02T09:14:51Z",
    "updated": "2026-02-02T09:14:51Z",
    "link": "http://arxiv.org/pdf/2602.01842v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jinbin Bai",
      "Yixuan Li",
      "Yuchen Zhu",
      "Yi Xin",
      "Qingyu Shi",
      "Aosong Feng",
      "Xiaohong Liu",
      "Molei Tao",
      "Jianru Xue",
      "Xiangtai Li",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01828v1",
    "title": "Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment",
    "summary": "Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking \"Is the graph hyperbolic?\" to also questioning \"Is the task aligned with hyperbolic geometry?\", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.",
    "published": "2026-02-02T09:01:58Z",
    "updated": "2026-02-02T09:01:58Z",
    "link": "http://arxiv.org/pdf/2602.01828v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Dionisia Naddeo",
      "Jonas Linkerhägner",
      "Nicola Toschi",
      "Geri Skenderi",
      "Veronica Lachi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01825v1",
    "title": "Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes",
    "summary": "We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.",
    "published": "2026-02-02T08:58:55Z",
    "updated": "2026-02-02T08:58:55Z",
    "link": "http://arxiv.org/pdf/2602.01825v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Mingyuan Xu",
      "Zongqi Xia",
      "Tianxi Cai",
      "Doudou Zhou",
      "Nian Si"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26005v2",
    "title": "BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields",
    "summary": "We introduce a formal active learning methodology for guiding the placement of Lagrangian observers to infer time-dependent vector fields -- a key task in oceanography, marine science, and ocean engineering -- using a physics-informed spatio-temporal Gaussian process surrogate model. The majority of existing placement campaigns either follow standard `space-filling' designs or relatively ad-hoc expert opinions. A key challenge to applying principled active learning in this setting is that Lagrangian observers are continuously advected through the vector field, so they make measurements at different locations and times. It is, therefore, important to consider the likely future trajectories of placed observers to account for the utility of candidate placement locations. To this end, we present BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable benefits of BALLAST-aided sequential observer placement strategies on both synthetic and high-fidelity ocean current models. In addition, we developed a novel GP inference method -- the Vanilla SPDE Exchange (VaSE) -- to boost the GP posterior sampling efficiency, which is also of independent interest.",
    "published": "2025-09-30T09:36:57Z",
    "updated": "2026-02-02T08:36:48Z",
    "link": "http://arxiv.org/pdf/2509.26005v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Rui-Yang Zhang",
      "Henry B. Moss",
      "Lachlan Astfalck",
      "Edward Cripps",
      "David S. Leslie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.18881v3",
    "title": "TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis",
    "summary": "A primary challenge in ITE estimation is sample selection bias. Traditional approaches utilize treatment regularization techniques such as the Integral Probability Metrics (IPM), re-weighting, and propensity score modeling to mitigate this bias. However, these regularizations may introduce undesirable information loss and limit the performance of the model. Furthermore, treatment effects vary across different external contexts, and the existing methods are insufficient in fully interacting with and utilizing these contextual features. To address these issues, we propose a Context-Aware uplift model based on the Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In the first stage, we train an uplift model, called CAN-U, which includes the treatment regularizations of IPM and propensity score prediction, to generate a complete dataset with counterfactual uplift labels. In the second stage, we train a model named CAN-D, which utilizes an isotonic output layer to directly model uplift effects, thereby eliminating the reliance on the regularization components. CAN-D adaptively corrects the errors estimated by CAN-U through reinforcing the factual samples, while avoiding the negative impacts associated with the aforementioned regularizations. Additionally, we introduce a Context-Aware Attention Layer throughout the two-stage process to manage the interactions between treatment, merchant, and contextual features, thereby modeling the varying treatment effect in different contexts. We conduct extensive experiments on two real-world datasets to validate the effectiveness of TSCAN. Ultimately, the deployment of our model for real-world merchant diagnosis on one of China's largest online food ordering platforms validates its practical utility and impact.",
    "published": "2025-04-26T10:00:16Z",
    "updated": "2026-02-02T08:36:11Z",
    "link": "http://arxiv.org/pdf/2504.18881v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hangtao Zhang",
      "Zhe Li",
      "Kairui Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08540v2",
    "title": "Biased Local SGD for Efficient Deep Learning on Heterogeneous Systems",
    "summary": "Most parallel neural network training methods assume homogeneous computing resources. For example, synchronous data-parallel SGD suffers from significant synchronization overhead under heterogeneous workloads, often forcing practitioners to rely only on the fastest devices (e.g., GPUs). In this work, we study local SGD for efficient parallel training on heterogeneous systems. We show that intentionally introducing bias in data sampling and model aggregation can effectively harmonize slower CPUs with faster GPUs. Our extensive empirical results demonstrate that a carefully controlled bias significantly accelerates local SGD while achieving comparable or even higher accuracy than synchronous SGD under the same epoch budget. For instance, our method trains ResNet20 on CIFAR-10 with 2 CPUs and 8 GPUs up to 32x faster than synchronous SGD, with nearly identical accuracy. These results provide practical insights into how to flexibly utilize diverse compute resources for deep learning.",
    "published": "2025-08-12T01:03:09Z",
    "updated": "2026-02-02T08:35:54Z",
    "link": "http://arxiv.org/pdf/2508.08540v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jihyun Lim",
      "Junhyuk Jo",
      "Chanhyeok Ko",
      "Young Min Go",
      "Jimin Hwa",
      "Sunwoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.01842v3",
    "title": "shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python",
    "summary": "This paper introduces the shapr R package, a versatile tool for generating Shapley value-based prediction explanations for machine learning and statistical regression models. Moreover, the shaprpy Python library brings the core capabilities of shapr to the Python ecosystem. Shapley values originate from cooperative game theory in the 1950s, but have over the past few years become a widely used method for quantifying how a model's features/covariates contribute to specific prediction outcomes. The shapr package emphasizes conditional Shapley value estimates, providing a comprehensive range of approaches for accurately capturing feature dependencies -- a crucial aspect for correct model explanation, typically lacking in similar software. In addition to regular tabular data, the shapr R package includes specialized functionality for explaining time series forecasts. The package offers a minimal set of user functions with sensible default values for most use cases while providing extensive flexibility for advanced users to fine-tune computations. Additional features include parallelized computations, iterative estimation with convergence detection, and rich visualization tools. shapr also extends its functionality to compute causal and asymmetric Shapley values when causal information is available. Overall, the shapr and shaprpy packages aim to enhance the interpretability of predictive models within a powerful and user-friendly framework.",
    "published": "2025-04-02T15:47:30Z",
    "updated": "2026-02-02T08:31:40Z",
    "link": "http://arxiv.org/pdf/2504.01842v3.pdf",
    "category": [
      "cs.LG",
      "stat.CO"
    ],
    "authors": [
      "Martin Jullum",
      "Lars Henry Berge Olsen",
      "Jon Lachmann",
      "Annabelle Redelmeier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21303v2",
    "title": "Data as a Lever: A Neighbouring Datasets Perspective on Predictive Multiplicity",
    "summary": "Multiplicity, the existence of equally good yet competing models, has received growing attention in recent years. While prior work has emphasized modelling choices, the critical role of data in shaping multiplicity has been largely overlooked. In this work, we first introduce a neighbouring datasets framework, arguing that much of data processing can be reframed as choosing between neighbouring datasets. Under this framework, we find a counterintuitive theoretical relationship: neighbouring datasets with greater inter-class distribution overlap exhibit lower multiplicity.\n  Building on this insight, we apply our framework to two domains: active learning and data imputation. For each, we establish natural extensions of the neighbouring datasets perspective, conduct the first systematic study of multiplicity in existing algorithms, and finally, propose novel multiplicity-aware methods, namely, multiplicity-aware data acquisition strategies for active learning and multiplicity-aware data imputation.",
    "published": "2025-10-24T10:01:40Z",
    "updated": "2026-02-02T08:22:04Z",
    "link": "http://arxiv.org/pdf/2510.21303v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Prakhar Ganesh",
      "Hsiang Hsu",
      "Golnoosh Farnadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01791v1",
    "title": "Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.",
    "published": "2026-02-02T08:13:13Z",
    "updated": "2026-02-02T08:13:13Z",
    "link": "http://arxiv.org/pdf/2602.01791v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zheng Zhang",
      "Ao Lu",
      "Yuanhao Zeng",
      "Ziwei Shan",
      "Jinjin Guo",
      "Lufei Li",
      "Yexin Li",
      "Kan Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12801v2",
    "title": "Transportability without Graphs: A Bayesian Approach to Identifying s-Admissible Backdoor Sets",
    "summary": "Transporting causal information across populations is a critical challenge in clinical decision-making. Causal modeling provides criteria for identifiability and transportability, but these require knowledge of the causal graph, which rarely holds in practice. We propose a Bayesian method that combines observational data from the target domain with experimental data from a different domain to identify s-admissible backdoor sets, which enable unbiased estimation of causal effects across populations, without requiring the causal graph. We prove that if such a set exists, we can always find one within the Markov boundary of the outcome, narrowing the search space, and we establish asymptotic convergence guarantees for our method. We develop a greedy algorithm that reframes transportability as a feature selection problem, selecting conditioning sets that maximize the marginal likelihood of experimental data given observational data. In simulated and semi-synthetic data, our method correctly identifies transportability bias, improves causal effect estimation, and performs favorably against alternatives.",
    "published": "2025-05-19T07:31:56Z",
    "updated": "2026-02-02T08:08:32Z",
    "link": "http://arxiv.org/pdf/2505.12801v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Konstantina Lelova",
      "Gregory F. Cooper",
      "Sofia Triantafillou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01776v1",
    "title": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting",
    "summary": "Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.",
    "published": "2026-02-02T08:01:11Z",
    "updated": "2026-02-02T08:01:11Z",
    "link": "http://arxiv.org/pdf/2602.01776v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mingyue Cheng",
      "Xiaoyu Tao",
      "Qi Liu",
      "Ze Guo",
      "Enhong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01774v1",
    "title": "Cost-Aware Bayesian Optimization for Prototyping Interactive Devices",
    "summary": "Deciding which idea is worth prototyping is a central concern in iterative design. A prototype should be produced when the expected improvement is high and the cost is low. However, this is hard to decide, because costs can vary drastically: a simple parameter tweak may take seconds, while fabricating hardware consumes material and energy. Such asymmetries, can discourage a designer from exploring the design space. In this paper, we present an extension of cost-aware Bayesian optimization to account for diverse prototyping costs. The method builds on the power of Bayesian optimization and requires only a minimal modification to the acquisition function. The key idea is to use designer-estimated costs to guide sampling toward more cost-effective prototypes. In technical evaluations, the method achieved comparable utility to a cost-agnostic baseline while requiring only ${\\approx}70\\%$ of the cost; under strict budgets, it outperformed the baseline threefold. A within-subjects study with 12 participants in a realistic joystick design task demonstrated similar benefits. These results show that accounting for prototyping costs can make Bayesian optimization more compatible with real-world design projects.",
    "published": "2026-02-02T07:59:50Z",
    "updated": "2026-02-02T07:59:50Z",
    "link": "http://arxiv.org/pdf/2602.01774v1.pdf",
    "category": [
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Thomas Langerak",
      "Renate Zhang",
      "Ziyuan Wang",
      "Per Ola Kristensson",
      "Antti Oulasvirta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05064v2",
    "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric",
    "summary": "Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. Existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when the forget and retain sets have semantically similar content and/or retraining the model from scratch on the retain set is impractical. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking to overcome these limitations. We introduce new benchmark datasets (with different levels of data similarity) for LLM unlearning that can be used to rigorously evaluate unlearning algorithms via WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.",
    "published": "2025-05-08T08:56:46Z",
    "updated": "2026-02-02T07:45:25Z",
    "link": "http://arxiv.org/pdf/2505.05064v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xinyang Lu",
      "Xinyuan Niu",
      "Gregory Kang Ruey Lau",
      "Bui Thi Cam Nhung",
      "Rachael Hwee Ling Sim",
      "John Russell Himawan",
      "Fanyu Wen",
      "Chuan-Sheng Foo",
      "See-Kiong Ng",
      "Bryan Kian Hsiang Low"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01751v1",
    "title": "MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network",
    "summary": "Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.",
    "published": "2026-02-02T07:35:08Z",
    "updated": "2026-02-02T07:35:08Z",
    "link": "http://arxiv.org/pdf/2602.01751v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.QM"
    ],
    "authors": [
      "Kunyi Fan",
      "Mengjie Chen",
      "Longlong Li",
      "Cunquan Qu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15552v4",
    "title": "The Multi-Query Paradox in Zeroth-Order Optimization",
    "summary": "Zeroth-order (ZO) optimization provides a powerful framework for problems where explicit gradients are unavailable and have to be approximated using only queries to function value. The prevalent single-query approach is simple, but suffers from high estimation variance, motivating a multi-query paradigm to improve estimation accuracy. This, however, creates a critical trade-off: under a fixed budget of queries (i.e. cost), queries per iteration and the total number of optimization iterations are inversely proportional to one another. How to best allocate this budget is a fundamental, under-explored question.\n  This work systematically resolves this query allocation problem. We analyze two aggregation methods: the de facto simple averaging (ZO-Avg), and a new Projection Alignment method (ZO-Align) we derive from local surrogate minimization. By deriving convergence rates for both methods that make the dependence on the number of queries explicit across strongly convex, convex, non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg, we prove that using more than one query per iteration is always query-inefficient, rendering the single-query approach optimal. On the contrary, ZO-Align generally performs better with more queries per iteration, resulting in a full-subspace estimation as the optimal approach. Thus, our work clarifies that the multi-query problem boils down to a choice not about an intermediate query size, but between two classic algorithms, a choice dictated entirely by the aggregation method used. These theoretical findings are also consistently validated by extensive experiments.",
    "published": "2025-09-19T03:10:45Z",
    "updated": "2026-02-02T07:28:48Z",
    "link": "http://arxiv.org/pdf/2509.15552v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Wei Lin",
      "Qingyu Song",
      "Hong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01737v1",
    "title": "Physics-Informed Chebyshev Polynomial Neural Operator for Parametric Partial Differential Equations",
    "summary": "Neural operators have emerged as powerful deep learning frameworks for approximating solution operators of parameterized partial differential equations (PDE). However, current methods predominantly rely on multilayer perceptrons (MLPs) for mapping inputs to solutions, which impairs training robustness in physics-informed settings due to inherent spectral biases and fixed activation functions. To overcome the architectural limitations, we introduce the Physics-Informed Chebyshev Polynomial Neural Operator (CPNO), a novel mesh-free framework that leverages a basis transformation to replace unstable monomial expansions with the numerically stable Chebyshev spectral basis. By integrating parameter dependent modulation mechanism to main net, CPNO constructs PDE solutions in a near-optimal functional space, decoupling the model from MLP-specific constraints and enhancing multi-scale representation. Theoretical analysis demonstrates the Chebyshev basis's near-minimax uniform approximation properties and superior conditioning, with Lebesgue constants growing logarithmically with degree, thereby mitigating spectral bias and ensuring stable gradient flow during optimization. Numerical experiments on benchmark parameterized PDEs show that CPNO achieves superior accuracy, faster convergence, and enhanced robustness to hyperparameters. The experiment of transonic airfoil flow has demonstrated the capability of CPNO in characterizing complex geometric problems.",
    "published": "2026-02-02T07:19:56Z",
    "updated": "2026-02-02T07:19:56Z",
    "link": "http://arxiv.org/pdf/2602.01737v1.pdf",
    "category": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "authors": [
      "Biao Chen",
      "Jing Wang",
      "Hairun Xie",
      "Qineng Wang",
      "Shuai Zhang",
      "Yifan Xia",
      "Jifa Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01736v1",
    "title": "Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting",
    "summary": "Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.",
    "published": "2026-02-02T07:19:16Z",
    "updated": "2026-02-02T07:19:16Z",
    "link": "http://arxiv.org/pdf/2602.01736v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Qinwei Ma",
      "Jingzhe Shi",
      "Jiahao Qiu",
      "Zaiwen Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01734v1",
    "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
    "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
    "published": "2026-02-02T07:18:45Z",
    "updated": "2026-02-02T07:18:45Z",
    "link": "http://arxiv.org/pdf/2602.01734v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Lianhai Ren",
      "Yucheng Ding",
      "Xiao Liu",
      "Qianxiao Li",
      "Peng Cheng",
      "Yeyun Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01733v1",
    "title": "ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation",
    "summary": "Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\\% to 1.12\\% on common benchmarks.",
    "published": "2026-02-02T07:18:35Z",
    "updated": "2026-02-02T07:18:35Z",
    "link": "http://arxiv.org/pdf/2602.01733v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Junxian Liu",
      "Hao Zeng",
      "Hongxin Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.18220v2",
    "title": "Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective",
    "summary": "The sparse identification of nonlinear dynamics (SINDy) approach can discover the governing equations of dynamical systems based on measurement data, where the dynamical model is identified as the sparse linear combination of the given basis functions. A major challenge in SINDy is the design of a library, which is a set of candidate basis functions, as the appropriate library is not trivial for many dynamical systems. To overcome this difficulty, this study proposes SINDy with library optimization mechanism (SINDy-LOM), which is a combination of the sparse regression technique and the novel learning strategy of the library. In the proposed approach, the basis functions are parametrized. The SINDy-LOM approach involves a two-layer optimization architecture: the inner-layer, in which the data-driven model is extracted as the sparse linear combination of the candidate basis functions, and the outer-layer, in which the basis functions are optimized from the viewpoint of the recursive long-term (RLT) prediction accuracy; thus, the library design is reformulated as the optimization of the parametrized basis functions. The dynamical model obtained by SINDy-LOM has good interpretability and usability, as this approach yields a parsimonious closed-form model. The library optimization mechanism significantly reduces user burden. The RLT perspective improves the reliability of the resulting model compared with the traditional SINDy approach that can only ensure the one-step-ahead prediction accuracy. The effectiveness of the proposed approach is verified through numerical experiments.",
    "published": "2025-07-24T09:15:26Z",
    "updated": "2026-02-02T07:15:21Z",
    "link": "http://arxiv.org/pdf/2507.18220v2.pdf",
    "category": [
      "cs.LG",
      "math.DS"
    ],
    "authors": [
      "Ansei Yonezawa",
      "Heisei Yonezawa",
      "Shuichi Yahagi",
      "Itsuro Kajiwara",
      "Shinya Kijimoto",
      "Hikaru Taniuchi",
      "Kentaro Murakami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.12094v2",
    "title": "Is This Predictor More Informative than Another? A Decision-Theoretical Comparison",
    "summary": "In many real-world applications, a model provider provides probabilistic forecasts to downstream decision-makers who use them to make decisions under diverse payoff objectives. The provider may have access to multiple predictive models, each potentially miscalibrated, and must choose which model to deploy in order to maximize the usefulness of predictions for downstream decisions. A central challenge arises: how can the provider meaningfully compare two predictors when neither is guaranteed to be well-calibrated, and when the relevant decision tasks may differ across users and contexts?\n  To answer this, our first contribution introduces the notion of the informativeness gap between any two predictors, defined as the maximum normalized payoff advantage one predictor offers over the other across all decision-making tasks. Our framework strictly generalizes several existing notions: it subsumes U-Calibration and Calibration Decision Loss, which compare a miscalibrated predictor to its calibrated counterpart, and it recovers Blackwell informativeness as a special case when both predictors are perfectly calibrated. Our second contribution is a dual characterization of the informativeness gap, which gives rise to a natural informativeness measure that can be viewed as a relaxed variant of the earth mover's distance between two prediction distributions. We show that this measure satisfies natural desiderata: it is complete and sound, and it can be estimated sample-efficiently in the prediction-only access setting. We complement our theory with experiments on LLM-based forecasters in real-world prediction tasks, showing that the informativeness gap offers a more decision-relevant alternative to traditional metrics, and provides a principled lens for evaluating how ad hoc calibration post-processing affects downstream decision usefulness.",
    "published": "2025-07-16T10:01:22Z",
    "updated": "2026-02-02T07:13:44Z",
    "link": "http://arxiv.org/pdf/2507.12094v2.pdf",
    "category": [
      "cs.LG",
      "cs.GT"
    ],
    "authors": [
      "Yiding Feng",
      "Liuhan Qian",
      "Wei Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22285v2",
    "title": "Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success",
    "summary": "Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific \"fingerprints\". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.",
    "published": "2026-01-29T20:00:26Z",
    "updated": "2026-02-02T07:07:31Z",
    "link": "http://arxiv.org/pdf/2601.22285v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Luca Zhou",
      "Bo Zhao",
      "Rose Yu",
      "Emanuele Rodolà"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01726v1",
    "title": "Cross-Domain Fake News Detection on Unseen Domains via LLM-Based Domain-Aware User Modeling",
    "summary": "Cross-domain fake news detection (CD-FND) transfers knowledge from a source domain to a target domain and is crucial for real-world fake news mitigation. This task becomes particularly important yet more challenging when the target domain is previously unseen (e.g., the COVID-19 outbreak or the Russia-Ukraine war). However, existing CD-FND methods overlook such scenarios and consequently suffer from the following two key limitations: (1) insufficient modeling of high-level semantics in news and user engagements; and (2) scarcity of labeled data in unseen domains. Targeting these limitations, we find that large language models (LLMs) offer strong potential for CD-FND on unseen domains, yet their effective use remains non-trivial. Nevertheless, two key challenges arise: (1) how to capture high-level semantics from both news content and user engagements using LLMs; and (2) how to make LLM-generated features more reliable and transferable for CD-FND on unseen domains. To tackle these challenges, we propose DAUD, a novel LLM-Based Domain-Aware framework for fake news detection on Unseen Domains. DAUD employs LLMs to extract high-level semantics from news content. It models users' single- and cross-domain engagements to generate domain-aware behavioral representations. In addition, DAUD captures the relations between original data-driven features and LLM-derived features of news, users, and user engagements. This allows it to extract more reliable domain-shared representations that improve knowledge transfer to unseen domains. Extensive experiments on real-world datasets demonstrate that DAUD outperforms state-of-the-art baselines in both general and unseen-domain CD-FND settings.",
    "published": "2026-02-02T07:04:13Z",
    "updated": "2026-02-02T07:04:13Z",
    "link": "http://arxiv.org/pdf/2602.01726v1.pdf",
    "category": [
      "cs.SI",
      "cs.LG"
    ],
    "authors": [
      "Xuankai Yang",
      "Yan Wang",
      "Jiajie Zhu",
      "Pengfei Ding",
      "Hongyang Liu",
      "Xiuzhen Zhang",
      "Huan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.04873v2",
    "title": "FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions",
    "summary": "Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships.\n  A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps.\n  Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures.",
    "published": "2026-01-08T12:18:41Z",
    "updated": "2026-02-02T06:58:54Z",
    "link": "http://arxiv.org/pdf/2601.04873v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Elisa Roldan",
      "Kirstie Andrews",
      "Stephen M. Richardson",
      "Reyhaneh Fatahian",
      "Glen Cooper",
      "Rasool Erfani",
      "Tasneem Sabir",
      "Neil D. Reeves"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24779v3",
    "title": "Are Your Generated Instances Truly Useful? GenBench-MILP: A Benchmark Suite for MILP Instance Generation",
    "summary": "The proliferation of machine learning-based methods for Mixed-Integer Linear Programming (MILP) instance generation has surged, driven by the need for diverse training datasets. However, a critical question remains: Are these generated instances truly useful and realistic? Current evaluation protocols often rely on superficial structural metrics or simple solvability checks, which frequently fail to capture the true computational complexity of real-world problems. To bridge this gap, we introduce GenBench-MILP, a comprehensive benchmark suite designed for the standardized and objective evaluation of MILP generators. Our framework assesses instance quality across four key dimensions: mathematical validity, structural similarity, computational hardness, and utility in downstream tasks. A distinctive innovation of GenBench-MILP is the analysis of solver-internal features -- including root node gaps, heuristic success rates, and cut plane usage. By treating the solver's dynamic behavior as an expert assessment, we reveal nuanced computational discrepancies that static graph features miss. Our experiments on instance generative models demonstrate that instances with high structural similarity scores can still exhibit drastically divergent solver interactions and difficulty levels. By providing this multifaceted evaluation toolkit, GenBench-MILP aims to facilitate rigorous comparisons and guide the development of high-fidelity instance generators.",
    "published": "2025-05-30T16:42:15Z",
    "updated": "2026-02-02T06:56:44Z",
    "link": "http://arxiv.org/pdf/2505.24779v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yidong Luo",
      "Chenguang Wang",
      "Dong Li",
      "Tianshu Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01718v1",
    "title": "Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift",
    "summary": "Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.",
    "published": "2026-02-02T06:56:33Z",
    "updated": "2026-02-02T06:56:33Z",
    "link": "http://arxiv.org/pdf/2602.01718v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sora Nakai",
      "Youssef Fadhloun",
      "Kacem Mathlouthi",
      "Kotaro Yoshida",
      "Ganesh Talluri",
      "Ioannis Mitliagkas",
      "Hiroki Naganuma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.06674v6",
    "title": "Impact of Dataset Properties on Membership Inference Vulnerability of Deep Transfer Learning",
    "summary": "Membership inference attacks (MIAs) are used to test practical privacy of machine learning models. MIAs complement formal guarantees from differential privacy (DP) under a more realistic adversary model. We analyse MIA vulnerability of fine-tuned neural networks both empirically and theoretically, the latter using a simplified model of fine-tuning. We show that the vulnerability of non-DP models when measured as the attacker advantage at a fixed false positive rate reduces according to a simple power law as the number of examples per class increases. A similar power-law applies even for the most vulnerable points, but the dataset size needed for adequate protection of the most vulnerable points is very large.",
    "published": "2024-02-07T14:23:01Z",
    "updated": "2026-02-02T06:09:55Z",
    "link": "http://arxiv.org/pdf/2402.06674v6.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Marlon Tobaben",
      "Hibiki Ito",
      "Joonas Jälkö",
      "Yuan He",
      "Antti Honkela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22081v2",
    "title": "Can Test-time Computation Mitigate Reproduction Bias in Neural Symbolic Regression?",
    "summary": "Mathematical expressions play a central role in scientific discovery. Symbolic regression aims to automatically discover such expressions from given numerical data. Recently, Neural symbolic regression (NSR) methods that involve Transformers pre-trained on synthetic datasets have gained attention for their fast inference, but they often perform poorly, especially with many input variables. In this study, we analyze NSR from both theoretical and empirical perspectives and show that (1) ordinary token-by-token generation is ill-suited for NSR, as Transformers cannot compositionally generate tokens while validating numerical consistency, and (2) the search space of NSR methods is greatly restricted due to reproduction bias, where the majority of generated expressions are merely copied from the training data. We further examine whether tailored test-time strategies can reduce reproduction bias and show that providing additional information at test time effectively mitigates it. These findings contribute to a deeper understanding of the limitation of NSR approaches and provide guidance for designing more robust and generalizable methods. Code is available at https://github.com/Shun-0922/Mem-Bias-NSR .",
    "published": "2025-05-28T08:01:25Z",
    "updated": "2026-02-02T05:59:06Z",
    "link": "http://arxiv.org/pdf/2505.22081v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shun Sato",
      "Issei Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.18595v2",
    "title": "Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows",
    "summary": "Predicting multiphysics dynamics is computationally expensive and challenging due to the severe coupling of multi-scale, heterogeneous physical processes. While neural surrogates promise a paradigm shift, the field currently suffers from an \"illusion of mastery\", as repeatedly emphasized in top-tier commentaries: existing evaluations overly rely on simplified, low-dimensional proxies, which fail to expose the models' inherent fragility in realistic regimes. To bridge this critical gap, we present REALM (REalistic AI Learning for Multiphysics), a rigorous benchmarking framework designed to test neural surrogates on challenging, application-driven reactive flows. REALM features 11 high-fidelity datasets spanning from canonical multiphysics problems to complex propulsion and fire safety scenarios, alongside a standardized end-to-end training and evaluation protocol that incorporates multiphysics-aware preprocessing and a robust rollout strategy. Using this framework, we systematically benchmark over a dozen representative surrogate model families, including spectral operators, convolutional models, Transformers, pointwise operators, and graph/mesh networks, and identify three robust trends: (i) a scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors; (ii) performance primarily controlled by architectural inductive biases rather than parameter count; and (iii) a persistent gap between nominal accuracy metrics and physically trustworthy behavior, where models with high correlations still miss key transient structures and integral quantities. Taken together, REALM exposes the limits of current neural surrogates on realistic multiphysics flows and offers a rigorous testbed to drive the development of next-generation physics-aware architectures.",
    "published": "2025-12-21T05:04:13Z",
    "updated": "2026-02-02T05:48:56Z",
    "link": "http://arxiv.org/pdf/2512.18595v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Runze Mao",
      "Rui Zhang",
      "Xuan Bai",
      "Tianhao Wu",
      "Teng Zhang",
      "Zhenyi Chen",
      "Minqi Lin",
      "Bocheng Zeng",
      "Yangchen Xu",
      "Yingxuan Xiang",
      "Haoze Zhang",
      "Shubham Goswami",
      "Pierre A. Dawe",
      "Yifan Xu",
      "Zhenhua An",
      "Mengtao Yan",
      "Xiaoyi Lu",
      "Yi Wang",
      "Rongbo Bai",
      "Haobu Gao",
      "Xiaohang Fang",
      "Han Li",
      "Hao Sun",
      "Zhi X. Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01682v1",
    "title": "Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets",
    "summary": "We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\\log T)$, as well as a finite but exponentially large bound of $\\exp(O(d\\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.",
    "published": "2026-02-02T05:48:54Z",
    "updated": "2026-02-02T05:48:54Z",
    "link": "http://arxiv.org/pdf/2602.01682v1.pdf",
    "category": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "authors": [
      "Taihei Oki",
      "Shinsaku Sakaue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01667v1",
    "title": "Quantifying Epistemic Predictive Uncertainty in Conformal Prediction",
    "summary": "We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \\emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.",
    "published": "2026-02-02T05:38:07Z",
    "updated": "2026-02-02T05:38:07Z",
    "link": "http://arxiv.org/pdf/2602.01667v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Siu Lun Chau",
      "Soroush H. Zargarbashi",
      "Yusuf Sale",
      "Michele Caprio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.23402v2",
    "title": "Quantized-Tinyllava: a new multimodal foundation model enables efficient split learning",
    "summary": "Multimodal foundation models are increasingly trained on sensitive data across domains such as finance, biomedicine, and personal identifiers. However, this distributed setup raises serious privacy concerns due to the need for cross-partition data sharing. Split learning addresses these concerns by enabling collaborative model training without raw data exchange between partitions, yet it introduces a significant challenge: transmitting high-dimensional intermediate feature representations between partitions leads to substantial communication costs. To address this challenge, we propose Quantized-TinyLLaVA, a multimodal foundation model with an integrated communication-efficient split learning framework. Our approach adopts a compression module that quantizes intermediate feature into discrete representations before transmission, substantially reducing communication overhead. Besides, we derive a principled quantization strategy grounded in entropy coding theory to determine the optimal number of discrete representation levels. We deploy our framework in a two-partition setting, with one partition operating as the client and the other as the server, to realistically simulate distributed training. Under this setup, Quantized-TinyLLaVA achieves an approximate \\textbf{87.5\\%} reduction in communication overhead with 2-bit quantization, while maintaining performance of the original 16-bit model across five benchmark datasets. Furthermore, our compressed representations exhibit enhanced resilience against feature inversion attacks, validating the privacy of transmission. The code is available at https://github.com/anonymous-1742/Quantized-TinyLLaVA.",
    "published": "2025-11-28T17:53:05Z",
    "updated": "2026-02-02T05:07:36Z",
    "link": "http://arxiv.org/pdf/2511.23402v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Jiajun Guo",
      "Xin Luo",
      "Jiayin Zheng",
      "Yiqun Wang",
      "Kai-Wei Chang",
      "Wei Wang",
      "Jie Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01833v1",
    "title": "Mixture of Disentangled Experts with Missing Modalities for Robust Multimodal Sentiment Analysis",
    "summary": "Multimodal Sentiment Analysis (MSA) integrates multiple modalities to infer human sentiment, but real-world noise often leads to missing or corrupted data. However, existing feature-disentangled methods struggle to handle the internal variations of heterogeneous information under uncertain missingness, making it difficult to learn effective multimodal representations from degraded modalities. To address this issue, we propose DERL, a Disentangled Expert Representation Learning framework for robust MSA. Specifically, DERL employs hybrid experts to adaptively disentangle multimodal inputs into orthogonal private and shared representation spaces. A multi-level reconstruction strategy is further developed to provide collaborative supervision, enhancing both the expressiveness and robustness of the learned representations. Finally, the disentangled features act as modality experts with distinct roles to generate importance-aware fusion results. Extensive experiments on two MSA benchmarks demonstrate that DERL outperforms state-of-the-art methods under various missing-modality conditions. For instance, our method achieves improvements of 2.47% in Acc-2 and 2.25% in MAE on MOSI under intra-modal missingness.",
    "published": "2026-02-02T09:06:38Z",
    "updated": "2026-02-02T09:06:38Z",
    "link": "http://arxiv.org/pdf/2602.01833v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Xiang Li",
      "Xiaoming Zhang",
      "Dezhuang Miao",
      "Xianfu Cheng",
      "Dawei Li",
      "Honggui Han",
      "Zhoujun Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01748v1",
    "title": "OFERA: Blendshape-driven 3D Gaussian Control for Occluded Facial Expression to Realistic Avatars in VR",
    "summary": "We propose OFERA, a novel framework for real-time expression control of photorealistic Gaussian head avatars for VR headset users. Existing approaches attempt to recover occluded facial expressions using additional sensors or internal cameras, but sensor-based methods increase device weight and discomfort, while camera-based methods raise privacy concerns and suffer from limited access to raw data. To overcome these limitations, we leverage the blendshape signals provided by commercial VR headsets as expression inputs. Our framework consists of three key components: (1) Blendshape Distribution Alignment (BDA), which applies linear regression to align the headset-provided blendshape distribution to a canonical input space; (2) an Expression Parameter Mapper (EPM) that maps the aligned blendshape signals into an expression parameter space for controlling Gaussian head avatars; and (3) a Mapper-integrated Avatar (MiA) that incorporates EPM into the avatar learning process to ensure distributional consistency. Furthermore, OFERA establishes an end-to-end pipeline that senses and maps expressions, updates Gaussian avatars, and renders them in real-time within VR environments. We show that EPM outperforms existing mapping methods on quantitative metrics, and we demonstrate through a user study that the full OFERA framework enhances expression fidelity while preserving avatar realism. By enabling real-time and photorealistic avatar expression control, OFERA significantly improves telepresence in VR communication. A project page is available at https://ysshwan147.github.io/projects/ofera/.",
    "published": "2026-02-02T07:34:15Z",
    "updated": "2026-02-02T07:34:15Z",
    "link": "http://arxiv.org/pdf/2602.01748v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Seokhwan Yang",
      "Boram Yoon",
      "Seoyoung Kang",
      "Hail Song",
      "Woontack Woo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02038v1",
    "title": "Frictional Contact Solving for Material Point Method",
    "summary": "Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.",
    "published": "2026-02-02T12:34:32Z",
    "updated": "2026-02-02T12:34:32Z",
    "link": "http://arxiv.org/pdf/2602.02038v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Etienne Ménager",
      "Justin Carpentier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02026v1",
    "title": "Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp",
    "summary": "We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.",
    "published": "2026-02-02T12:21:27Z",
    "updated": "2026-02-02T12:21:27Z",
    "link": "http://arxiv.org/pdf/2602.02026v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhenwei Niu",
      "Xiaoyi Chen",
      "Jiayu Hu",
      "Zhaoyang Liu",
      "Xiaozu Ju"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02006v1",
    "title": "Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements",
    "summary": "Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.\n  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.",
    "published": "2026-02-02T12:04:34Z",
    "updated": "2026-02-02T12:04:34Z",
    "link": "http://arxiv.org/pdf/2602.02006v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Thomas Jantos",
      "Giulio Delama",
      "Stephan Weiss",
      "Jan Steinbrener"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01948v1",
    "title": "A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications",
    "summary": "Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.",
    "published": "2026-02-02T10:58:53Z",
    "updated": "2026-02-02T10:58:53Z",
    "link": "http://arxiv.org/pdf/2602.01948v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Patrick Frank",
      "Christian Friedrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.08278v5",
    "title": "Line-Search Filter Differential Dynamic Programming for Optimal Control with Nonlinear Equality Constraints",
    "summary": "We present FilterDDP, a differential dynamic programming algorithm for solving discrete-time, optimal control problems (OCPs) with nonlinear equality constraints. Unlike prior methods based on merit functions or the augmented Lagrangian class of algorithms, FilterDDP uses a step filter in conjunction with a line search to handle equality constraints. We identify two important design choices for the step filter criteria which lead to robust numerical performance: 1) we use the Lagrangian instead of the cost in the step acceptance criterion and, 2) in the backward pass, we perturb the value function Hessian. Both choices are rigorously justified, for 2) in particular by a formal proof of local quadratic convergence. In addition to providing a primal-dual interior point extension for handling OCPs with both equality and inequality constraints, we validate FilterDDP on three contact implicit trajectory optimisation problems which arise in robotics.",
    "published": "2025-04-11T06:18:46Z",
    "updated": "2026-02-02T10:26:28Z",
    "link": "http://arxiv.org/pdf/2504.08278v5.pdf",
    "category": [
      "math.OC",
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Ming Xu",
      "Stephen Gould",
      "Iman Shames"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01916v1",
    "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
    "summary": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/",
    "published": "2026-02-02T10:20:11Z",
    "updated": "2026-02-02T10:20:11Z",
    "link": "http://arxiv.org/pdf/2602.01916v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Keyu Chen",
      "Wenchao Sun",
      "Hao Cheng",
      "Zheng Fu",
      "Sifa Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01892v1",
    "title": "Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study",
    "summary": "This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.",
    "published": "2026-02-02T10:03:37Z",
    "updated": "2026-02-02T10:03:37Z",
    "link": "http://arxiv.org/pdf/2602.01892v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Alexandre Lombard",
      "Florent Perronnet",
      "Nicolas Gaud",
      "Abdeljalil Abbas-Turki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01880v1",
    "title": "Multimodal Large Language Models for Real-Time Situated Reasoning",
    "summary": "In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.",
    "published": "2026-02-02T09:52:11Z",
    "updated": "2026-02-02T09:52:11Z",
    "link": "http://arxiv.org/pdf/2602.01880v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Giulio Antonio Abbo",
      "Senne Lenaerts",
      "Tony Belpaeme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01870v1",
    "title": "BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models",
    "summary": "Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.",
    "published": "2026-02-02T09:43:17Z",
    "updated": "2026-02-02T09:43:17Z",
    "link": "http://arxiv.org/pdf/2602.01870v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Riccardo Andrea Izzo",
      "Gianluca Bardaro",
      "Matteo Matteucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06552v2",
    "title": "Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics",
    "summary": "Whenever humans and robots work together, it is essential that unexpected robot behavior can be explained to the user. Especially in applications such as shared control the user and the robot must share the same model of the objects in the world, and the actions that can be performed on these objects.\n  In this paper, we achieve this with a so-called model reconciliation framework. We leverage a Large Language Model to predict and explain the difference between the robot's and the human's mental models, without the need of a formal mental model of the user. Furthermore, our framework aims to solve the model divergence after the explanation by allowing the human to correct the robot. We provide an implementation in an assistive robotics domain, where we conduct a set of experiments with a real wheelchair-based mobile manipulator and its digital twin.",
    "published": "2026-01-10T12:38:08Z",
    "updated": "2026-02-02T09:39:25Z",
    "link": "http://arxiv.org/pdf/2601.06552v2.pdf",
    "category": [
      "cs.RO",
      "cs.HC"
    ],
    "authors": [
      "Britt Besch",
      "Tai Mai",
      "Jeremias Thun",
      "Markus Huff",
      "Jörn Vogel",
      "Freek Stulp",
      "Samuel Bustamante"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01860v1",
    "title": "Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach",
    "summary": "Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.",
    "published": "2026-02-02T09:32:19Z",
    "updated": "2026-02-02T09:32:19Z",
    "link": "http://arxiv.org/pdf/2602.01860v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Filip Novák",
      "Matěj Petrlík",
      "Matej Novosad",
      "Parakh M. Gupta",
      "Robert Pěnička",
      "Martin Saska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01834v1",
    "title": "Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models",
    "summary": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.",
    "published": "2026-02-02T09:06:43Z",
    "updated": "2026-02-02T09:06:43Z",
    "link": "http://arxiv.org/pdf/2602.01834v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Siqi Wen",
      "Shu Yang",
      "Shaopeng Fu",
      "Jingfeng Zhang",
      "Lijie Hu",
      "Di Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.05248v2",
    "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model",
    "summary": "Vision-Language-Action (VLA) models have recently shown strong generalization, with some approaches seeking to explicitly generate linguistic reasoning traces or predict future observations prior to execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching during deployment. Across 10 real-world tasks spanning tabletop, mobile, and dexterous hand manipulation, LaST$_0$ improves mean success rates by 13%, 14% and 14% over prior SOTA VLA methods, respectively.",
    "published": "2026-01-08T18:59:53Z",
    "updated": "2026-02-02T08:46:04Z",
    "link": "http://arxiv.org/pdf/2601.05248v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhuoyang Liu",
      "Jiaming Liu",
      "Hao Chen",
      "Jiale Yu",
      "Ziyu Guo",
      "Chengkai Hou",
      "Chenyang Gu",
      "Xiangju Mi",
      "Renrui Zhang",
      "Kun Wu",
      "Zhengping Che",
      "Jian Tang",
      "Pheng-Ann Heng",
      "Shanghang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01811v1",
    "title": "From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models",
    "summary": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.",
    "published": "2026-02-02T08:44:40Z",
    "updated": "2026-02-02T08:44:40Z",
    "link": "http://arxiv.org/pdf/2602.01811v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Wentao Zhang",
      "Aolan Sun",
      "Wentao Mo",
      "Xiaoyang Qu",
      "Yuxin Zheng",
      "Jianzong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.20377v2",
    "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
    "summary": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.",
    "published": "2026-01-28T08:43:48Z",
    "updated": "2026-02-02T08:31:09Z",
    "link": "http://arxiv.org/pdf/2601.20377v2.pdf",
    "category": [
      "cs.RO",
      "eess.SP"
    ],
    "authors": [
      "Xinyan Chen",
      "Qinchun Li",
      "Ruiqin Ma",
      "Jiaqi Bai",
      "Li Yi",
      "Jianfei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01789v1",
    "title": "RFS: Reinforcement learning with Residual flow steering for dexterous manipulation",
    "summary": "Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \\emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.",
    "published": "2026-02-02T08:11:57Z",
    "updated": "2026-02-02T08:11:57Z",
    "link": "http://arxiv.org/pdf/2602.01789v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Entong Su",
      "Tyler Westenbroek",
      "Anusha Nagabandi",
      "Abhishek Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.13531v3",
    "title": "A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots",
    "summary": "This paper presents a control framework designed to enhance the stability and robustness of legged robots in the presence of uncertainties, including model uncertainties, external disturbances, and faults. The framework enables the full-state feedback estimator to estimate and compensate for uncertainties in the whole-body dynamics of the legged robots. First, we propose a novel moving horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate noise in legged systems, which can be integrated into the framework for disturbance compensation. Second, we introduce a three-level whole-body disturbance rejection control framework (T-WB-DRC). Unlike the previous two-level approach, this three-level framework considers both the plan based on whole-body dynamics without uncertainties and the plan based on dynamics with uncertainties, significantly improving payload transportation, external disturbance rejection, and fault tolerance. Third, simulations of both humanoid and quadruped robots in the Gazebo simulator demonstrate the effectiveness and versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped robot validate the robustness and stability of the system when using T-WB-DRC under various disturbance conditions.",
    "published": "2025-08-19T05:43:54Z",
    "updated": "2026-02-02T08:06:14Z",
    "link": "http://arxiv.org/pdf/2508.13531v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Bolin Li",
      "Gewei Zuo",
      "Zhixiang Wang",
      "Xiaotian Ke",
      "Lijun Zhu",
      "Han Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08416v2",
    "title": "A Gait Driven Reinforcement Learning Framework for Humanoid Robots",
    "summary": "This paper presents a real-time gait driven training framework for humanoid robots. First, we introduce a novel gait planner that incorporates dynamics to design the desired joint trajectory. In the gait design process, the 3D robot model is decoupled into two 2D models, which are then approximated as hybrid inverted pendulums (H-LIP) for trajectory planning. The gait planner operates in parallel in real time within the robot's learning environment. Second, based on this gait planner, we design three effective reward functions within a reinforcement learning framework, forming a reward composition to achieve periodic bipedal gait. This reward composition reduces the robot's learning time and enhances locomotion performance. Finally, a gait design example, along with simulation and experimental comparisons, is presented to demonstrate the effectiveness of the proposed method.",
    "published": "2025-06-10T03:42:04Z",
    "updated": "2026-02-02T07:53:19Z",
    "link": "http://arxiv.org/pdf/2506.08416v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Bolin Li",
      "Yuzhi Jiang",
      "Linwei Sun",
      "Xuecong Huang",
      "Lijun Zhu",
      "Han Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01731v1",
    "title": "Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion",
    "summary": "Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.",
    "published": "2026-02-02T07:12:27Z",
    "updated": "2026-02-02T07:12:27Z",
    "link": "http://arxiv.org/pdf/2602.01731v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiwoo Hwang",
      "Taegeun Yang",
      "Jeil Jeong",
      "Minsung Yoon",
      "Sung-Eui Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01700v1",
    "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
    "summary": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.",
    "published": "2026-02-02T06:15:33Z",
    "updated": "2026-02-02T06:15:33Z",
    "link": "http://arxiv.org/pdf/2602.01700v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ruoyu Wang",
      "Xuchen Liu",
      "Zongzhou Wu",
      "Zixuan Guo",
      "Wendi Ding",
      "Ben M. Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01693v1",
    "title": "GSR: Learning Structured Reasoning for Embodied Manipulation",
    "summary": "Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.",
    "published": "2026-02-02T06:07:42Z",
    "updated": "2026-02-02T06:07:42Z",
    "link": "http://arxiv.org/pdf/2602.01693v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kewei Hu",
      "Michael Zhang",
      "Wei Ying",
      "Tianhao Liu",
      "Guoqiang Hao",
      "Zimeng Li",
      "Wanchan Yu",
      "Jiajian Jing",
      "Fangwen Chen",
      "Hanwen Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01662v1",
    "title": "AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act",
    "summary": "Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.",
    "published": "2026-02-02T05:30:14Z",
    "updated": "2026-02-02T05:30:14Z",
    "link": "http://arxiv.org/pdf/2602.01662v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Pengyuan Guo",
      "Zhonghao Mai",
      "Zhengtong Xu",
      "Kaidi Zhang",
      "Heng Zhang",
      "Zichen Miao",
      "Arash Ajoudani",
      "Zachary Kingston",
      "Qiang Qiu",
      "Yu She"
    ]
  }
]