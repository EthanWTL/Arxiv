[
  {
    "id": "http://arxiv.org/abs/2508.13187v2",
    "title": "\"Not in My Backyard\": LLMs Uncover Online and Offline Social Biases Against Homelessnes",
    "summary": "Homelessness is a persistent social challenge, impacting millions worldwide. Over 876,000 people experienced homelessness (PEH) in the U.S. in 2025. Social bias is a significant barrier to alleviation, shaping public perception and influencing policymaking. Given that online textual media and offline city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases against PEH. We present a new, manually-annotated multi-domain dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across ten U.S. cities. Our 16-category multi-label taxonomy creates a challenging long-tail classification problem: some categories appear in less than 1% of samples, while others exceed 70%. We find that small human-annotated datasets (1,702 samples) are insufficient for training effective classifiers, whether used to fine-tune encoder models or as few-shot examples for LLMs. To address this, we use GPT-4.1 to generate pseudo-labels on a larger unlabeled corpus. Training on this expanded dataset enables even small encoder models (ModernBERT, 150M parameters) to achieve 35.23 macro-F1, approaching GPT-4.1's 41.57. This demonstrates that \\textbf{data quantity matters more than model size}, enabling low-cost, privacy-preserving deployment without relying on commercial APIs. Our results reveal that negative bias against PEH is prevalent both offline and online (especially on Reddit), with \"not in my backyard\" narratives showing the highest engagement. These findings uncover a type of ostracism that directly impacts poverty-reduction policymaking and provide actionable insights for practitioners addressing homelessness.",
    "published": "2025-08-14T17:58:34Z",
    "updated": "2026-01-27T18:56:57Z",
    "link": "http://arxiv.org/pdf/2508.13187v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jonathan A. Karr",
      "Benjamin F. Herbst",
      "Matthew L. Sisk",
      "Xueyun Li",
      "Ting Hua",
      "Matthew Hauenstein",
      "Georgina Curto",
      "Nitesh V. Chawla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19888v1",
    "title": "M-SGWR: Multiscale Similarity and Geographically Weighted Regression",
    "summary": "The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes \"near\" and \"related\" remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local regression models, such as Geographically Weighted Regression (GWR) and Multiscale GWR (MGWR), quantify spatial relationships solely through geographic proximity. In an era of globalization and digital connectivity, however, geographic proximity alone may be insufficient to capture how locations are interconnected. To address this limitation, we propose a new multiscale local regression framework, termed M-SGWR, which characterizes spatial interaction across two dimensions: geographic proximity and attribute (variable) similarity. For each predictor, geographic and attribute-based weight matrices are constructed separately and then combined using an optimized parameter, alpha, which governs their relative contribution to local model fitting. Analogous to variable-specific bandwidths in MGWR, the optimal alpha varies by predictor, allowing the model to flexibly account for geographic, mixed, or non-spatial (remote similarity) effects. Results from two simulation experiments and one empirical application demonstrate that M-SGWR consistently outperforms GWR, SGWR, and MGWR across all goodness-of-fit metrics.",
    "published": "2026-01-27T18:55:12Z",
    "updated": "2026-01-27T18:55:12Z",
    "link": "http://arxiv.org/pdf/2601.19888v1.pdf",
    "category": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "M. Naser Lessani",
      "Zhenlong Li",
      "Manzhu Yu",
      "Helen Greatrex",
      "Chan Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02091v4",
    "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning",
    "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.",
    "published": "2025-10-02T14:57:13Z",
    "updated": "2026-01-27T18:53:30Z",
    "link": "http://arxiv.org/pdf/2510.02091v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xinyuan Song",
      "Keyu Wang",
      "PengXiang Li",
      "Lu Yin",
      "Shiwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19886v1",
    "title": "AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability",
    "summary": "The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized academics and smaller companies. Simultaneously, increased energy expenditure, due to growing AI use, has led to mounting environmental costs. In response to accessibility and sustainability concerns, we argue for research into, and implementation of, market-based methods that incentivize AI efficiency. We believe that incentivizing efficient operations and approaches will reduce emissions while opening new opportunities for academics and smaller companies. As a call to action, we propose a cap-and-trade system for AI. Our system provably reduces computations for AI deployment, thereby lowering emissions and monetizing efficiency to the benefit of of academics and smaller companies.",
    "published": "2026-01-27T18:53:21Z",
    "updated": "2026-01-27T18:53:21Z",
    "link": "http://arxiv.org/pdf/2601.19886v1.pdf",
    "category": [
      "econ.GN",
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "authors": [
      "Marco Bornstein",
      "Amrit Singh Bedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16504v2",
    "title": "LOGICAL-COMMONSENSEQA: A Benchmark for Logical Commonsense Reasoning",
    "summary": "Commonsense reasoning often involves evaluating multiple plausible interpretations rather than selecting a single atomic answer, yet most benchmarks rely on single-label evaluation, obscuring whether statements are jointly plausible, mutually exclusive, or jointly implausible. We introduce LOGICAL-COMMONSENSEQA, a benchmark that re-frames commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators (AND, OR, NEITHER/NOR). Evaluating instruction-tuned, reasoning-specialized, and fine-tuned models under zero-shot, few-shot, and chain-of-thought prompting, we find that while models perform reasonably on conjunctive and moderately on disjunctive reasoning, performance degrades sharply on negation-based questions. LOGICAL-COMMONSENSEQA exposes fundamental reasoning limitations and provides a controlled framework for advancing compositional commonsense reasoning.",
    "published": "2026-01-23T07:07:19Z",
    "updated": "2026-01-27T18:33:20Z",
    "link": "http://arxiv.org/pdf/2601.16504v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Obed Junias",
      "Maria Leonor Pacheco"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08512v2",
    "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding",
    "summary": "Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines.",
    "published": "2025-06-10T07:20:12Z",
    "updated": "2026-01-27T18:07:12Z",
    "link": "http://arxiv.org/pdf/2506.08512v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhiyi Zhu",
      "Xiaoyu Wu",
      "Zihao Liu",
      "Linlin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12121v5",
    "title": "Learning Dynamic Representations via An Optimally-Weighted Maximum Mean Discrepancy Optimization Framework for Continual Learning",
    "summary": "Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance.",
    "published": "2025-01-21T13:33:45Z",
    "updated": "2026-01-27T18:04:09Z",
    "link": "http://arxiv.org/pdf/2501.12121v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "KaiHui Huang",
      "RunQing Wu",
      "JinHui Sheng",
      "HanYi Zhang",
      "Ling Ge",
      "JinYu Guo",
      "Fei Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19839v1",
    "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs",
    "summary": "Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.",
    "published": "2026-01-27T17:45:04Z",
    "updated": "2026-01-27T17:45:04Z",
    "link": "http://arxiv.org/pdf/2601.19839v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Jeanne Malécot",
      "Hamed Rahimi",
      "Jeanne Cattoni",
      "Marie Samson",
      "Mouad Abrini",
      "Mahdi Khoramshahi",
      "Maribel Pino",
      "Mohamed Chetouani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26383v4",
    "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
    "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.",
    "published": "2025-09-30T15:14:24Z",
    "updated": "2026-01-27T17:44:43Z",
    "link": "http://arxiv.org/pdf/2509.26383v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jinyeop Song",
      "Song Wang",
      "Julian Shun",
      "Yada Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19834v1",
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.",
    "published": "2026-01-27T17:40:07Z",
    "updated": "2026-01-27T17:40:07Z",
    "link": "http://arxiv.org/pdf/2601.19834v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jialong Wu",
      "Xiaoying Zhang",
      "Hongyi Yuan",
      "Xiangcheng Zhang",
      "Tianhao Huang",
      "Changjing He",
      "Chaoyi Deng",
      "Renrui Zhang",
      "Youbin Wu",
      "Mingsheng Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19827v1",
    "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering",
    "summary": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.",
    "published": "2026-01-27T17:35:05Z",
    "updated": "2026-01-27T17:35:05Z",
    "link": "http://arxiv.org/pdf/2601.19827v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Mahdi Astaraki",
      "Mohammad Arshi Saloot",
      "Ali Shiraee Kasmaee",
      "Hamidreza Mahyar",
      "Soheila Samiee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14242v2",
    "title": "APEX-Agents",
    "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.",
    "published": "2026-01-20T18:53:44Z",
    "updated": "2026-01-27T17:31:16Z",
    "link": "http://arxiv.org/pdf/2601.14242v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Bertie Vidgen",
      "Austin Mann",
      "Abby Fennelly",
      "John Wright Stanly",
      "Lucas Rothman",
      "Marco Burstein",
      "Julien Benchek",
      "David Ostrofsky",
      "Anirudh Ravichandran",
      "Debnil Sur",
      "Neel Venugopal",
      "Alannah Hsia",
      "Isaac Robinson",
      "Calix Huang",
      "Olivia Varones",
      "Daniyal Khan",
      "Michael Haines",
      "Zach Richards",
      "Chirag Mahapatra",
      "Brendan Foody",
      "Osvald Nitski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19825v1",
    "title": "Routing End User Queries to Enterprise Databases",
    "summary": "We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.",
    "published": "2026-01-27T17:30:19Z",
    "updated": "2026-01-27T17:30:19Z",
    "link": "http://arxiv.org/pdf/2601.19825v1.pdf",
    "category": [
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Saikrishna Sudarshan",
      "Tanay Kulkarni",
      "Manasi Patwardhan",
      "Lovekesh Vig",
      "Ashwin Srinivasan",
      "Tanmay Tulsidas Verlekar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19824v1",
    "title": "An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care",
    "summary": "There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we address these challenges with a recommendation model that leverages the structure of psychometric data to provide visual explanations that are faithful to the model and interpretable by care professionals. We focus on a narrow healthcare niche, gerontological primary care, to show that the proposed recommendation model can assist the attending professional in the creation of personalised care plans. We report results of a comparative offline performance evaluation of the proposed model on healthcare datasets that were collected by research partners in Brazil, as well as the results of a user study that evaluates the interpretability of the visual explanations the model generates. The results suggest that the proposed model can advance the application of recommender systems in this healthcare niche, which is expected to grow in demand , opportunities, and information technology needs as demographic changes become more pronounced.",
    "published": "2026-01-27T17:29:21Z",
    "updated": "2026-01-27T17:29:21Z",
    "link": "http://arxiv.org/pdf/2601.19824v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.SI"
    ],
    "authors": [
      "Andre Paulino de Lima",
      "Paula Castro",
      "Suzana Carvalho Vaz de Andrade",
      "Rosa Maria Marcucci",
      "Ruth Caldeira de Melo",
      "Marcelo Garcia Manzato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.14780v3",
    "title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting",
    "summary": "Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (<500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.",
    "published": "2025-02-20T18:01:41Z",
    "updated": "2026-01-27T17:16:10Z",
    "link": "http://arxiv.org/pdf/2502.14780v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Abhijit Mishra",
      "Mingda Li",
      "Hsiang Fu",
      "Richard Noh",
      "Minji Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19811v1",
    "title": "Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts",
    "summary": "Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance.",
    "published": "2026-01-27T17:12:15Z",
    "updated": "2026-01-27T17:12:15Z",
    "link": "http://arxiv.org/pdf/2601.19811v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ME"
    ],
    "authors": [
      "TrungKhang Tran",
      "TrungTin Nguyen",
      "Gersende Fort",
      "Tung Doan",
      "Hien Duy Nguyen",
      "Binh T. Nguyen",
      "Florence Forbes",
      "Christopher Drovandi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19810v1",
    "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
    "summary": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.",
    "published": "2026-01-27T17:10:29Z",
    "updated": "2026-01-27T17:10:29Z",
    "link": "http://arxiv.org/pdf/2601.19810v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Octavio Pappalardo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00961v2",
    "title": "LLM-Generated Explanations Do Not Suffice for Ultra-Strong Machine Learning",
    "summary": "Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. We introduce LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic framework that combines symbolic program synthesis with large language models (LLMs). This framework automatically generates natural language explanations of learned logic programs, replacing hand-crafted templates used in prior USML work. Using LLMs-as-judges evaluation and expert validation, we show that LENS produces higher-quality explanations than both direct LLM prompting and hand-crafted templates. We then examine whether LENS explanations suffice for achieving USML in a human trial teaching active learning strategies across three related domains. Our exploratory analysis suggests that concise, expert-written explanations may benefit learners with higher initial performance, while LLM-generated explanations provide no advantage over human self learning despite being rated as higher quality. This case study reveals that achieving USML requires methods grounded in human learning, where current LLM-generated explanations do not capture human cognitive constraints and LLMs-as-judges evaluations do not reflect what effectively supports human learning.",
    "published": "2025-08-31T19:04:31Z",
    "updated": "2026-01-27T17:01:53Z",
    "link": "http://arxiv.org/pdf/2509.00961v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Lun Ai",
      "Johannes Langer",
      "Ute Schmid",
      "Stephen Muggleton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19793v1",
    "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing",
    "summary": "Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.",
    "published": "2026-01-27T16:52:47Z",
    "updated": "2026-01-27T16:52:47Z",
    "link": "http://arxiv.org/pdf/2601.19793v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shanyv Liu",
      "Xuyang Yuan",
      "Tao Chen",
      "Zijun Zhan",
      "Zhu Han",
      "Danyang Zheng",
      "Weishan Zhang",
      "Shaohua Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19792v1",
    "title": "LVLMs and Humans Ground Differently in Referential Communication",
    "summary": "For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.",
    "published": "2026-01-27T16:52:20Z",
    "updated": "2026-01-27T16:52:20Z",
    "link": "http://arxiv.org/pdf/2601.19792v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Peter Zeng",
      "Weiling Li",
      "Amie Paige",
      "Zhengxiang Wang",
      "Panagiotis Kaliosis",
      "Dimitris Samaras",
      "Gregory Zelinsky",
      "Susan Brennan",
      "Owen Rambow"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19778v1",
    "title": "Reimagining Peer Review Process Through Multi-Agent Mechanism Design",
    "summary": "The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as \"broken.\" This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.",
    "published": "2026-01-27T16:43:11Z",
    "updated": "2026-01-27T16:43:11Z",
    "link": "http://arxiv.org/pdf/2601.19778v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "cs.SE"
    ],
    "authors": [
      "Ahmad Farooq",
      "Kamran Iqbal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.08005v7",
    "title": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection",
    "summary": "Out-of-distribution (OOD) detection holds significant importance across many applications. While semantic and domain-shift OOD problems are well-studied, this work focuses on covariate shifts - subtle variations in the data distribution that can degrade machine learning performance. We hypothesize that detecting these subtle shifts can improve our understanding of in-distribution boundaries, ultimately improving OOD detection. In adversarial discriminators trained with Batch Normalization (BN), real and adversarial samples form distinct domains with unique batch statistics - a property we exploit for OOD detection. We introduce DisCoPatch, an unsupervised Adversarial Variational Autoencoder (VAE) framework that harnesses this mechanism. During inference, batches consist of patches from the same image, ensuring a consistent data distribution that allows the model to rely on batch statistics. DisCoPatch uses the VAE's suboptimal outputs (generated and reconstructed) as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this boundary, DisCoPatch achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior methods on public Near-OOD (95.0%) benchmarks. With a compact model size of 25MB, it achieves high OOD detection performance at notably lower latency than existing methods, making it an efficient and practical solution for real-world OOD detection applications. The code is publicly available.",
    "published": "2025-01-14T10:49:26Z",
    "updated": "2026-01-27T16:37:43Z",
    "link": "http://arxiv.org/pdf/2501.08005v7.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Francisco Caetano",
      "Christiaan Viviers",
      "Luis A. Zavala-Mondragón",
      "Peter H. N. de With",
      "Fons van der Sommen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.16468v2",
    "title": "Quantifying Fidelity: A Decisive Feature Approach to Comparing Synthetic and Real Imagery",
    "summary": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on consistent decision evidence in both real and simulated environments, not just whether images \"look real\" to humans. To this end this paper proposes a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity, that is, agreement in the model-specific decisive evidence that drives the SUT's decisions across domains. DFF leverages explainable-AI methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.",
    "published": "2025-12-18T12:39:13Z",
    "updated": "2026-01-27T16:34:36Z",
    "link": "http://arxiv.org/pdf/2512.16468v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Danial Safaei",
      "Siddartha Khastgir",
      "Mohsen Alirezaei",
      "Jeroen Ploeg",
      "Son Tong",
      "Xingyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03871v2",
    "title": "Optimal Scaling Needs Optimal Norm",
    "summary": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. For Adam and Scion optimizers, we discover that joint optimal scaling across model and dataset sizes is conditioned on a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(η^{\\ast}, B^{\\ast})$ consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(η, B)$ reach the optimal norm, only a unique $(η^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient condition, we provide the first measurement of $(η^{\\ast}, B^{\\ast})$ scaling with dataset size for Scion, and find that the scaling rules are consistent with those of Adam. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.",
    "published": "2025-10-04T16:48:36Z",
    "updated": "2026-01-27T16:32:23Z",
    "link": "http://arxiv.org/pdf/2510.03871v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Oleg Filatov",
      "Jiangtao Wang",
      "Jan Ebert",
      "Stefan Kesselheim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19768v1",
    "title": "GAVEL: Towards rule-based safety through activation monitoring",
    "summary": "Large language models (LLMs) are increasingly paired with activation-based monitoring to detect and prevent harmful behaviors that may not be apparent at the surface-text level. However, existing activation safety approaches, trained on broad misuse datasets, struggle with poor precision, limited flexibility, and lack of interpretability. This paper introduces a new paradigm: rule-based activation safety, inspired by rule-sharing practices in cybersecurity. We propose modeling activations as cognitive elements (CEs), fine-grained, interpretable factors such as ''making a threat'' and ''payment processing'', that can be composed to capture nuanced, domain-specific behaviors with higher precision. Building on this representation, we present a practical framework that defines predicate rules over CEs and detects violations in real time. This enables practitioners to configure and update safeguards without retraining models or detectors, while supporting transparency and auditability. Our results show that compositional rule-based activation safety improves precision, supports domain customization, and lays the groundwork for scalable, interpretable, and auditable AI governance. We will release GAVEL as an open-source framework and provide an accompanying automated rule creation tool.",
    "published": "2026-01-27T16:31:39Z",
    "updated": "2026-01-27T16:31:39Z",
    "link": "http://arxiv.org/pdf/2601.19768v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Shir Rozenfeld",
      "Rahul Pankajakshan",
      "Itay Zloczower",
      "Eyal Lenga",
      "Gilad Gressel",
      "Yisroel Mirsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22562v2",
    "title": "Activation Function Design Sustains Plasticity in Continual Learning",
    "summary": "In independent, identically distributed (i.i.d.) training regimes, activation functions have been benchmarked extensively, and their differences often shrink once model size and optimization are tuned. In continual learning, however, the picture is different: beyond catastrophic forgetting, models can progressively lose the ability to adapt (referred to as loss of plasticity) and the role of the non-linearity in this failure mode remains underexplored. We show that activation choice is a primary, architecture-agnostic lever for mitigating plasticity loss. Building on a property-level analysis of negative-branch shape and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky and Randomized Smooth-Leaky) and evaluate them in two complementary settings: (i) supervised class-incremental benchmarks and (ii) reinforcement learning with non-stationary MuJoCo environments designed to induce controlled distribution and dynamics shifts. We also provide a simple stress protocol and diagnostics that link the shape of the activation to the adaptation under change. The takeaway is straightforward: thoughtful activation design offers a lightweight, domain-general way to sustain plasticity in continual learning without extra capacity or task-specific tuning.",
    "published": "2025-09-26T16:41:47Z",
    "updated": "2026-01-27T16:19:30Z",
    "link": "http://arxiv.org/pdf/2509.22562v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Lute Lillo",
      "Nick Cheney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16846v4",
    "title": "BASIL: Bayesian Assessment of Sycophancy in LLMs",
    "summary": "Sycophancy (overly agreeable or flattering behavior) poses a fundamental challenge for human-AI collaboration, particularly in high-stakes decision-making domains such as health, law, and education. A central difficulty in studying sycophancy in large language models (LLMs) is disentangling sycophantic belief shifts from rational changes in behavior driven by new evidence or user-provided information. Existing approaches either measure descriptive behavior changes or apply normative evaluations that rely on objective ground truth, limiting their applicability to subjective or uncertain tasks. We introduce a Bayesian probabilistic framework, grounded in behavioral economics and rational decision theory, that explicitly separates sycophancy from rational belief updating. Within this framework, we achieve three objectives: (i) a descriptive metric that measures sycophancy while controlling for rational responses to evidence; (ii) a normative metric that quantifies how sycophancy leads models astray from Bayesian-consistent belief updating; and (iii) the ability to apply both metrics in settings without ground-truth labels. Applying our framework across multiple LLMs and three uncertainty-driven tasks, we find robust evidence of sycophantic belief shifts and show that their impact on rationality depends on whether models systematically over- or under-update their beliefs. Finally, we demonstrate that a post-hoc calibration method and two fine-tuning strategies (SFT and DPO) substantially reduce Bayesian inconsistency, with particularly strong improvements under explicit sycophancy prompting.",
    "published": "2025-08-23T00:11:00Z",
    "updated": "2026-01-27T16:15:18Z",
    "link": "http://arxiv.org/pdf/2508.16846v4.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Katherine Atwell",
      "Pedram Heydari",
      "Anthony Sicilia",
      "Malihe Alikhani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19752v1",
    "title": "Agentic Design Patterns: A System-Theoretic Framework",
    "summary": "With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.",
    "published": "2026-01-27T16:14:08Z",
    "updated": "2026-01-27T16:14:08Z",
    "link": "http://arxiv.org/pdf/2601.19752v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Minh-Dung Dao",
      "Quy Minh Le",
      "Hoang Thanh Lam",
      "Duc-Trong Le",
      "Quoc-Viet Pham",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19747v1",
    "title": "Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation",
    "summary": "In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.",
    "published": "2026-01-27T16:10:23Z",
    "updated": "2026-01-27T16:10:23Z",
    "link": "http://arxiv.org/pdf/2601.19747v1.pdf",
    "category": [
      "cs.AR",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Jiale Liu",
      "Taiyu Zhou",
      "Tianqi Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.13474v2",
    "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning",
    "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited \"out-of-the-box\" capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.",
    "published": "2025-06-16T13:32:01Z",
    "updated": "2026-01-27T16:05:43Z",
    "link": "http://arxiv.org/pdf/2506.13474v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "David Bani-Harouni",
      "Chantal Pellegrini",
      "Ege Özsoy",
      "Matthias Keicher",
      "Nassir Navab"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.14056v2",
    "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
    "summary": "Recent work in continual learning has highlighted the stability gap -- a temporary performance drop on previously learned tasks when new ones are introduced. This phenomenon reflects a mismatch between rapid adaptation and strong retention at task boundaries, underscoring the need for optimization mechanisms that balance plasticity and stability over abrupt distribution changes. While optimizers such as momentum-SGD and Adam introduce implicit multi-timescale behavior, they still exhibit pronounced stability gaps. Importantly, these gaps persist even under ideal joint training, making it crucial to study them in this setting to isolate their causes from other sources of forgetting. Motivated by how noradrenergic (neuromodulatory) bursts transiently increase neuronal gain under uncertainty, we introduce a dynamic gain scaling mechanism as a two-timescale optimization technique that balances adaptation and retention by modulating effective learning rates and flattening the local landscape through an effective reparameterization. Across domain- and class-incremental MNIST, CIFAR, and mini-ImageNet benchmarks under task-agnostic joint training, dynamic gain scaling effectively attenuates stability gaps while maintaining competitive accuracy, improving robustness at task transitions.",
    "published": "2025-07-18T16:34:06Z",
    "updated": "2026-01-27T16:05:29Z",
    "link": "http://arxiv.org/pdf/2507.14056v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "authors": [
      "Alejandro Rodriguez-Garcia",
      "Anindya Ghosh",
      "Srikanth Ramaswamy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.02623v4",
    "title": "Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models",
    "summary": "A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness.",
    "published": "2025-03-04T13:48:50Z",
    "updated": "2026-01-27T16:03:47Z",
    "link": "http://arxiv.org/pdf/2503.02623v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "David Bani-Harouni",
      "Chantal Pellegrini",
      "Paul Stangel",
      "Ege Özsoy",
      "Kamilia Zaripova",
      "Matthias Keicher",
      "Nassir Navab"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19739v1",
    "title": "TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching",
    "summary": "Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: https://runjia.tech/iclr_tokenseek/",
    "published": "2026-01-27T15:58:36Z",
    "updated": "2026-01-27T15:58:36Z",
    "link": "http://arxiv.org/pdf/2601.19739v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Runjia Zeng",
      "Qifan Wang",
      "Qiang Guan",
      "Ruixiang Tang",
      "Lifu Huang",
      "Zhenting Wang",
      "Xueling Zhang",
      "Cheng Han",
      "Dongfang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19738v1",
    "title": "Quantum Circuit Pre-Synthesis: Learning Local Edits to Reduce $T$-count",
    "summary": "Compiling quantum circuits into Clifford+$T$ gates is a central task for fault-tolerant quantum computing using stabilizer codes. In the near term, $T$ gates will dominate the cost of fault tolerant implementations, and any reduction in the number of such expensive gates could mean the difference between being able to run a circuit or not. While exact synthesis is exponentially hard in the number of qubits, local synthesis approaches are commonly used to compile large circuits by decomposing them into substructures. However, composing local methods leads to suboptimal compilations in key metrics such as $T$-count or circuit depth, and their performance strongly depends on circuit representation. In this work, we address this challenge by proposing \\textsc{Q-PreSyn}, a strategy that, given a set of local edits preserving circuit equivalence, uses a RL agent to identify effective sequences of such actions and thereby obtain circuit representations that yield a reduced $T$-count upon synthesis. Experimental results of our proposed strategy, applied on top of well-known synthesis algorithms, show up to a $20\\%$ reduction in $T$-count on circuits with up to 25 qubits, without introducing any additional approximation error prior to synthesis.",
    "published": "2026-01-27T15:58:05Z",
    "updated": "2026-01-27T15:58:05Z",
    "link": "http://arxiv.org/pdf/2601.19738v1.pdf",
    "category": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Daniele Lizzio Bosco",
      "Lukasz Cincio",
      "Giuseppe Serra",
      "M. Cerezo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11478v2",
    "title": "Designing and Evaluating a Conversational Agent for Early Diagnosis of Alzheimer's Disease and Related Dementias",
    "summary": "Early diagnosis of Alzheimer's disease and related dementias (ADRD) is critical for timely intervention, yet most diagnoses are delayed until advanced stages. While comprehensive patient narratives are essential for accurate diagnosis, prior work has largely focused on screening studies that classify cognitive status from interactions rather than supporting the diagnostic process. We designed voice-interactive conversational agents, leveraging large language models (LLMs), to elicit narratives relevant to ADRD from patients and informants. We evaluated the agent with 30 adults with suspected ADRD through conversation analysis, user surveys, and analysis of symptom elicitation compared to blinded specialist interviews. Symptoms detected by the agent showed promising agreement with those identified by specialists. Users appreciated the agent's patience and systematic questioning, which supported engagement and expression of complex, hard-to-describe experiences. While these findings suggest potential for conversational agents as structured diagnostic support tools, further validation with larger samples and assessment of clinical utility is needed before deployment.",
    "published": "2025-09-14T23:55:01Z",
    "updated": "2026-01-27T15:50:29Z",
    "link": "http://arxiv.org/pdf/2509.11478v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Andrew G. Breithaupt",
      "Nayoung Choi",
      "James D. Finch",
      "Jeanne M. Powell",
      "Arin L. Nelson",
      "Oz A. Alon",
      "Howard J. Rosen",
      "Jinho D. Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19726v1",
    "title": "RvB: Automating AI System Hardening via Iterative Red-Blue Games",
    "summary": "The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\\% and 45\\% across the respective tasks while maintaining near 0\\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.",
    "published": "2026-01-27T15:49:58Z",
    "updated": "2026-01-27T15:49:58Z",
    "link": "http://arxiv.org/pdf/2601.19726v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Lige Huang",
      "Zicheng Liu",
      "Jie Zhang",
      "Lewen Yan",
      "Dongrui Liu",
      "Jing Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19723v1",
    "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
    "summary": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.",
    "published": "2026-01-27T15:47:22Z",
    "updated": "2026-01-27T15:47:22Z",
    "link": "http://arxiv.org/pdf/2601.19723v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yifan Wang",
      "Jichen Zheng",
      "Jingyuan Sun",
      "Yunhao Zhang",
      "Chunyu Ye",
      "Jixing Li",
      "Chengqing Zong",
      "Shaonan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19709v1",
    "title": "Hyperbolic Additive Margin Softmax with Hierarchical Information for Speaker Verification",
    "summary": "Speaker embedding learning based on Euclidean space has achieved significant progress, but it is still insufficient in modeling hierarchical information within speaker features. Hyperbolic space, with its negative curvature geometric properties, can efficiently represent hierarchical information within a finite volume, making it more suitable for the feature distribution of speaker embeddings. In this paper, we propose Hyperbolic Softmax (H-Softmax) and Hyperbolic Additive Margin Softmax (HAM-Softmax) based on hyperbolic space. H-Softmax incorporates hierarchical information into speaker embeddings by projecting embeddings and speaker centers into hyperbolic space and computing hyperbolic distances. HAM-Softmax further enhances inter-class separability by introducing margin constraint on this basis. Experimental results show that H-Softmax and HAM-Softmax achieve average relative EER reductions of 27.84% and 14.23% compared with standard Softmax and AM-Softmax, respectively, demonstrating that the proposed methods effectively improve speaker verification performance and at the same time preserve the capability of hierarchical structure modeling. The code will be released at https://github.com/PunkMale/HAM-Softmax.",
    "published": "2026-01-27T15:33:47Z",
    "updated": "2026-01-27T15:33:47Z",
    "link": "http://arxiv.org/pdf/2601.19709v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Zhihua Fang",
      "Liang He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09014v4",
    "title": "MSCCL++: Rethinking GPU Communication Abstractions for AI Inference",
    "summary": "AI applications increasingly run on fast-evolving, heterogeneous hardware to maximize performance, but general-purpose libraries lag in supporting these features. Performance-minded programmers often build custom communication stacks that are fast but error-prone and non-portable.\n  This paper introduces MSCCL++, a design methodology for developing high-performance, portable communication kernels. It provides (1) a low-level, performance-preserving primitive interface that exposes minimal hardware abstractions while hiding the complexities of synchronization and consistency, (2) a higher-level DSL for application developers to implement workload-specific communication algorithms, and (3) a library of efficient algorithms implementing the standard collective API, enabling adoption by users with minimal expertise.\n  Compared to state-of-the-art baselines, MSCCL++ achieves geomean speedups of $1.7\\times$ (up to $5.4\\times$) for collective communication and $1.2\\times$ (up to $1.38\\times$) for AI inference workloads. MSCCL++ is in production of multiple AI services provided by Microsoft Azure, and has also been adopted by RCCL, the GPU collective communication library maintained by AMD. MSCCL++ is open source and available at https://github.com/microsoft/mscclpp . Our two years of experience with MSCCL++ suggests that its abstractions are robust, enabling support for new hardware features, such as multimem, within weeks of development.",
    "published": "2025-04-11T23:51:54Z",
    "updated": "2026-01-27T15:31:47Z",
    "link": "http://arxiv.org/pdf/2504.09014v4.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Changho Hwang",
      "Peng Cheng",
      "Roshan Dathathri",
      "Abhinav Jangda",
      "Saeed Maleki",
      "Madan Musuvathi",
      "Olli Saarikivi",
      "Aashaka Shah",
      "Ziyue Yang",
      "Binyang Li",
      "Caio Rocha",
      "Qinghua Zhou",
      "Mahdieh Ghazimirsaeed",
      "Sreevatsa Anantharamu",
      "Jithin Jose"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19702v1",
    "title": "SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation",
    "summary": "The performance evaluation remains a complex challenge in audio separation, and existing evaluation metrics are often misaligned with human perception, course-grained, relying on ground truth signals. On the other hand, subjective listening tests remain the gold standard for real-world evaluation, but they are expensive, time-consuming, and difficult to scale. This paper addresses the growing need for automated systems capable of evaluating audio separation without human intervention. The proposed evaluation metric, SAM Audio Judge (SAJ), is a multimodal fine-grained reference-free objective metric, which shows highly alignment with human perceptions. SAJ supports three audio domains (speech, music and general sound events) and three prompt inputs (text, visual and span), covering four different dimensions of evaluation (recall, percision, faithfulness, and overall). SAM Audio Judge also shows potential applications in data filtering, pseudo-labeling large datasets and reranking in audio separation models. We release our code and pre-trained models at: https://github.com/facebookresearch/sam-audio.",
    "published": "2026-01-27T15:29:02Z",
    "updated": "2026-01-27T15:29:02Z",
    "link": "http://arxiv.org/pdf/2601.19702v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Helin Wang",
      "Bowen Shi",
      "Andros Tjandra",
      "John Hoffman",
      "Yi-Chiao Wu",
      "Apoorv Vyas",
      "Najim Dehak",
      "Ann Lee",
      "Wei-Ning Hsu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19700v1",
    "title": "Out-of-Distribution Generalization via Invariant Trajectories for Multimodal Large Language Model Editing",
    "summary": "Knowledge editing emerges as a crucial technique for efficiently correcting incorrect or outdated knowledge in large language models (LLM). Existing editing methods for unimodal LLM rely on a rigid parameter-to-output mapping, which causes causal-underfit and causal-overfit in cascaded reasoning for Multimodal LLM (MLLM). In this paper, we reformulate MLLM editing as an out-of-distribution (OOD) generalization problem, where the goal is to discern semantic shift with factual shift and thus achieve robust editing among diverse cross-modal prompting. The key challenge of this OOD problem lies in identifying invariant causal trajectories that generalize accurately while suppressing spurious correlations. To address it, we propose ODEdit, a plug-and-play invariant learning based framework that optimizes the tripartite OOD risk objective to simultaneously enhance editing reliability, locality, and generality.We further introduce an edit trajectory invariant learning method, which integrates a total variation penalty into the risk minimization objective to stabilize edit trajectories against environmental variations. Theoretical analysis and extensive experiments demonstrate the effectiveness of ODEdit.",
    "published": "2026-01-27T15:25:07Z",
    "updated": "2026-01-27T15:25:07Z",
    "link": "http://arxiv.org/pdf/2601.19700v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiajie Su",
      "Haoyuan Wang",
      "Xiaohua Feng",
      "Yunshan Ma",
      "Xiaobo Xia",
      "Yuyuan Li",
      "Xiaolin Zheng",
      "Jianmao Xiao",
      "Chaochao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21961v4",
    "title": "Entropy-Gated Branching for Efficient Test-Time Reasoning",
    "summary": "Test-time compute methods can significantly improve the reasoning capabilities and problem-solving accuracy of large language models (LLMs). However, these approaches require substantially more computational resources, with most compute wasted on exploring low-diversity branches where the model already exhibits high confidence. We observe that a small subset of uncertain reasoning steps has a disproportionately large impact on final prediction accuracy, and branching at these critical junctures tends to yield more diverse and higher-quality candidate reasoning steps. We propose Entropy-Gated Branching (EGB), which branches only at high-uncertainty steps and prunes expansions with a lightweight verifier. On mathematical and financial reasoning benchmarks, EGB improves accuracy by 22.6% over standard inference while operating 31%-75% faster across math benchmarks than test-time beam search with higher performance. Our results show that dynamic resource allocation during inference can substantially improve both efficiency and effectiveness, offering a more scalable pathway to enhanced LLM reasoning capabilities.",
    "published": "2025-03-27T20:18:22Z",
    "updated": "2026-01-27T15:24:36Z",
    "link": "http://arxiv.org/pdf/2503.21961v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Xianzhi Li",
      "Ethan Callanan",
      "Abdellah Ghassel",
      "Xiaodan Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19697v1",
    "title": "AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion",
    "summary": "Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.",
    "published": "2026-01-27T15:23:14Z",
    "updated": "2026-01-27T15:23:14Z",
    "link": "http://arxiv.org/pdf/2601.19697v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Tianyue Jiang",
      "Yanli Wang",
      "Yanlin Wang",
      "Daya Guo",
      "Ensheng Shi",
      "Yuchi Ma",
      "Jiachi Chen",
      "Zibin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01858v2",
    "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents",
    "summary": "Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner",
    "published": "2025-08-03T17:17:52Z",
    "updated": "2026-01-27T15:19:33Z",
    "link": "http://arxiv.org/pdf/2508.01858v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuhan Guo",
      "Cong Guo",
      "Aiwen Sun",
      "Hongliang He",
      "Xinyu Yang",
      "Yue Lu",
      "Yingji Zhang",
      "Xuntao Guo",
      "Dong Zhang",
      "Jianzhuang Liu",
      "Jiang Duan",
      "Yijia Xiao",
      "Liangjian Wen",
      "Hai-Ming Xu",
      "Yong Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02032v2",
    "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges",
    "summary": "The rapid adoption of machine learning (ML) technologies has driven organizations across diverse sectors to seek efficient and reliable methods to accelerate model development-to-deployment. Machine Learning Operations (MLOps) has emerged as an integrative approach addressing these requirements by unifying relevant roles and streamlining ML workflows. As the MLOps market continues to grow, securing these pipelines has become increasingly critical. However, the unified nature of MLOps ecosystem introduces vulnerabilities, making them susceptible to adversarial attacks where a single misconfiguration can lead to compromised credentials, severe financial losses, damaged public trust, and the poisoning of training data. Our paper presents a systematic application of the MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) framework, supplemented by reviews of white and grey literature, to systematically assess attacks across different phases of the MLOps ecosystem. We begin by reviewing prior work in this domain, then present our taxonomy and introduce a threat model that captures attackers with different knowledge and capabilities. We then present a structured taxonomy of attack techniques explicitly mapped to corresponding phases of the MLOps ecosystem, supported by examples drawn from red-teaming exercises and real-world incidents. This is followed by a taxonomy of mitigation strategies aligned with these attack categories, offering actionable early-stage defenses to strengthen the security of MLOps ecosystem. Given the gradual evolution and adoption of MLOps, we further highlight key research gaps that require immediate attention. Our work emphasizes the importance of implementing robust security protocols from the outset, empowering practitioners to safeguard MLOps ecosystem against evolving cyber attacks.",
    "published": "2025-05-30T17:45:31Z",
    "updated": "2026-01-27T15:08:48Z",
    "link": "http://arxiv.org/pdf/2506.02032v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Raj Patel",
      "Himanshu Tripathi",
      "Jasper Stone",
      "Noorbakhsh Amiri Golilarz",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Vini Chaudhary"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19674v1",
    "title": "Cross-Domain Offshore Wind Power Forecasting: Transfer Learning Through Meteorological Clusters",
    "summary": "Ambitious decarbonisation targets are catalysing growth in orders of new offshore wind farms. For these newly commissioned plants to run, accurate power forecasts are needed from the onset. These allow grid stability, good reserve management and efficient energy trading. Despite machine learning models having strong performances, they tend to require large volumes of site-specific data that new farms do not yet have. To overcome this data scarcity, we propose a novel transfer learning framework that clusters power output according to covariate meteorological features. Rather than training a single, general-purpose model, we thus forecast with an ensemble of expert models, each trained on a cluster. As these pre-trained models each specialise in a distinct weather pattern, they adapt efficiently to new sites and capture transferable, climate-dependent dynamics. Through the expert models' built-in calibration to seasonal and meteorological variability, we remove the industry-standard requirement of local measurements over a year. Our contributions are two-fold - we propose this novel framework and comprehensively evaluate it on eight offshore wind farms, achieving accurate cross-domain forecasting with under five months of site-specific data. Our experiments achieve a MAE of 3.52\\%, providing empirical verification that reliable forecasts do not require a full annual cycle. Beyond power forecasting, this climate-aware transfer learning method opens new opportunities for offshore wind applications such as early-stage wind resource assessment, where reducing data requirements can significantly accelerate project development whilst effectively mitigating its inherent risks.",
    "published": "2026-01-27T14:54:27Z",
    "updated": "2026-01-27T14:54:27Z",
    "link": "http://arxiv.org/pdf/2601.19674v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ME"
    ],
    "authors": [
      "Dominic Weisser",
      "Chloé Hashimoto-Cullen",
      "Benjamin Guedj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19673v1",
    "title": "A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models",
    "summary": "The present benchmarks for testing the audio modality of multimodal large language models concentrate on testing various audio tasks such as speaker diarization or gender identification in isolation. Whether a multimodal model can answer the questions that require reasoning skills to combine audio tasks of different categories, cannot be verified with their use. To address this issue, we propose Audio Reasoning Tasks (ART), a new benchmark for assessing the ability of multimodal models to solve problems that require reasoning over audio signal.",
    "published": "2026-01-27T14:54:10Z",
    "updated": "2026-01-27T14:54:10Z",
    "link": "http://arxiv.org/pdf/2601.19673v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Iwona Christop",
      "Mateusz Czyżnikiewicz",
      "Paweł Skórzewski",
      "Łukasz Bondaruk",
      "Jakub Kubiak",
      "Marcin Lewandowski",
      "Marek Kubis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19672v1",
    "title": "ProToken: Token-Level Attribution for Federated Large Language Models",
    "summary": "Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.",
    "published": "2026-01-27T14:53:12Z",
    "updated": "2026-01-27T14:53:12Z",
    "link": "http://arxiv.org/pdf/2601.19672v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Waris Gill",
      "Ahmad Humayun",
      "Ali Anwar",
      "Muhammad Ali Gulzar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15197v4",
    "title": "LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose LangForce, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $π(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, LangForce significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
    "published": "2026-01-21T17:15:22Z",
    "updated": "2026-01-27T14:51:48Z",
    "link": "http://arxiv.org/pdf/2601.15197v4.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Shijie Lian",
      "Bin Yu",
      "Xiaopeng Lin",
      "Laurence T. Yang",
      "Zhaolong Shen",
      "Changti Wu",
      "Yuzhuo Miao",
      "Cong Huang",
      "Kai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19667v1",
    "title": "SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking",
    "summary": "We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.",
    "published": "2026-01-27T14:47:17Z",
    "updated": "2026-01-27T14:47:17Z",
    "link": "http://arxiv.org/pdf/2601.19667v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Adam Remaki",
      "Christel Gérardin",
      "Eulàlia Farré-Maduell",
      "Martin Krallinger",
      "Xavier Tannier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.03156v2",
    "title": "Prompt-Counterfactual Explanations for Generative AI System Behavior",
    "summary": "As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -- the prompt -- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.",
    "published": "2026-01-06T16:33:19Z",
    "updated": "2026-01-27T14:42:02Z",
    "link": "http://arxiv.org/pdf/2601.03156v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Sofie Goethals",
      "Foster Provost",
      "João Sedoc"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23597v2",
    "title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting",
    "summary": "Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models, suggesting that their robustness and interpretability warrant deeper theoretical investigation. This paper presents a systematic study of linear models for time series forecasting, with a focus on the role of characteristic roots in temporal dynamics. We begin by analyzing the noise-free setting, where we show that characteristic roots govern long-term behavior and explain how design choices such as instance normalization and channel independence affect model capabilities. We then extend our analysis to the noisy regime, revealing that models tend to produce spurious roots. This leads to the identification of a key data-scaling property: mitigating the influence of noise requires disproportionately large training data, highlighting the need for structural regularization. To address these challenges, we propose two complementary strategies for robust root restructuring. The first uses rank reduction techniques, including Reduced-Rank Regression and Direct Weight Rank Reduction, to recover the low-dimensional latent dynamics. The second, a novel adaptive method called Root Purge, encourages the model to learn a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate the effectiveness of both approaches, validating our theoretical insights and achieving state-of-the-art results in several settings. Our findings underscore the potential of integrating classical theories for linear systems with modern learning techniques to build robust, interpretable, and data-efficient forecasting models.",
    "published": "2025-09-28T03:06:30Z",
    "updated": "2026-01-27T14:36:37Z",
    "link": "http://arxiv.org/pdf/2509.23597v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zheng Wang",
      "Kaixuan Zhang",
      "Wanfang Chen",
      "Xiaonan Lu",
      "Longyuan Li",
      "Tobias Schlagenhauf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19657v1",
    "title": "One Token Is Enough: Improving Diffusion Language Models with a Sink Token",
    "summary": "Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.",
    "published": "2026-01-27T14:32:36Z",
    "updated": "2026-01-27T14:32:36Z",
    "link": "http://arxiv.org/pdf/2601.19657v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zihou Zhang",
      "Zheyong Xie",
      "Li Zhong",
      "Haifeng Liu",
      "Shaosheng Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.15600v2",
    "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism",
    "summary": "The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly.",
    "published": "2025-10-17T12:47:50Z",
    "updated": "2026-01-27T14:28:41Z",
    "link": "http://arxiv.org/pdf/2510.15600v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Haoran Sun",
      "Yankai Jiang",
      "Zhenyu Tang",
      "Yaning Pan",
      "Shuang Gu",
      "Zekai Lin",
      "Lilong Wang",
      "Wenjie Lou",
      "Lei Liu",
      "Lei Bai",
      "Xiaosong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19644v1",
    "title": "Robustness of Constraint Automata for Description Logics with Concrete Domains",
    "summary": "Decidability or complexity issues about the consistency problem for description logics with concrete domains have already been analysed with tableaux-based or type elimination methods. Concrete domains in ontologies are essential to consider concrete objects and predefined relations. In this work, we expose an automata-based approach leading to the optimal upper bound EXPTIME, that is designed by enriching the transitions with symbolic constraints. We show that the nonemptiness problem for such automata belongs to EXPTIME if the concrete domains satisfy a few simple properties. Then, we provide a reduction from the consistency problem for ontologies, yielding EXPTIME-membership.Thanks to the expressivity of constraint automata, the results are extended to additional ingredients such as inverse roles, functional role names and constraint assertions, while maintaining EXPTIME-membership, which illustrates the robustness of the approach",
    "published": "2026-01-27T14:19:56Z",
    "updated": "2026-01-27T14:19:56Z",
    "link": "http://arxiv.org/pdf/2601.19644v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI"
    ],
    "authors": [
      "Stéphane Demri",
      "Tianwen Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10802v4",
    "title": "MSCloudCAM: Multi-Scale Context Adaptation with Convolutional Cross-Attention for Multispectral Cloud Segmentation",
    "summary": "Clouds remain a major obstacle in optical satellite imaging, limiting accurate environmental and climate analysis. To address the strong spectral variability and the large scale differences among cloud types, we propose MSCloudCAM, a novel multi-scale context adapter network with convolution based cross-attention tailored for multispectral and multi-sensor cloud segmentation. A key contribution of MSCloudCAM is the explicit modeling of multiple complementary multi-scale context extractors. And also, rather than simply stacking or concatenating their outputs, our formulation uses one extractor's fine-resolution features and the other extractor's global contextual representations enabling dynamic, scale-aware feature selection. Building on this idea, we design a new convolution-based cross attention adapter that effectively fuses localized, detailed information with broader multi-scale context. Integrated with a hierarchical vision backbone and refined through channel and spatial attention mechanisms, MSCloudCAM achieves strong spectral-spatial discrimination. Experiments on various multisensor datatsets e.g. CloudSEN12 (Sentinel-2) and L8Biome (Landsat-8), demonstrate that MSCloudCAM achieves superior overall segmentation performance and competitive class-wise accuracy compared to recent state-of-the-art models, while maintaining competitive model complexity, highlighting the novelty and effectiveness of the proposed design for large-scale Earth observation.",
    "published": "2025-10-12T20:40:22Z",
    "updated": "2026-01-27T14:16:53Z",
    "link": "http://arxiv.org/pdf/2510.10802v4.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Md Abdullah Al Mazid",
      "Liangdong Deng",
      "Naphtali Rishe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04424v3",
    "title": "EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models",
    "summary": "With the integration of Multimodal large language models (MLLMs) into robotic systems and various AI applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. Existing static, text-based, or text-image benchmarks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for evaluating MLLMs' EI. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs on EmoBench-M reveal a significant performance gap between them and humans, highlighting the need to further advance their EI capabilities. All benchmark resources, including code and datasets, are publicly available at https://emo-gml.github.io/.",
    "published": "2025-02-06T18:13:35Z",
    "updated": "2026-01-27T14:10:59Z",
    "link": "http://arxiv.org/pdf/2502.04424v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "He Hu",
      "Yucheng Zhou",
      "Lianzhong You",
      "Hongbo Xu",
      "Qianning Wang",
      "Zheng Lian",
      "Fei Richard Yu",
      "Fei Ma",
      "Laizhong Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10350v4",
    "title": "Geometric Dynamics of Agentic Loops in Large Language Models",
    "summary": "Iterative LLM systems(self-refinement, chain-of-thought, autonomous agents) are increasingly deployed, yet their temporal dynamics remain uncharacterized. Prior work evaluates task performance at convergence but ignores the trajectory: how does semantic content evolve across iterations? Does it stabilize, drift, or oscillate? Without answering these questions, we cannot predict system behavior, guarantee stability, or systematically design iterative architectures.\n  We formalize agentic loops as discrete dynamical systems in semantic space. Borrowing from dynamical systems theory, we define trajectories, attractors and dynamical regimes for recursive LLM transformations, providing rigorous geometric definitions adapted to this setting. Our framework reveals that agentic loops exhibit classifiable dynamics: contractive (convergence toward stable semantic attractors), oscillatory (cycling among attractors), or exploratory (unbounded divergence).\n  Experiments on singular loops validate the framework. Iterative paraphrasing produces contractive dynamics with measurable attractor formation and decreasing dispersion. Iterative negation produces exploratory dynamics with no stable structure. Crucially, prompt design directly controls the dynamical regime - the same model exhibits fundamentally different geometric behaviors depending solely on the transformation applied.\n  This work establishes that iterative LLM dynamics are predictable and controllable, opening new directions for stability analysis, trajectory forecasting, and principled design of composite loops that balance convergence and exploration.",
    "published": "2025-12-11T07:06:14Z",
    "updated": "2026-01-27T14:01:32Z",
    "link": "http://arxiv.org/pdf/2512.10350v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nicolas Tacheny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09201v4",
    "title": "Learning to Detect Unseen Jailbreak Attacks in Large Vision-Language Models",
    "summary": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks. To mitigate these risks, existing detection methods are essential, yet they face two major challenges: generalization and accuracy. While learning-based methods trained on specific attacks fail to generalize to unseen attacks, learning-free methods based on hand-crafted heuristics suffer from limited accuracy and reduced efficiency. To address these limitations, we propose Learning to Detect (LoD), a learnable framework that eliminates the need for any attack data or hand-crafted heuristics. LoD operates by first extracting layer-wise safety representations directly from the model's internal activations using Multi-modal Safety Concept Activation Vectors classifiers, and then converting the high-dimensional representations into a one-dimensional anomaly score for detection via a Safety Pattern Auto-Encoder. Extensive experiments demonstrate that LoD consistently achieves state-of-the-art detection performance (AUROC) across diverse unseen jailbreak attacks on multiple LVLMs, while also significantly improving efficiency. Code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.",
    "published": "2025-08-08T16:13:28Z",
    "updated": "2026-01-27T13:58:13Z",
    "link": "http://arxiv.org/pdf/2508.09201v4.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Shuang Liang",
      "Zhihao Xu",
      "Jiaqi Weng",
      "Jialing Tao",
      "Hui Xue",
      "Xiting Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19624v1",
    "title": "Tracking Drift: Variation-Aware Entropy Scheduling for Non-Stationary Reinforcement Learning",
    "summary": "Real-world reinforcement learning often faces environment drift, but most existing methods rely on static entropy coefficients/target entropy, causing over-exploration during stable periods and under-exploration after drift (thus slow recovery), and leaving unanswered the principled question of how exploration intensity should scale with drift magnitude. We prove that entropy scheduling under non-stationarity can be reduced to a one-dimensional, round-by-round trade-off, faster tracking of the optimal solution after drift vs. avoiding gratuitous randomness when the environment is stable, so exploration strength can be driven by measurable online drift signals. Building on this, we propose AES (Adaptive Entropy Scheduling), which adaptively adjusts the entropy coefficient/temperature online using observable drift proxies during training, requiring almost no structural changes and incurring minimal overhead. Across 4 algorithm variants, 12 tasks, and 4 drift modes, AES significantly reduces the fraction of performance degradation caused by drift and accelerates recovery after abrupt changes.",
    "published": "2026-01-27T13:58:11Z",
    "updated": "2026-01-27T13:58:11Z",
    "link": "http://arxiv.org/pdf/2601.19624v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Tongxi Wang",
      "Zhuoyang Xia",
      "Xinran Chen",
      "Shan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19622v1",
    "title": "Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search",
    "summary": "Heuristic functions are essential to the performance of tree search algorithms such as A*, where their accuracy and efficiency directly impact search outcomes. Traditionally, such heuristics are handcrafted, requiring significant expertise. Recent advances in large language models (LLMs) and evolutionary frameworks have opened the door to automating heuristic design. In this paper, we extend the Evolution of Heuristics (EoH) framework to investigate the automated generation of guiding heuristics for A* search. We introduce a novel domain-agnostic prompt augmentation strategy that includes the A* code into the prompt to leverage in-context learning, named Algorithmic - Contextual EoH (A-CEoH). To evaluate the effectiveness of A-CeoH, we study two problem domains: the Unit-Load Pre-Marshalling Problem (UPMP), a niche problem from warehouse logistics, and the classical sliding puzzle problem (SPP). Our computational experiments show that A-CEoH can significantly improve the quality of the generated heuristics and even outperform expert-designed heuristics.",
    "published": "2026-01-27T13:55:58Z",
    "updated": "2026-01-27T13:55:58Z",
    "link": "http://arxiv.org/pdf/2601.19622v1.pdf",
    "category": [
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Thomas Bömer",
      "Nico Koltermann",
      "Max Disselnmeyer",
      "Bastian Amberg",
      "Anne Meyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19620v1",
    "title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning",
    "summary": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.",
    "published": "2026-01-27T13:55:34Z",
    "updated": "2026-01-27T13:55:34Z",
    "link": "http://arxiv.org/pdf/2601.19620v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhizheng Jiang",
      "Kang Zhao",
      "Weikai Xu",
      "Xinkui Lin",
      "Wei Liu",
      "Jian Luan",
      "Shuo Shang",
      "Peng Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19618v1",
    "title": "The role of self-supervised pretraining in differentially private medical image analysis",
    "summary": "Differential privacy (DP) provides formal protection for sensitive data but typically incurs substantial losses in diagnostic performance. Model initialization has emerged as a critical factor in mitigating this degradation, yet the role of modern self-supervised learning under full-model DP remains poorly understood. Here, we present a large-scale evaluation of initialization strategies for differentially private medical image analysis, using chest radiograph classification as a representative benchmark with more than 800,000 images. Using state-of-the-art ConvNeXt models trained with DP-SGD across realistic privacy regimes, we compare non-domain-specific supervised ImageNet initialization, non-domain-specific self-supervised DINOv3 initialization, and domain-specific supervised pretraining on MIMIC-CXR, the largest publicly available chest radiograph dataset. Evaluations are conducted across five external datasets spanning diverse institutions and acquisition settings. We show that DINOv3 initialization consistently improves diagnostic utility relative to ImageNet initialization under DP, but remains inferior to domain-specific supervised pretraining, which achieves performance closest to non-private baselines. We further demonstrate that initialization choice strongly influences demographic fairness, cross-dataset generalization, and robustness to data scale and model capacity under privacy constraints. The results establish initialization strategy as a central determinant of utility, fairness, and generalization in differentially private medical imaging.",
    "published": "2026-01-27T13:50:43Z",
    "updated": "2026-01-27T13:50:43Z",
    "link": "http://arxiv.org/pdf/2601.19618v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Soroosh Tayebi Arasteh",
      "Mina Farajiamiri",
      "Mahshad Lotfinia",
      "Behrus Hinrichs-Puladi",
      "Jonas Bienzeisler",
      "Mohamed Alhaskir",
      "Mirabela Rusu",
      "Christiane Kuhl",
      "Sven Nebelung",
      "Daniel Truhn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.09342v2",
    "title": "Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework",
    "summary": "This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.",
    "published": "2026-01-14T10:20:32Z",
    "updated": "2026-01-27T13:48:59Z",
    "link": "http://arxiv.org/pdf/2601.09342v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Ewelina Gajewska",
      "Katarzyna Budzynska",
      "Jarosław A Chudziak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08323v2",
    "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
    "summary": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.",
    "published": "2026-01-13T08:22:28Z",
    "updated": "2026-01-27T13:47:50Z",
    "link": "http://arxiv.org/pdf/2601.08323v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yupeng Huo",
      "Yaxi Lu",
      "Zhong Zhang",
      "Haotian Chen",
      "Yankai Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19613v1",
    "title": "Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs",
    "summary": "Key Information Extraction (KIE) from visually-rich documents (VrDs) is a critical task, for which recent Large Language Models (LLMs) and Multi-Modal Large Language Models (MLLMs) have demonstrated strong potential. However, their reliance on autoregressive inference, which generates outputs sequentially, creates a significant efficiency bottleneck, especially as KIE tasks often involve extracting multiple, semantically independent fields. To overcome this limitation, we introduce PIP: a Parallel Inference Paradigm for KIE. Our approach reformulates the problem by using \"[mask]\" tokens as placeholders for all target values, enabling their simultaneous generation in a single forward pass. To facilitate this paradigm, we develop a tailored mask pre-training strategy and construct large-scale supervised datasets. Experimental results show that our PIP-models achieve a 5-36x inference speedup with negligible performance degradation compared to traditional autoregressive base models. By substantially improving efficiency while maintaining high accuracy, PIP paves the way for scalable and practical real-world KIE solutions.",
    "published": "2026-01-27T13:45:30Z",
    "updated": "2026-01-27T13:45:30Z",
    "link": "http://arxiv.org/pdf/2601.19613v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Xinzhong Wang",
      "Ya Guo",
      "Jing Li",
      "Huan Chen",
      "Yi Tu",
      "Yijie Hong",
      "Gongshen Liu",
      "Huijia Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19612v1",
    "title": "Safe Exploration via Policy Priors",
    "summary": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.",
    "published": "2026-01-27T13:45:28Z",
    "updated": "2026-01-27T13:45:28Z",
    "link": "http://arxiv.org/pdf/2601.19612v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Manuel Wendl",
      "Yarden As",
      "Manish Prajapat",
      "Anton Pollak",
      "Stelian Coros",
      "Andreas Krause"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19611v1",
    "title": "Explicit Multi-head Attention for Inter-head Interaction in Large Language Models",
    "summary": "In large language models built upon the Transformer architecture, recent studies have shown that inter-head interaction can enhance attention performance. Motivated by this, we propose Multi-head Explicit Attention (MEA), a simple yet effective attention variant that explicitly models cross-head interaction. MEA consists of two key components: a Head-level Linear Composition (HLC) module that separately applies learnable linear combinations to the key and value vectors across heads, thereby enabling rich inter-head communication; and a head-level Group Normalization layer that aligns the statistical properties of the recombined heads. MEA shows strong robustness in pretraining, which allows the use of larger learning rates that lead to faster convergence, ultimately resulting in lower validation loss and improved performance across a range of tasks. Furthermore, we explore the parameter efficiency of MEA by reducing the number of attention heads and leveraging HLC to reconstruct them using low-rank \"virtual heads\". This enables a practical key-value cache compression strategy that reduces KV-cache memory usage by 50% with negligible performance loss on knowledge-intensive and scientific reasoning tasks, and only a 3.59% accuracy drop for Olympiad-level mathematical benchmarks.",
    "published": "2026-01-27T13:45:03Z",
    "updated": "2026-01-27T13:45:03Z",
    "link": "http://arxiv.org/pdf/2601.19611v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Runyu Peng",
      "Yunhua Zhou",
      "Demin Song",
      "Kai Lv",
      "Bo Wang",
      "Qipeng Guo",
      "Xipeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19607v1",
    "title": "ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks",
    "summary": "Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and verification capabilities. To address this, we present ComAgent, a multi-LLM agentic AI framework. ComAgent employs a closed-loop Perception-Planning-Action-Reflection cycle, coordinating specialized agents for literature search, coding, and scoring to autonomously generate solver-ready formulations and reproducible simulations. By iteratively decomposing problems and self-correcting errors, the framework effectively bridges the gap between user intent and execution. Evaluations demonstrate that ComAgent achieves expert-comparable performance in complex beamforming optimization and outperforms monolithic LLMs across diverse wireless tasks, highlighting its potential for automating design in emerging wireless networks.",
    "published": "2026-01-27T13:43:59Z",
    "updated": "2026-01-27T13:43:59Z",
    "link": "http://arxiv.org/pdf/2601.19607v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Haoyun Li",
      "Ming Xiao",
      "Kezhi Wang",
      "Robert Schober",
      "Dong In Kim",
      "Yong Liang Guan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19606v1",
    "title": "GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining",
    "summary": "Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.",
    "published": "2026-01-27T13:43:32Z",
    "updated": "2026-01-27T13:43:32Z",
    "link": "http://arxiv.org/pdf/2601.19606v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Shentong Mo",
      "Zehua Chen",
      "Jun Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13687v2",
    "title": "Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue",
    "summary": "Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.",
    "published": "2026-01-20T07:41:26Z",
    "updated": "2026-01-27T13:35:01Z",
    "link": "http://arxiv.org/pdf/2601.13687v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhichao Liang",
      "Satoshi Nakamura"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19595v1",
    "title": "Intersectional Fairness via Mixed-Integer Optimization",
    "summary": "The deployment of Artificial Intelligence in high-risk domains, such as finance and healthcare, necessitates models that are both fair and transparent. While regulatory frameworks, including the EU's AI Act, mandate bias mitigation, they are deliberately vague about the definition of bias. In line with existing research, we argue that true fairness requires addressing bias at the intersections of protected groups. We propose a unified framework that leverages Mixed-Integer Optimization (MIO) to train intersectionally fair and intrinsically interpretable classifiers. We prove the equivalence of two measures of intersectional fairness (MSD and SPSF) in detecting the most unfair subgroup and empirically demonstrate that our MIO-based algorithm improves performance in finding bias. We train high-performing, interpretable classifiers that bound intersectional bias below an acceptable threshold, offering a robust solution for regulated industries and beyond.",
    "published": "2026-01-27T13:29:25Z",
    "updated": "2026-01-27T13:29:25Z",
    "link": "http://arxiv.org/pdf/2601.19595v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Jiří Němeček",
      "Mark Kozdoba",
      "Illia Kryvoviaz",
      "Tomáš Pevný",
      "Jakub Mareček"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17756v2",
    "title": "MV-S2V: Multi-View Subject-Consistent Video Generation",
    "summary": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at: https://szy-young.github.io/mv-s2v",
    "published": "2026-01-25T09:02:33Z",
    "updated": "2026-01-27T13:24:40Z",
    "link": "http://arxiv.org/pdf/2601.17756v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "authors": [
      "Ziyang Song",
      "Xinyu Gong",
      "Bangya Liu",
      "Zelin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19588v1",
    "title": "From Atoms to Chains: Divergence-Guided Reasoning Curriculum for Unlabeled LLM Domain Adaptation",
    "summary": "Adapting Large Language Models (LLMs) to specialized domains without human-annotated data is a crucial yet formidable challenge. Widely adopted knowledge distillation methods often devolve into coarse-grained mimicry, where the student model inefficiently targets its own weaknesses and risks inheriting the teacher's reasoning flaws. This exposes a critical pedagogical dilemma: how to devise a reliable curriculum when the teacher itself is not an infallible expert. Our work resolves this by capitalizing on a key insight: while LLMs may exhibit fallibility in complex, holistic reasoning, they often exhibit high fidelity on focused, atomic sub-problems. Based on this, we propose Divergence-Guided Reasoning Curriculum (DGRC), which constructs a learning path from atomic knowledge to reasoning chains by dynamically deriving two complementary curricula from disagreements in reasoning pathways. When a student and teacher produce conflicting results, DGRC directs the teacher to perform a diagnostic analysis: it analyzes both reasoning paths to formulate atomic queries that target the specific points of divergence, and then self-answers these queries to create high-confidence atomic question-answer pairs. These pairs then serve a dual purpose: (1) providing an atomic curriculum to rectify the student's knowledge gaps, and (2) serving as factual criteria to filter the teacher's original reasoning chains, yielding a verified CoT curriculum that teaches the student how to integrate atomic knowledge into complete reasoning paths. Experiments across the medical and legal domains on student models of various sizes demonstrate the effectiveness of our DGRC framework. Notably, our method achieves a 7.76% relative improvement for the 1.5B student model in the medical domain over strong unlabeled baseline.",
    "published": "2026-01-27T13:23:40Z",
    "updated": "2026-01-27T13:23:40Z",
    "link": "http://arxiv.org/pdf/2601.19588v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yongqi Wang",
      "Xiaofeng Ji",
      "Jie Wang",
      "Qingbin Li",
      "Xiao Xiong",
      "Zheming Yang",
      "Jian Xu",
      "Minghui Qiu",
      "Xinxiao Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19585v1",
    "title": "LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation",
    "summary": "Interactive recommender systems can dynamically adapt to user feedback, but often suffer from content homogeneity and filter bubble effects due to overfitting short-term user preferences. While recent efforts aim to improve content diversity, they predominantly operate in static or one-shot settings, neglecting the long-term evolution of user interests. Reinforcement learning provides a principled framework for optimizing long-term user satisfaction by modeling sequential decision-making processes. However, its application in recommendation is hindered by sparse, long-tailed user-item interactions and limited semantic planning capabilities. In this work, we propose LLM-Enhanced Reinforcement Learning (LERL), a novel hierarchical recommendation framework that integrates the semantic planning power of LLM with the fine-grained adaptability of RL. LERL consists of a high-level LLM-based planner that selects semantically diverse content categories, and a low-level RL policy that recommends personalized items within the selected semantic space. This hierarchical design narrows the action space, enhances planning efficiency, and mitigates overexposure to redundant content. Extensive experiments on real-world datasets demonstrate that LERL significantly improves long-term user satisfaction when compared with state-of-the-art baselines. The implementation of LERL is available at https://anonymous.4open.science/r/code3-18D3/.",
    "published": "2026-01-27T13:22:30Z",
    "updated": "2026-01-27T13:22:30Z",
    "link": "http://arxiv.org/pdf/2601.19585v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Chongjun Xia",
      "Yanchun Peng",
      "Xianzhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26201v2",
    "title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing",
    "summary": "Large Language Models (LLMs) have garnered significant attention for several years now. Recently, their use as independently reasoning agents has been proposed. In this work, we test the potential of such agents for knowledge discovery in materials science. We repurpose LangGraph's tool functionality to supply agents with a black box function to interrogate. In contrast to process optimization or performing specific, user-defined tasks, knowledge discovery consists of freely exploring the system, posing and verifying statements about the behavior of this black box, with the sole objective of generating and verifying generalizable statements. We provide proof of concept for this approach through a children's parlor game, demonstrating the role of trial-and-error and persistence in knowledge discovery, and the strong path-dependence of results. We then apply the same strategy to show that LLM agents can explore, discover, and exploit diverse chemical interactions in an advanced Atomic Layer Processing reactor simulation using intentionally limited probe capabilities without explicit instructions.",
    "published": "2025-09-30T13:01:44Z",
    "updated": "2026-01-27T13:15:05Z",
    "link": "http://arxiv.org/pdf/2509.26201v2.pdf",
    "category": [
      "cs.AI",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "authors": [
      "Andreas Werbrouck",
      "Marshall B. Lindsay",
      "Matthew Maschmann",
      "Matthias J. Young"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19514v4",
    "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback",
    "summary": "Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.",
    "published": "2025-05-26T04:56:48Z",
    "updated": "2026-01-27T13:12:54Z",
    "link": "http://arxiv.org/pdf/2505.19514v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yaoning Yu",
      "Ye Yu",
      "Peiyan Zhang",
      "Kai Wei",
      "Haojing Luo",
      "Haohan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03785v2",
    "title": "SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons",
    "summary": "Recent advances in artificial intelligence (AI), in particular foundation models, have improved the state of the art in many application domains including geosciences. Some specific problems, however, could not benefit from this progress yet. Soil horizon classification, for instance, remains challenging because of its multimodal and multitask characteristics and a complex hierarchically structured label taxonomy. Accurate classification of soil horizons is crucial for monitoring soil condition. In this work, we propose \\textit{SoilNet} - a multimodal multitask model to tackle this problem through a structured modularized pipeline. In contrast to omnipurpose AI foundation models, our approach is designed to be inherently transparent by following the task structure human experts developed for solving this challenging annotation task. The proposed approach integrates image data and geotemporal metadata to first predict depth markers, segmenting the soil profile into horizon candidates. Each segment is characterized by a set of horizon-specific morphological features. Finally, horizon labels are predicted based on the multimodal concatenated feature vector, leveraging a graph-based label representation to account for the complex hierarchical relationships among soil horizons. Our method is designed to address complex hierarchical classification, where the number of possible labels is very large, imbalanced and non-trivially structured. We demonstrate the effectiveness of our approach on a real-world soil profile dataset and a comprehensive user study with domain experts. Our empirical evaluations demonstrate that SoilNet reliably predicts soil horizons that are plausible and accurate. User study results indicate that SoilNet achieves predictive performance on par with or better than that of human experts. All code can be found at: https://github.com/calgo-lab/BGR/",
    "published": "2025-08-05T15:29:57Z",
    "updated": "2026-01-27T13:06:52Z",
    "link": "http://arxiv.org/pdf/2508.03785v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Vipin Singh",
      "Teodor Chiaburu",
      "Einar Eberhardt",
      "Stefan Broda",
      "Joey Prüssing",
      "Frank Haußer",
      "Felix Bießmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19568v1",
    "title": "Learning Adaptive Parallel Execution for Efficient Code Localization",
    "summary": "Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\\% redundant invocation rate, which negates parallelism benefits. We propose \\textbf{FuseSearch}, reformulating parallel code localization as a \\textbf{joint quality-efficiency optimization} task. Through defining \\textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\\% file-level and 56.4\\% function-level $F_1$ scores) with 93.6\\% speedup, utilizing 67.7\\% fewer turns and 68.9\\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.",
    "published": "2026-01-27T12:59:31Z",
    "updated": "2026-01-27T12:59:31Z",
    "link": "http://arxiv.org/pdf/2601.19568v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Ke Xu",
      "Siyang Xiao",
      "Ming Liang",
      "Yichen Yu",
      "Zhixiang Wang",
      "Jingxuan Xu",
      "Dajun Chen",
      "Wei Jiang",
      "Yong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07981v3",
    "title": "Why is Your Language Model a Poor Implicit Reward Model?",
    "summary": "Reward models are key to language model post-training and inference pipelines. Conveniently, recent work showed that every language model defines an implicit reward model (IM-RM), without requiring any architectural changes. However, such IM-RMs tend to generalize worse, especially out-of-distribution, compared to explicit reward models (EX-RMs) that apply a dedicated linear head over the hidden representations of a language model. The existence of a generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They can be trained using the same data, loss function, and language model, and differ only in how the reward is computed. Toward a fundamental understanding of the implicit biases underlying different reward model types, we investigate the root cause of this gap. Our main finding, backed by theory and experiments, is that IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts, as well as in-distribution. Furthermore, we provide evidence against alternative hypotheses for the generalization gap. Most notably, we challenge the claim that IM-RMs struggle in tasks where generation is harder than verification because they can operate both as a verifier and a generator. Overall, our results highlight that seemingly minor design choices can substantially impact the generalization behavior of reward models.",
    "published": "2025-07-10T17:55:05Z",
    "updated": "2026-01-27T12:59:25Z",
    "link": "http://arxiv.org/pdf/2507.07981v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Noam Razin",
      "Yong Lin",
      "Jiarui Yao",
      "Sanjeev Arora"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2304.13894v2",
    "title": "CNN-based IoT Device Identification: A Comparative Study on Payload vs. Fingerprint",
    "summary": "The proliferation of the Internet of Things (IoT) has introduced a massive influx of devices into the market, bringing with them significant security vulnerabilities. In this diverse ecosystem, robust IoT device identification is a critical preventive measure for network security and vulnerability management. This study proposes a deep learning-based method to identify IoT devices using the Aalto dataset. We employ Convolutional Neural Networks (CNN) to classify devices by converting network packet payloads into pseudo-images. Furthermore, we compare the performance of this payload-based approach against a feature-based fingerprinting method. Our results indicate that while the fingerprint-based method is significantly faster (approximately 10x), the payload-based image classification achieves comparable accuracy, highlighting the trade-offs between computational efficiency and data granularity in IoT security.",
    "published": "2023-04-27T00:37:16Z",
    "updated": "2026-01-27T12:58:33Z",
    "link": "http://arxiv.org/pdf/2304.13894v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Kahraman Kostas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.11234v4",
    "title": "Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning",
    "summary": "Offline reinforcement learning (RL) is a powerful approach for data-driven decision-making and control. Compared to model-free methods, offline model-based reinforcement learning (MBRL) explicitly learns world models from a static dataset and uses them as surrogate simulators, improving the data efficiency and enabling the learned policy to potentially generalize beyond the dataset support. However, there could be various MDPs that behave identically on the offline dataset and dealing with the uncertainty about the true MDP can be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process (BAMDP), which is a principled framework for addressing model uncertainty. We further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state and action spaces with stochastic transitions. This planning process is based on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy improvement operator in policy iteration. Our \"RL + Search\" framework follows in the footsteps of superhuman AIs like AlphaZero, improving on current offline MBRL methods by incorporating more computation input. The proposed algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three challenging, stochastic tokamak control tasks. The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.",
    "published": "2024-10-15T03:36:43Z",
    "updated": "2026-01-27T12:54:32Z",
    "link": "http://arxiv.org/pdf/2410.11234v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiayu Chen",
      "Le Xu",
      "Wentse Chen",
      "Jeff Schneider"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19561v1",
    "title": "AROMMA: Unifying Olfactory Embeddings for Single Molecules and Mixtures",
    "summary": "Public olfaction datasets are small and fragmented across single molecules and mixtures, limiting learning of generalizable odor representations. Recent works either learn single-molecule embeddings or address mixtures via similarity or pairwise label prediction, leaving representations separate and unaligned. In this work, we propose AROMMA, a framework that learns a unified embedding space for single molecules and two-molecule mixtures. Each molecule is encoded by a chemical foundation model and the mixtures are composed by an attention-based aggregator, ensuring both permutation invariance and asymmetric molecular interactions. We further align odor descriptor sets using knowledge distillation and class-aware pseudo-labeling to enrich missing mixture annotations. AROMMA achieves state-of-the-art performance in both single-molecule and molecule-pair datasets, with up to 19.1% AUROC improvement, demonstrating a robust generalization in two domains.",
    "published": "2026-01-27T12:54:31Z",
    "updated": "2026-01-27T12:54:31Z",
    "link": "http://arxiv.org/pdf/2601.19561v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Dayoung Kang",
      "JongWon Kim",
      "Jiho Park",
      "Keonseock Lee",
      "Ji-Woong Choi",
      "Jinhyun So"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21778v2",
    "title": "Beyond Structure: Invariant Crystal Property Prediction with Pseudo-Particle Ray Diffraction",
    "summary": "Crystal property prediction, governed by quantum mechanical principles, is computationally prohibitive to solve exactly for large many-body systems using traditional density functional theory. While machine learning models have emerged as efficient approximations for large-scale applications, their performance is strongly influenced by the choice of atomic representation. Although modern graph-based approaches have progressively incorporated more structural information, they often fail to capture long-range atomic interactions due to finite receptive fields and local encoding schemes. This limitation leads to distinct crystals being mapped to identical representations, hindering accurate property prediction. To address this, we introduce PRDNet that leverages unique reciprocal-space diffraction besides graph representations. To enhance sensitivity to elemental and environmental variations, we employ a data-driven pseudo-particle to generate a synthetic diffraction pattern. PRDNet ensures full invariance to crystallographic symmetries. Extensive experiments are conducted on Materials Project, JARVIS-DFT, and MatBench, demonstrating that the proposed model achieves state-of-the-art performance. The code is openly available at https://github.com/Bin-Cao/PRDNet.",
    "published": "2025-09-26T02:30:23Z",
    "updated": "2026-01-27T12:53:48Z",
    "link": "http://arxiv.org/pdf/2509.21778v2.pdf",
    "category": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "authors": [
      "Bin Cao",
      "Yang Liu",
      "Longhan Zhang",
      "Yifan Wu",
      "Zhixun Li",
      "Yuyu Luo",
      "Hong Cheng",
      "Yang Ren",
      "Tong-Yi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15547v2",
    "title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling",
    "summary": "Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator(LANO) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. LANO achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.",
    "published": "2026-01-22T00:33:38Z",
    "updated": "2026-01-27T12:52:58Z",
    "link": "http://arxiv.org/pdf/2601.15547v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jingren Hou",
      "Hong Wang",
      "Pengyu Xu",
      "Chang Gao",
      "Huafeng Liu",
      "Liping Jing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19551v1",
    "title": "Scale-Consistent State-Space Dynamics via Fractal of Stationary Transformations",
    "summary": "Recent deep learning models increasingly rely on depth without structural guarantees on the validity of intermediate representations, rendering early stopping and adaptive computation ill-posed. We address this limitation by formulating a structural requirement for state-space model's scale-consistent latent dynamics across iterative refinement, and derive Fractal of Stationary Transformations (FROST), which enforces a self-similar representation manifold through a fractal inductive bias. Under this geometry, intermediate states correspond to different resolutions of a shared representation, and we provide a geometric analysis establishing contraction and stable convergence across iterations. As a consequence of this scale-consistent structure, halting naturally admits a ranking-based formulation driven by intrinsic feature quality rather than extrinsic objectives. Controlled experiments on ImageNet-100 empirically verify the predicted scale-consistent behavior, showing that adaptive efficiency emerges from the aligned latent geometry.",
    "published": "2026-01-27T12:44:20Z",
    "updated": "2026-01-27T12:44:20Z",
    "link": "http://arxiv.org/pdf/2601.19551v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Geunhyeok Yu",
      "Hyoseok Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21372v2",
    "title": "Improving LLM-based Global Optimization with Search Space Partitioning",
    "summary": "Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading global optimization methods, while substantially outperforming global LLM-based sampling strategies.",
    "published": "2025-05-27T16:01:49Z",
    "updated": "2026-01-27T12:40:04Z",
    "link": "http://arxiv.org/pdf/2505.21372v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Andrej Schwanke",
      "Lyubomir Ivanov",
      "David Salinas",
      "Fabio Ferreira",
      "Aaron Klein",
      "Frank Hutter",
      "Arber Zela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.07639v3",
    "title": "PowerGraph-LLM: Novel Power Grid Graph Embedding and Optimization with Large Language Models",
    "summary": "Efficiently solving Optimal Power Flow (OPF) problems in power systems is crucial for operational planning and grid management. There is a growing need for scalable algorithms capable of handling the increasing variability, constraints, and uncertainties in modern power networks while providing accurate and fast solutions. To address this, machine learning techniques, particularly Graph Neural Networks (GNNs) have emerged as promising approaches. This letter introduces PowerGraph-LLM, the first framework explicitly designed for solving OPF problems using Large Language Models (LLMs). The proposed approach combines graph and tabular representations of power grids to effectively query LLMs, capturing the complex relationships and constraints in power systems. A new implementation of in-context learning and fine-tuning protocols for LLMs is introduced, tailored specifically for the OPF problem. PowerGraph-LLM demonstrates reliable performances using off-the-shelf LLM. Our study reveals the impact of LLM architecture, size, and fine-tuning and demonstrates our framework's ability to handle realistic grid components and constraints.",
    "published": "2025-01-13T19:01:58Z",
    "updated": "2026-01-27T12:33:04Z",
    "link": "http://arxiv.org/pdf/2501.07639v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Fabien Bernier",
      "Jun Cao",
      "Maxime Cordy",
      "Salah Ghamizi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23785v2",
    "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale",
    "summary": "This position paper argues that large language models (LLMs) can make cultural context, and therefore human meaning, legible at an unprecedented scale in AI-based sociotechnical systems. We argue that such systems have previously been unable to represent human meaning because they rely on thin descriptions (numerical representations that enforce standardization and therefore strip human activity of the cultural context which gives it meaning). By contrast, scholars in the humanities and qualitative social sciences have developed frameworks for representing meaning through thick description (verbal representations that accommodate heterogeneity and retain contextual information needed to represent human meaning). The verbal capabilities of LLMs now provide a means of at least partially automating the generation and processing of thick descriptions, offering new ways to deploy them at scale. We argue that the problem of rendering human meaning legible is not just about selecting better metrics but about developing new representational formats based on thick description. We frame this as a crucial direction for the application of generative AI and identify five key challenges: preserving context, maintaining interpretive pluralism, integrating perspectives based on lived experience and critical distance, distinguishing qualitative content from quantitative magnitude, and acknowledging meaning as dynamic rather than static.",
    "published": "2025-05-23T04:10:42Z",
    "updated": "2026-01-27T12:30:30Z",
    "link": "http://arxiv.org/pdf/2505.23785v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Cody Kommers",
      "Drew Hemment",
      "Maria Antoniak",
      "Joel Z. Leibo",
      "Hoyt Long",
      "Emily Robinson",
      "Adam Sobey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11358v2",
    "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation",
    "summary": "Retrieval-augmented generation (RAG) is typically optimized for topical relevance, yet its success ultimately depends on whether retrieved passages are useful for a large language model (LLM) to generate correct and complete answers. We argue that such utility is often LLM-specific rather than universal, due to differences in models' knowledge, reasoning, and ability to leverage evidence. We formalize LLM-specific utility as the performance improvement of a target LLM when a passage is provided, compared to answering without evidence. To systematically study LLM-specific utility, we construct a benchmark of LLM-specific gold utilitarian passages for four LLMs (Qwen3-8B/14B/32B and Llama3.1-8B) on three QA datasets (Natural Questions, TriviaQA, and MS MARCO-FQA). Our analysis shows that utilitarian passages are model-dependent and non-transferable: each LLM performs best with its own utilitarian evidence, while evidence optimized for other LLMs is consistently suboptimal. Human-annotated evidence remains a strong general baseline but does not fully match individual LLM utility needs. We further introduce the LLM-specific utility judgment task and find that existing utility-aware selection and scoring methods largely capture model-agnostic usefulness and struggle to reliably estimate LLM-specific utility. Overall, our findings highlight the limitations of current utility-aware retrieval and motivate generator-tailored evidence selection for improving RAG.",
    "published": "2025-10-13T12:57:45Z",
    "updated": "2026-01-27T12:24:06Z",
    "link": "http://arxiv.org/pdf/2510.11358v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Jiaming Zhang",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xueqi Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19533v1",
    "title": "SLM-SS: Speech Language Model for Generative Speech Separation",
    "summary": "Speech separation (SS) has advanced significantly with neural network-based methods, showing improved performance on signal-level metrics. However, these methods often struggle to maintain speech intelligibility in the separated signals, which can negatively affect the performance of downstream tasks such as speech recognition. In this work, we propose SLM-SS, a novel approach that applies speech language models to SS, aiming to enhance the intelligibility and coherence of the separated signals. We frame SS as discrete multi-codebook sequence generation, using Encoder-Decoder models to map quantized speech mixtures to target tokens. In addition to the autoregressive modeling strategy, we introduce a non-autoregressive model to improve decoding efficiency for residual tokens. Experimental results on the LibriMix dataset demonstrate that our approach shows significantly better preservation of speech intelligibility, leading to improved linguistic consistency in a variety of downstream tasks compared to existing approaches.",
    "published": "2026-01-27T12:22:43Z",
    "updated": "2026-01-27T12:22:43Z",
    "link": "http://arxiv.org/pdf/2601.19533v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Tianhua Li",
      "Chenda Li",
      "Wei Wang",
      "Xin Zhou",
      "Xihui Chen",
      "Jianqing Gao",
      "Yanmin Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19532v1",
    "title": "Benchmarks Saturate When The Model Gets Smarter Than The Judge",
    "summary": "Benchmarks are important tools to track progress in the development of Large Language Models (LLMs), yet inaccuracies in datasets and evaluation methods consistently undermine their effectiveness. Here, we present Omni-MATH-2, a manually revised version of the Omni-MATH dataset comprising a clean, exact-answer subset ($n{=}4181$) and a tagged, non-standard subset ($n{=}247$). Each problem was audited to ensure LaTeX compilability, solvability and verifiability, which involved adding missing figures or information, labeling problems requiring a proof, estimation or image, and removing clutter. This process significantly reduces dataset-induced noise, thereby providing a more precise assessment of model performance. The annotated dataset also allows us to evaluate judge-induced noise by comparing GPT-5 mini with the original Omni-Judge, revealing substantial discrepancies between judges on both the clean and tagged problem subsets. Expert annotations reveal that Omni-Judge is wrong in $96.4\\%$ of the judge disagreements, indicating its inability to differentiate between models' abilities, even well before saturation of the benchmark occurs. As problems become more challenging, we find that increasingly competent judges become essential in order to prevent judge errors from masking genuine differences between models. Finally, neither judge identifies the present failure modes for the subset of tagged problems, demonstrating that dataset quality and judge reliability are both critical to develop accurate benchmarks of model performance.",
    "published": "2026-01-27T12:20:44Z",
    "updated": "2026-01-27T12:20:44Z",
    "link": "http://arxiv.org/pdf/2601.19532v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Marthe Ballon",
      "Andres Algaba",
      "Brecht Verbeken",
      "Vincent Ginis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06047v2",
    "title": "PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks",
    "summary": "Irregular temporal data, characterized by varying recording frequencies, differing observation durations, and missing values, presents significant challenges across fields like mobility, healthcare, and environmental science. Existing research communities often overlook or address these challenges in isolation, leading to fragmented tools and methods. To bridge this gap, we introduce a unified framework, and the first standardized dataset repository for irregular time series classification, built on a common array format to enhance interoperability. This repository comprises 34 datasets on which we benchmark 12 classifier models from diverse domains and communities. This work aims to centralize research efforts and enable a more robust evaluation of irregular temporal data analysis methods.",
    "published": "2025-05-09T13:43:43Z",
    "updated": "2026-01-27T12:19:49Z",
    "link": "http://arxiv.org/pdf/2505.06047v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Francesco Spinnato",
      "Cristiano Landi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18418v2",
    "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
    "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...",
    "published": "2026-01-26T12:20:18Z",
    "updated": "2026-01-27T12:16:16Z",
    "link": "http://arxiv.org/pdf/2601.18418v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Ji Zeng",
      "Dayuan Fu",
      "Tiantian Mi",
      "Yumin Zhuang",
      "Yaxing Huang",
      "Xuefeng Li",
      "Lyumanshan Ye",
      "Muhang Xie",
      "Qishuo Hua",
      "Zhen Huang",
      "Mohan Jiang",
      "Hanning Wang",
      "Jifan Lin",
      "Yang Xiao",
      "Jie Sun",
      "Yunze Wu",
      "Pengfei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.10495v2",
    "title": "SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models",
    "summary": "Latent Diffusion Models (LDMs) have established themselves as powerful tools in the rapidly evolving field of image generation, capable of producing highly realistic images. However, their widespread adoption raises critical concerns about copyright infringement and the misuse of generated content. Watermarking techniques have emerged as a promising solution, enabling copyright identification and misuse tracing through imperceptible markers embedded in generated images. Among these, latent-based watermarking techniques are particularly promising, as they embed watermarks directly into the latent noise without altering the underlying LDM architecture. In this work, we demonstrate that such latent-based watermarks are practically vulnerable to detection and compromise through systematic analysis of output images' statistical patterns for the first time. To counter this, we propose SWA-LDM (Stealthy Watermark for LDM), a lightweight framework that enhances stealth by dynamically randomizing the embedded watermarks using the Gaussian-distributed latent noise inherent to diffusion models. By embedding unique, pattern-free signatures per image, SWA-LDM eliminates detectable artifacts while preserving image quality and extraction robustness. Experiments demonstrate an average of 20% improvement in stealth over state-of-the-art methods, enabling secure deployment of watermarked generative AI in real-world applications.",
    "published": "2025-02-14T16:55:45Z",
    "updated": "2026-01-27T12:10:27Z",
    "link": "http://arxiv.org/pdf/2502.10495v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zhonghao Yang",
      "Linye Lyu",
      "Xuanhang Chang",
      "Daojing He",
      "YU LI"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19527v1",
    "title": "Fuzzy expert system for the process of collecting and purifying acidic water: a digital twin approach",
    "summary": "Purifying sour water is essential for reducing emissions, minimizing corrosion risks, enabling the reuse of treated water in industrial or domestic applications, and ultimately lowering operational costs. Moreover, automating the purification process helps reduce the risk of worker harm by limiting human involvement. Crude oil contains acidic components such as hydrogen sulfide, carbon dioxide, and other chemical compounds. During processing, these substances are partially released into sour water. If not properly treated, sour water poses serious environmental threats and accelerates the corrosion of pipelines and equipment. This paper presents a fuzzy expert system, combined with a custom-generated digital twin, developed from a documented industrial process to maintain key parameters at desired levels by mimicking human reasoning. The control strategy is designed to be simple and intuitive, allowing junior or non-expert personnel to interact with the system effectively. The digital twin was developed using Honeywell UniSim Design R492 to simulate real industrial behavior accurately. Valve dynamics were modeled through system identification in MATLAB, and real-time data exchange between the simulator and controller was established using OPC DA. The fuzzy controller applies split-range control to two valves and was tested under 21 different initial pressure conditions using five distinct defuzzification strategies, resulting in a total of 105 unique test scenarios. System performance was evaluated using both error-based metrics (MSE, RMSE, MAE, IAE, ISE, ITAE) and dynamic response metrics, including overshoot, undershoot, rise time, fall time, settling time, and steady-state error. A web-based simulation interface was developed in Python using the Streamlit framework. Although demonstrated here for sour water treatment, the proposed fuzzy expert system is general-purpose.",
    "published": "2026-01-27T12:08:11Z",
    "updated": "2026-01-27T12:08:11Z",
    "link": "http://arxiv.org/pdf/2601.19527v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Temirbolat Maratuly",
      "Pakizar Shamoi",
      "Timur Samigulin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19519v1",
    "title": "Mocap Anywhere: Towards Pairwise-Distance based Motion Capture in the Wild (for the Wild)",
    "summary": "We introduce a novel motion capture system that reconstructs full-body 3D motion using only sparse pairwise distance (PWD) measurements from body-mounted(UWB) sensors. Using time-of-flight ranging between wireless nodes, our method eliminates the need for external cameras, enabling robust operation in uncontrolled and outdoor environments. Unlike traditional optical or inertial systems, our approach is shape-invariant and resilient to environmental constraints such as lighting and magnetic interference. At the core of our system is Wild-Poser (WiP for short), a compact, real-time Transformer-based architecture that directly predicts 3D joint positions from noisy or corrupted PWD measurements, which can later be used for joint rotation reconstruction via learned methods. WiP generalizes across subjects of varying morphologies, including non-human species, without requiring individual body measurements or shape fitting. Operating in real time, WiP achieves low joint position error and demonstrates accurate 3D motion reconstruction for both human and animal subjects in-the-wild. Our empirical analysis highlights its potential for scalable, low-cost, and general purpose motion capture in real-world settings.",
    "published": "2026-01-27T11:58:34Z",
    "updated": "2026-01-27T11:58:34Z",
    "link": "http://arxiv.org/pdf/2601.19519v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "authors": [
      "Ofir Abramovich",
      "Ariel Shamir",
      "Andreas Aristidou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.17498v2",
    "title": "Improving Value-based Process Verifier via Structural Prior Injection",
    "summary": "In the Large Language Model(LLM) reasoning scenario, people often estimate state value via Monte Carlo sampling. Though Monte Carlo estimation is an elegant method with less inductive bias, noise and errors are inevitably introduced due to the limited sampling. To handle the problem, we inject the structural prior into the value representation and transfer the scalar value into the expectation of a pre-defined categorical distribution, representing the noise and errors from a distribution perspective. Specifically, by treating the result of Monte Carlo sampling as a single sample from the prior ground-truth Binomial distribution, we quantify the sampling error as the mismatch between posterior estimated distribution and ground-truth distribution, which is thus optimized via distribution selection optimization. We test the performance of value-based process verifiers on Best-of-N task and Beam search task. Compared with the scalar value representation, we show that reasonable structural prior injection induced by different objective functions or optimization methods can improve the performance of value-based process verifiers for about 1$\\sim$2 points at little-to-no cost. We also show that under different structural prior, the verifiers' performances vary greatly despite having the same optimal solution, indicating the importance of reasonable structural prior injection.",
    "published": "2025-02-21T07:57:59Z",
    "updated": "2026-01-27T11:51:05Z",
    "link": "http://arxiv.org/pdf/2502.17498v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zetian Sun",
      "Dongfang Li",
      "Baotian Hu",
      "Jun Yu",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19503v1",
    "title": "GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs",
    "summary": "Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight downstream datasets. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is publicly available.",
    "published": "2026-01-27T11:41:26Z",
    "updated": "2026-01-27T11:41:26Z",
    "link": "http://arxiv.org/pdf/2601.19503v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Wei Huang",
      "Anda Cheng",
      "Yinggui Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19498v1",
    "title": "Cortex-Grounded Diffusion Models for Brain Image Generation",
    "summary": "Synthetic neuroimaging data can mitigate critical limitations of real-world datasets, including the scarcity of rare phenotypes, domain shifts across scanners, and insufficient longitudinal coverage. However, existing generative models largely rely on weak conditioning signals, such as labels or text, which lack anatomical grounding and often produce biologically implausible outputs. To this end, we introduce Cor2Vox, a cortex-grounded generative framework for brain magnetic resonance image (MRI) synthesis that ties image generation to continuous structural priors of the cerebral cortex. It leverages high-resolution cortical surfaces to guide a 3D shape-to-image Brownian bridge diffusion process, enabling topologically faithful synthesis and precise control over underlying anatomies. To support the generation of new, realistic brain shapes, we developed a large-scale statistical shape model of cortical morphology derived from over 33,000 UK Biobank scans. We validated the fidelity of Cor2Vox based on traditional image quality metrics, advanced cortical surface reconstruction, and whole-brain segmentation quality, outperforming many baseline methods. Across three applications, namely (i) anatomically consistent synthesis, (ii) simulation of progressive gray matter atrophy, and (iii) harmonization of in-house frontotemporal dementia scans with public datasets, Cor2Vox preserved fine-grained cortical morphology at the sub-voxel level, exhibiting remarkable robustness to variations in cortical geometry and disease phenotype without retraining.",
    "published": "2026-01-27T11:34:43Z",
    "updated": "2026-01-27T11:34:43Z",
    "link": "http://arxiv.org/pdf/2601.19498v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Fabian Bongratz",
      "Yitong Li",
      "Sama Elbaroudy",
      "Christian Wachinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19494v1",
    "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context",
    "summary": "High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an \"AI-assisted, Expert-verified\" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .",
    "published": "2026-01-27T11:28:44Z",
    "updated": "2026-01-27T11:28:44Z",
    "link": "http://arxiv.org/pdf/2601.19494v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Lei Zhang",
      "Yongda Yu",
      "Minghui Yu",
      "Xinxin Guo",
      "Zhengqi Zhuang",
      "Guoping Rong",
      "Dong Shao",
      "Haifeng Shen",
      "Hongyu Kuang",
      "Zhengfeng Li",
      "Boge Wang",
      "Guoan Zhang",
      "Bangyu Xiang",
      "Xiaobing Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.23530v5",
    "title": "There and Back Again: On the relation between Noise and Image Inversions in Diffusion Models",
    "summary": "Diffusion Models achieve state-of-the-art performance in generating new samples but lack a low-dimensional latent space that encodes the data into editable features. Inversion-based methods address this by reversing the denoising trajectory, transferring images to their approximated starting noise. In this work, we thoroughly analyze this procedure and focus on the relation between the initial noise, the generated samples, and their corresponding latent encodings obtained through the DDIM inversion. First, we show that latents exhibit structural patterns in the form of less diverse noise predicted for smooth image areas (e.g., plain sky). Through a series of analyses, we trace this issue to the first inversion steps, which fail to provide accurate and diverse noise. Consequently, the DDIM inversion space is notably less manipulative than the original noise. We show that prior inversion methods do not fully resolve this issue, but our simple fix, where we replace the first DDIM Inversion steps with a forward diffusion process, successfully decorrelates latent encodings and enables higher quality editions and interpolations. The code is available at https://github.com/luk-st/taba.",
    "published": "2024-10-31T00:30:35Z",
    "updated": "2026-01-27T11:28:29Z",
    "link": "http://arxiv.org/pdf/2410.23530v5.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Łukasz Staniszewski",
      "Łukasz Kuciński",
      "Kamil Deja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19487v1",
    "title": "LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment",
    "summary": "Safety-aligned LLMs suffer from two failure modes: jailbreak (answering harmful inputs) and over-refusal (declining benign queries). Existing vector steering methods adjust the magnitude of answer vectors, but this creates a fundamental trade-off -- reducing jailbreak increases over-refusal and vice versa. We identify the root cause: LLMs encode the decision to answer (answer vector $v_a$) and the judgment of input safety (benign vector $v_b$) as nearly orthogonal directions, treating them as independent processes. We propose LLM-VA, which aligns $v_a$ with $v_b$ through closed-form weight updates, making the model's willingness to answer causally dependent on its safety assessment -- without fine-tuning or architectural changes. Our method identifies vectors at each layer using SVMs, selects safety-relevant layers, and iteratively aligns vectors via minimum-norm weight modifications. Experiments on 12 LLMs demonstrate that LLM-VA achieves 11.45% higher F1 than the best baseline while preserving 95.92% utility, and automatically adapts to each model's safety bias without manual tuning. Code and models are available at https://hotbento.github.io/LLM-VA-Web/.",
    "published": "2026-01-27T11:19:19Z",
    "updated": "2026-01-27T11:19:19Z",
    "link": "http://arxiv.org/pdf/2601.19487v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haonan Zhang",
      "Dongxia Wang",
      "Yi Liu",
      "Kexin Chen",
      "Wenhai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01098v2",
    "title": "Towards Automated Smart Contract Generation: Evaluation, Benchmarking, and Retrieval-Augmented Repair",
    "summary": "Smart contracts, predominantly written in Solidity and deployed on blockchains such as Ethereum, are immutable after deployment, making functional correctness critical. However, existing evaluations of Solidity code generation rely largely on surface-level metrics (e.g., BLEU, CrystalBLEU) or manual inspection, which correlate poorly with functional correctness. In contrast to Python, Solidity lacks large-scale, execution-based benchmarks, limiting systematic evaluation of large language models for smart contract development.\n  We introduce SolBench, a comprehensive benchmark and automated testing pipeline for Solidity that emphasizes functional correctness via differential fuzzing. SolBench consists of 28825 functions extracted from 7604 real-world smart contracts collected from Etherscan (genesis-2024), spanning ten application domains. We benchmark 14 diverse LLMs, covering open and closed models, 1.3B-671B parameters, and both general-purpose and code-specialized architectures. The dominant failure mode is missing critical intra-contract information, such as state variables and type definitions. Providing full-contract context improves accuracy but incurs prohibitive inference costs.\n  To address this, we propose Retrieval-Augmented Repair (RAR), a cost-effective framework that integrates execution feedback into code repair. RAR uses compiler and runtime error messages to retrieve only the minimal contract snippets needed to correct a target function, avoiding full-context inference. This significantly reduces input length while improving functional correctness. We further analyze retrieval and repair strategies within RAR, demonstrating consistent gains in accuracy and efficiency. SolBench and RAR enable principled, execution-based evaluation and economical improvement of Solidity code generation. Dataset and code are publicly available at https://github.com/ZaoyuChen/SolBench.",
    "published": "2025-03-03T01:55:20Z",
    "updated": "2026-01-27T11:18:24Z",
    "link": "http://arxiv.org/pdf/2503.01098v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zaoyu Chen",
      "Haoran Qin",
      "Nuo Chen",
      "Xiangyu Zhao",
      "Lei Xue",
      "Xiapu Luo",
      "Xiao-Ming Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.10402v2",
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
    "published": "2026-01-15T13:52:04Z",
    "updated": "2026-01-27T11:18:14Z",
    "link": "http://arxiv.org/pdf/2601.10402v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xinyu Zhu",
      "Yuzhu Cai",
      "Zexi Liu",
      "Bingyang Zheng",
      "Cheng Wang",
      "Rui Ye",
      "Jiaao Chen",
      "Hanrui Wang",
      "Wei-Chen Wang",
      "Yuzhi Zhang",
      "Linfeng Zhang",
      "Weinan E",
      "Di Jin",
      "Siheng Chen",
      "Yanfeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14401v2",
    "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems",
    "summary": "A growing body of multi-agent studies with LLMs explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without explicit knowledge of the payoff structure or how individual actions translate into long-run outcomes, relying instead on heuristics, communication, and enforcement. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.",
    "published": "2025-10-16T07:59:31Z",
    "updated": "2026-01-27T11:13:05Z",
    "link": "http://arxiv.org/pdf/2510.14401v2.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Prateek Gupta",
      "Qiankun Zhong",
      "Hiromu Yakura",
      "Thomas Eisenmann",
      "Iyad Rahwan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19479v1",
    "title": "Time-to-Injury Forecasting in Elite Female Football: A DeepHit Survival Approach",
    "summary": "Injury occurrence in football poses significant challenges for athletes and teams, carrying personal, competitive, and financial consequences. While machine learning has been applied to injury prediction before, existing approaches often rely on static pre-season data and binary outcomes, limiting their real-world utility. This study investigates the feasibility of using a DeepHit neural network to forecast time-to-injury from longitudinal athlete monitoring data, while providing interpretable predictions. The analysis utilised the publicly available SoccerMon dataset, containing two seasons of training, match, and wellness records from elite female footballers. Data was pre-processed through cleaning, feature engineering, and the application of three imputation strategies. Baseline models (Random Forest, XGBoost, Logistic Regression) were optimised via grid search for benchmarking, while the DeepHit model, implemented with a multilayer perceptron backbone, was evaluated using chronological and leave-one-player-out (LOPO) validation. DeepHit achieved a concordance index of 0.762, outperforming baseline models and delivering individualised, time-varying risk estimates. Shapley Additive Explanations (SHAP) identified clinically relevant predictors consistent with established risk factors, enhancing interpretability. Overall, this study provides a novel proof of concept: survival modelling with DeepHit shows strong potential to advance injury forecasting in football, offering accurate, explainable, and actionable insights for injury prevention across competitive levels.",
    "published": "2026-01-27T11:11:52Z",
    "updated": "2026-01-27T11:11:52Z",
    "link": "http://arxiv.org/pdf/2601.19479v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Victoria Catterall",
      "Cise Midoglu",
      "Stephen Lynch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17912v2",
    "title": "Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN",
    "summary": "Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, remain underexplored. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying TabPFN (and similar) models in practice and the need for further fairness interventions.",
    "published": "2026-01-25T17:17:12Z",
    "updated": "2026-01-27T11:11:17Z",
    "link": "http://arxiv.org/pdf/2601.17912v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qinyi Liu",
      "Mohammad Khalil",
      "Naman Goel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09156v2",
    "title": "Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems",
    "summary": "We present a framework for fine-tuning flow-matching generative models to enforce physical constraints and solve inverse problems in scientific systems. Starting from a model trained on low-fidelity or observational data, we apply a differentiable post-training procedure that minimizes weak-form residuals of governing partial differential equations (PDEs), promoting physical consistency and adherence to boundary conditions without distorting the underlying learned distribution. To infer unknown physical inputs, such as source terms, material parameters, or boundary data, we augment the generative process with a learnable latent parameter predictor and propose a joint optimization strategy. The resulting model produces physically valid field solutions alongside plausible estimates of hidden parameters, effectively addressing ill-posed inverse problems in a data-driven yet physicsaware manner. We validate our method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE constraints and accurate recovery of latent coefficients. Our approach bridges generative modelling and scientific inference, opening new avenues for simulation-augmented discovery and data-efficient modelling of physical systems.",
    "published": "2025-08-05T09:32:04Z",
    "updated": "2026-01-27T10:52:42Z",
    "link": "http://arxiv.org/pdf/2508.09156v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "authors": [
      "Jan Tauberschmidt",
      "Sophie Fellenz",
      "Sebastian J. Vollmer",
      "Andrew B. Duncan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12542v2",
    "title": "Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery",
    "summary": "Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.4% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.",
    "published": "2026-01-18T19:12:41Z",
    "updated": "2026-01-27T10:52:36Z",
    "link": "http://arxiv.org/pdf/2601.12542v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Lukas Weidener",
      "Marko Brkić",
      "Mihailo Jovanović",
      "Ritvik Singh",
      "Chiara Baccin",
      "Emre Ulgac",
      "Alex Dobrin",
      "Aakaash Meduri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12576v2",
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "summary": "While reinforcement learning has achieved impressive progress in language model reasoning, it is constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the probabilities that LLMs generate reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
    "published": "2025-12-14T07:03:51Z",
    "updated": "2026-01-27T10:47:25Z",
    "link": "http://arxiv.org/pdf/2512.12576v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Xueru Wen",
      "Jie Lou",
      "Yanjiang Liu",
      "Hongyu Lin",
      "Ben He",
      "Xianpei Han",
      "Le Sun",
      "Yaojie Lu",
      "Debing Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19452v1",
    "title": "APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition",
    "summary": "Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior's applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.",
    "published": "2026-01-27T10:38:32Z",
    "updated": "2026-01-27T10:38:32Z",
    "link": "http://arxiv.org/pdf/2601.19452v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Finn Rietz",
      "Pedro Zuidberg dos Martires",
      "Johannes Andreas Stork"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19447v1",
    "title": "KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking",
    "summary": "Claim verification is a core component of automated fact-checking systems, aimed at determining the truthfulness of a statement by assessing it against reliable evidence sources such as documents or knowledge bases. This work presents KG-CRAFT, a method that improves automatic claim verification by leveraging large language models (LLMs) augmented with contrastive questions grounded in a knowledge graph. KG-CRAFT first constructs a knowledge graph from claims and associated reports, then formulates contextually relevant contrastive questions based on the knowledge graph structure. These questions guide the distillation of evidence-based reports, which are synthesised into a concise summary that is used for veracity assessment by LLMs. Extensive evaluations on two real-world datasets (LIAR-RAW and RAWFC) demonstrate that our method achieves a new state-of-the-art in predictive performance. Comprehensive analyses validate in detail the effectiveness of our knowledge graph-based contrastive reasoning approach in improving LLMs' fact-checking capabilities.",
    "published": "2026-01-27T10:32:42Z",
    "updated": "2026-01-27T10:32:42Z",
    "link": "http://arxiv.org/pdf/2601.19447v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Vítor N. Lourenço",
      "Aline Paes",
      "Tillman Weyde",
      "Audrey Depeige",
      "Mohnish Dubey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19435v1",
    "title": "Ad Insertion in LLM-Generated Responses",
    "summary": "Sustainable monetization of Large Language Models (LLMs) remains a critical open challenge. Traditional search advertising, which relies on static keywords, fails to capture the fleeting, context-dependent user intents--the specific information, goods, or services a user seeks--embedded in conversational flows. Beyond the standard goal of social welfare maximization, effective LLM advertising imposes additional requirements on contextual coherence (ensuring ads align semantically with transient user intents) and computational efficiency (avoiding user interaction latency), as well as adherence to ethical and regulatory standards, including preserving privacy and ensuring explicit ad disclosure. Although various recent solutions have explored bidding on token-level and query-level, both categories of approaches generally fail to holistically satisfy this multifaceted set of constraints.\n  We propose a practical framework that resolves these tensions through two decoupling strategies. First, we decouple ad insertion from response generation to ensure safety and explicit disclosure. Second, we decouple bidding from specific user queries by using ``genres'' (high-level semantic clusters) as a proxy. This allows advertisers to bid on stable categories rather than sensitive real-time response, reducing computational burden and privacy risks. We demonstrate that applying the VCG auction mechanism to this genre-based framework yields approximately dominant strategy incentive compatibility (DSIC) and individual rationality (IR), as well as approximately optimal social welfare, while maintaining high computational efficiency. Finally, we introduce an \"LLM-as-a-Judge\" metric to estimate contextual coherence. Our experiments show that this metric correlates strongly with human ratings (Spearman's $ρ\\approx 0.66$), outperforming 80% of individual human evaluators.",
    "published": "2026-01-27T10:16:03Z",
    "updated": "2026-01-27T10:16:03Z",
    "link": "http://arxiv.org/pdf/2601.19435v1.pdf",
    "category": [
      "cs.GT",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shengwei Xu",
      "Zhaohua Chen",
      "Xiaotie Deng",
      "Zhiyi Huang",
      "Grant Schoenebeck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21028v2",
    "title": "Who Gets Cited Most? Benchmarking Long-Context Reasoning on Scientific Articles",
    "summary": "We introduce SciTrek, a novel question-answering benchmark designed to evaluate long-context reasoning capabilities of large language models (LLMs) using scientific articles. Current long-context benchmarks often focus on simple information retrieval tasks, or employ artificial contexts. SciTrek addresses these limitations by creating benchmark questions that require information aggregation and synthesis across multiple full-text scientific articles. The questions and their ground-truth answers are automatically generated by formulating them as SQL queries over a database constructed from article metadata (i.e., titles, authors, and references). These SQL queries provide explicit, verifiable reasoning processes that enable fine-grained error analysis on model answers, and the data construction scales to contexts of up to 1M tokens with minimal supervision. Experiments on open-weight and proprietary LLMs show that SciTrek poses significant challenges as the context length increases, with supervised fine-tuning and reinforcement learning offering only limited gains. Our analysis reveals systematic shortcomings of frontier LLMs' ability to effectively perform numerical operations and accurately locate information in long contexts.",
    "published": "2025-09-25T11:36:09Z",
    "updated": "2026-01-27T10:01:24Z",
    "link": "http://arxiv.org/pdf/2509.21028v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Miao Li",
      "Alexander Gurung",
      "Irina Saparina",
      "Mirella Lapata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12946v3",
    "title": "AI-generated data contamination erodes pathological variability and diagnostic reliability",
    "summary": "Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination remain unexplored. Here, we show that in the absence of mandatory human verification, this self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability. By analysing more than 800,000 synthetic data points across clinical text generation, vision-language reporting, and medical image synthesis, we find that models progressively converge toward generic phenotypes regardless of the model architecture. Specifically, rare but critical findings, including pneumothorax and effusions, vanish from the synthetic content generated by AI models, while demographic representations skew heavily toward middle-aged male phenotypes. Crucially, this degradation is masked by false diagnostic confidence; models continue to issue reassuring reports while failing to detect life-threatening pathology, with false reassurance rates tripling to 40%. Blinded physician evaluation confirms that this decoupling of confidence and accuracy renders AI-generated documentation clinically useless after just two generations. We systematically evaluate three mitigation strategies, finding that while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity. Ultimately, our results suggest that without policy-mandated human oversight, the deployment of generative AI threatens to degrade the very healthcare data ecosystems it relies upon.",
    "published": "2026-01-19T10:54:03Z",
    "updated": "2026-01-27T09:59:35Z",
    "link": "http://arxiv.org/pdf/2601.12946v3.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Hongyu He",
      "Shaowen Xiang",
      "Ye Zhang",
      "Yingtao Zhu",
      "Jin Zhang",
      "Hao Deng",
      "Emily Alsentzer",
      "Qingyu Chen",
      "Kun-Hsing Yu",
      "Andrew Marshall",
      "Tingting Chen",
      "Srinivas Anumasa",
      "Daniel Ebner",
      "Dean Ho",
      "Kee Yuan Ngiam",
      "Ching-Yu Cheng",
      "Dianbo Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11001v3",
    "title": "DND: Boosting Large Language Models with Dynamic Nested Depth",
    "summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.",
    "published": "2025-10-13T04:22:57Z",
    "updated": "2026-01-27T09:59:08Z",
    "link": "http://arxiv.org/pdf/2510.11001v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tieyuan Chen",
      "Xiaodong Chen",
      "Haoxing Chen",
      "Zhenzhong Lan",
      "Weiyao Lin",
      "Jianguo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03252v3",
    "title": "Universal Multi-Domain Translation via Diffusion Routers",
    "summary": "Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.",
    "published": "2025-09-26T07:32:43Z",
    "updated": "2026-01-27T09:56:22Z",
    "link": "http://arxiv.org/pdf/2510.03252v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Duc Kieu",
      "Kien Do",
      "Tuan Hoang",
      "Thao Minh Le",
      "Tung Kieu",
      "Dang Nguyen",
      "Thin Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19406v1",
    "title": "Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation",
    "summary": "Synthetic simulation data and real-world human data provide scalable alternatives to circumvent the prohibitive costs of robot data collection. However, these sources suffer from the sim-to-real visual gap and the human-to-robot embodiment gap, respectively, which limits the policy's generalization to real-world scenarios. In this work, we identify a natural yet underexplored complementarity between these sources: simulation offers the robot action that human data lacks, while human data provides the real-world observation that simulation struggles to render. Motivated by this insight, we present SimHum, a co-training framework to simultaneously extract kinematic prior from simulated robot actions and visual prior from real-world human observations. Based on the two complementary priors, we achieve data-efficient and generalizable robotic manipulation in real-world tasks. Empirically, SimHum outperforms the baseline by up to $\\mathbf{40\\%}$ under the same data collection budget, and achieves a $\\mathbf{62.5\\%}$ OOD success with only 80 real data, outperforming the real only baseline by $7.1\\times$. Videos and additional information can be found at \\href{https://kaipengfang.github.io/sim-and-human}{project website}.",
    "published": "2026-01-27T09:41:28Z",
    "updated": "2026-01-27T09:41:28Z",
    "link": "http://arxiv.org/pdf/2601.19406v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Kaipeng Fang",
      "Weiqing Liang",
      "Yuyang Li",
      "Ji Zhang",
      "Pengpeng Zeng",
      "Lianli Gao",
      "Jingkuan Song",
      "Heng Tao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18739v2",
    "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
    "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.",
    "published": "2026-01-26T18:01:46Z",
    "updated": "2026-01-27T09:40:10Z",
    "link": "http://arxiv.org/pdf/2601.18739v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ignacio Antequera-Sánchez",
      "Juan Luis Suárez-Díaz",
      "Rosana Montes",
      "Francisco Herrera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09239v2",
    "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting",
    "summary": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced Novel View Synthesis (NVS) through explicit scene representation, enabling real-time photorealistic rendering. However, existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. This is exacerbated by conflicting gradient directions that prevent effective splitting of these Gaussians; (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation. This redundancy significantly increases memory overhead due to unnecessary data retention. We present Gradient-Direction-Aware Gaussian Splatting (GDAGS) to address these challenges. Our key innovations: the Gradient Coherence Ratio (GCR), computed through normalized gradient vector norms, which explicitly discriminates Gaussians with concordant versus conflicting gradient directions; and a nonlinear dynamic weighting mechanism leverages the GCR to enable gradient-direction-aware density control. Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation. Comprehensive evaluations across diverse real-world benchmarks demonstrate that GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations.",
    "published": "2025-08-12T13:12:54Z",
    "updated": "2026-01-27T09:40:08Z",
    "link": "http://arxiv.org/pdf/2508.09239v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zheng Zhou",
      "Yu-Jie Xiong",
      "Jia-Chen Zhang",
      "Chun-Ming Xia",
      "Xihe Qiu",
      "Hongjian Zhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19404v1",
    "title": "RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization",
    "summary": "Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.",
    "published": "2026-01-27T09:38:32Z",
    "updated": "2026-01-27T09:38:32Z",
    "link": "http://arxiv.org/pdf/2601.19404v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Hongzhu Yi",
      "Xinming Wang",
      "Zhenghao zhang",
      "Tianyu Zong",
      "Yuanxiang Wang",
      "Jun Xie",
      "Tao Yu",
      "Haopeng Jin",
      "Zhepeng Wang",
      "Kaixin Xu",
      "Feng Chen",
      "Jiahuan Chen",
      "Yujia Yang",
      "Zhenyu Guan",
      "Bingkang Shi",
      "Jungang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19403v1",
    "title": "Learned split-spectrum metalens for obstruction-free broadband imaging in the visible",
    "summary": "Obstructions such as raindrops, fences, or dust degrade captured images, especially when mechanical cleaning is infeasible. Conventional solutions to obstructions rely on a bulky compound optics array or computational inpainting, which compromise compactness or fidelity. Metalenses composed of subwavelength meta-atoms promise compact imaging, but simultaneous achievement of broadband and obstruction-free imaging remains a challenge, since a metalens that images distant scenes across a broadband spectrum cannot properly defocus near-depth occlusions. Here, we introduce a learned split-spectrum metalens that enables broadband obstruction-free imaging. Our approach divides the spectrum of each RGB channel into pass and stop bands with multi-band spectral filtering and learns the metalens to focus light from far objects through pass bands, while filtering focused near-depth light through stop bands. This optical signal is further enhanced using a neural network. Our learned split-spectrum metalens achieves broadband and obstruction-free imaging with relative PSNR gains of 32.29% and improves object detection and semantic segmentation accuracies with absolute gains of +13.54% mAP, +48.45% IoU, and +20.35% mIoU over a conventional hyperbolic design. This promises robust obstruction-free sensing and vision for space-constrained systems, such as mobile robots, drones, and endoscopes.",
    "published": "2026-01-27T09:38:26Z",
    "updated": "2026-01-27T09:38:26Z",
    "link": "http://arxiv.org/pdf/2601.19403v1.pdf",
    "category": [
      "physics.optics",
      "cs.AI",
      "cs.CV",
      "physics.app-ph"
    ],
    "authors": [
      "Seungwoo Yoon",
      "Dohyun Kang",
      "Eunsue Choi",
      "Sohyun Lee",
      "Seoyeon Kim",
      "Minho Choi",
      "Hyeonsu Heo",
      "Dong-ha Shin",
      "Suha Kwak",
      "Arka Majumdar",
      "Junsuk Rho",
      "Seung-Hwan Baek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19402v1",
    "title": "PROTEUS: SLA-Aware Routing via Lagrangian RL for Multi-LLM Serving Systems",
    "summary": "Production LLM deployments serve diverse workloads where cost and quality requirements vary by customer tier, time of day, and query criticality. Model serving systems accept latency SLOs directly. LLM routers do not. They force operators to tune parameters offline and guess what accuracy might result. The relationship between parameters and outcomes is indirect, non-monotonic, and dataset-dependent. Operators need to specify accuracy targets, not infer them from opaque settings. We present PROTEUS (Polymorphic Router for Operational Target Enforcement with Unified SLA), a router that accepts accuracy targets tau as runtime input. PROTEUS uses Lagrangian dual control. A learned dual variable lambda tracks constraint violations during training and conditions the policy network. This lets the router translate specified tau values into routing decisions that satisfy them. A single trained model serves the full accuracy spectrum without retraining.We evaluate on RouterBench (11 models, 405K queries) and SPROUT (14 models, 45K queries). PROTEUS achieves consistent floor compliance where accuracy meets or exceeds tau. The target-response correlation reaches 0.97 to 0.98. The closest baseline, OmniRouter, meets floors only 22% of the time despite also using Lagrangian optimization. PROTEUS operates across tau in [0.85, 0.95] from a single model. On RouterBench it achieves 90.1% accuracy, within 1.3% of oracle. On SPROUT it achieves 94.0% accuracy, within 4.6% of oracle. Cost savings reach 89.8% versus the best fixed model.",
    "published": "2026-01-27T09:38:16Z",
    "updated": "2026-01-27T09:38:16Z",
    "link": "http://arxiv.org/pdf/2601.19402v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Amit Singh Bhatti",
      "Vishal Vaddina",
      "Dagnachew Birru"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19399v1",
    "title": "Residual Tokens Enhance Masked Autoencoders for Speech Modeling",
    "summary": "Recent speech modeling relies on explicit attributes such as pitch, content, and speaker identity, but these alone cannot capture the full richness of natural speech. We introduce RT-MAE, a novel masked autoencoder framework that augments the supervised attributes-based modeling with unsupervised residual trainable tokens, designed to encode the information not explained by explicit labeled factors (e.g., timbre variations, noise, emotion etc). Experiments show that RT-MAE improves reconstruction quality, preserving content and speaker similarity while enhancing expressivity. We further demonstrate its applicability to speech enhancement, removing noise at inference while maintaining controllability and naturalness.",
    "published": "2026-01-27T09:30:31Z",
    "updated": "2026-01-27T09:30:31Z",
    "link": "http://arxiv.org/pdf/2601.19399v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Samir Sadok",
      "Stéphane Lathuilière",
      "Xavier Alameda-Pineda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17477v2",
    "title": "LingoQ: Bridging the Gap between EFL Learning and Work through AI-Generated Work-Related Quizzes",
    "summary": "Non-native English speakers performing English-related tasks at work struggle to sustain EFL learning, despite their motivation. Often, study materials are disconnected from their work context. Our formative study revealed that reviewing work-related English becomes burdensome with current systems, especially after work. Although workers rely on LLM-based assistants to address their immediate needs, these interactions may not directly contribute to their English skills. We present LingoQ, an AI-mediated system that allows workers to practice English using quizzes generated from their LLM queries during work. LingoQ leverages these on-the-fly queries using AI to generate personalized quizzes that workers can review and practice on their smartphones. We conducted a three-week deployment study with 28 EFL workers to evaluate LingoQ. Participants valued the quality-assured, work-situated quizzes and constantly engaging with the app during the study. This active engagement improved self-efficacy and led to learning gains for beginners and, potentially, for intermediate learners. Drawing on these results, we discuss design implications for leveraging workers' growing reliance on LLMs to foster proficiency and engagement while respecting work boundaries and ethics.",
    "published": "2025-09-22T08:12:10Z",
    "updated": "2026-01-27T09:23:51Z",
    "link": "http://arxiv.org/pdf/2509.17477v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yeonsun Yang",
      "Sang Won Lee",
      "Jean Y. Song",
      "Sangdoo Yun",
      "Young-Ho Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.12481v2",
    "title": "SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling",
    "summary": "The past years have seen Large Language Models (LLMs) strive not only as generative models but also as agents solving textual sequential decision-making tasks. When facing complex environments where their zero-shot abilities are insufficient, recent work showed online Reinforcement Learning (RL) could be used for the LLM agent to discover and learn efficient strategies interactively. However, most prior work sticks to on-policy algorithms, which greatly reduces the scope of methods such agents could use for both exploration and exploitation, such as experience replay and hindsight relabeling. Yet, such methods may be key for LLM learning agents, and in particular when designing autonomous intrinsically motivated agents sampling and pursuing their own goals (i.e. autotelic agents). This paper presents and studies an adaptation of Soft Actor-Critic and hindsight relabeling to LLM agents. Our method not only paves the path towards autotelic LLM agents that learn online but can also outperform on-policy methods in more classic multi-goal RL environments.",
    "published": "2024-10-16T11:59:27Z",
    "updated": "2026-01-27T09:14:35Z",
    "link": "http://arxiv.org/pdf/2410.12481v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Loris Gaven",
      "Clement Romac",
      "Thomas Carta",
      "Sylvain Lamprier",
      "Olivier Sigaud",
      "Pierre-Yves Oudeyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15098v4",
    "title": "TextMineX: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action",
    "summary": "Humanitarian Mine Action (HMA) addresses the challenge of detecting and removing landmines from conflict regions. Much of the life-saving operational knowledge produced by HMA agencies is buried in unstructured reports, limiting the transferability of information between agencies. To address this issue, we propose TextMineX: the first dataset, evaluation framework and ontology-guided large language model (LLM) pipeline for knowledge extraction from text in the HMA domain. TextMineX structures HMA reports into (subject, relation, object)-triples, thus creating domain-specific knowledge. To ensure real-world relevance, we utilized the dataset from our collaborator Cambodian Mine Action Centre (CMAC). We further introduce a bias-aware evaluation framework that combines human-annotated triples with an LLM-as-Judge protocol to mitigate position bias in reference-free scoring. Our experiments show that ontology-aligned prompts improve extraction accuracy by up to 44.2%, reduce hallucinations by 22.5%, and enhance format adherence by 20.9% compared to baseline models. We publicly release the dataset and code.",
    "published": "2025-09-18T15:55:19Z",
    "updated": "2026-01-27T09:09:23Z",
    "link": "http://arxiv.org/pdf/2509.15098v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chenyue Zhou",
      "Gürkan Solmaz",
      "Flavio Cirillo",
      "Kiril Gashteovski",
      "Jonathan Fürst"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19380v1",
    "title": "Tri-Reader: An Open-Access, Multi-Stage AI Pipeline for First-Pass Lung Nodule Annotation in Screening CT",
    "summary": "Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards.",
    "published": "2026-01-27T09:05:45Z",
    "updated": "2026-01-27T09:05:45Z",
    "link": "http://arxiv.org/pdf/2601.19380v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Fakrul Islam Tushar",
      "Joseph Y. Lo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19376v1",
    "title": "Teaching Machine Learning Fundamentals with LEGO Robotics",
    "summary": "This paper presents the web-based platform Machine Learning with Bricks and an accompanying two-day course designed to teach machine learning concepts to students aged 12 to 17 through programming-free robotics activities. Machine Learning with Bricks is an open source platform and combines interactive visualizations with LEGO robotics to teach three core algorithms: KNN, linear regression, and Q-learning. Students learn by collecting data, training models, and interacting with robots via a web-based interface. Pre- and post-surveys with 14 students demonstrate significant improvements in conceptual understanding of machine learning algorithms, positive shifts in AI perception, high platform usability, and increased motivation for continued learning. This work demonstrates that tangible, visualization-based approaches can make machine learning concepts accessible and engaging for young learners while maintaining technical depth. The platform is freely available at https://learning-and-dynamics.github.io/ml-with-bricks/, with video tutorials guiding students through the experiments at https://youtube.com/playlist?list=PLx1grFu4zAcwfKKJZ1Ux4LwRqaePCOA2J.",
    "published": "2026-01-27T08:59:57Z",
    "updated": "2026-01-27T08:59:57Z",
    "link": "http://arxiv.org/pdf/2601.19376v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Viacheslav Sydora",
      "Guner Dilsad Er",
      "Michael Muehlebach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19375v1",
    "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection",
    "summary": "Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering",
    "published": "2026-01-27T08:56:25Z",
    "updated": "2026-01-27T08:56:25Z",
    "link": "http://arxiv.org/pdf/2601.19375v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Quy-Anh Dang",
      "Chris Ngo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16632v2",
    "title": "Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting",
    "summary": "Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.",
    "published": "2026-01-23T10:33:34Z",
    "updated": "2026-01-27T08:51:39Z",
    "link": "http://arxiv.org/pdf/2601.16632v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haonan Yang",
      "Jianchao Tang",
      "Zhuo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19362v1",
    "title": "Revisiting Parameter Server in LLM Post-Training",
    "summary": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \\textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.",
    "published": "2026-01-27T08:44:46Z",
    "updated": "2026-01-27T08:44:46Z",
    "link": "http://arxiv.org/pdf/2601.19362v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Xinyi Wan",
      "Penghui Qi",
      "Guangxing Huang",
      "Chaoyi Ruan",
      "Min Lin",
      "Jialin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23634v2",
    "title": "Monotone and Separable Set Functions: Characterizations and Neural Models",
    "summary": "Motivated by applications for set containment problems, we consider the following fundamental problem: can we design set-to-vector functions so that the natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if and only if } F(S)\\leq F(T) $. We call functions satisfying this property Monotone and Separating (MAS) set functions. % We establish lower and upper bounds for the vector dimension necessary to obtain MAS functions, as a function of the cardinality of the multisets and the underlying ground set. In the important case of an infinite ground set, we show that MAS functions do not exist, but provide a model called our which provably enjoys a relaxed MAS property we name \"weakly MAS\" and is stable in the sense of Holder continuity. We also show that MAS functions can be used to construct universal models that are monotone by construction and can approximate all monotone set functions. Experimentally, we consider a variety of set containment tasks. The experiments show the benefit of using our our model, in comparison with standard set models which do not incorporate set containment as an inductive bias. Our code is available in https://github.com/yonatansverdlov/Monotone-Embedding.",
    "published": "2025-10-24T09:59:07Z",
    "updated": "2026-01-27T08:38:27Z",
    "link": "http://arxiv.org/pdf/2510.23634v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Soutrik Sarangi",
      "Yonatan Sverdlov",
      "Nadav Dym",
      "Abir De"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19341v1",
    "title": "Robust Uncertainty Estimation under Distribution Shift via Difference Reconstruction",
    "summary": "Estimating uncertainty in deep learning models is critical for reliable decision-making in high-stakes applications such as medical imaging. Prior research has established that the difference between an input sample and its reconstructed version produced by an auxiliary model can serve as a useful proxy for uncertainty. However, directly comparing reconstructions with the original input is degraded by information loss and sensitivity to superficial details, which limits its effectiveness. In this work, we propose Difference Reconstruction Uncertainty Estimation (DRUE), a method that mitigates this limitation by reconstructing inputs from two intermediate layers and measuring the discrepancy between their outputs as the uncertainty score. To evaluate uncertainty estimation in practice, we follow the widely used out-of-distribution (OOD) detection paradigm, where in-distribution (ID) training data are compared against datasets with increasing domain shift. Using glaucoma detection as the ID task, we demonstrate that DRUE consistently achieves superior AUC and AUPR across multiple OOD datasets, highlighting its robustness and reliability under distribution shift. This work provides a principled and effective framework for enhancing model reliability in uncertain environments.",
    "published": "2026-01-27T08:23:00Z",
    "updated": "2026-01-27T08:23:00Z",
    "link": "http://arxiv.org/pdf/2601.19341v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xinran Xu",
      "Li Rong Wang",
      "Xiuyi Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19337v1",
    "title": "SETA: Statistical Fault Attribution for Compound AI Systems",
    "summary": "Modern AI systems increasingly comprise multiple interconnected neural networks to tackle complex inference tasks. Testing such systems for robustness and safety entails significant challenges. Current state-of-the-art robustness testing techniques, whether black-box or white-box, have been proposed and implemented for single-network models and do not scale well to multi-network pipelines. We propose a modular robustness testing framework that applies a given set of perturbations to test data. Our testing framework supports (1) a component-wise system analysis to isolate errors and (2) reasoning about error propagation across the neural network modules. The testing framework is architecture and modality agnostic and can be applied across domains. We apply the framework to a real-world autonomous rail inspection system composed of multiple deep networks and successfully demonstrate how our approach enables fine-grained robustness analysis beyond conventional end-to-end metrics.",
    "published": "2026-01-27T08:21:28Z",
    "updated": "2026-01-27T08:21:28Z",
    "link": "http://arxiv.org/pdf/2601.19337v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Sayak Chowdhury",
      "Meenakshi D'Souza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19336v1",
    "title": "From Observations to Events: Event-Aware World Model for Reinforcement Learning",
    "summary": "While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.",
    "published": "2026-01-27T08:20:44Z",
    "updated": "2026-01-27T08:20:44Z",
    "link": "http://arxiv.org/pdf/2601.19336v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhao-Han Peng",
      "Shaohui Li",
      "Zhi Li",
      "Shulan Ruan",
      "Yu Liu",
      "You He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19334v1",
    "title": "When Benchmarks Leak: Inference-Time Decontamination for LLMs",
    "summary": "Benchmark-based evaluation is the de facto standard for comparing large language models (LLMs). However, its reliability is increasingly threatened by test set contamination, where test samples or their close variants leak into training data and artificially inflate reported performance. To address this issue, prior work has explored two main lines of mitigation. One line attempts to identify and remove contaminated benchmark items before evaluation, but this inevitably alters the evaluation set itself and becomes unreliable when contamination is moderate or severe. The other line preserves the benchmark and instead suppresses contaminated behavior at evaluation time; however, such interventions often interfere with normal inference and lead to noticeable performance degradation on clean inputs. We propose DeconIEP, a decontamination framework that operates entirely during evaluation by applying small, bounded perturbations in the input embedding space. Guided by a relatively less-contaminated reference model, DeconIEP learns an instance-adaptive perturbation generator that steers the evaluated model away from memorization-driven shortcut pathways. Across multiple open-weight LLMs and benchmarks, extensive empirical results show that DeconIEP achieves strong decontamination effectiveness while incurring only minimal degradation in benign utility.",
    "published": "2026-01-27T08:19:40Z",
    "updated": "2026-01-27T08:19:40Z",
    "link": "http://arxiv.org/pdf/2601.19334v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jianzhe Chai",
      "Yu Zhe",
      "Jun Sakuma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05580v2",
    "title": "MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption",
    "summary": "Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, incur high compute costs, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.",
    "published": "2025-10-07T04:54:39Z",
    "updated": "2026-01-27T08:17:08Z",
    "link": "http://arxiv.org/pdf/2510.05580v2.pdf",
    "category": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Chen Li",
      "Zhantao Yang",
      "Han Zhang",
      "Fangyi Chen",
      "Chenchen Zhu",
      "Anudeepsekhar Bolimera",
      "Marios Savvides"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19325v1",
    "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
    "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.",
    "published": "2026-01-27T08:12:18Z",
    "updated": "2026-01-27T08:12:18Z",
    "link": "http://arxiv.org/pdf/2601.19325v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zichen Wen",
      "Boxue Yang",
      "Shuang Chen",
      "Yaojie Zhang",
      "Yuhang Han",
      "Junlong Ke",
      "Cong Wang",
      "Yicheng Fu",
      "Jiawang Zhao",
      "Jiangchao Yao",
      "Xi Fang",
      "Zhen Wang",
      "Henxing Cai",
      "Lin Yao",
      "Zhifeng Gao",
      "Yanhui Hong",
      "Nang Yuan",
      "Yixuan Li",
      "Guojiang Zhao",
      "Haoyi Tao",
      "Nan Wang",
      "Han Lyu",
      "Guolin Ke",
      "Ning Liao",
      "Xiaoxing Wang",
      "Kai Chen",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Sihan Hu",
      "Kun Chen",
      "Yanfeng Wang",
      "Weinan E",
      "Linfeng Zhang",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16778v2",
    "title": "GTA: Generative Traffic Agents for Simulating Realistic Mobility Behavior",
    "summary": "People's transportation choices reflect complex trade-offs shaped by personal preferences, social norms, and technology acceptance. Predicting such behavior at scale is a critical challenge with major implications for urban planning and sustainable transport. Traditional methods use handcrafted assumptions and costly data collection, making them impractical for early-stage evaluations of new technologies or policies. We introduce Generative Traffic Agents (GTA) for simulating large-scale, context-sensitive transportation choices using LLM-powered, persona-based agents. GTA generates artificial populations from census-based sociodemographic data. It simulates activity schedules and mode choices, enabling scalable, human-like simulations without handcrafted rules. We evaluate GTA in Berlin-scale experiments, comparing simulation results against empirical data. While agents replicate patterns, such as modal split by socioeconomic status, they show systematic biases in trip length and mode preference. GTA offers new opportunities for modeling how future innovations, from bike lanes to transit apps, shape mobility decisions.",
    "published": "2026-01-23T14:24:09Z",
    "updated": "2026-01-27T08:03:15Z",
    "link": "http://arxiv.org/pdf/2601.16778v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Simon Lämmer",
      "Mark Colley",
      "Patrick Ebel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19320v1",
    "title": "StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths",
    "summary": "Quantization-aware training (QAT) is essential for deploying large models under strict memory and latency constraints, yet achieving stable and robust optimization at ultra-low bitwidths remains challenging. Common approaches based on the straight-through estimator (STE) or soft quantizers often suffer from gradient mismatch, instability, or high computational overhead. As such, we propose StableQAT, a unified and efficient QAT framework that stabilizes training in ultra low-bit settings via a novel, lightweight, and theoretically grounded surrogate for backpropagation derived from a discrete Fourier analysis of the rounding operator. StableQAT strictly generalizes STE as the latter arises as a special case of our more expressive surrogate family, yielding smooth, bounded, and inexpensive gradients that improve QAT training performance and stability across various hyperparameter choices. In experiments, StableQAT exhibits stable and efficient QAT at 2-4 bit regimes, demonstrating improved training stability, robustness, and superior performance with negligible training overhead against standard QAT techniques. Our code is available at https://github.com/microsoft/StableQAT.",
    "published": "2026-01-27T08:00:57Z",
    "updated": "2026-01-27T08:00:57Z",
    "link": "http://arxiv.org/pdf/2601.19320v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Tianyi Chen",
      "Sihan Chen",
      "Xiaoyi Qu",
      "Dan Zhao",
      "Ruomei Yan",
      "Jongwoo Ko",
      "Luming Liang",
      "Pashmina Cameron"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05568v3",
    "title": "Large Multimodal Models for Low-Resource Languages: A Survey",
    "summary": "In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey.",
    "published": "2025-02-08T13:29:44Z",
    "updated": "2026-01-27T07:58:08Z",
    "link": "http://arxiv.org/pdf/2502.05568v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Marian Lupascu",
      "Ana-Cristina Rogoz",
      "Mihai Sorin Stupariu",
      "Radu Tudor Ionescu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10530v2",
    "title": "Is On-Policy Data always the Best Choice for Direct Preference Optimization-based LM Alignment?",
    "summary": "The alignment of language models~(LMs) with human preferences is critical for building reliable AI systems. The problem is typically framed as optimizing an LM policy to maximize the expected reward that reflects human preferences. Recently, Direct Preference Optimization~(DPO) was proposed as a LM alignment method that directly optimize the policy from static preference data, and further improved by incorporating on-policy sampling~(i.e., preference candidates generated during the training loop) for better LM alignment. However, we show on-policy data is not always optimal, with systematic effectiveness difference emerging between static and on-policy preference candidates. For example, on-policy data can result in a $3\\times$ effectiveness compared with static data for Llama-3, and a $0.4\\times$ effectiveness for Zephyr. To explain the phenomenon, we propose the alignment stage assumption, which divides the alignment process into two distinct stages: the preference injection stage, which benefits from diverse data, and the preference fine-tuning stage, which favors high-quality data. Through theoretical and empirical analysis, we characterize these stages and propose an effective algorithm to identify the boundaries between them. We perform experiments on $5$ models~(Llama, Zephyr, Phi-2, Qwen, Pythia) and $2$ alignment methods~(DPO, SLiC-HF) to show the generalizability of alignment stage assumption and the effectiveness of the boundary measurement algorithm.",
    "published": "2025-08-14T11:05:18Z",
    "updated": "2026-01-27T07:56:31Z",
    "link": "http://arxiv.org/pdf/2508.10530v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zetian Sun",
      "Dongfang Li",
      "Xuhui Chen",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19314v1",
    "title": "Instance-Guided Radar Depth Estimation for 3D Object Detection",
    "summary": "Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.",
    "published": "2026-01-27T07:53:24Z",
    "updated": "2026-01-27T07:53:24Z",
    "link": "http://arxiv.org/pdf/2601.19314v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chen-Chou Lo",
      "Patrick Vandewalle"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19311v1",
    "title": "Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems",
    "summary": "As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.",
    "published": "2026-01-27T07:49:55Z",
    "updated": "2026-01-27T07:49:55Z",
    "link": "http://arxiv.org/pdf/2601.19311v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Anh Khoa Ngo Ho",
      "Martin Chauvin",
      "Simon Gosset",
      "Philippe Cordier",
      "Boris Gamazaychikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.12529v2",
    "title": "Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing",
    "summary": "Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing - an endeavor requiring precision, multimodal synthesis, and domain expertise - remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2 x 2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.",
    "published": "2025-11-16T09:49:01Z",
    "updated": "2026-01-27T07:48:51Z",
    "link": "http://arxiv.org/pdf/2511.12529v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sanchaita Hazra",
      "Doeun Lee",
      "Bodhisattwa Prasad Majumder",
      "Sachin Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19306v1",
    "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
    "summary": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.",
    "published": "2026-01-27T07:46:05Z",
    "updated": "2026-01-27T07:46:05Z",
    "link": "http://arxiv.org/pdf/2601.19306v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Sijia Li",
      "Xiaoyu Tan",
      "Shahir Ali",
      "Niels Schmidt",
      "Gengchen Ma",
      "Xihe Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08931v3",
    "title": "Astra: General Interactive World Model with Autoregressive Denoising",
    "summary": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.",
    "published": "2025-12-09T18:59:57Z",
    "updated": "2026-01-27T07:42:06Z",
    "link": "http://arxiv.org/pdf/2512.08931v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yixuan Zhu",
      "Jiaqi Feng",
      "Wenzhao Zheng",
      "Yuan Gao",
      "Xin Tao",
      "Pengfei Wan",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.20691v3",
    "title": "Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs",
    "summary": "Knowledge Graph Question Answering aims to answer natural language questions by reasoning over structured knowledge graphs. While large language models have advanced KGQA through their strong reasoning capabilities, existing methods continue to struggle to fully exploit both the rich knowledge encoded in KGs and the reasoning capabilities of LLMs, particularly in complex scenarios. They often assume complete KG coverage and lack mechanisms to judge when external information is needed, and their reasoning remains locally myopic, failing to maintain coherent multi-step planning, leading to reasoning failures even when relevant knowledge exists. We propose Graph-RFT, a novel two-stage reinforcement fine-tuning KGQA framework with a 'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to perform autonomous planning and adaptive retrieval scheduling across KG and web sources under incomplete knowledge conditions. Graph-RFT introduces a chain-of-thought fine-tuning method with a customized plan-retrieval dataset activates structured reasoning and resolves the GRPO cold-start problem. It then introduces a novel plan-retrieval guided reinforcement learning process integrates explicit planning and retrieval actions with a multi-reward design, enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired planning module to decompose complex questions into ordered subquestions, and logical expression to guide tool invocation for globally consistent multi-step reasoning. This reasoning retrieval process is optimized with a multi-reward combining outcome and retrieval specific signals, enabling the model to learn when and how to combine KG and web retrieval effectively.",
    "published": "2025-10-23T16:04:13Z",
    "updated": "2026-01-27T07:40:44Z",
    "link": "http://arxiv.org/pdf/2510.20691v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yanlin Song",
      "Ben Liu",
      "Víctor Gutiérrez-Basulto",
      "Zhiwei Hu",
      "Qianqian Xie",
      "Min Peng",
      "Sophia Ananiadou",
      "Jeff Z. Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13550v2",
    "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models",
    "summary": "The development of large language models (LLMs) has successfully transformed knowledge-based systems such as open domain question nswering, which can automatically produce vast amounts of seemingly coherent information. Yet, those models have several disadvantages like hallucinations or confident generation of incorrect or unverifiable facts. In this paper, we introduce a new approach to the development of expert systems using LLMs in a controlled and transparent way. By limiting the domain and employing a well-structured prompt-based extraction approach, we produce a symbolic representation of knowledge in Prolog, which can be validated and corrected by human experts. This approach also guarantees interpretability, scalability and reliability of the developed expert systems. Via quantitative and qualitative experiments with Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic coherence on our generated knowledge bases. We present a transparent hybrid solution that combines the recall capacity of LLMs with the precision of symbolic systems, thereby laying the foundation for dependable AI applications in sensitive domains.",
    "published": "2025-07-17T21:57:37Z",
    "updated": "2026-01-27T07:35:24Z",
    "link": "http://arxiv.org/pdf/2507.13550v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "authors": [
      "Eduardo C. Garrido-Merchán",
      "Cristina Puente"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15828v2",
    "title": "Can professional translators identify machine-generated text?",
    "summary": "This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.",
    "published": "2026-01-22T10:25:52Z",
    "updated": "2026-01-27T07:23:21Z",
    "link": "http://arxiv.org/pdf/2601.15828v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Michael Farrell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10539v2",
    "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction",
    "summary": "Large language models (LLMs) have achieved remarkable success in a wide range of tasks. However, their reasoning capabilities, particularly in complex domains like mathematics, remain a significant challenge. Value-based process verifiers, which estimate the probability of a partial reasoning chain leading to a correct solution, are a promising approach for improving reasoning. Nevertheless, their effectiveness is often hindered by estimation error in their training annotations, a consequence of the limited number of Monte Carlo (MC) samples feasible due to the high cost of LLM inference. In this paper, we identify that the estimation error primarily arises from high variance rather than bias, and the MC estimator is a Minimum Variance Unbiased Estimator (MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte \\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased estimator by linearly combining the MC estimators from the current and subsequent steps. Theoretically, we show that our method leads to a predictable reduction in variance, while maintaining an unbiased estimation without additional LLM inference cost. We also perform empirical experiments on the MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method. Notably, ComMCS outperforms regression-based optimization method by 2.8 points, the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32 sampling experiment.",
    "published": "2025-08-14T11:22:29Z",
    "updated": "2026-01-27T07:18:31Z",
    "link": "http://arxiv.org/pdf/2508.10539v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zetian Sun",
      "Dongfang Li",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18776v2",
    "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field",
    "summary": "Large language models (LLMs), as a novel information technology, are seeing increasing adoption in the Architecture, Engineering, and Construction (AEC) field. They have shown their potential to streamline processes throughout the building lifecycle. However, the robustness and reliability of LLMs in such a specialized and safety-critical domain remain to be evaluated. To address this challenge, this paper establishes AECBench, a comprehensive benchmark designed to quantify the strengths and limitations of current LLMs in the AEC domain. The benchmark features a five-level, cognition-oriented evaluation framework (i.e., Knowledge Memorization, Understanding, Reasoning, Calculation, and Application). Based on the framework, 23 representative evaluation tasks were defined. These tasks were derived from authentic AEC practice, with scope ranging from codes retrieval to specialized documents generation. Subsequently, a 4,800-question dataset encompassing diverse formats, including open-ended questions, was crafted primarily by engineers and validated through a two-round expert review. Furthermore, an \"LLM-as-a-Judge\" approach was introduced to provide a scalable and consistent methodology for evaluating complex, long-form responses leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear performance decline across five cognitive levels was revealed. Despite demonstrating proficiency in foundational tasks at the Knowledge Memorization and Understanding levels, the models showed significant performance deficits, particularly in interpreting knowledge from tables in building codes, executing complex reasoning and calculation, and generating domain-specific documents. Consequently, this study lays the groundwork for future research and development aimed at the robust and reliable integration of LLMs into safety-critical engineering practices.",
    "published": "2025-09-23T08:09:58Z",
    "updated": "2026-01-27T07:17:23Z",
    "link": "http://arxiv.org/pdf/2509.18776v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chen Liang",
      "Zhaoqi Huang",
      "Haofen Wang",
      "Fu Chai",
      "Chunying Yu",
      "Huanhuan Wei",
      "Zhengjie Liu",
      "Yanpeng Li",
      "Hongjun Wang",
      "Ruifeng Luo",
      "Xianzhong Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19280v1",
    "title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning",
    "summary": "Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.\n  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.",
    "published": "2026-01-27T07:10:41Z",
    "updated": "2026-01-27T07:10:41Z",
    "link": "http://arxiv.org/pdf/2601.19280v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Kishan Panaganti",
      "Zhenwen Liang",
      "Wenhao Yu",
      "Haitao Mi",
      "Dong Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.00452v4",
    "title": "Attention-Aided MMSE for OFDM Channel Estimation: Learning Linear Filters with Attention",
    "summary": "In orthogonal frequency division multiplexing (OFDM), accurate channel estimation is crucial. Classical signal processing-based approaches, such as linear minimum mean-squared error (LMMSE) estimation, often require second-order statistics that are difficult to obtain in practice. Recent deep neural network (DNN)-based methods have been introduced to address this; yet they often suffer from high inference complexity. This paper proposes an Attention-aided MMSE (A-MMSE), a model-based DNN framework that learns the linear MMSE filter via the Attention Transformer. Once trained, the A-MMSE performs channel estimation through a single linear operation, eliminating nonlinear activations during inference and thus reducing computational complexity. To improve the learning efficiency of the A-MMSE, we develop a two-stage Attention encoder that captures the frequency and temporal correlation structure of OFDM channels. We also introduce a rank-adaptive extension that enables a flexible performance-complexity trade-off. Numerical simulations show that the proposed A-MMSE consistently outperforms other baseline methods in terms of normalized MSE across a wide range of signal-to-noise ratio (SNR) conditions. In particular, the A-MMSE and its rank-adaptive extension provide an improved performance-complexity trade-off, providing a powerful and highly efficient solution for practical channel estimation.",
    "published": "2025-05-31T08:12:04Z",
    "updated": "2026-01-27T07:07:41Z",
    "link": "http://arxiv.org/pdf/2506.00452v4.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "TaeJun Ha",
      "Chaehyun Jung",
      "Hyeonuk Kim",
      "Jeongwoo Park",
      "Jeonghun Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19276v1",
    "title": "Talos: Optimizing Top-$K$ Accuracy in Recommender Systems",
    "summary": "Recommender systems (RS) aim to retrieve a small set of items that best match individual user preferences. Naturally, RS place primary emphasis on the quality of the Top-$K$ results rather than performance across the entire item set. However, estimating Top-$K$ accuracy (e.g., Precision@$K$, Recall@$K$) requires determining the ranking positions of items, which imposes substantial computational overhead and poses significant challenges for optimization. In addition, RS often suffer from distribution shifts due to evolving user preferences or data biases, further complicating the task.\n  To address these issues, we propose Talos, a loss function that is specifically designed to optimize the Talos recommendation accuracy. Talos leverages a quantile technique that replaces the complex ranking-dependent operations into simpler comparisons between predicted scores and learned score thresholds. We further develop a sampling-based regression algorithm for efficient and accurate threshold estimation, and introduce a constraint term to maintain optimization stability by preventing score inflation. Additionally, we incorporate a tailored surrogate function to address discontinuity and enhance robustness against distribution shifts. Comprehensive theoretical analyzes and empirical experiments are conducted to demonstrate the effectiveness, efficiency, convergence, and distributional robustness of Talos. The code is available at https://github.com/cynthia-shengjia/WWW-2026-Talos.",
    "published": "2026-01-27T07:04:09Z",
    "updated": "2026-01-27T07:04:09Z",
    "link": "http://arxiv.org/pdf/2601.19276v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shengjia Zhang",
      "Weiqin Yang",
      "Jiawei Chen",
      "Peng Wu",
      "Yuegang Sun",
      "Gang Wang",
      "Qihao Shi",
      "Can Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19275v1",
    "title": "Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist",
    "summary": "Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.",
    "published": "2026-01-27T07:04:01Z",
    "updated": "2026-01-27T07:04:01Z",
    "link": "http://arxiv.org/pdf/2601.19275v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Tatsuya Kamijo",
      "Mai Nishimura",
      "Cristian C. Beltran-Hernandez",
      "Nodoka Shibasaki",
      "Masashi Hamaya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19273v1",
    "title": "Riddle Quest : The Enigma of Words",
    "summary": "Riddles are concise linguistic puzzles that describe an object or idea through indirect, figurative, or playful clues. They are a longstanding form of creative expression, requiring the solver to interpret hints, recognize patterns, and draw inferences to identify the answers. In this work, we introduce a simple pipeline for creating and evaluating analogy-based riddles. The system includes a triples creator that builds structured facts about a concept, a semantic mapper that selects attributes useful for analogy, a stylized generator that turns them into riddle clues, and a validator that collects all possible answers the riddle could point to. We use this validator to study whether large language models can recover the full answer set for different riddle types. Our case study shows that while models often guess the main intended answer, they frequently miss other valid interpretations. This highlights the value of riddles as a lightweight tool for examining reasoning coverage and ambiguity handling in language models.",
    "published": "2026-01-27T07:03:29Z",
    "updated": "2026-01-27T07:03:29Z",
    "link": "http://arxiv.org/pdf/2601.19273v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IT"
    ],
    "authors": [
      "Niharika Sri Parasa",
      "Chaitali Diwan",
      "Srinath Srinivasa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17717v2",
    "title": "The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \\textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.",
    "published": "2026-01-25T06:40:25Z",
    "updated": "2026-01-27T06:44:57Z",
    "link": "http://arxiv.org/pdf/2601.17717v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kaituo Zhang",
      "Mingzhi Hu",
      "Hoang Anh Duy Le",
      "Fariha Kabir Torsha",
      "Zhimeng Jiang",
      "Minh Khai Bui",
      "Chia-Yuan Chang",
      "Yu-Neng Chuang",
      "Zhen Xiong",
      "Ying Lin",
      "Guanchu Wang",
      "Na Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01812v4",
    "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment",
    "summary": "Singing voice generation progresses rapidly, yet evaluating singing quality remains a critical challenge. Human subjective assessment, typically in the form of listening tests, is costly and time consuming, while existing objective metrics capture only limited perceptual aspects. In this work, we introduce SingMOS-Pro, a dataset for automatic singing quality assessment. Building on our preview version SingMOS, which provides only overall ratings, SingMOS-Pro extends the annotations of the additional data to include lyrics, melody, and overall quality, offering broader coverage and greater diversity. The dataset contains 7,981 singing clips generated by 41 models across 12 datasets, spanning from early systems to recent state-of-the-art approaches. Each clip is rated by at least five experienced annotators to ensure reliability and consistency. Furthermore, we investigate strategies for effectively utilizing MOS data annotated under heterogeneous standards and benchmark several widely used evaluation methods from related tasks on SingMOS-Pro, establishing strong baselines and practical references for future research. The dataset is publicly available at https://huggingface.co/datasets/TangRain/SingMOS-Pro.",
    "published": "2025-10-02T08:53:49Z",
    "updated": "2026-01-27T06:41:44Z",
    "link": "http://arxiv.org/pdf/2510.01812v4.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Yuxun Tang",
      "Lan Liu",
      "Wenhao Feng",
      "Yiwen Zhao",
      "Jionghao Han",
      "Yifeng Yu",
      "Jiatong Shi",
      "Qin Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19261v1",
    "title": "Decoupled Split Learning via Auxiliary Loss",
    "summary": "Split learning is a distributed training paradigm where a neural network is partitioned between clients and a server, which allows data to remain at the client while only intermediate activations are shared. Traditional split learning relies on end-to-end backpropagation across the client-server split point. This incurs a large communication overhead (i.e., forward activations and backward gradients need to be exchanged every iteration) and significant memory use (for storing activations and gradients). In this paper, we develop a beyond-backpropagation training method for split learning. In this approach, the client and server train their model partitions semi-independently, using local loss signals instead of propagated gradients. In particular, the client's network is augmented with a small auxiliary classifier at the split point to provide a local error signal, while the server trains on the client's transmitted activations using the true loss function. This decoupling removes the need to send backward gradients, which cuts communication costs roughly in half and also reduces memory overhead (as each side only stores local activations for its own backward pass). We evaluate our approach on CIFAR-10 and CIFAR-100. Our experiments show two key results. First, the proposed approach achieves performance on par with standard split learning that uses backpropagation. Second, it significantly reduces communication (of transmitting activations/gradient) by 50% and peak memory usage by up to 58%.",
    "published": "2026-01-27T06:41:42Z",
    "updated": "2026-01-27T06:41:42Z",
    "link": "http://arxiv.org/pdf/2601.19261v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Anower Zihad",
      "Felix Owino",
      "Haibo Yang",
      "Ming Tang",
      "Chao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19258v1",
    "title": "GhostUI: Unveiling Hidden Interactions in Mobile UI",
    "summary": "Modern mobile applications rely on hidden interactions--gestures without visual cues like long presses and swipes--to provide functionality without cluttering interfaces. While experienced users may discover these interactions through prior use or onboarding tutorials, their implicit nature makes them difficult for most users to uncover. Similarly, mobile agents--systems designed to automate tasks on mobile user interfaces, powered by vision language models (VLMs)--struggle to detect veiled interactions or determine actions for completing tasks. To address this challenge, we present GhostUI, a new dataset designed to enable the detection of hidden interactions in mobile applications. GhostUI provides before-and-after screenshots, simplified view hierarchies, gesture metadata, and task descriptions, allowing VLMs to better recognize concealed gestures and anticipate post-interaction states. Quantitative evaluations with VLMs show that models fine-tuned on GhostUI outperform baseline VLMs, particularly in predicting hidden interactions and inferring post-interaction screens, underscoring GhostUI's potential as a foundation for advancing mobile task automation.",
    "published": "2026-01-27T06:40:29Z",
    "updated": "2026-01-27T06:40:29Z",
    "link": "http://arxiv.org/pdf/2601.19258v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Minkyu Kweon",
      "Seokhyeon Park",
      "Soohyun Lee",
      "You Been Lee",
      "Jeongmin Rhee",
      "Jinwook Seo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19257v1",
    "title": "PCEvo: Path-Consistent Molecular Representation via Virtual Evolutionary",
    "summary": "Molecular representation learning aims to learn vector embeddings that capture molecular structure and geometry, thereby enabling property prediction and downstream scientific applications. In many AI for science tasks, labeled data are expensive to obtain and therefore limited in availability. Under the few-shot setting, models trained with scarce supervision often learn brittle structure-property relationships, resulting in substantially higher prediction errors and reduced generalization to unseen molecules. To address this limitation, we propose PCEvo, a path-consistent representation method that learns from virtual paths through dynamic structural evolution. PCEvo enumerates multiple chemically feasible edit paths between retrieved similar molecular pairs under topological dependency constraints. It transforms the labels of the two molecules into stepwise supervision along each virtual evolutionary path. It introduces a path-consistency objective that enforces prediction invariance across alternative paths connecting the same two molecules. Comprehensive experiments on the QM9 and MoleculeNet datasets demonstrate that PCEvo substantially improves the few-shot generalization performance of baseline methods. The code is available at https://anonymous.4open.science/r/PCEvo-4BF2.",
    "published": "2026-01-27T06:40:11Z",
    "updated": "2026-01-27T06:40:11Z",
    "link": "http://arxiv.org/pdf/2601.19257v1.pdf",
    "category": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kun Li",
      "Longtao Hu",
      "Yida Xiong",
      "Jiajun Yu",
      "Hongzhi Zhang",
      "Jiameng Chen",
      "Xiantao Cai",
      "Jia Wu",
      "Wenbin Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19255v1",
    "title": "LLM-Assisted Logic Rule Learning: Scaling Human Expertise for Time Series Anomaly Detection",
    "summary": "Time series anomaly detection is critical for supply chain management to take proactive operations, but faces challenges: classical unsupervised anomaly detection based on exploiting data patterns often yields results misaligned with business requirements and domain knowledge, while manual expert analysis cannot scale to millions of products in the supply chain. We propose a framework that leverages large language models (LLMs) to systematically encode human expertise into interpretable, logic-based rules for detecting anomaly patterns in supply chain time series data. Our approach operates in three stages: 1) LLM-based labeling of training data instructed by domain knowledge, 2) automated generation and iterative improvements of symbolic rules through LLM-driven optimization, and 3) rule augmentation with business-relevant anomaly categories supported by LLMs to enhance interpretability. The experiment results showcase that our approach outperforms the unsupervised learning methods in both detection accuracy and interpretability. Furthermore, compared to direct LLM deployment for time series anomaly detection, our approach provides consistent, deterministic results with low computational latency and cost, making it ideal for production deployment. The proposed framework thus demonstrates how LLMs can bridge the gap between scalable automation and expert-driven decision-making in operational settings.",
    "published": "2026-01-27T06:37:37Z",
    "updated": "2026-01-27T06:37:37Z",
    "link": "http://arxiv.org/pdf/2601.19255v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haoting Zhang",
      "Shekhar Jain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19249v1",
    "title": "GLOVE: Global Verifier for LLM Memory-Environment Realignment",
    "summary": "Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.",
    "published": "2026-01-27T06:32:05Z",
    "updated": "2026-01-27T06:32:05Z",
    "link": "http://arxiv.org/pdf/2601.19249v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xingkun Yin",
      "Hongyang Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.20772v3",
    "title": "Damper-B-PINN: Damper Characteristics-Based Bayesian Physics-Informed Neural Network for Vehicle State Estimation",
    "summary": "Accurate state estimation is fundamental to intelligent vehicles. Wheel load, one of the most important chassis states, serves as an essential input for advanced driver assistance systems (ADAS) and exerts a direct influence on vehicle stability and safety. However, wheel load estimation remains challenging due to the complexity of chassis modeling and the susceptibility of nonlinear systems to noise. To address these issues, this paper first introduces a refined suspension linkage-level modeling approach that constructs a nonlinear instantaneous dynamic model by explicitly considering the complex geometric structure of the suspension. Building upon this, we propose a damper characteristics-based Bayesian physics-informed neural network (Damper-B-PINN) framework to estimate dynamic wheel load, which leverages the suspension dynamics as physical guidance of PINN while employing Bayesian inference to mitigate the effects of system noise and uncertainty. Moreover, a damper-characteristic physics conditioning (DPC) module is designed for embedding physical prior. The proposed Damper-B-PINN is evaluated using both high-fidelity simulation datasets generated by CarSim software and real-world datasets collected from a Formula Student race car. Experimental results demonstrate that our Damper-B-PINN consistently outperforms existing methods across various test conditions, particularly extreme ones. These findings highlight the potential of the proposed Damper-B-PINN framework to enhance the accuracy and robustness of dynamic wheel load estimation, thereby improving the reliability and safety of ADAS applications.",
    "published": "2025-02-28T06:46:21Z",
    "updated": "2026-01-27T06:31:30Z",
    "link": "http://arxiv.org/pdf/2502.20772v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Tianyi Zeng",
      "Tianyi Wang",
      "Zimo Zeng",
      "Feiyang Zhang",
      "Jiseop Byeon",
      "Yujin Wang",
      "Yajie Zou",
      "Yangyang Wang",
      "Junfeng Jiao",
      "Christian Claudel",
      "Xinbo Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19245v1",
    "title": "Beyond In-Domain Detection: SpikeScore for Cross-Domain Hallucination Detection",
    "summary": "Hallucination detection is critical for deploying large language models (LLMs) in real-world applications. Existing hallucination detection methods achieve strong performance when the training and test data come from the same domain, but they suffer from poor cross-domain generalization. In this paper, we study an important yet overlooked problem, termed generalizable hallucination detection (GHD), which aims to train hallucination detectors on data from a single domain while ensuring robust performance across diverse related domains. In studying GHD, we simulate multi-turn dialogues following LLMs initial response and observe an interesting phenomenon: hallucination-initiated multi-turn dialogues universally exhibit larger uncertainty fluctuations than factual ones across different domains. Based on the phenomenon, we propose a new score SpikeScore, which quantifies abrupt fluctuations in multi-turn dialogues. Through both theoretical analysis and empirical validation, we demonstrate that SpikeScore achieves strong cross-domain separability between hallucinated and non-hallucinated responses. Experiments across multiple LLMs and benchmarks demonstrate that the SpikeScore-based detection method outperforms representative baselines in cross-domain generalization and surpasses advanced generalization-oriented methods, verifying the effectiveness of our method in cross-domain hallucination detection.",
    "published": "2026-01-27T06:28:35Z",
    "updated": "2026-01-27T06:28:35Z",
    "link": "http://arxiv.org/pdf/2601.19245v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yongxin Deng",
      "Zhen Fang",
      "Yixuan Li",
      "Ling Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.03709v2",
    "title": "AutoGameUI: Constructing High-Fidelity GameUI via Multimodal Correspondence Matching",
    "summary": "Game UI development is essential to the game industry. However, the traditional workflow requires substantial manual effort to integrate pairwise UI and UX designs into a cohesive game user interface (GameUI). The inconsistency between the aesthetic UI design and the functional UX design typically results in mismatches and inefficiencies. To address the issue, we present an automatic system, AutoGameUI, for efficiently and accurately constructing GameUI. The system centers on a two-stage multimodal learning pipeline to obtain the optimal correspondences between UI and UX designs. The first stage learns the comprehensive representations of UI and UX designs from multimodal perspectives. The second stage incorporates grouped cross-attention modules with constrained integer programming to estimate the optimal correspondences through top-down hierarchical matching. The optimal correspondences enable the automatic GameUI construction. We create the GAMEUI dataset, comprising pairwise UI and UX designs from real-world games, to train and validate the proposed method. Besides, an interactive web tool is implemented to ensure high-fidelity effects and facilitate human-in-the-loop construction. Extensive experiments on the GAMEUI and RICO datasets demonstrate the effectiveness of our system in maintaining consistency between the constructed GameUI and the original designs. When deployed in the workflow of several mobile games, AutoGameUI achieves a 3$\\times$ improvement in time efficiency, conveying significant practical value for game UI development.",
    "published": "2024-11-06T07:16:54Z",
    "updated": "2026-01-27T06:22:17Z",
    "link": "http://arxiv.org/pdf/2411.03709v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Zhongliang Tang",
      "Qingrong Cheng",
      "Mengchen Tan",
      "Yongxiang Zhang",
      "Fei Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.18925v2",
    "title": "Beyond the Prompt: An Empirical Study of Cursor Rules",
    "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.\n  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.",
    "published": "2025-12-21T23:51:02Z",
    "updated": "2026-01-27T06:15:28Z",
    "link": "http://arxiv.org/pdf/2512.18925v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Shaokang Jiang",
      "Daye Nam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17581v2",
    "title": "How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests",
    "summary": "AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.",
    "published": "2026-01-24T20:27:04Z",
    "updated": "2026-01-27T06:14:36Z",
    "link": "http://arxiv.org/pdf/2601.17581v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Daniel Ogenrwot",
      "John Businge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19232v1",
    "title": "Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model",
    "summary": "RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.",
    "published": "2026-01-27T06:04:02Z",
    "updated": "2026-01-27T06:04:02Z",
    "link": "http://arxiv.org/pdf/2601.19232v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qi Si",
      "Xuyang Liu",
      "Penglei Wang",
      "Xin Guo",
      "Yuan Qi",
      "Yuan Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14988v6",
    "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits",
    "summary": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at ensuring fair outcomes across agents while maximizing overall system performance. A key challenge in this setting is decision-making under limited information about arm rewards. To address this, we introduce a novel probing framework that strategically gathers information about selected arms before allocation. In the offline setting, where reward distributions are known, we leverage submodular properties to design a greedy probing algorithm with a provable performance bound. For the more complex online setting, we develop an algorithm that achieves sublinear regret while maintaining fairness. Extensive experiments on synthetic and real-world datasets show that our approach outperforms baseline methods, achieving better fairness and efficiency.",
    "published": "2025-06-17T21:43:21Z",
    "updated": "2026-01-27T05:57:08Z",
    "link": "http://arxiv.org/pdf/2506.14988v6.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Tianyi Xu",
      "Jiaxin Liu",
      "Nicholas Mattei",
      "Zizhan Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19225v1",
    "title": "RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering",
    "summary": "Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.",
    "published": "2026-01-27T05:46:32Z",
    "updated": "2026-01-27T05:46:32Z",
    "link": "http://arxiv.org/pdf/2601.19225v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Kaehyun Um",
      "KyuHwan Yeom",
      "Haerim Yang",
      "Minyoung Choi",
      "Hyeongjun Yang",
      "Kyong-Ho Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19222v1",
    "title": "UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection",
    "summary": "Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research.",
    "published": "2026-01-27T05:42:45Z",
    "updated": "2026-01-27T05:42:45Z",
    "link": "http://arxiv.org/pdf/2601.19222v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Fuxiang Sun",
      "Xi Jiang",
      "Jiansheng Wu",
      "Haigang Zhang",
      "Feng Zheng",
      "Jinfeng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19216v1",
    "title": "Bridging Visual and Wireless Sensing: A Unified Radiation Field for 3D Radio Map Construction",
    "summary": "The emerging applications of next-generation wireless networks (e.g., immersive 3D communication, low-altitude networks, and integrated sensing and communication) necessitate high-fidelity environmental intelligence. 3D radio maps have emerged as a critical tool for this purpose, enabling spectrum-aware planning and environment-aware sensing by bridging the gap between physical environments and electromagnetic signal propagation. However, constructing accurate 3D radio maps requires fine-grained 3D geometric information and a profound understanding of electromagnetic wave propagation. Existing approaches typically treat optical and wireless knowledge as distinct modalities, failing to exploit the fundamental physical principles governing both light and electromagnetic propagation. To bridge this gap, we propose URF-GS, a unified radio-optical radiation field representation framework for accurate and generalizable 3D radio map construction based on 3D Gaussian splatting (3D-GS) and inverse rendering. By fusing visual and wireless sensing observations, URF-GS recovers scene geometry and material properties while accurately predicting radio signal behavior at arbitrary transmitter-receiver (Tx-Rx) configurations. Experimental results demonstrate that URF-GS achieves up to a 24.7% improvement in spatial spectrum prediction accuracy and a 10x increase in sample efficiency for 3D radio map construction compared with neural radiance field (NeRF)-based methods. This work establishes a foundation for next-generation wireless networks by integrating perception, interaction, and communication through holistic radiation field reconstruction.",
    "published": "2026-01-27T05:35:50Z",
    "updated": "2026-01-27T05:35:50Z",
    "link": "http://arxiv.org/pdf/2601.19216v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Chaozheng Wen",
      "Jingwen Tong",
      "Zehong Lin",
      "Chenghong Bian",
      "Jun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15751v2",
    "title": "Tabular Incremental Inference",
    "summary": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.",
    "published": "2026-01-22T08:24:31Z",
    "updated": "2026-01-27T05:31:04Z",
    "link": "http://arxiv.org/pdf/2601.15751v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xinda Chen",
      "Zhen Xing",
      "Hanyu Zhang",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19214v1",
    "title": "A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews",
    "summary": "Extracting actionable suggestions from customer reviews is essential for operational decision-making, yet these directives are often embedded within mixed-intent, unstructured text. Existing approaches either classify suggestion-bearing sentences or generate high-level summaries, but rarely isolate the precise improvement instructions businesses need. We evaluate a hybrid pipeline combining a high-recall RoBERTa classifier trained with a precision-recall surrogate to reduce unrecoverable false negatives with a controlled, instruction-tuned LLM for suggestion extraction, categorization, clustering, and summarization. Across real-world hospitality and food datasets, the hybrid system outperforms prompt-only, rule-based, and classifier-only baselines in extraction accuracy and cluster coherence. Human evaluations further confirm that the resulting suggestions and summaries are clear, faithful, and interpretable. Overall, our results show that hybrid reasoning architectures achieve meaningful improvements fine-grained actionable suggestion mining while highlighting challenges in domain adaptation and efficient local deployment.",
    "published": "2026-01-27T05:30:29Z",
    "updated": "2026-01-27T05:30:29Z",
    "link": "http://arxiv.org/pdf/2601.19214v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Aakash Trivedi",
      "Aniket Upadhyay",
      "Pratik Narang",
      "Dhruv Kumar",
      "Praveen Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13948v2",
    "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
    "summary": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
    "published": "2026-01-20T13:23:44Z",
    "updated": "2026-01-27T05:25:34Z",
    "link": "http://arxiv.org/pdf/2601.13948v2.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Nikita Kuzmin",
      "Songting Liu",
      "Kong Aik Lee",
      "Eng Siong Chng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.01857v3",
    "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
    "summary": "As agent systems powered by large language models (LLMs) advance, improving performance in context understanding, tool usage, and long-horizon execution has become critical. However, existing agent frameworks and benchmarks provide limited visibility into execution-level behavior, making failures in tool invocation, state tracking, and context management difficult to diagnose. This paper presents Jenius-Agent, a system-level agent framework grounded in real-world deployment experience. It integrates adaptive prompt generation, context-aware tool orchestration, and layered memory mechanism to stabilize execution and improve robustness in long-horizon, tool-augmented tasks. Beyond system design, we introduce an evaluation methodology that jointly measures procedural fidelity, semantic correctness, and efficiency. This framework makes agent behavior observable as a structured execution process and enables systematic analysis of failure modes not captured by output-only metrics. Experiments on Jenius-bench show substantial improvements in task completion rate, with up to a 35 percent relative gain over the base agent, along with reduced token consumption, response latency, and tool invocation failures. The framework is already deployed in Jenius ({https://www.jenius.cn}), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
    "published": "2026-01-05T07:35:12Z",
    "updated": "2026-01-27T05:16:28Z",
    "link": "http://arxiv.org/pdf/2601.01857v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Defei Xia",
      "Bingfeng Pi",
      "Shenbin Zhang",
      "Song Hua",
      "Yunfei Wei",
      "Lei Zuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.06832v2",
    "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges",
    "summary": "The mainstream paradigm of remote sensing image interpretation has long been dominated by vision-centered models, which rely on visual features for semantic understanding. However, these models face inherent limitations in handling multi-modal reasoning, semantic abstraction, and interactive decision-making. While recent advances have introduced Large Language Models (LLMs) into remote sensing workflows, existing studies primarily focus on downstream applications, lacking a unified theoretical framework that explains the cognitive role of language. This review advocates a paradigm shift from vision-centered to language-centered remote sensing interpretation. Drawing inspiration from the Global Workspace Theory (GWT) of human cognition, We propose a language-centered framework for remote sensing interpretation that treats LLMs as the cognitive central hub integrating perceptual, task, knowledge and action spaces to enable unified understanding, reasoning, and decision-making. We first explore the potential of LLMs as the central cognitive component in remote sensing interpretation, and then summarize core technical challenges, including unified multimodal representation, knowledge association, and reasoning and decision-making. Furthermore, we construct a global workspace-driven interpretation mechanism and review how language-centered solutions address each challenge. Finally, we outline future research directions from four perspectives: adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to provide a conceptual foundation for the next generation of remote sensing interpretation systems and establish a roadmap toward cognition-driven intelligent geospatial analysis.",
    "published": "2025-08-09T05:10:38Z",
    "updated": "2026-01-27T05:14:39Z",
    "link": "http://arxiv.org/pdf/2508.06832v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Haifeng Li",
      "Wang Guo",
      "Haiyang Wu",
      "Mengwei Wu",
      "Jipeng Zhang",
      "Qing Zhu",
      "Yu Liu",
      "Xin Huang",
      "Chao Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19205v1",
    "title": "EnzyPGM: Pocket-conditioned Generative Model for Substrate-specific Enzyme Design",
    "summary": "Designing enzymes with substrate-binding pockets is a critical challenge in protein engineering, as catalytic activity depends on the precise interaction between pockets and substrates. Currently, generative models dominate functional protein design but cannot model pocket-substrate interactions, which limits the generation of enzymes with precise catalytic environments. To address this issue, we propose EnzyPGM, a unified framework that jointly generates enzymes and substrate-binding pockets conditioned on functional priors and substrates, with a particular focus on learning accurate pocket-substrate interactions. At its core, EnzyPGM includes two main modules: a Residue-atom Bi-scale Attention (RBA) that jointly models intra-residue dependencies and fine-grained interactions between pocket residues and substrate atoms, and a Residue Function Fusion (RFF) that incorporates enzyme function priors into residue representations. Also, we curate EnzyPock, an enzyme-pocket dataset comprising 83,062 enzyme-substrate pairs across 1,036 four-level enzyme families. Extensive experiments demonstrate that EnzyPGM achieves state-of-the-art performance on EnzyPock. Notably, EnzyPGM reduces the average binding energy of 0.47 kcal/mol over EnzyGen, showing its superior performance on substrate-specific enzyme design. The code and dataset will be released later.",
    "published": "2026-01-27T05:07:55Z",
    "updated": "2026-01-27T05:07:55Z",
    "link": "http://arxiv.org/pdf/2601.19205v1.pdf",
    "category": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zefeng Lin",
      "Zhihang Zhang",
      "Weirong Zhu",
      "Tongchang Han",
      "Xianyong Fang",
      "Tianfan Fu",
      "Xiaohua Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19204v1",
    "title": "MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning",
    "summary": "Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.",
    "published": "2026-01-27T05:06:54Z",
    "updated": "2026-01-27T05:06:54Z",
    "link": "http://arxiv.org/pdf/2601.19204v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Zhixi Cai",
      "Fucai Ke",
      "Kevin Leo",
      "Sukai Huang",
      "Maria Garcia de la Banda",
      "Peter J. Stuckey",
      "Hamid Rezatofighi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19899v1",
    "title": "Evaluation of Oncotimia: An LLM based system for supporting tumour boards",
    "summary": "Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.",
    "published": "2026-01-27T18:59:38Z",
    "updated": "2026-01-27T18:59:38Z",
    "link": "http://arxiv.org/pdf/2601.19899v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Luis Lorenzo",
      "Marcos Montana-Mendez",
      "Sergio Figueiras",
      "Miguel Boubeta",
      "Cristobal Bernardo-Castineira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19895v1",
    "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
    "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.",
    "published": "2026-01-27T18:58:46Z",
    "updated": "2026-01-27T18:58:46Z",
    "link": "http://arxiv.org/pdf/2601.19895v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Chen Chen",
      "Lai Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19871v1",
    "title": "Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection",
    "summary": "Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Translation, a prompt-based framework in which a model generates an initial translation, produces a structured self-critique, and then uses this reflection to generate a refined translation. The approach is evaluated on English-isiZulu and English-isiXhosa translation using OPUS-100 and NTREX-African, across multiple prompting strategies and confidence thresholds. Results show consistent improvements in both BLEU and COMET scores between first- and second-pass translations, with average gains of up to +0.22 BLEU and +0.18 COMET. Statistical significance testing using paired nonparametric tests confirms that these improvements are robust. The proposed method is model-agnostic, requires no fine-tuning, and introduces a reflection-augmented dataset that can support future supervised or analysis-driven work. These findings demonstrate that structured self-reflection is a practical and effective mechanism for improving translation quality in low-resource settings.",
    "published": "2026-01-27T18:37:09Z",
    "updated": "2026-01-27T18:37:09Z",
    "link": "http://arxiv.org/pdf/2601.19871v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nicholas Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.19378v4",
    "title": "TableMaster: A Recipe to Advance Table Understanding with Language Models",
    "summary": "Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines. We hope this work will serve as a practical step toward more robust and reliable table understanding.",
    "published": "2025-01-31T18:31:31Z",
    "updated": "2026-01-27T18:04:05Z",
    "link": "http://arxiv.org/pdf/2501.19378v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lang Cao",
      "Hanbing Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.17768v2",
    "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "summary": "Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its efficiency-accuracy trade-offs remain unclear due to the lack of comprehensive evaluation. We address this gap with the largest-scale empirical analysis to date of training-free sparse attention, evaluating six methods across multiple model families and sizes, sequences up to 128K tokens, and sparsity levels up to 0.95 (i.e., $1/20$ attention budget) on nine diverse tasks. We first organise the rapidly evolving landscape of sparse attention methods into a taxonomy along four design axes. Our analysis then yields actionable insights: 1) sparse attention is effective -- larger sparse models outperform smaller dense ones at equivalent cost, improving the Pareto frontier; 2) due to computational constraints, token-to-page importance estimation is unfeasible during prefilling, where the choice of an alternative solution (global-to-token or block-to-block) depends on the task, but is possible during decoding, enabling better generalisation and tolerance to higher sparsity; 3) longer sequences tolerate higher sparsity, suggesting that fixed-budget methods in production are suboptimal. Together, these findings provide practical guidance for deploying sparse attention and methodological recommendations for future evaluations. Our code is available at https://github.com/PiotrNawrot/sparse-frontier.",
    "published": "2025-04-24T17:39:25Z",
    "updated": "2026-01-27T17:59:04Z",
    "link": "http://arxiv.org/pdf/2504.17768v2.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Piotr Nawrot",
      "Robert Li",
      "Renjie Huang",
      "Sebastian Ruder",
      "Kelly Marchisio",
      "Edoardo M. Ponti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19847v1",
    "title": "Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering",
    "summary": "Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases. Experiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25. Moreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost.",
    "published": "2026-01-27T17:53:01Z",
    "updated": "2026-01-27T17:53:01Z",
    "link": "http://arxiv.org/pdf/2601.19847v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Fangan Dong",
      "Zuming Yan",
      "Xuri Ge",
      "Zhiwei Xu",
      "Mengqi Zhang",
      "Xuanang Chen",
      "Ben He",
      "Xin Xin",
      "Zhumin Chen",
      "Ying Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.01509v2",
    "title": "PROPHET: An Inferable Future Forecasting Benchmark with Causal Intervened Likelihood Estimation",
    "summary": "Predicting future events based on news on the Web stands as one of the ultimate aspirations of artificial intelligence. Recent advances in large language model (LLM)-based systems have shown remarkable potential in forecasting future events, thereby garnering significant interest in the research community. Currently, several benchmarks have been established to evaluate the forecasting capabilities by formalizing the event prediction as a retrieval-augmented generation (RAG)-and-reasoning task. In these benchmarks, each prediction question is answered with relevant retrieved news articles downloaded from the Web. However, because there is no consideration of whether the questions can be supported by valid or sufficient supporting rationales, some of the questions in these benchmarks may be inherently noninferable. To address this issue, we introduce a new benchmark, PROPHET, which comprises inferable forecasting questions paired with relevant news for retrieval. To ensure the inferability of the benchmark, we propose Causal Intervened Likelihood (CIL), a statistical measure that assesses inferability through causal inference. In constructing this benchmark, we first collected recent trend forecasting questions, and then filtered the data using CIL resulting in an inferable benchmark for future forecasting. Through extensive experiments, we first demonstrate the validity of CIL and in-depth investigations into future forecasting with the aid of CIL. Subsequently, we evaluate several representative prediction methods on PROPHET. The overall results draws valuable insights for task of future directions.",
    "published": "2025-04-02T08:57:42Z",
    "updated": "2026-01-27T17:41:45Z",
    "link": "http://arxiv.org/pdf/2504.01509v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhengwei Tao",
      "Pu Wu",
      "Zhi Jin",
      "Xiaoying Bai",
      "Haiyan Zhao",
      "Chengfeng Dou",
      "Xiancai Chen",
      "Jia Li",
      "Linyu Li",
      "Chongyang Tao",
      "Wentao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19831v1",
    "title": "Neural Neural Scaling Laws",
    "summary": "Neural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors. To address this, we propose Neural Neural Scaling Laws (NeuNeu), a neural network that frames scaling law prediction as time-series extrapolation. NeuNeu combines temporal context from observed accuracy trajectories with token-level validation losses, learning to predict future performance without assuming any bottleneck or functional form. Trained entirely on open-source model checkpoints from HuggingFace, NeuNeu achieves 2.04% mean absolute error in predicting model accuracy on 66 downstream tasks -- a 38% reduction compared to logistic scaling laws (3.29% MAE). Furthermore, NeuNeu generalizes zero-shot to unseen model families, parameter counts, and downstream tasks. Our work suggests that predicting downstream scaling laws directly from data outperforms parametric alternatives.",
    "published": "2026-01-27T17:38:11Z",
    "updated": "2026-01-27T17:38:11Z",
    "link": "http://arxiv.org/pdf/2601.19831v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Michael Y. Hu",
      "Jane Pan",
      "Ayush Rajesh Jhaveri",
      "Nicholas Lourie",
      "Kyunghyun Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19802v1",
    "title": "Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation",
    "summary": "Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to automatically identify multiple target-stance pairs from text without prior target knowledge. We construct a Chinese social media stance detection dataset and design multi-dimensional evaluation metrics. We explore both integrated and two-stage fine-tuning strategies for large language models (LLMs) and evaluate various baseline models. Experimental results demonstrate that fine-tuned LLMs achieve superior performance on this task: the two-stage fine-tuned Qwen2.5-7B attains the highest comprehensive target recognition score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B achieves a stance detection F1 score of 79.26%.",
    "published": "2026-01-27T17:04:18Z",
    "updated": "2026-01-27T17:04:18Z",
    "link": "http://arxiv.org/pdf/2601.19802v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Aohua Li",
      "Yuanshuo Zhang",
      "Ge Gao",
      "Bo Chen",
      "Xiaobing Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11051v3",
    "title": "CAMEO: Collection of Multilingual Emotional Speech Corpora",
    "summary": "This paper presents CAMEO -- a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a standardized benchmark for evaluating speech emotion recognition (SER) systems across different emotional states and languages. The paper describes the dataset selection criteria, the curation and normalization process, and provides performance results for several models. The collection, along with metadata, and a leaderboard, is publicly available via the Hugging Face platform.",
    "published": "2025-05-16T09:52:00Z",
    "updated": "2026-01-27T16:56:40Z",
    "link": "http://arxiv.org/pdf/2505.11051v3.pdf",
    "category": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Iwona Christop",
      "Maciej Czajka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19786v1",
    "title": "Rethinking Discrete Speech Representation Tokens for Accent Generation",
    "summary": "Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phonetic and speaker information in DSRTs, how accent information is encoded in DSRTs remains largely unexplored. In this paper, we present the first systematic investigation of accent information in DSRTs. We propose a unified evaluation framework that measures both accessibility of accent information via a novel Accent ABX task and recoverability via cross-accent Voice Conversion (VC) resynthesis. Using this framework, we analyse DSRTs derived from a variety of speech encoders. Our results reveal that accent information is substantially reduced when ASR supervision is used to fine-tune the encoder, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. Based on these findings, we propose new content-only and content-accent DSRTs that significantly outperform existing designs in controllable accent generation. Our work highlights the importance of accent-aware evaluation and provides practical guidance for designing DSRTs for accent-controlled speech generation.",
    "published": "2026-01-27T16:48:48Z",
    "updated": "2026-01-27T16:48:48Z",
    "link": "http://arxiv.org/pdf/2601.19786v1.pdf",
    "category": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      "Jinzuomu Zhong",
      "Yi Wang",
      "Korin Richmond",
      "Peter Bell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21850v2",
    "title": "SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models",
    "summary": "Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to bridge the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.",
    "published": "2025-10-22T17:47:12Z",
    "updated": "2026-01-27T16:39:04Z",
    "link": "http://arxiv.org/pdf/2510.21850v2.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Gyubeum Lim",
      "Yemo Koo",
      "Vijay Krishna Madisetti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19773v1",
    "title": "Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis",
    "summary": "Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \\rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark .",
    "published": "2026-01-27T16:36:35Z",
    "updated": "2026-01-27T16:36:35Z",
    "link": "http://arxiv.org/pdf/2601.19773v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhuohan Long",
      "Zhijie Bao",
      "Zhongyu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11254v2",
    "title": "Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality",
    "summary": "Psychometric tests are increasingly used to assess psychological constructs in large language models (LLMs). However, it remains unclear whether these tests -- originally developed for humans -- yield meaningful results when applied to LLMs. In this study, we systematically evaluate the reliability and validity of human psychometric tests on 17 LLMs for three constructs: sexism, racism, and morality. We find moderate reliability across multiple item and prompt variations. Validity is evaluated through both convergent (i.e., testing theory-based inter-test correlations) and ecological approaches (i.e., testing the alignment between tests scores and behavior in real-world downstream tasks). Crucially, we find that psychometric test scores do not align, and in some cases even negatively correlate with, model behavior in downstream tasks, indicating low ecological validity. Our results highlight that systematic evaluations of psychometric tests on LLMs are essential before interpreting their scores. Our findings also suggest that psychometric tests designed for humans cannot be applied directly to LLMs without adaptation.",
    "published": "2025-10-13T10:43:49Z",
    "updated": "2026-01-27T16:13:26Z",
    "link": "http://arxiv.org/pdf/2510.11254v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jana Jung",
      "Marlene Lutz",
      "Indira Sen",
      "Markus Strohmaier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10113v4",
    "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs",
    "summary": "In this paper, we introduce S-MedQA, an English medical question-answering (QA) dataset designed for benchmarking large language models (LLMs) in fine-grained clinical specialties. S-MedQA consists of over 24k examples, covering 15 medical specialties, with QA pairs that can have multiple specialty annotations, such as when a question is cross-disciplinary. The dataset is constructed using both machine and expert verification to maximize data availability and reliability. We use S-MedQA to investigate the role of clinical specialties in the knowledge-intensive scenario of medical QA. Our results show that training on data from a clinical specialty does not necessarily lead to the best performance on that specialty. Additionally, regardless of the specialty the LLM was fine-tuned on, token probabilities of clinically relevant terms consistently increase across all specialties. Based on these findings, we hypothesize that improvement gains, at least in our settings, are derived primarily from domain shifting (e.g., general to medical) rather than from injecting specialty-specific knowledge. This suggests a need to rethink the role of fine-tuning data in the medical domain. To encourage further advancements in the clinical NLP field, we release S-MedQA along with all the code required to reproduce our experiments for the research community.",
    "published": "2025-05-15T09:35:26Z",
    "updated": "2026-01-27T15:33:01Z",
    "link": "http://arxiv.org/pdf/2505.10113v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xinlan Yan",
      "Di Wu",
      "Yibin Lei",
      "Christof Monz",
      "Iacer Calixto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10077v2",
    "title": "A-IPO: Adaptive Intent-driven Preference Optimization",
    "summary": "Human preferences are diverse and dynamic, shaped by regional, cultural, and social factors. Existing alignment methods like Direct Preference Optimization (DPO) and its variants often default to majority views, overlooking minority opinions and failing to capture latent user intentions in prompts.\n  To address these limitations, we introduce \\underline{\\textbf{A}}daptive \\textbf{\\underline{I}}ntent-driven \\textbf{\\underline{P}}reference \\textbf{\\underline{O}}ptimization (\\textbf{A-IPO}). Specifically,A-IPO introduces an intention module that infers the latent intent behind each user prompt and explicitly incorporates this inferred intent into the reward function, encouraging stronger alignment between the preferred model's responses and the user's underlying intentions. We demonstrate, both theoretically and empirically, that incorporating an intention--response similarity term increases the preference margin (by a positive shift of $λ\\,Δ\\mathrm{sim}$ in the log-odds), resulting in clearer separation between preferred and dispreferred responses compared to DPO.\n  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess real-world and adversarial preference alignment.\n  Through explicit modeling of diverse user intents,A-IPO facilitates pluralistic preference optimization while simultaneously enhancing adversarial robustness in preference alignment. Comprehensive empirical evaluation demonstrates that A-IPO consistently surpasses existing baselines, yielding substantial improvements across key metrics: up to +24.8 win-rate and +45.6 Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention Consistency Score on GlobalOpinionQA-Ext.",
    "published": "2025-10-11T07:29:11Z",
    "updated": "2026-01-27T15:30:25Z",
    "link": "http://arxiv.org/pdf/2510.10077v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Wenqing Wang",
      "Muhammad Asif Ali",
      "Ali Shoker",
      "Ruohan Yang",
      "Junyang Chen",
      "Ying Sha",
      "Huan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16435v3",
    "title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
    "summary": "Humans develop perception through a bottom-up hierarchy: from basic primitives and Gestalt principles to high-level semantics. In contrast, current Multimodal Large Language Models (MLLMs) are trained directly on complex downstream tasks, often bypassing these foundational visual capabilities. To systematically investigate this gap, we introduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from FRCT, a well-established cognitive psychology assessment spanning four domains of human visual cognition. Furthermore, we design algorithms to automatically construct and validate unlimited test cases with controllable difficulty. Using VisFactor, we evaluate 23 frontier MLLMs, including both proprietary (e.g., GPT, Gemini) and open-source (e.g., LLaMA, Qwen) models. The best model achieves a score of only 30.17%. Models consistently fail on tasks such as mental rotation, spatial relation inference, and figure-ground discrimination, regardless of model size or prompting strategy. These findings suggest that performance improvements on existing general benchmarks might represent castles in the air instead of a genuine mastery of human-like visual cognition.",
    "published": "2025-02-23T04:21:32Z",
    "updated": "2026-01-27T15:14:06Z",
    "link": "http://arxiv.org/pdf/2502.16435v3.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Jen-Tse Huang",
      "Dasen Dai",
      "Jen-Yuan Huang",
      "Youliang Yuan",
      "Xiaoyuan Liu",
      "Wenxuan Wang",
      "Wenxiang Jiao",
      "Pinjia He",
      "Zhaopeng Tu",
      "Haodong Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11811v7",
    "title": "Less is More: Compact Clue Selection for Efficient Retrieval-Augmented Generation Reasoning",
    "summary": "Current RAG retrievers are designed primarily for human readers, emphasizing complete, readable, and coherent paragraphs. However, Large Language Models (LLMs) benefit more from precise, compact, and well-structured input, which enhances reasoning quality and efficiency. Existing methods rely on reranking or summarization to identify key sentences, but may introduce semantic breaks and unfaithfulness. Thus, efficiently extracting and organizing answer-relevant clues from large-scale documents while reducing LLM reasoning costs remains challenging in RAG systems. Inspired by Occam's razor, we frame LLM-centric retrieval as MinMax optimization: maximizing the extraction of potential clues and reranking them for well-organization, while minimizing reasoning costs by truncating to the smallest sufficient set of clues. In this paper, we propose CompSelect, a compact clue selection mechanism for LLM-centric RAG, consisting of a clue extractor, a reranker, and a truncator. (1) The clue extractor first uses answer-containing sentences as fine-tuning targets, aiming to extract sufficient potential clues; (2) The reranker is trained to prioritize effective clues based on real LLM feedback; (3) The truncator uses the truncated text containing the minimum sufficient clues for answering the question as fine-tuning targets, thereby enabling efficient RAG reasoning. Experiments on three QA datasets demonstrate that CompSelect improves performance while reducing both total and online latency compared to a range of baseline methods. Further analysis also confirms its robustness to unreliable retrieval and generalization across different scenarios.",
    "published": "2025-02-17T13:55:42Z",
    "updated": "2026-01-27T14:37:51Z",
    "link": "http://arxiv.org/pdf/2502.11811v7.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Qianchi Zhang",
      "Hainan Zhang",
      "Liang Pang",
      "Yongxin Tong",
      "Hongwei Zheng",
      "Zhiming Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19637v1",
    "title": "RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems",
    "summary": "Reviewer assignment is increasingly critical yet challenging in the LLM era, where rapid topic shifts render many pre-2023 benchmarks outdated and where proxy signals poorly reflect true reviewer familiarity. We address this evaluation bottleneck by introducing LR-bench, a high-fidelity, up-to-date benchmark curated from 2024-2025 AI/NLP manuscripts with five-level self-assessed familiarity ratings collected via a large-scale email survey, yielding 1055 expert-annotated paper-reviewer-score annotations. We further propose RATE, a reviewer-centric ranking framework that distills each reviewer's recent publications into compact keyword-based profiles and fine-tunes an embedding model with weak preference supervision constructed from heuristic retrieval signals, enabling matching each manuscript against a reviewer profile directly. Across LR-bench and the CMU gold-standard dataset, our approach consistently achieves state-of-the-art performance, outperforming strong embedding baselines by a clear margin. We release LR-bench at https://huggingface.co/datasets/Gnociew/LR-bench, and a GitHub repository at https://github.com/Gnociew/RATE-Reviewer-Assign.",
    "published": "2026-01-27T14:13:46Z",
    "updated": "2026-01-27T14:13:46Z",
    "link": "http://arxiv.org/pdf/2601.19637v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Weicong Liu",
      "Zixuan Yang",
      "Yibo Zhao",
      "Xiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19605v1",
    "title": "Decompose-and-Formalise: Recursively Verifiable Natural Language Inference",
    "summary": "Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $θ$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.",
    "published": "2026-01-27T13:43:30Z",
    "updated": "2026-01-27T13:43:30Z",
    "link": "http://arxiv.org/pdf/2601.19605v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xin Quan",
      "Marco Valentino",
      "Louise A. Dennis",
      "André Freitas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.10441v2",
    "title": "Analogical Structure, Minimal Contextual Cues and Contrastive Distractors: Input Design for Sample-Efficient Linguistic Rule Induction",
    "summary": "Large language models achieve strong performance on many tasks, but their training makes it hard to see which properties of the input support efficient linguistic rule learning. We ask how three cognitively-inspired principles of input design support sample-efficient linguistic rule induction: analogical structure, contrastive learning, and minimal contextual cue. We also ask how their effects compare to those of LLMs on the same controlled tasks. We implement these principles in structured sentence completion tasks that test English verb alternations. Lightweight models trained on hundreds to one-thousand such examples learn the alternation rules with high F1 on these tasks. Ablation studies show that analogical organisation is the main driver of sample efficiency, and contrastive distractors and minimal context help further gains. We also evaluate zero- and few-shot LLMs on the same tasks. In this controlled setting, the lightweight models reach higher F1 with far fewer task-specific data. We treat this contrast as a comparison between learning regimes rather than a general verdict on LLMs. Our results show that careful input organisation supports sample-efficient learning of linguistic rules and reveals distinct learning signatures for trained lightweight models and prompted LLMs.",
    "published": "2025-11-13T16:04:46Z",
    "updated": "2026-01-27T13:15:24Z",
    "link": "http://arxiv.org/pdf/2511.10441v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Chunyang Jiang",
      "Paola Merlo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19578v1",
    "title": "Yunque DeepResearch Technical Report",
    "summary": "Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.",
    "published": "2026-01-27T13:10:00Z",
    "updated": "2026-01-27T13:10:00Z",
    "link": "http://arxiv.org/pdf/2601.19578v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuxuan Cai",
      "Xinyi Lai",
      "Peng Yuan",
      "Weiting Liu",
      "Huajian Li",
      "Mingda Li",
      "Xinghua Wang",
      "Shengxie Zheng",
      "Yanchao Hao",
      "Yuyang Yin",
      "Zheng Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01900v2",
    "title": "High-Layer Attention Pruning with Rescaling",
    "summary": "Pruning is a highly effective approach for compressing large language models (LLMs), significantly reducing inference latency. However, conventional training-free structured pruning methods often employ a heuristic metric that indiscriminately removes some attention heads across all pruning layers, without considering their positions within the network architecture. In this work, we propose a novel pruning algorithm that strategically prunes attention heads in the model's higher layers. Since the removal of attention heads can alter the magnitude of token representations, we introduce an adaptive rescaling parameter that calibrates the representation scale post-pruning to counteract this effect. We conduct comprehensive experiments on a wide range of LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our evaluation includes both generation and discriminative tasks across 27 datasets. The results consistently demonstrate that our method outperforms existing structured pruning methods. This improvement is particularly notable in generation tasks, where our approach significantly outperforms existing baselines. Code is available at https://github.com/SongtaoLiu0823/HARP.",
    "published": "2025-07-02T17:15:05Z",
    "updated": "2026-01-27T12:54:05Z",
    "link": "http://arxiv.org/pdf/2507.01900v2.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Songtao Liu",
      "Peng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.10246v2",
    "title": "coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts",
    "summary": "Access to mental healthcare is increasingly strained by workforce shortages and rising demand, motivating the development of intelligent systems that can support mental healthcare experts. We introduce coTherapist, a unified framework utilizing a small language model to emulate core therapeutic competencies through domain-specific fine-tuning, retrieval augmentation, and agentic reasoning. Evaluation on clinical queries demonstrates that coTherapist generates more relevant and clinically grounded responses than contemporary baselines. Using our novel T-BARS rubric and psychometric profiling, we confirm coTherapist exhibits high empathy and therapist-consistent personality traits. Furthermore, human evaluation by domain experts validates that coTherapist delivers accurate, trustworthy, and safe responses. coTherapist was deployed and tested by clinical experts. Collectively, these findings demonstrate that small models can be engineered to exhibit expert-like behavior, offering a scalable pathway for digital mental health tools.",
    "published": "2026-01-15T10:06:28Z",
    "updated": "2026-01-27T12:15:26Z",
    "link": "http://arxiv.org/pdf/2601.10246v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Prottay Kumar Adhikary",
      "Reena Rawat",
      "Tanmoy Chakraborty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19513v1",
    "title": "Enhancing Academic Paper Recommendations Using Fine-Grained Knowledge Entities and Multifaceted Document Embeddings",
    "summary": "In the era of explosive growth in academic literature, the burden of literature review on scholars are increasing. Proactively recommending academic papers that align with scholars' literature needs in the research process has become one of the crucial pathways to enhance research efficiency and stimulate innovative thinking. Current academic paper recommendation systems primarily focus on broad and coarse-grained suggestions based on general topic or field similarities. While these systems effectively identify related literature, they fall short in addressing scholars' more specific and fine-grained needs, such as locating papers that utilize particular research methods, or tackle distinct research tasks within the same topic. To meet the diverse and specific literature needs of scholars in the research process, this paper proposes a novel academic paper recommendation method. This approach embeds multidimensional information by integrating new types of fine-grained knowledge entities, title and abstract of document, and citation data. Recommendations are then generated by calculating the similarity between combined paper vectors. The proposed recommendation method was evaluated using the STM-KG dataset, a knowledge graph that incorporates scientific concepts derived from papers across ten distinct domains. The experimental results indicate that our method outperforms baseline models, achieving an average precision of 27.3% among the top 50 recommendations. This represents an improvement of 6.7% over existing approaches.",
    "published": "2026-01-27T11:55:10Z",
    "updated": "2026-01-27T11:55:10Z",
    "link": "http://arxiv.org/pdf/2601.19513v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL",
      "cs.DL"
    ],
    "authors": [
      "Haixu Xi",
      "Heng Zhang",
      "Chengzhi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19510v1",
    "title": "ALRM: Agentic LLM for Robotic Manipulation",
    "summary": "Large Language Models (LLMs) have recently empowered agentic frameworks to exhibit advanced reasoning and planning capabilities. However, their integration in robotic control pipelines remains limited in two aspects: (1) prior \\ac{llm}-based approaches often lack modular, agentic execution mechanisms, limiting their ability to plan, reflect on outcomes, and revise actions in a closed-loop manner; and (2) existing benchmarks for manipulation tasks focus on low-level control and do not systematically evaluate multistep reasoning and linguistic variation. In this paper, we propose Agentic LLM for Robot Manipulation (ALRM), an LLM-driven agentic framework for robotic manipulation. ALRM integrates policy generation with agentic execution through a ReAct-style reasoning loop, supporting two complementary modes: Code-asPolicy (CaP) for direct executable control code generation, and Tool-as-Policy (TaP) for iterative planning and tool-based action execution. To enable systematic evaluation, we also introduce a novel simulation benchmark comprising 56 tasks across multiple environments, capturing linguistically diverse instructions. Experiments with ten LLMs demonstrate that ALRM provides a scalable, interpretable, and modular approach for bridging natural language reasoning with reliable robotic execution. Results reveal Claude-4.1-Opus as the top closed-source model and Falcon-H1-7B as the top open-source model under CaP.",
    "published": "2026-01-27T11:54:14Z",
    "updated": "2026-01-27T11:54:14Z",
    "link": "http://arxiv.org/pdf/2601.19510v1.pdf",
    "category": [
      "cs.RO",
      "cs.CL"
    ],
    "authors": [
      "Vitor Gaboardi dos Santos",
      "Ibrahim Khadraoui",
      "Ibrahim Farhat",
      "Hamza Yous",
      "Samy Teffahi",
      "Hakim Hacid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19507v1",
    "title": "Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs",
    "summary": "Large vision-language models (LVLMs) exhibit remarkable capabilities in cross-modal tasks but face significant safety challenges, which undermine their reliability in real-world applications. Efforts have been made to build LVLM safety evaluation benchmarks to uncover their vulnerability. However, existing benchmarks are hindered by their labor-intensive construction process, static complexity, and limited discriminative power. Thus, they may fail to keep pace with rapidly evolving models and emerging risks. To address these limitations, we propose VLSafetyBencher, the first automated system for LVLM safety benchmarking. VLSafetyBencher introduces four collaborative agents: Data Preprocessing, Generation, Augmentation, and Selection agents to construct and select high-quality samples. Experiments validates that VLSafetyBencher can construct high-quality safety benchmarks within one week at a minimal cost. The generated benchmark effectively distinguish safety, with a safety rate disparity of 70% between the most and least safe models.",
    "published": "2026-01-27T11:51:30Z",
    "updated": "2026-01-27T11:51:30Z",
    "link": "http://arxiv.org/pdf/2601.19507v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xiangyang Zhu",
      "Yuan Tian",
      "Zicheng Zhang",
      "Qi Jia",
      "Chunyi Li",
      "Renrui Zhang",
      "Heng Li",
      "Zongrui Wang",
      "Wei Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.13547v4",
    "title": "ThinkNote: Enhancing Knowledge Integration and Utilization of Large Language Models via Constructivist Cognition Modeling",
    "summary": "Large Language Models (LLMs) have demonstrated strong performance across a wide range of NLP tasks. However, they often exhibit suboptimal behaviors and inconsistencies when exposed to unfamiliar external information, underscoring their limitations in effectively leveraging such knowledge. Inspired by constructivist learning theory, we propose ThinkNote, a novel framework that enhances the external knowledge utilization of LLMs through a two-stage constructivist cognitive modeling process. Specifically, ThinkNote performs knowledge assimilation to align new information with the model's parametric memory, forming a coherent internal representation. It then applies thought accommodation to adapt internal reasoning, thereby promoting more consistent and reliable outputs. Extensive experimental results demonstrate that ThinkNote achieves a 10% improvement over strong baseline methods on various question-answering benchmarks. Further analysis indicates that ThinkNote effectively integrates and utilizes external knowledge to help LLMs generate accurate responses and improves their self-consistency. All data and codes are available at https://github.com/OpenMatch/ThinkNote.",
    "published": "2024-02-21T06:04:53Z",
    "updated": "2026-01-27T11:49:38Z",
    "link": "http://arxiv.org/pdf/2402.13547v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhipeng Xu",
      "Zhenghao Liu",
      "Yukun Yan",
      "Shuo Wang",
      "Shi Yu",
      "Zheni Zeng",
      "Chaojun Xiao",
      "Zhiyuan Liu",
      "Ge Yu",
      "Chenyan Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19490v1",
    "title": "ClaimPT: A Portuguese Dataset of Annotated Claims in News Articles",
    "summary": "Fact-checking remains a demanding and time-consuming task, still largely dependent on manual verification and unable to match the rapid spread of misinformation online. This is particularly important because debunking false information typically takes longer to reach consumers than the misinformation itself; accelerating corrections through automation can therefore help counter it more effectively. Although many organizations perform manual fact-checking, this approach is difficult to scale given the growing volume of digital content. These limitations have motivated interest in automating fact-checking, where identifying claims is a crucial first step. However, progress has been uneven across languages, with English dominating due to abundant annotated data. Portuguese, like other languages, still lacks accessible, licensed datasets, limiting research, NLP developments and applications. In this paper, we introduce ClaimPT, a dataset of European Portuguese news articles annotated for factual claims, comprising 1,308 articles and 6,875 individual annotations. Unlike most existing resources based on social media or parliamentary transcripts, ClaimPT focuses on journalistic content, collected through a partnership with LUSA, the Portuguese News Agency. To ensure annotation quality, two trained annotators labeled each article, with a curator validating all annotations according to a newly proposed scheme. We also provide baseline models for claim detection, establishing initial benchmarks and enabling future NLP and IR applications. By releasing ClaimPT, we aim to advance research on low-resource fact-checking and enhance understanding of misinformation in news media.",
    "published": "2026-01-27T11:22:00Z",
    "updated": "2026-01-27T11:22:00Z",
    "link": "http://arxiv.org/pdf/2601.19490v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ricardo Campos",
      "Raquel Sequeira",
      "Sara Nerea",
      "Inês Cantante",
      "Diogo Folques",
      "Luís Filipe Cunha",
      "João Canavilhas",
      "António Branco",
      "Alípio Jorge",
      "Sérgio Nunes",
      "Nuno Guimarães",
      "Purificação Silvano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14105v2",
    "title": "Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks",
    "summary": "This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means",
    "published": "2026-01-20T16:04:09Z",
    "updated": "2026-01-27T10:46:38Z",
    "link": "http://arxiv.org/pdf/2601.14105v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Olesya Razuvayevskaya",
      "Kalina Bontcheva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19451v1",
    "title": "Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition",
    "summary": "Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.",
    "published": "2026-01-27T10:37:03Z",
    "updated": "2026-01-27T10:37:03Z",
    "link": "http://arxiv.org/pdf/2601.19451v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Isha Pandey",
      "Ashish Mittal",
      "Vartul Bahuguna",
      "Ganesh Ramakrishnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19410v1",
    "title": "Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?",
    "summary": "Automatic post-editing (APE) aims to refine machine translations by correcting residual errors. Although recent large language models (LLMs) demonstrate strong translation capabilities, their effectiveness for APE--especially under document-level context--remains insufficiently understood. We present a systematic comparison of proprietary and open-weight LLMs under a naive document-level prompting setup, analyzing APE quality, contextual behavior, robustness, and efficiency.\n  Our results show that proprietary LLMs achieve near human-level APE quality even with simple one-shot prompting, regardless of whether document context is provided. While these models exhibit higher robustness to data poisoning attacks than open-weight counterparts, this robustness also reveals a limitation: they largely fail to exploit document-level context for contextual error correction. Furthermore, standard automatic metrics do not reliably reflect these qualitative improvements, highlighting the continued necessity of human evaluation. Despite their strong performance, the substantial cost and latency overheads of proprietary LLMs render them impractical for real-world APE deployment. Overall, our findings elucidate both the promise and current limitations of LLM-based document-aware APE, and point toward the need for more efficient long-context modeling approaches for translation refinement.",
    "published": "2026-01-27T09:45:29Z",
    "updated": "2026-01-27T09:45:29Z",
    "link": "http://arxiv.org/pdf/2601.19410v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ahrii Kim",
      "Seong-heum Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19360v1",
    "title": "Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement",
    "summary": "We present a comprehensive approach for multiword expression (MWE) identification that combines binary token-level classification, linguistic feature integration, and data augmentation. Our DeBERTa-v3-large model achieves 69.8% F1 on the CoAM dataset, surpassing the best results (Qwen-72B, 57.8% F1) on this dataset by 12 points while using 165x fewer parameters. We achieve this performance by (1) reformulating detection as binary token-level START/END/INSIDE classification rather than span-based prediction, (2) incorporating NP chunking and dependency features that help discontinuous and NOUN-type MWEs identification, and (3) applying oversampling that addresses severe class imbalance in the training data. We confirm the generalization of our method on the STREUSLE dataset, achieving 78.9% F1. These results demonstrate that carefully designed smaller models can substantially outperform LLMs on structured NLP tasks, with important implications for resource-constrained deployments.",
    "published": "2026-01-27T08:42:54Z",
    "updated": "2026-01-27T08:42:54Z",
    "link": "http://arxiv.org/pdf/2601.19360v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Diego Rossini",
      "Lonneke van der Plas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19350v1",
    "title": "Cross-Examination Framework: A Task-Agnostic Diagnostic for Information Fidelity in Text-to-Text Generation",
    "summary": "Traditional metrics like BLEU and BERTScore fail to capture semantic fidelity in generative text-to-text tasks. We adapt the Cross-Examination Framework (CEF) for a reference-free, multi-dimensional evaluation by treating the source and candidate as independent knowledge bases. CEF generates verifiable questions from each text and performs a cross-examination to derive three interpretable scores: Coverage, Conformity, and Consistency. Validated across translation, summarization and clinical note-generation, our framework identifies critical errors, such as content omissions and factual contradictions, missed by standard metrics. A key contribution is a systematic robustness analysis to select a stable judge model. Crucially, the strong correlation between our reference-free and with-reference modes validates CEF's reliability without gold references. Furthermore, human expert validation demonstrates that CEF mismatching questions align with meaning-altering semantic errors higher than with non-semantic errors, particularly excelling at identifying entity-based and relational distortions.",
    "published": "2026-01-27T08:30:13Z",
    "updated": "2026-01-27T08:30:13Z",
    "link": "http://arxiv.org/pdf/2601.19350v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tathagata Raha",
      "Clement Christophe",
      "Nada Saadi",
      "Hamza A Javed",
      "Marco AF Pimentel",
      "Ronnie Rajan",
      "Praveenkumar Kanithi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.17048v2",
    "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
    "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
    "published": "2024-12-22T14:59:19Z",
    "updated": "2026-01-27T08:10:34Z",
    "link": "http://arxiv.org/pdf/2412.17048v2.pdf",
    "category": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      "Hankun Wang",
      "Haoran Wang",
      "Yiwei Guo",
      "Zhihan Li",
      "Chenpeng Du",
      "Kai Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19302v1",
    "title": "Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics",
    "summary": "Prompting techniques such as Chain-of-Thought (CoT) and Program-of-Thought (PoT) improve LLM mathematical reasoning by structuring intermediate steps in natural language or code. However, applied mathematics problems in domains like finance, physics, and cryptography often require recalling or deriving governing equations, a step that current approaches do not explicitly leverage. We propose Formula-One Prompting (F-1), a two-phase approach that uses mathematical equations as an intermediate representation before adaptive solving. F-1 first formulates governing equations from problem descriptions, then selects a solving strategy among CoT, PoT, or direct computation based on the generated equations, all within a single LLM call. Results across five models and four benchmarks show F-1 outperforms CoT by +5.76% and PoT by +8.42% on average. Crucially, gains are largest in applied domains: +13.30% on FinanceMath over CoT, and within OlympiadBench, larger gains on physics (+2.55%) than pure math (+0.44%). This demonstrates that F-1 is more effective than CoT in applied mathematics problems.",
    "published": "2026-01-27T07:42:20Z",
    "updated": "2026-01-27T07:42:20Z",
    "link": "http://arxiv.org/pdf/2601.19302v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Natapong Nitarach",
      "Pittawat Taveekitworachai",
      "Kunat Pipatanakul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.07295v3",
    "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation",
    "summary": "As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel Cross-lingual and Cross-modal Factuality benchmark (CCFQA). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMs' cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at https://github.com/yxduir/ccfqa.",
    "published": "2025-08-10T11:09:41Z",
    "updated": "2026-01-27T07:24:49Z",
    "link": "http://arxiv.org/pdf/2508.07295v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yexing Du",
      "Kaiyuan Liu",
      "Youcheng Pan",
      "Zheng Chu",
      "Bo Yang",
      "Xiaocheng Feng",
      "Ming Liu",
      "Yang Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19290v1",
    "title": "MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning",
    "summary": "Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.",
    "published": "2026-01-27T07:24:35Z",
    "updated": "2026-01-27T07:24:35Z",
    "link": "http://arxiv.org/pdf/2601.19290v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yimeng Wang",
      "Jiaxing Zhao",
      "Hongbin Xie",
      "Hexing Ma",
      "Yuzhen Lei",
      "Shuangxue Liu",
      "Xuan Song",
      "Zichen Zhang",
      "Haoran Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19286v1",
    "title": "ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction",
    "summary": "Electronic Health Records (EHRs) provide crucial information for clinical decision-making. However, their high-dimensionality, heterogeneity, and sparsity make clinical prediction challenging. Large Language Models (LLMs) allowed progress towards addressing this challenge by leveraging parametric medical knowledge to enhance EHR data for clinical prediction tasks. Despite the significant achievements made so far, most of the existing approaches are fundamentally task-agnostic in the sense that they deploy LLMs as EHR encoders or EHR completion modules without fully integrating signals from the prediction tasks. This naturally hinders task performance accuracy. In this work, we propose Rewrite-To-Predict (ReToP), an LLM-based framework that addresses this limitation through an end-to-end training of an EHR rewriter and a clinical predictor. To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter. ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score that enables the EHR rewriter to generate clinically relevant rewrites that directly enhance prediction. Our ReToP framework surpasses strong baseline models across three clinical tasks on MIMIC-IV. Moreover, the analysis of ReToP shows its generalizability to unseen datasets and tasks with minimal fine-tuning while preserving faithful rewrites and emphasizing task-relevant predictive features.",
    "published": "2026-01-27T07:20:46Z",
    "updated": "2026-01-27T07:20:46Z",
    "link": "http://arxiv.org/pdf/2601.19286v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jesus Lovon-Melgarejo",
      "Jose G. Moreno",
      "Christine Damase-Michel",
      "Lynda Tamine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19278v1",
    "title": "DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference",
    "summary": "Speculative decoding is an effective and lossless approach for accelerating LLM inference. However, existing widely adopted model-based draft designs, such as EAGLE3, improve accuracy at the cost of multi-step autoregressive inference, resulting in high drafting latency and ultimately rendering the drafting stage itself a performance bottleneck. Inspired by diffusion-based large language models (dLLMs), we propose DART, which leverages parallel generation to reduce drafting latency. DART predicts logits for multiple future masked positions in parallel within a single forward pass based on hidden states of the target model, thereby eliminating autoregressive rollouts in the draft model while preserving a lightweight design. Based on these parallel logit predictions, we further introduce an efficient tree pruning algorithm that constructs high-quality draft token trees with N-gram-enforced semantic continuity. DART substantially reduces draft-stage overhead while preserving high draft accuracy, leading to significantly improved end-to-end decoding speed. Experimental results demonstrate that DART achieves a 2.03x--3.44x wall-clock time speedup across multiple datasets, surpassing EAGLE3 by 30% on average and offering a practical speculative decoding framework. Code is released at https://github.com/fvliang/DART.",
    "published": "2026-01-27T07:04:24Z",
    "updated": "2026-01-27T07:04:24Z",
    "link": "http://arxiv.org/pdf/2601.19278v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Fuliang Liu",
      "Xue Li",
      "Ketai Zhao",
      "Yinxi Gao",
      "Ziyan Zhou",
      "Zhonghui Zhang",
      "Zhibin Wang",
      "Wanchun Dou",
      "Sheng Zhong",
      "Chen Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03870v2",
    "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
    "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
    "published": "2025-12-03T15:22:00Z",
    "updated": "2026-01-27T07:00:22Z",
    "link": "http://arxiv.org/pdf/2512.03870v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hongzhan Lin",
      "Zhiqi Bai",
      "Xinmiao Zhang",
      "Sen Yang",
      "Xiang Li",
      "Siran Yang",
      "Yunlong Xu",
      "Jiaheng Liu",
      "Yongchi Zhao",
      "Jiamang Wang",
      "Yuchi Xu",
      "Wenbo Su",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19267v1",
    "title": "DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models",
    "summary": "Accurate dialogue description in audiovisual video captioning is crucial for downstream understanding and generation tasks. However, existing models generally struggle to produce faithful dialogue descriptions within audiovisual captions. To mitigate this limitation, we propose DiaDem, a powerful audiovisual video captioning model capable of generating captions with more precise dialogue descriptions while maintaining strong overall performance. We first synthesize a high-quality dataset for SFT, then employ a difficulty-partitioned two-stage GRPO strategy to further enhance dialogue descriptions. To enable systematic evaluation of dialogue description capabilities, we introduce DiaDemBench, a comprehensive benchmark designed to evaluate models across diverse dialogue scenarios, emphasizing both speaker attribution accuracy and utterance transcription fidelity in audiovisual captions. Extensive experiments on DiaDemBench reveal even commercial models still exhibit substantial room for improvement in dialogue-aware captioning. Notably, DiaDem not only outperforms the Gemini series in dialogue description accuracy but also achieves competitive performance on general audiovisual captioning benchmarks, demonstrating its overall effectiveness.",
    "published": "2026-01-27T06:55:21Z",
    "updated": "2026-01-27T06:55:21Z",
    "link": "http://arxiv.org/pdf/2601.19267v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xinlong Chen",
      "Weihong Lin",
      "Jingyun Hua",
      "Linli Yao",
      "Yue Ding",
      "Bozhou Li",
      "Bohan Zeng",
      "Yang Shi",
      "Qiang Liu",
      "Yuanxing Zhang",
      "Pengfei Wan",
      "Liang Wang",
      "Tieniu Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12344v2",
    "title": "Propaganda AI: An Analysis of Semantic Divergence in Large Language Models",
    "summary": "Large language models (LLMs) can exhibit concept-conditioned semantic divergence: common high-level cues (e.g., ideologies, public figures) elicit unusually uniform, stance-like responses that evade token-trigger audits. This behavior falls in a blind spot of current safety evaluations, yet carries major societal stakes, as such concept cues can steer content exposure at scale. We formalize this phenomenon and present RAVEN (Response Anomaly Vigilance), a black-box audit that flags cases where a model is simultaneously highly certain and atypical among peers by coupling semantic entropy over paraphrastic samples with cross-model disagreement. In a controlled LoRA fine-tuning study, we implant a concept-conditioned stance using a small biased corpus, demonstrating feasibility without rare token triggers. Auditing five LLM families across twelve sensitive topics (360 prompts per model) and clustering via bidirectional entailment, RAVEN surfaces recurrent, model-specific divergences in 9/12 topics. Concept-level audits complement token-level defenses and provide a practical early-warning signal for release evaluation and post-deployment monitoring against propaganda-like influence.",
    "published": "2025-04-15T16:43:15Z",
    "updated": "2026-01-27T06:40:02Z",
    "link": "http://arxiv.org/pdf/2504.12344v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nay Myat Min",
      "Long H. Pham",
      "Yige Li",
      "Jun Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09225v2",
    "title": "Unsupervised lexicon learning from speech is limited by representations rather than clustering",
    "summary": "Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels. Despite progress, the induced lexicons are still far from perfect. In an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types. We combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (K-means, hierarchical, graph-based) on English and Mandarin data. The best system uses graph clustering with dynamic time warping on continuous features. Faster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences. Through controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type -- rather than clustering -- is the primary factor limiting performance.",
    "published": "2025-10-10T10:12:11Z",
    "updated": "2026-01-27T06:34:07Z",
    "link": "http://arxiv.org/pdf/2510.09225v2.pdf",
    "category": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      "Danel Slabbert",
      "Simon Malan",
      "Herman Kamper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.01035v5",
    "title": "Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning",
    "summary": "Large language models demonstrate impressive performance on downstream tasks, yet they require extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions (TSDs), which are critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Additionally, based on our exploration of TSD, we focus on an important issue in PEFT: the initialization of LoRA. While some works have pointed out the significance of initialization for LoRA's performance and proposed various strategies, these methods are often empirical and not task-specific. To address this issue, we propose LoRA-Init. Starting from TSD, we identify the directions that require the most adjustment during fine-tuning for downstream tasks. By initializing the matrices in LoRA with these directions, LoRA-Init significantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash and LoRA-Init to create the final version of LoRA based on TSDs, which we refer to as LoRA-TSD. Extensive experiments have conclusively demonstrated the effectiveness of these methods, and in-depth analyses further reveal the underlying mechanisms behind their success.",
    "published": "2024-09-02T08:10:51Z",
    "updated": "2026-01-27T06:31:07Z",
    "link": "http://arxiv.org/pdf/2409.01035v5.pdf",
    "category": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Chongjie Si",
      "Zhiyi Shi",
      "Shifan Zhang",
      "Xiaokang Yang",
      "Hanspeter Pfister",
      "Wei Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12815v4",
    "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
    "summary": "Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.",
    "published": "2026-01-19T08:21:46Z",
    "updated": "2026-01-27T06:04:33Z",
    "link": "http://arxiv.org/pdf/2601.12815v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhaolu Kang",
      "Junhao Gong",
      "Qingxi Chen",
      "Hao Zhang",
      "Jiaxin Liu",
      "Rong Fu",
      "Zhiyuan Feng",
      "Yuan Wang",
      "Simon Fong",
      "Kaiyue Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19221v1",
    "title": "DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models",
    "summary": "Modern Recurrent Neural Networks (RNNs), such as RWKV, are distinguished by their powerful short-range modeling capabilities and efficient fixed-size states, which constitute a core advantage over standard Transformers. However, there is a significant lack of research into their internal state as an editable knowledge representation. To fill this gap, we first explore the representational properties of the RWKV state by proposing the DREAMSTATE framework. This framework utilizes a conditional Diffusion Transformer (DiT) to directly model the probability manifold of the state, enabling its generation and editing. The structural nature of this representation is validated through t-SNE visualizations and controlled generation experiments. After successfully uncovering and modeling the state's representational potential, we further propose a novel hybrid architecture that combines the local advantages of RNNs with global context adaptability. This architecture features a parallel DiT that processes a variable-length global context to dynamically generate and adjust the core recurrent module's WKV parameters, transforming the fixed recurrence mechanism into a context-aware dynamic function. Experiments demonstrate that this hybrid model can be trained stably via a multi-objective loss, validating its design feasibility. Our work not only opens a new research direction for RNN state representation but also provides a concrete architectural reference for future model design. The code is publicly available at: https://huggingface.co/2dgx41s/DreamState.",
    "published": "2026-01-27T05:42:25Z",
    "updated": "2026-01-27T05:42:25Z",
    "link": "http://arxiv.org/pdf/2601.19221v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Liu Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19208v1",
    "title": "How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability",
    "summary": "Semantic associations such as the link between \"bird\" and \"flew\" are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data in attention-based language models through the lens of training dynamics. By leveraging a leading-term approximation of the gradients, we develop closed-form expressions for the weights at early stages of training that explain how semantic associations first take shape. Through our analysis, we reveal that each set of weights of the transformer has closed-form expressions as simple compositions of three basis functions (bigram, token-interchangeability, and context mappings), reflecting the statistics of the text corpus and uncovering how each component of the transformer captures semantic associations based on these compositions. Experiments on real-world LLMs demonstrate that our theoretical weight characterizations closely match the learned weights, and qualitative analyses further show how our theorem shines light on interpreting the learned associations in transformers.",
    "published": "2026-01-27T05:22:34Z",
    "updated": "2026-01-27T05:22:34Z",
    "link": "http://arxiv.org/pdf/2601.19208v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Shawn Im",
      "Changdae Oh",
      "Zhen Fang",
      "Sharon Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19202v1",
    "title": "Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs",
    "summary": "Vision-Language Models (VLMs) have shown strong multimodal reasoning capabilities on Visual-Question-Answering (VQA) benchmarks. However, their robustness against textual misinformation remains under-explored. While existing research has studied the effect of misinformation in text-only domains, it is not clear how VLMs arbitrate between contradictory information from different modalities. To bridge the gap, we first propose the CONTEXT-VQA (i.e., Conflicting Text) dataset, consisting of image-question pairs together with systematically generated persuasive prompts that deliberately conflict with visual evidence. Then, a thorough evaluation framework is designed and executed to benchmark the susceptibility of various models to these conflicting multimodal inputs. Comprehensive experiments over 11 state-of-the-art VLMs reveal that these models are indeed vulnerable to misleading textual prompts, often overriding clear visual evidence in favor of the conflicting text, and show an average performance drop of over 48.2% after only one round of persuasive conversation. Our findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.",
    "published": "2026-01-27T05:04:38Z",
    "updated": "2026-01-27T05:04:38Z",
    "link": "http://arxiv.org/pdf/2601.19202v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Chi Zhang",
      "Wenxuan Ding",
      "Jiale Liu",
      "Mingrui Wu",
      "Qingyun Wu",
      "Ray Mooney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19898v1",
    "title": "DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding",
    "summary": "Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.",
    "published": "2026-01-27T18:59:19Z",
    "updated": "2026-01-27T18:59:19Z",
    "link": "http://arxiv.org/pdf/2601.19898v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shubham Patle",
      "Sara Ghaboura",
      "Hania Tariq",
      "Mohammad Usman Khan",
      "Omkar Thawakar",
      "Rao Muhammad Anwer",
      "Salman Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19887v1",
    "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
    "summary": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.",
    "published": "2026-01-27T18:54:29Z",
    "updated": "2026-01-27T18:54:29Z",
    "link": "http://arxiv.org/pdf/2601.19887v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Dominic Maggio",
      "Luca Carlone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19884v1",
    "title": "SONIC: Spectral Oriented Neural Invariant Convolutions",
    "summary": "Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.",
    "published": "2026-01-27T18:51:11Z",
    "updated": "2026-01-27T18:51:11Z",
    "link": "http://arxiv.org/pdf/2601.19884v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Gijs Joppe Moens",
      "Regina Beets-Tan",
      "Eduardo H. P. Pooch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.11236v3",
    "title": "Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing",
    "summary": "In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters. Our code and dataset are available at https://github.com/cao-cong/FSMSE.",
    "published": "2025-11-14T12:40:21Z",
    "updated": "2026-01-27T18:27:31Z",
    "link": "http://arxiv.org/pdf/2511.11236v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cong Cao",
      "Yujie Xu",
      "Xiaodong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17535v2",
    "title": "Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries",
    "summary": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.",
    "published": "2026-01-24T17:30:23Z",
    "updated": "2026-01-27T18:04:35Z",
    "link": "http://arxiv.org/pdf/2601.17535v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kevin Robbins",
      "Xiaotong Liu",
      "Yu Wu",
      "Le Sun",
      "Grady McPeak",
      "Abby Stylianou",
      "Robert Pless"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19850v1",
    "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
    "summary": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL",
    "published": "2026-01-27T17:58:12Z",
    "updated": "2026-01-27T17:58:12Z",
    "link": "http://arxiv.org/pdf/2601.19850v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Binzhu Xie",
      "Shi Qiu",
      "Sicheng Zhang",
      "Yinqiao Wang",
      "Hao Xu",
      "Muzammal Naseer",
      "Chi-Wing Fu",
      "Pheng-Ann Heng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19849v1",
    "title": "HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation",
    "summary": "Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.",
    "published": "2026-01-27T17:56:49Z",
    "updated": "2026-01-27T17:56:49Z",
    "link": "http://arxiv.org/pdf/2601.19849v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haya Alyoussef",
      "Ahmad Bdeir",
      "Diego Coello de Portugal Mecke",
      "Tom Hanika",
      "Niels Landwehr",
      "Lars Schmidt-Thieme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18829v2",
    "title": "Assessing the Effectiveness of Deep Embeddings for Tree Species Classification in the Dutch Forest Inventory",
    "summary": "National Forest Inventory serves as the primary source of forest information, however, maintaining these inventories requires labor-intensive on-site campaigns by forestry experts to identify and document tree species. Embeddings from deep pre-trained remote sensing models offer new opportunities to update NFIs more frequently and at larger scales. While training new deep learning models on few data points remains challenging, we show that using pre-computed embeddings can proven effective for distinguishing tree species through seasonal canopy reflectance patternsin combination with Random Forest. This work systematically investigates how deep embeddings improve tree species classification accuracy in the Netherlands with few annotated data. We evaluate this question on three embedding models: Presto, Alpha Earth, and Tessera, using three tree species datasets of varying difficulty. Data-wise, we compare the available embeddings from Alpha Earth and Tessera with dynamically calculated embeddings from a pre-trained Presto model. Our results demonstrate that fine-tuning a publicly available remote sensing time series pre-trained model outperforms the current state-of-the-art in NFI classification in the Netherlands, yielding performance gains of approximately 2-9 percentage points across datasets and evaluation metrics. This indicates that classic hand-defined features are too simple for this task and highlights the potential of using deep embeddings for data-limited applications such as NFI classification. By leveraging openly available satellite data and deep embeddings from pre-trained models, this approach significantly improves classification accuracy compared to traditional methods and can effectively complement existing forest inventory processes.",
    "published": "2025-08-26T09:06:14Z",
    "updated": "2026-01-27T17:25:21Z",
    "link": "http://arxiv.org/pdf/2508.18829v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Takayuki Ishikawa",
      "Carmelo Bonannella",
      "Bas J. W. Lerink",
      "Marc Rußwurm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19821v1",
    "title": "Query-Guided Spatial-Temporal-Frequency Interaction for Music Audio-Visual Question Answering",
    "summary": "Audio--Visual Question Answering (AVQA) is a challenging multimodal task that requires jointly reasoning over audio, visual, and textual information in a given video to answer natural language questions. Inspired by recent advances in Video QA, many existing AVQA approaches primarily focus on visual information processing, leveraging pre-trained models to extract object-level and motion-level representations. However, in those methods, the audio input is primarily treated as complementary to video analysis, and the textual question information contributes minimally to audio--visual understanding, as it is typically integrated only in the final stages of reasoning. To address these limitations, we propose a novel Query-guided Spatial--Temporal--Frequency (QSTar) interaction method, which effectively incorporates question-guided clues and exploits the distinctive frequency-domain characteristics of audio signals, alongside spatial and temporal perception, to enhance audio--visual understanding. Furthermore, we introduce a Query Context Reasoning (QCR) block inspired by prompting, which guides the model to focus more precisely on semantically relevant audio and visual features. Extensive experiments conducted on several AVQA benchmarks demonstrate the effectiveness of our proposed method, achieving significant performance improvements over existing Audio QA, Visual QA, Video QA, and AVQA approaches. The code and pretrained models will be released after publication.",
    "published": "2026-01-27T17:24:32Z",
    "updated": "2026-01-27T17:24:32Z",
    "link": "http://arxiv.org/pdf/2601.19821v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kun Li",
      "Michael Ying Yang",
      "Sami Sebastian Brandt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19798v1",
    "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision",
    "summary": "Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.",
    "published": "2026-01-27T17:01:16Z",
    "updated": "2026-01-27T17:01:16Z",
    "link": "http://arxiv.org/pdf/2601.19798v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhixiang Wei",
      "Yi Li",
      "Zhehan Kan",
      "Xinghua Jiang",
      "Zuwei Long",
      "Shifeng Liu",
      "Hongze Shen",
      "Wei Liu",
      "Xiaoyu Tan",
      "Haojia Lin",
      "Yubo Zhu",
      "Qianyu Li",
      "Di Yin",
      "Haoyu Cao",
      "Weibo Gu",
      "Xin Li",
      "Yinsong Liu",
      "Deqiang Jiang",
      "Xing Sun",
      "Yunsheng Wu",
      "Mingkong Tang",
      "Shuangyin Liu",
      "Lexiang Tang",
      "Haodong Lin",
      "Junru Lu",
      "Jiarui Qin",
      "Lingfeng Qiao",
      "Ruizhi Qiao",
      "Bo Ke",
      "Jianfeng He",
      "Ke Li",
      "Yangning Li",
      "Yunhang Shen",
      "Mengdan Zhang",
      "Peixian Chen",
      "Kun Yin",
      "Bing Liu",
      "Yunfei Wu",
      "Huang Chen",
      "Zhongpeng Cai",
      "Xiaotian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19795v1",
    "title": "Diffusion for De-Occlusion: Accessory-Aware Diffusion Inpainting for Robust Ear Biometric Recognition",
    "summary": "Ear occlusions (arising from the presence of ear accessories such as earrings and earphones) can negatively impact performance in ear-based biometric recognition systems, especially in unconstrained imaging circumstances. In this study, we assess the effectiveness of a diffusion-based ear inpainting technique as a pre-processing aid to mitigate the issues of ear accessory occlusions in transformer-based ear recognition systems. Given an input ear image and an automatically derived accessory mask, the inpainting model reconstructs clean and anatomically plausible ear regions by synthesizing missing pixels while preserving local geometric coherence along key ear structures, including the helix, antihelix, concha, and lobule. We evaluate the effectiveness of this pre-processing aid in transformer-based recognition systems for several vision transformer models and different patch sizes for a range of benchmark datasets. Experiments show that diffusion-based inpainting can be a useful pre-processing aid to alleviate ear accessory occlusions to improve overall recognition performance.",
    "published": "2026-01-27T16:55:35Z",
    "updated": "2026-01-27T16:55:35Z",
    "link": "http://arxiv.org/pdf/2601.19795v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Deeksha Arun",
      "Kevin W. Bowyer",
      "Patrick Flynn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19785v1",
    "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
    "summary": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.",
    "published": "2026-01-27T16:47:35Z",
    "updated": "2026-01-27T16:47:35Z",
    "link": "http://arxiv.org/pdf/2601.19785v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haozhi Zhu",
      "Miaomiao Zhao",
      "Dingyao Liu",
      "Runze Tian",
      "Yan Zhang",
      "Jie Guo",
      "Fenggen Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19771v1",
    "title": "PaW-ViT: A Patch-based Warping Vision Transformer for Robust Ear Verification",
    "summary": "The rectangular tokens common to vision transformer methods for visual recognition can strongly affect performance of these methods due to incorporation of information outside the objects to be recognized. This paper introduces PaW-ViT, Patch-based Warping Vision Transformer, a preprocessing approach rooted in anatomical knowledge that normalizes ear images to enhance the efficacy of ViT. By accurately aligning token boundaries to detected ear feature boundaries, PaW-ViT obtains greater robustness to shape, size, and pose variation. By aligning feature boundaries to natural ear curvature, it produces more consistent token representations for various morphologies. Experiments confirm the effectiveness of PaW-ViT on various ViT models (ViT-T, ViT-S, ViT-B, ViT-L) and yield reasonable alignment robustness to variation in shape, size, and pose. Our work aims to solve the disconnect between ear biometric morphological variation and transformer architecture positional sensitivity, presenting a possible avenue for authentication schemes.",
    "published": "2026-01-27T16:34:43Z",
    "updated": "2026-01-27T16:34:43Z",
    "link": "http://arxiv.org/pdf/2601.19771v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Deeksha Arun",
      "Kevin W. Bowyer",
      "Patrick Flynn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19753v1",
    "title": "WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration",
    "summary": "Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.",
    "published": "2026-01-27T16:14:34Z",
    "updated": "2026-01-27T16:14:34Z",
    "link": "http://arxiv.org/pdf/2601.19753v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinrui Zhang",
      "Yufeng Wang",
      "Shuangkang Fang",
      "Zesheng Wang",
      "Dacheng Qi",
      "Wenrui Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.22603v3",
    "title": "Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMs",
    "summary": "Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.",
    "published": "2025-10-26T09:44:20Z",
    "updated": "2026-01-27T16:14:08Z",
    "link": "http://arxiv.org/pdf/2510.22603v3.pdf",
    "category": [
      "eess.AS",
      "cs.CV",
      "cs.SD"
    ],
    "authors": [
      " Anand",
      "Umberto Cappellazzo",
      "Stavros Petridis",
      "Maja Pantic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19750v1",
    "title": "Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues",
    "summary": "Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.\n  We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.",
    "published": "2026-01-27T16:13:26Z",
    "updated": "2026-01-27T16:13:26Z",
    "link": "http://arxiv.org/pdf/2601.19750v1.pdf",
    "category": [
      "cs.MM",
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Junchen Fu",
      "Wenhao Deng",
      "Kaiwen Zheng",
      "Alexandros Karatzoglou",
      "Ioannis Arapakis",
      "Yu Ye",
      "Yongxin Ni",
      "Joemon M. Jose",
      "Xuri Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17835v2",
    "title": "Geometry-Grounded Gaussian Splatting",
    "summary": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.",
    "published": "2026-01-25T13:32:53Z",
    "updated": "2026-01-27T16:05:11Z",
    "link": "http://arxiv.org/pdf/2601.17835v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Baowen Zhang",
      "Chenxing Jiang",
      "Heng Li",
      "Shaojie Shen",
      "Ping Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19743v1",
    "title": "Interpretable and backpropagation-free Green Learning for efficient multi-task echocardiographic segmentation and classification",
    "summary": "Echocardiography is a cornerstone for managing heart failure (HF), with Left Ventricular Ejection Fraction (LVEF) being a critical metric for guiding therapy. However, manual LVEF assessment suffers from high inter-observer variability, while existing Deep Learning (DL) models are often computationally intensive and data-hungry \"black boxes\" that impede clinical trust and adoption. Here, we propose a backpropagation-free multi-task Green Learning (MTGL) framework that performs simultaneous Left Ventricle (LV) segmentation and LVEF classification. Our framework integrates an unsupervised VoxelHop encoder for hierarchical spatio-temporal feature extraction with a multi-level regression decoder and an XG-Boost classifier. On the EchoNet-Dynamic dataset, our MTGL model achieves state-of-the-art classification and segmentation performance, attaining a classification accuracy of 94.3% and a Dice Similarity Coefficient (DSC) of 0.912, significantly outperforming several advanced 3D DL models. Crucially, our model achieves this with over an order of magnitude fewer parameters, demonstrating exceptional computational efficiency. This work demonstrates that the GL paradigm can deliver highly accurate, efficient, and interpretable solutions for complex medical image analysis, paving the way for more sustainable and trustworthy artificial intelligence in clinical practice.",
    "published": "2026-01-27T16:04:42Z",
    "updated": "2026-01-27T16:04:42Z",
    "link": "http://arxiv.org/pdf/2601.19743v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jyun-Ping Kao",
      "Jiaxing Yang",
      "C. -C. Jay Kuo",
      "Jonghye Woo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27647v2",
    "title": "NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception",
    "summary": "Collaborative perception improves task performance by expanding the perception range through information sharing among agents. . Immutable heterogeneity poses a significant challenge in collaborative perception, as participating agents may employ different and fixed perception models. This leads to domain gaps in the intermediate features shared among agents, consequently degrading collaborative performance. Aligning the features of all agents to a common representation can eliminate domain gaps with low training cost. However, in existing methods, the common representation is designated as the representation of a specific agent, making it difficult for agents with significant domain discrepancies from this specific agent to achieve proper alignment. This paper proposes NegoCollab, a heterogeneous collaboration method based on the negotiated common representation. It introduces a negotiator during training to derive the common representation from the local representations of each modality's agent, effectively reducing the inherent domain gap with the various local representations. In NegoCollab, the mutual transformation of features between the local representation space and the common representation space is achieved by a pair of sender and receiver. To better align local representations to the common representation containing multimodal information, we introduce structural alignment loss and pragmatic alignment loss in addition to the distribution alignment loss to supervise the training. This enables the knowledge in the common representation to be fully distilled into the sender.",
    "published": "2025-10-31T17:20:54Z",
    "updated": "2026-01-27T15:59:14Z",
    "link": "http://arxiv.org/pdf/2510.27647v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Congzhang Shao",
      "Quan Yuan",
      "Guiyang Luo",
      "Yue Hu",
      "Danni Wang",
      "Yilin Liu",
      "Rui Pan",
      "Bo Chen",
      "Jinglin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00506v2",
    "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool",
    "summary": "Machine learning for remote sensing imaging relies on up-to-date and accurate labels for model training and testing. Labelling remote sensing imagery is time and cost intensive, requiring expert analysis. Previous labelling tools rely on pre-labelled data for training in order to label new unseen data. In this work, we define an unsupervised pipeline for finding and labelling geographical areas of similar context and content within Sentinel-2 satellite imagery. Our approach removes limitations of previous methods by utilising segmentation with convolutional and graph neural networks to encode a more robust feature space for image comparison. Unlike previous approaches we segment the image into homogeneous regions of pixels that are grouped based on colour and spatial similarity. Graph neural networks are used to aggregate information about the surrounding segments enabling the feature representation to encode the local neighbourhood whilst preserving its own local information. This reduces outliers in the labelling tool, allows users to label at a granular level, and allows a rotationally invariant semantic relationship at the image level to be formed within the encoding space. Our pipeline achieves high contextual consistency, with similarity scores of SSIM = 0.96 and SAM = 0.21 under context-aware evaluation, demonstrating robust organisation of the feature space for interactive labelling.",
    "published": "2025-08-01T10:35:32Z",
    "updated": "2026-01-27T15:42:32Z",
    "link": "http://arxiv.org/pdf/2508.00506v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tulsi Patel",
      "Mark W. Jones",
      "Thomas Redfern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19717v1",
    "title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization",
    "summary": "3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.",
    "published": "2026-01-27T15:41:11Z",
    "updated": "2026-01-27T15:41:11Z",
    "link": "http://arxiv.org/pdf/2601.19717v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yitong Yang",
      "Xuexin Liu",
      "Yinglin Wang",
      "Jing Wang",
      "Hao Dou",
      "Changshuo Wang",
      "Shuting He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19694v1",
    "title": "Self-Supervised Weight Templates for Scalable Vision Model Initialization",
    "summary": "The increasing scale and complexity of modern model parameters underscore the importance of pre-trained models. However, deployment often demands architectures of varying sizes, exposing limitations of conventional pre-training and fine-tuning. To address this, we propose SWEET, a self-supervised framework that performs constraint-based pre-training to enable scalable initialization in vision tasks. Instead of pre-training a fixed-size model, we learn a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths. Target models are subsequently initialized by composing and reweighting the template through lightweight weight scalers, whose parameters can be efficiently learned from minimal training data. To further enhance flexibility in width expansion, we introduce width-wise stochastic scaling, which regularizes the template along width-related dimensions and encourages robust, width-invariant representations for improved cross-width generalization. Extensive experiments on \\textsc{classification}, \\textsc{detection}, \\textsc{segmentation} and \\textsc{generation} tasks demonstrate the state-of-the-art performance of SWEET for initializing variable-sized vision models.",
    "published": "2026-01-27T15:15:17Z",
    "updated": "2026-01-27T15:15:17Z",
    "link": "http://arxiv.org/pdf/2601.19694v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yucheng Xie",
      "Fu Feng",
      "Ruixiao Shi",
      "Jing Wang",
      "Yong Rui",
      "Xin Geng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17046v2",
    "title": "Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning",
    "summary": "We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.",
    "published": "2026-01-20T19:35:53Z",
    "updated": "2026-01-27T15:07:04Z",
    "link": "http://arxiv.org/pdf/2601.17046v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Matan Leibovich",
      "Mai Tan",
      "Ramon Manzorro-Ureba",
      "Adria Marcos-Morales",
      "Sreyas Mohan",
      "Peter A. Crozier",
      "Carlos Fernandez-Granda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19690v1",
    "title": "DSVM-UNet : Enhancing VM-UNet with Dual Self-distillation for Medical Image Segmentation",
    "summary": "Vision Mamba models have been extensively researched in various fields, which address the limitations of previous models by effectively managing long-range dependencies with a linear-time overhead. Several prospective studies have further designed Vision Mamba based on UNet(VM-UNet) for medical image segmentation. These approaches primarily focus on optimizing architectural designs by creating more complex structures to enhance the model's ability to perceive semantic features. In this paper, we propose a simple yet effective approach to improve the model by Dual Self-distillation for VM-UNet (DSVM-UNet) without any complex architectural designs. To achieve this goal, we develop double self-distillation methods to align the features at both the global and local levels. Extensive experiments conducted on the ISIC2017, ISIC2018, and Synapse benchmarks demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. Code is available at https://github.com/RoryShao/DSVM-UNet.git.",
    "published": "2026-01-27T15:06:38Z",
    "updated": "2026-01-27T15:06:38Z",
    "link": "http://arxiv.org/pdf/2601.19690v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Renrong Shao",
      "Dongyang Li",
      "Dong Xia",
      "Lin Shao",
      "Jiangdong Lu",
      "Fen Zheng",
      "Lulu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19686v1",
    "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
    "summary": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at https://github.com/zywang0104/Video-KTR.",
    "published": "2026-01-27T15:02:23Z",
    "updated": "2026-01-27T15:02:23Z",
    "link": "http://arxiv.org/pdf/2601.19686v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziyue Wang",
      "Sheng Jin",
      "Zhongrong Zuo",
      "Jiawei Wu",
      "Han Qiu",
      "Qi She",
      "Hao Zhang",
      "Xudong Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19683v1",
    "title": "SharpNet: Enhancing MLPs to Represent Functions with Controlled Non-differentiability",
    "summary": "Multi-layer perceptrons (MLPs) are a standard tool for learning and function approximation, but they inherently yield outputs that are globally smooth. As a result, they struggle to represent functions that are continuous yet deliberately non-differentiable (i.e., with prescribed $C^0$ sharp features) without relying on ad hoc post-processing. We present SharpNet, a modified MLP architecture capable of encoding functions with user-defined sharp features by enriching the network with an auxiliary feature function, which is defined as the solution to a Poisson equation with jump Neumann boundary conditions. It is evaluated via an efficient local integral that is fully differentiable with respect to the feature locations, enabling our method to jointly optimize both the feature locations and the MLP parameters to recover the target functions/models. The $C^0$-continuity of SharpNet is precisely controllable, ensuring $C^0$-continuity at the feature locations and smoothness elsewhere. We validate SharpNet on 2D problems and 3D CAD model reconstruction, and compare it against several state-of-the-art baselines. In both types of tasks, SharpNet accurately recovers sharp edges and corners while maintaining smooth behavior away from those features, whereas existing methods tend to smooth out gradient discontinuities. Both qualitative and quantitative evaluations highlight the benefits of our approach.",
    "published": "2026-01-27T15:01:11Z",
    "updated": "2026-01-27T15:01:11Z",
    "link": "http://arxiv.org/pdf/2601.19683v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hanting Niu",
      "Junkai Deng",
      "Fei Hou",
      "Wencheng Wang",
      "Ying He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19680v1",
    "title": "A new Image Similarity Metric for a Perceptual and Transparent Geometric and Chromatic Assessment",
    "summary": "In the literature, several studies have shown that state-of-the-art image similarity metrics are not perceptual metrics; moreover, they have difficulty evaluating images, especially when texture distortion is also present. In this work, we propose a new perceptual metric composed of two terms. The first term evaluates the dissimilarity between the textures of two images using Earth Mover's Distance. The second term evaluates the chromatic dissimilarity between two images in the Oklab perceptual color space. We evaluated the performance of our metric on a non-traditional dataset, called Berkeley-Adobe Perceptual Patch Similarity, which contains a wide range of complex distortions in shapes and colors. We have shown that our metric outperforms the state of the art, especially when images contain shape distortions, confirming also its greater perceptiveness. Furthermore, although deep black-box metrics could be very accurate, they only provide similarity scores between two images, without explaining their main differences and similarities. Our metric, on the other hand, provides visual explanations to support the calculated score, making the similarity assessment transparent and justified.",
    "published": "2026-01-27T14:59:01Z",
    "updated": "2026-01-27T14:59:01Z",
    "link": "http://arxiv.org/pdf/2601.19680v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Antonio Di Marino",
      "Vincenzo Bevilacqua",
      "Emanuel Di Nardo",
      "Angelo Ciaramella",
      "Ivanoe De Falco",
      "Giovanna Sannino"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03623v2",
    "title": "Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map",
    "summary": "Synthetic dataset generation in Computer Vision, particularly for industrial applications, is still underexplored. Industrial defect segmentation, for instance, requires highly accurate labels, yet acquiring such data is costly and time-consuming. To address this challenge, we propose a novel diffusion-based pipeline for generating high-fidelity industrial datasets with minimal supervision. Our approach conditions the diffusion model on enriched bounding box representations to produce precise segmentation masks, ensuring realistic and accurately localized defect synthesis. Compared to existing layout-conditioned generative methods, our approach improves defect consistency and spatial accuracy. We introduce two quantitative metrics to evaluate the effectiveness of our method and assess its impact on a downstream segmentation task trained on real and synthetic data. Our results demonstrate that diffusion-based synthesis can bridge the gap between artificial and real-world industrial data, fostering more reliable and cost-efficient segmentation models. The code is publicly available at https://github.com/covisionlab/diffusion_labeling.",
    "published": "2025-05-06T15:21:36Z",
    "updated": "2026-01-27T14:58:31Z",
    "link": "http://arxiv.org/pdf/2505.03623v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Emanuele Caruso",
      "Alessandro Simoni",
      "Francesco Pelosin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19659v1",
    "title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation",
    "summary": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.",
    "published": "2026-01-27T14:38:57Z",
    "updated": "2026-01-27T14:38:57Z",
    "link": "http://arxiv.org/pdf/2601.19659v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Mao-Lin Luo",
      "Zi-Hao Zhou",
      "Yi-Lin Zhang",
      "Yuanyu Wan",
      "Tong Wei",
      "Min-Ling Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19640v1",
    "title": "Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework",
    "summary": "Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.",
    "published": "2026-01-27T14:17:04Z",
    "updated": "2026-01-27T14:17:04Z",
    "link": "http://arxiv.org/pdf/2601.19640v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Chang",
      "Zhihui Wang",
      "Lingxiang Wu",
      "Peijin Wang",
      "Wenhui Diao",
      "Jinqiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22522v2",
    "title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation",
    "summary": "Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems.",
    "published": "2025-09-26T16:04:00Z",
    "updated": "2026-01-27T14:13:52Z",
    "link": "http://arxiv.org/pdf/2509.22522v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Guillem Capellera",
      "Luis Ferraz",
      "Antonio Rubio",
      "Alexandre Alahi",
      "Antonio Agudo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.01510v2",
    "title": "Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation",
    "summary": "We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation, where we train a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. Our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently at training-time, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware. Overall, our evaluation shows that our method achieves new state-of-the-art performance in DG for medical image segmentation, even matching the performance of the in-domain baseline in several settings. We will release our source code upon acceptance of this manuscript.",
    "published": "2025-12-01T10:35:45Z",
    "updated": "2026-01-27T14:10:51Z",
    "link": "http://arxiv.org/pdf/2512.01510v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Franz Thaler",
      "Martin Urschler",
      "Mateusz Kozinski",
      "Matthias AF Gsell",
      "Gernot Plank",
      "Darko Stern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.14961v3",
    "title": "Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities",
    "summary": "Person identification systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently present with missing or degraded modalities. To address this challenge, we propose a multimodal person identification framework incorporating upper-body motion, face, and voice. Experimental results demonstrate that body motion outperforms traditional modalities such as face and voice in within-session evaluations, while serving as a complementary cue that enhances performance in multi-session scenarios. Our model employs a unified hybrid fusion strategy, fusing both feature-level and score-level information to maximize representational richness and decision accuracy. Specifically, it leverages multi-task learning to process modalities independently, followed by cross-attention and gated fusion mechanisms to exploit both unimodal information and cross-modal interactions. Finally, a confidence-weighted strategy and mistake-correction mechanism dynamically adapt to missing data, ensuring that our single classification head achieves optimal performance even in unimodal and bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark in this work for the first time. Our results demonstrate that the proposed trimodal system achieves 99.51% Top-1 accuracy on person identification tasks. In addition, we evaluate our model on the VoxCeleb1 dataset as a widely used evaluation protocol and reach 99.92% accuracy in bimodal mode, outperforming conventional approaches. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.",
    "published": "2025-12-16T22:59:24Z",
    "updated": "2026-01-27T13:42:30Z",
    "link": "http://arxiv.org/pdf/2512.14961v3.pdf",
    "category": [
      "cs.CV",
      "cs.SD",
      "eess.AS",
      "eess.IV"
    ],
    "authors": [
      "Aref Farhadipour",
      "Teodora Vukovic",
      "Volker Dellwo",
      "Petr Motlicek",
      "Srikanth Madikeri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19593v1",
    "title": "Localized Latent Editing for Dose-Response Modeling in Botulinum Toxin Injection Planning",
    "summary": "Botulinum toxin (Botox) injections are the gold standard for managing facial asymmetry and aesthetic rejuvenation, yet determining the optimal dosage remains largely intuitive, often leading to suboptimal outcomes. We propose a localized latent editing framework that simulates Botulinum Toxin injection effects for injection planning through dose-response modeling. Our key contribution is a Region-Specific Latent Axis Discovery method that learns localized muscle relaxation trajectories in StyleGAN2's latent space, enabling precise control over specific facial regions without global side effects. By correlating these localized latent trajectories with injected toxin units, we learn a predictive dose-response model. We rigorously compare two approaches: direct metric regression versus image-based generative simulation on a clinical dataset of N=360 images from 46 patients. On a hold-out test set, our framework demonstrates moderate-to-strong structural correlations for geometric asymmetry metrics, confirming that the generative model correctly captures the direction of morphological changes. While biological variability limits absolute precision, we introduce a hybrid \"Human-in-the-Loop\" workflow where clinicians interactively refine simulations, bridging the gap between pathological reconstruction and cosmetic planning.",
    "published": "2026-01-27T13:26:29Z",
    "updated": "2026-01-27T13:26:29Z",
    "link": "http://arxiv.org/pdf/2601.19593v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Estèphe Arnaud",
      "Mohamed Daoudi",
      "Pierre Guerreschi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19582v1",
    "title": "ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving",
    "summary": "In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.",
    "published": "2026-01-27T13:17:50Z",
    "updated": "2026-01-27T13:17:50Z",
    "link": "http://arxiv.org/pdf/2601.19582v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yujin Wang",
      "Yutong Zheng",
      "Wenxian Fan",
      "Tianyi Wang",
      "Hongqing Chu",
      "Daxin Tian",
      "Bingzhao Gao",
      "Jianqiang Wang",
      "Hong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17124v2",
    "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code",
    "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ",
    "published": "2026-01-23T19:00:35Z",
    "updated": "2026-01-27T13:16:28Z",
    "link": "http://arxiv.org/pdf/2601.17124v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bin Lin",
      "Zongjian Li",
      "Yuwei Niu",
      "Kaixiong Gong",
      "Yunyang Ge",
      "Yunlong Lin",
      "Mingzhe Zheng",
      "JianWei Zhang",
      "Miles Yang",
      "Zhao Zhong",
      "Liefeng Bo",
      "Li Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19580v1",
    "title": "QuaMo: Quaternion Motions for Vision-based 3D Human Kinematics Capture",
    "summary": "Vision-based 3D human motion capture from videos remains a challenge in computer vision. Traditional 3D pose estimation approaches often ignore the temporal consistency between frames, causing implausible and jittery motion. The emerging field of kinematics-based 3D motion capture addresses these issues by estimating the temporal transitioning between poses instead. A major drawback in current kinematics approaches is their reliance on Euler angles. Despite their simplicity, Euler angles suffer from discontinuity that leads to unstable motion reconstructions, especially in online settings where trajectory refinement is unavailable. Contrarily, quaternions have no discontinuity and can produce continuous transitions between poses. In this paper, we propose QuaMo, a novel Quaternion Motions method using quaternion differential equations (QDE) for human kinematics capture. We utilize the state-space model, an effective system for describing real-time kinematics estimations, with quaternion state and the QDE describing quaternion velocity. The corresponding angular acceleration is computed from a meta-PD controller with a novel acceleration enhancement that adaptively regulates the control signals as the human quickly changes to a new pose. Unlike previous work, our QDE is solved under the quaternion unit-sphere constraint that results in more accurate estimations. Experimental results show that our novel formulation of the QDE with acceleration enhancement accurately estimates 3D human kinematics with no discontinuity and minimal implausibilities. QuaMo outperforms comparable state-of-the-art methods on multiple datasets, namely Human3.6M, Fit3D, SportsPose and AIST. The code is available at https://github.com/cuongle1206/QuaMo",
    "published": "2026-01-27T13:12:08Z",
    "updated": "2026-01-27T13:12:08Z",
    "link": "http://arxiv.org/pdf/2601.19580v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cuong Le",
      "Pavlo Melnyk",
      "Urs Waldmann",
      "Mårten Wadenbäck",
      "Bastian Wandt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19577v1",
    "title": "MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation",
    "summary": "Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.",
    "published": "2026-01-27T13:06:47Z",
    "updated": "2026-01-27T13:06:47Z",
    "link": "http://arxiv.org/pdf/2601.19577v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ronglai Zuo",
      "Rolandos Alexandros Potamias",
      "Qi Sun",
      "Evangelos Ververas",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19557v1",
    "title": "The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments",
    "summary": "We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.",
    "published": "2026-01-27T12:49:40Z",
    "updated": "2026-01-27T12:49:40Z",
    "link": "http://arxiv.org/pdf/2601.19557v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Riccardo Giubilato",
      "Marcus Gerhard Müller",
      "Marco Sewtz",
      "Laura Alejandra Encinar Gonzalez",
      "John Folkesson",
      "Rudolph Triebel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19526v1",
    "title": "A Non-Invasive 3D Gait Analysis Framework for Quantifying Psychomotor Retardation in Major Depressive Disorder",
    "summary": "Predicting the status of Major Depressive Disorder (MDD) from objective, non-invasive methods is an active research field. Yet, extracting automatically objective, interpretable features for a detailed analysis of the patient state remains largely unexplored.\n  Among MDD's symptoms, Psychomotor retardation (PMR) is a core item, yet its clinical assessment remains largely subjective. While 3D motion capture offers an objective alternative, its reliance on specialized hardware often precludes routine clinical use. In this paper, we propose a non-invasive computational framework that transforms monocular RGB video into clinically relevant 3D gait kinematics. Our pipeline uses Gravity-View Coordinates along with a novel trajectory-correction algorithm that leverages the closed-loop topology of our adapted Timed Up and Go (TUG) protocol to mitigate monocular depth errors. This novel pipeline enables the extraction of 297 explicit gait biomechanical biomarkers from a single camera capture.\n  To address the challenges of small clinical datasets, we introduce a stability-based machine learning framework that identifies robust motor signatures while preventing overfitting. Validated on the CALYPSO dataset, our method achieves an 83.3% accuracy in detecting PMR and explains 64% of the variance in overall depression severity (R^2=0.64). Notably, our study reveals a strong link between reduced ankle propulsion and restricted pelvic mobility to the depressive motor phenotype. These results demonstrate that physical movement serves as a robust proxy for the cognitive state, offering a transparent and scalable tool for the objective monitoring of depression in standard clinical environments.",
    "published": "2026-01-27T12:07:21Z",
    "updated": "2026-01-27T12:07:21Z",
    "link": "http://arxiv.org/pdf/2601.19526v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fouad Boutaleb",
      "Emery Pierson",
      "Mohamed Daoudi",
      "Clémence Nineuil",
      "Ali Amad",
      "Fabien D'Hondt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19506v1",
    "title": "Bridging Information Asymmetry: A Hierarchical Framework for Deterministic Blind Face Restoration",
    "summary": "Blind face restoration remains a persistent challenge due to the inherent ill-posedness of reconstructing holistic structures from severely constrained observations. Current generative approaches, while capable of synthesizing realistic textures, often suffer from information asymmetry -- the intrinsic disparity between the information-sparse low quality inputs and the information-dense high quality outputs. This imbalance leads to a one-to-many mapping, where insufficient constraints result in stochastic uncertainty and hallucinatory artifacts. To bridge this gap, we present \\textbf{Pref-Restore}, a hierarchical framework that integrates discrete semantic logic with continuous texture generation to achieve deterministic, preference-aligned restoration. Our methodology fundamentally addresses this information disparity through two complementary strategies: (1) Augmenting Input Density: We employ an auto-regressive integrator to reformulate textual instructions into dense latent queries, injecting high-level semantic stability to constrain the degraded signals; (2) Pruning Output Distribution: We pioneer the integration of on-policy reinforcement learning directly into the diffusion restoration loop. By transforming human preferences into differentiable constraints, we explicitly penalize stochastic deviations, thereby sharpening the posterior distribution toward the desired high-fidelity outcomes. Extensive experiments demonstrate that Pref-Restore achieves state-of-the-art performance across synthetic and real-world benchmarks. Furthermore, empirical analysis confirms that our preference-aligned strategy significantly reduces solution entropy, establishing a robust pathway toward reliable and deterministic blind restoration.",
    "published": "2026-01-27T11:50:31Z",
    "updated": "2026-01-27T11:50:31Z",
    "link": "http://arxiv.org/pdf/2601.19506v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhengjian Yao",
      "Jiakui Hu",
      "Kaiwen Li",
      "Hangzhou He",
      "Xinliang Zhang",
      "Shuang Zeng",
      "Lei Zhu",
      "Yanye Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25787v4",
    "title": "Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking",
    "summary": "Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM's own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model's iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM's perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM's zero-shot performance by 31.8% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks. Furthermore, the framework demonstrates significant flexibility, allowing it to be stacked with pre-trained IQA models to bolster generalization on unseen datasets.",
    "published": "2025-09-30T04:57:26Z",
    "updated": "2026-01-27T11:48:21Z",
    "link": "http://arxiv.org/pdf/2509.25787v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wen Wen",
      "Tianwu Zhi",
      "Kanglong Fan",
      "Yang Li",
      "Xinge Peng",
      "Yabin Zhang",
      "Yiting Liao",
      "Junlin Li",
      "Li Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19489v1",
    "title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction",
    "summary": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition.",
    "published": "2026-01-27T11:20:37Z",
    "updated": "2026-01-27T11:20:37Z",
    "link": "http://arxiv.org/pdf/2601.19489v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziyu Zhang",
      "Tianle Liu",
      "Diantao Tu",
      "Shuhan Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16117v2",
    "title": "Distillation-based Layer Dropping (DLD): Effective End-to-end Framework for Dynamic Speech Networks",
    "summary": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.",
    "published": "2026-01-22T17:11:44Z",
    "updated": "2026-01-27T11:20:19Z",
    "link": "http://arxiv.org/pdf/2601.16117v2.pdf",
    "category": [
      "cs.SD",
      "cs.CV"
    ],
    "authors": [
      "Abdul Hannan",
      "Daniele Falavigna",
      "Shah Nawaz",
      "Mubashir Noman",
      "Markus Schedl",
      "Alessio Brutti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19488v1",
    "title": "Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation",
    "summary": "Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.",
    "published": "2026-01-27T11:19:53Z",
    "updated": "2026-01-27T11:19:53Z",
    "link": "http://arxiv.org/pdf/2601.19488v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yizhao Han",
      "Tianxing Shi",
      "Zhao Wang",
      "Zifan Xu",
      "Zhiyuan Pu",
      "Mingxiao Li",
      "Qian Zhang",
      "Wei Yin",
      "Xiao-Xiao Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19484v1",
    "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
    "summary": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.",
    "published": "2026-01-27T11:16:42Z",
    "updated": "2026-01-27T11:16:42Z",
    "link": "http://arxiv.org/pdf/2601.19484v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yin Wang",
      "Zhiying Leng",
      "Haitian Liu",
      "Frederick W. B. Li",
      "Mu Li",
      "Xiaohui Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19461v1",
    "title": "Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods",
    "summary": "Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \\times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.",
    "published": "2026-01-27T10:45:29Z",
    "updated": "2026-01-27T10:45:29Z",
    "link": "http://arxiv.org/pdf/2601.19461v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.15809v4",
    "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion",
    "summary": "Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.",
    "published": "2025-07-21T17:10:16Z",
    "updated": "2026-01-27T10:40:32Z",
    "link": "http://arxiv.org/pdf/2507.15809v4.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "physics.geo-ph",
      "stat.AP"
    ],
    "authors": [
      "Roberto Miele",
      "Niklas Linde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19446v1",
    "title": "DSTCS: Dual-Student Teacher Framework with Segment Anything Model for Semi-Supervised Pubic Symphysis Fetal Head Segmentation",
    "summary": "Segmentation of the pubic symphysis and fetal head (PSFH) is a critical procedure in intrapartum monitoring and is essential for evaluating labor progression and identifying potential delivery complications. However, achieving accurate segmentation remains a significant challenge due to class imbalance, ambiguous boundaries, and noise interference in ultrasound images, compounded by the scarcity of high-quality annotated data. Current research on PSFH segmentation predominantly relies on CNN and Transformer architectures, leaving the potential of more powerful models underexplored. In this work, we propose a Dual-Student and Teacher framework combining CNN and SAM (DSTCS), which integrates the Segment Anything Model (SAM) into a dual student-teacher architecture. A cooperative learning mechanism between the CNN and SAM branches significantly improves segmentation accuracy. The proposed scheme also incorporates a specialized data augmentation strategy optimized for boundary processing and a novel loss function. Extensive experiments on the MICCAI 2023 and 2024 PSFH segmentation benchmarks demonstrate that our method exhibits superior robustness and significantly outperforms existing techniques, providing a reliable segmentation tool for clinical practice.",
    "published": "2026-01-27T10:32:28Z",
    "updated": "2026-01-27T10:32:28Z",
    "link": "http://arxiv.org/pdf/2601.19446v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yalin Luo",
      "Shun Long",
      "Huijin Wang",
      "Jieyun Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.14568v5",
    "title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity",
    "summary": "The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeTickets. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, FreeTickets, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, FreeTickets has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. FreeTickets surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, FreeTickets outperforms the naive deep ensemble with ResNet50 on ImageNet using around only 1/5 of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.",
    "published": "2021-06-28T10:48:20Z",
    "updated": "2026-01-27T10:28:35Z",
    "link": "http://arxiv.org/pdf/2106.14568v5.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Shiwei Liu",
      "Tianlong Chen",
      "Zahra Atashgahi",
      "Xiaohan Chen",
      "Ghada Sokar",
      "Elena Mocanu",
      "Mykola Pechenizkiy",
      "Zhangyang Wang",
      "Decebal Constantin Mocanu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.15766v2",
    "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
    "summary": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
    "published": "2026-01-22T08:57:36Z",
    "updated": "2026-01-27T10:12:42Z",
    "link": "http://arxiv.org/pdf/2601.15766v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuhan Chen",
      "Ying Fang",
      "Guofa Li",
      "Wenxuan Yu",
      "Yicui Shi",
      "Jingrui Zhang",
      "Kefei Qian",
      "Wenbo Chu",
      "Keqiang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19433v1",
    "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
    "summary": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.",
    "published": "2026-01-27T10:10:55Z",
    "updated": "2026-01-27T10:10:55Z",
    "link": "http://arxiv.org/pdf/2601.19433v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jisheng Chu",
      "Wenrui Li",
      "Rui Zhao",
      "Wangmeng Zuo",
      "Shifeng Chen",
      "Xiaopeng Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19430v1",
    "title": "Unveiling Perceptual Artifacts: A Fine-Grained Benchmark for Interpretable AI-Generated Image Detection",
    "summary": "Current AI-Generated Image (AIGI) detection approaches predominantly rely on binary classification to distinguish real from synthetic images, often lacking interpretable or convincing evidence to substantiate their decisions. This limitation stems from existing AIGI detection benchmarks, which, despite featuring a broad collection of synthetic images, remain restricted in their coverage of artifact diversity and lack detailed, localized annotations. To bridge this gap, we introduce a fine-grained benchmark towards eXplainable AI-Generated image Detection, named X-AIGD, which provides pixel-level, categorized annotations of perceptual artifacts, spanning low-level distortions, high-level semantics, and cognitive-level counterfactuals. These comprehensive annotations facilitate fine-grained interpretability evaluation and deeper insight into model decision-making processes. Our extensive investigation using X-AIGD provides several key insights: (1) Existing AIGI detectors demonstrate negligible reliance on perceptual artifacts, even at the most basic distortion level. (2) While AIGI detectors can be trained to identify specific artifacts, they still substantially base their judgment on uninterpretable features. (3) Explicitly aligning model attention with artifact regions can increase the interpretability and generalization of detectors. The data and code are available at: https://github.com/Coxy7/X-AIGD.",
    "published": "2026-01-27T10:09:17Z",
    "updated": "2026-01-27T10:09:17Z",
    "link": "http://arxiv.org/pdf/2601.19430v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yao Xiao",
      "Weiyan Chen",
      "Jiahao Chen",
      "Zijie Cao",
      "Weijian Deng",
      "Binbin Yang",
      "Ziyi Dong",
      "Xiangyang Ji",
      "Wei Ke",
      "Pengxu Wei",
      "Liang Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.03660v2",
    "title": "MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding",
    "summary": "Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.",
    "published": "2026-01-07T07:16:46Z",
    "updated": "2026-01-27T09:31:44Z",
    "link": "http://arxiv.org/pdf/2601.03660v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiangyuan Liu",
      "Yuhao Zhao",
      "Hongxuan Ma",
      "Zhe Liu",
      "Jian Wang",
      "Wei Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23624v2",
    "title": "DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation",
    "summary": "Deep generative models have advanced text-to-online handwriting generation (TOHG), which aims to synthesize realistic pen trajectories conditioned on textual input and style references. However, most existing methods still primarily focus on character- or word-level generation, resulting in inefficiency and a lack of holistic structural modeling when applied to full text lines. To address these issues, we propose DiffInk, the first latent diffusion Transformer framework for full-line handwriting generation. We first introduce InkVAE, a novel sequential variational autoencoder enhanced with two complementary latent-space regularization losses: (1) an OCR-based loss enforcing glyph-level accuracy, and (2) a style-classification loss preserving writing style. This dual regularization yields a semantically structured latent space where character content and writer styles are effectively disentangled. We then introduce InkDiT, a novel latent diffusion Transformer that integrates target text and reference styles to generate coherent pen trajectories. Experimental results demonstrate that DiffInk outperforms existing state-of-the-art methods in both glyph accuracy and style fidelity, while significantly improving generation efficiency.",
    "published": "2025-09-28T03:58:15Z",
    "updated": "2026-01-27T09:19:26Z",
    "link": "http://arxiv.org/pdf/2509.23624v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wei Pan",
      "Huiguo He",
      "Hiuyi Cheng",
      "Yilin Shi",
      "Lianwen Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.11946v2",
    "title": "R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors",
    "summary": "Mesh reconstruction from multi-view images is a fundamental problem in computer vision, but its performance degrades significantly under sparse-view conditions, especially in unseen regions where no ground-truth observations are available. While recent advances in diffusion models have demonstrated strong capabilities in synthesizing novel views from limited inputs, their outputs often suffer from visual artifacts and lack 3D consistency, posing challenges for reliable mesh optimization. In this paper, we propose a novel framework that leverages diffusion models to enhance sparse-view mesh reconstruction in a principled and reliable manner. To address the instability of diffusion outputs, we propose a Consensus Diffusion Module that filters unreliable generations via interquartile range (IQR) analysis and performs variance-aware image fusion to produce robust pseudo-supervision. Building on this, we design an online reinforcement learning strategy based on the Upper Confidence Bound (UCB) to adaptively select the most informative viewpoints for enhancement, guided by diffusion loss. Finally, the fused images are used to jointly supervise a NeRF-based model alongside sparse-view ground truth, ensuring consistency across both geometry and appearance. Extensive experiments demonstrate that our method achieves significant improvements in both geometric quality and rendering quality.",
    "published": "2025-04-16T10:23:59Z",
    "updated": "2026-01-27T09:02:36Z",
    "link": "http://arxiv.org/pdf/2504.11946v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haoyang Wang",
      "Liming Liu",
      "Peiheng Wang",
      "Junlin Hao",
      "Jiangkai Wu",
      "Xinggong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19378v1",
    "title": "Establishing dermatopathology encyclopedia DermpathNet with Artificial Intelligence-Based Workflow",
    "summary": "Accessing high-quality, open-access dermatopathology image datasets for learning and cross-referencing is a common challenge for clinicians and dermatopathology trainees. To establish a comprehensive open-access dermatopathology dataset for educational, cross-referencing, and machine-learning purposes, we employed a hybrid workflow to curate and categorize images from the PubMed Central (PMC) repository. We used specific keywords to extract relevant images, and classified them using a novel hybrid method that combined deep learning-based image modality classification with figure caption analyses. Validation on 651 manually annotated images demonstrated the robustness of our workflow, with an F-score of 89.6\\% for the deep learning approach, 61.0\\% for the keyword-based retrieval method, and 90.4\\% for the hybrid approach. We retrieved over 7,772 images across 166 diagnoses and released this fully annotated dataset, reviewed by board-certified dermatopathologists. Using our dataset as a challenging task, we found the current image analysis algorithm from OpenAI inadequate for analyzing dermatopathology images. In conclusion, we have developed a large, peer-reviewed, open-access dermatopathology image dataset, DermpathNet, which features a semi-automated curation workflow.",
    "published": "2026-01-27T09:02:29Z",
    "updated": "2026-01-27T09:02:29Z",
    "link": "http://arxiv.org/pdf/2601.19378v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziyang Xu",
      "Mingquan Lin",
      "Yiliang Zhou",
      "Zihan Xu",
      "Seth J. Orlow",
      "Zihan Xu",
      "Shane A. Meehan",
      "Alexandra Flamm",
      "Ata S. Moshiri",
      "Yifan Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19365v1",
    "title": "Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation",
    "summary": "Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.",
    "published": "2026-01-27T08:47:01Z",
    "updated": "2026-01-27T08:47:01Z",
    "link": "http://arxiv.org/pdf/2601.19365v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jinming Zhang",
      "Xi Yang",
      "Youpeng Yang",
      "Haosen Shi",
      "Yuyao Yan",
      "Qiufeng Wang",
      "Guangliang Cheng",
      "Kaizhu Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09665v2",
    "title": "Revealing Subtle Phenotypes in Small Microscopy Datasets Using Latent Diffusion Models",
    "summary": "Identifying subtle phenotypic variations in cellular images is critical for advancing biological research and accelerating drug discovery. These variations are often masked by the inherent cellular heterogeneity, making it challenging to distinguish differences between experimental conditions. Recent advancements in deep generative models have demonstrated significant potential for revealing these nuanced phenotypes through image translation, opening new frontiers in cellular and molecular biology as well as the identification of novel biomarkers. Among these generative models, diffusion models stand out for their ability to produce high-quality, realistic images. However, training diffusion models typically requires large datasets and substantial computational resources, both of which can be limited in biological research. In this work, we propose a novel approach that leverages pre-trained latent diffusion models to uncover subtle phenotypic changes. We validate our approach qualitatively and quantitatively on several small datasets of microscopy images. Our findings reveal that our approach enables effective detection of phenotypic variations, capturing both visually apparent and imperceptible differences. Ultimately, our results highlight the promising potential of this approach for phenotype detection, especially in contexts constrained by limited data and computational capacity.",
    "published": "2025-02-12T15:45:19Z",
    "updated": "2026-01-27T08:47:00Z",
    "link": "http://arxiv.org/pdf/2502.09665v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anis Bourou",
      "Biel Castaño Segade",
      "Thomas Boyer",
      "Valérie Mezger",
      "Auguste Genovesio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01573v2",
    "title": "A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation",
    "summary": "Remote sensing semantic segmentation must address both what the ground objects are within an image and where they are located. Consequently, segmentation models must ensure not only the semantic correctness of large-scale patches (low-frequency information) but also the precise localization of boundaries between patches (high-frequency information). However, most existing approaches rely heavily on discriminative learning, which excels at capturing low-frequency features, while overlooking its inherent limitations in learning high-frequency features for semantic segmentation. Recent studies have revealed that diffusion generative models excel at generating high-frequency details. Our theoretical analysis confirms that the diffusion denoising process significantly enhances the model's ability to learn high-frequency features; however, we also observe that these models exhibit insufficient semantic inference for low-frequency features when guided solely by the original image. Therefore, we integrate the strengths of both discriminative and generative learning, proposing the Integration of Discriminative and diffusion-based Generative learning for Boundary Refinement (IDGBR) framework. The framework first generates a coarse segmentation map using a discriminative backbone model. This map and the original image are fed into a conditioning guidance network to jointly learn a guidance representation subsequently leveraged by an iterative denoising diffusion process refining the coarse segmentation. Extensive experiments across five remote sensing semantic segmentation datasets (binary and multi-class segmentation) confirm our framework's capability of consistent boundary refinement for coarse results from diverse discriminative architectures.",
    "published": "2025-07-02T10:47:59Z",
    "updated": "2026-01-27T08:37:49Z",
    "link": "http://arxiv.org/pdf/2507.01573v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Wang",
      "Keyan Hu",
      "Xin Guo",
      "Haifeng Li",
      "Chao Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19349v1",
    "title": "AMGFormer: Adaptive Multi-Granular Transformer for Brain Tumor Segmentation with Missing Modalities",
    "summary": "Multimodal MRI is essential for brain tumor segmentation, yet missing modalities in clinical practice cause existing methods to exhibit >40% performance variance across modality combinations, rendering them clinically unreliable. We propose AMGFormer, achieving significantly improved stability through three synergistic modules: (1) QuadIntegrator Bridge (QIB) enabling spatially adaptive fusion maintaining consistent predictions regardless of available modalities, (2) Multi-Granular Attention Orchestrator (MGAO) focusing on pathological regions to reduce background sensitivity, and (3) Modality Quality-Aware Enhancement (MQAE) preventing error propagation from corrupted sequences. On BraTS 2018, our method achieves 89.33% WT, 82.70% TC, 67.23% ET Dice scores with <0.5% variance across 15 modality combinations, solving the stability crisis. Single-modality ET segmentation shows 40-81% relative improvements over state-of-the-art methods. The method generalizes to BraTS 2020/2021, achieving up to 92.44% WT, 89.91% TC, 84.57% ET. The model demonstrates potential for clinical deployment with 1.2s inference. Code: https://github.com/guochengxiangives/AMGFormer.",
    "published": "2026-01-27T08:29:02Z",
    "updated": "2026-01-27T08:29:02Z",
    "link": "http://arxiv.org/pdf/2601.19349v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Chengxiang Guo",
      "Jian Wang",
      "Junhua Fei",
      "Xiao Li",
      "Chunling Chen",
      "Yun Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12766v4",
    "title": "Probing Deep into Temporal Profile Makes the Infrared Small Target Detector Much Better",
    "summary": "Infrared small target (IRST) detection is challenging in simultaneously achieving precise, robust, and efficient performance due to extremely dim targets and strong interference. Current learning-based methods attempt to leverage ``more\" information from both the spatial and the short-term temporal domains, but suffer from unreliable performance under complex conditions while incurring computational redundancy. In this paper, we explore the ``more essential\" information from a more crucial domain for the detection. Through theoretical analysis, we reveal that the global temporal saliency and correlation information in the temporal profile demonstrate significant superiority in distinguishing target signals from other signals. To investigate whether such superiority is preferentially leveraged by well-trained networks, we built the first prediction attribution tool in this field and verified the importance of the temporal profile information. Inspired by the above conclusions, we remodel the IRST detection task as a one-dimensional signal anomaly detection task, and propose an efficient deep temporal probe network (DeepPro) that only performs calculations in the time dimension for IRST detection. We conducted extensive experiments to fully validate the effectiveness of our method. The experimental results are exciting, as our DeepPro outperforms existing state-of-the-art IRST detection methods on widely-used benchmarks with extremely high efficiency, and achieves a significant improvement on dim targets and in complex scenarios. We provide a new modeling domain, a new insight, a new method, and a new performance, which can promote the development of IRST detection. Codes are available at https://github.com/TinaLRJ/DeepPro.",
    "published": "2025-06-15T08:19:32Z",
    "updated": "2026-01-27T08:23:07Z",
    "link": "http://arxiv.org/pdf/2506.12766v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ruojing Li",
      "Wei An",
      "Yingqian Wang",
      "Xinyi Ying",
      "Yimian Dai",
      "Longguang Wang",
      "Miao Li",
      "Yulan Guo",
      "Li Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.18523v2",
    "title": "Token Caching for Diffusion Transformer Acceleration",
    "summary": "Diffusion transformers have gained substantial interest in diffusion generative modeling due to their outstanding performance. However, their computational demands, particularly the quadratic complexity of attention mechanisms and multi-step inference processes, present substantial bottlenecks that limit their practical applications. To address these challenges, we propose TokenCache, a novel acceleration method that leverages the token-based multi-block architecture of transformers to reduce redundant computations. TokenCache tackles three critical questions: (1) Which tokens should be pruned and reused by the caching mechanism to eliminate redundancy? (2) Which blocks should be targeted for efficient caching? (3) At which time steps should caching be applied to balance speed and quality? In response to these challenges, TokenCache introduces a Cache Predictor that hierarchically addresses these issues by (1) Token pruning: assigning importance scores to each token to determine which tokens to prune and reuse; (2) Block selection: allocating pruning ratio to each block to adaptively select blocks for caching; (3) Temporal Scheduling: deciding at which time steps to apply caching strategies. Experimental results across various models demonstrate that TokenCache achieves an effective trade-off between generation quality and inference speed for diffusion transformers.",
    "published": "2024-09-27T08:05:34Z",
    "updated": "2026-01-27T08:13:13Z",
    "link": "http://arxiv.org/pdf/2409.18523v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Jinming Lou",
      "Wenyang Luo",
      "Yufan Liu",
      "Bing Li",
      "Xinmiao Ding",
      "Weiming Hu",
      "Yuming Li",
      "Chenguang Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13386v4",
    "title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis",
    "summary": "In order to be widely applicable, speech-driven 3D head avatars must articulate their lips in accordance with speech, while also conveying the appropriate emotions with dynamically changing facial expressions. The key problem is that deterministic models produce high-quality lip-sync but without rich expressions, whereas stochastic models generate diverse expressions but with lower lip-sync quality. To get the best of both, we seek a stochastic model with accurate lip-sync. To that end, we develop a new approach based on the following observation: if a method generates realistic 3D lip motions, it should be possible to infer the spoken audio from the lip motion. The inferred speech should match the original input audio, and erroneous predictions create a novel supervision signal for training 3D talking head avatars with accurate lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under Neural Differentiable Elocution Reconstruction), a 3D talking head avatar framework that introduces a novel supervision mechanism via differentiable sound production. First, we train a novel mesh-to-speech model that regresses audio from facial animation. Then, we incorporate this model into a diffusion-based talking avatar framework. During training, the mesh-to-speech model takes the generated animation and produces a sound that is compared to the input speech, creating a differentiable analysis-by-audio-synthesis supervision loop. Our extensive qualitative and quantitative experiments demonstrate that THUNDER significantly improves the quality of the lip-sync of talking head avatars while still allowing for generation of diverse, high-quality, expressive facial animations. The code and models will be available at https://thunder.is.tue.mpg.de/",
    "published": "2025-04-18T00:24:52Z",
    "updated": "2026-01-27T08:03:40Z",
    "link": "http://arxiv.org/pdf/2504.13386v4.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Radek Daněček",
      "Carolin Schmitt",
      "Senya Polikovsky",
      "Michael J. Black"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.11018v2",
    "title": "KOCOBrain: Kuramoto-Guided Graph Network for Uncovering Structure-Function Coupling in Adolescent Prenatal Drug Exposure",
    "summary": "Exposure to psychoactive substances during pregnancy, such as cannabis, can disrupt neurodevelopment and alter large-scale brain networks, yet identifying their neural signatures remains challenging. We introduced KOCOBrain: KuramotO COupled Brain Graph Network; a unified graph neural network framework that integrates structural and functional connectomes via Kuramoto-based phase dynamics and cognition-aware attention. The Kuramoto layer models neural synchronization over anatomical connections, generating phase-informed embeddings that capture structure-function coupling, while cognitive scores modulate information routing in a subject-specific manner followed by a joint objective enhancing robustness under class imbalance scenario. Applied to the ABCD cohort, KOCOBrain improved prenatal drug exposure prediction over relevant baselines and revealed interpretable structure-function patterns that reflect disrupted brain network coordination associated with early exposure.",
    "published": "2026-01-16T06:26:57Z",
    "updated": "2026-01-27T08:02:08Z",
    "link": "http://arxiv.org/pdf/2601.11018v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.CV"
    ],
    "authors": [
      "Badhan Mazumder",
      "Lei Wu",
      "Sir-Lord Wiafe",
      "Vince D. Calhoun",
      "Dong Hye Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19318v1",
    "title": "Perception-to-Pursuit: Track-Centric Temporal Reasoning for Open-World Drone Detection and Autonomous Chasing",
    "summary": "Autonomous drone pursuit requires not only detecting drones but also predicting their trajectories in a manner that enables kinematically feasible interception. Existing tracking methods optimize for prediction accuracy but ignore pursuit feasibility, resulting in trajectories that are physically impossible to intercept 99.9% of the time. We propose Perception-to-Pursuit (P2P), a track-centric temporal reasoning framework that bridges detection and actionable pursuit planning. Our method represents drone motion as compact 8-dimensional tokens capturing velocity, acceleration, scale, and smoothness, enabling a 12-frame causal transformer to reason about future behavior. We introduce the Intercept Success Rate (ISR) metric to measure pursuit feasibility under realistic interceptor constraints. Evaluated on the Anti-UAV-RGBT dataset with 226 real drone sequences, P2P achieves 28.12 pixel average displacement error and 0.597 ISR, representing a 77% improvement in trajectory prediction and 597x improvement in pursuit feasibility over tracking-only baselines, while maintaining perfect drone classification accuracy (100%). Our work demonstrates that temporal reasoning over motion patterns enables both accurate prediction and actionable pursuit planning.",
    "published": "2026-01-27T07:57:29Z",
    "updated": "2026-01-27T07:57:29Z",
    "link": "http://arxiv.org/pdf/2601.19318v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Venkatakrishna Reddy Oruganti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19309v1",
    "title": "Beyond Shadows: A Large-Scale Benchmark and Multi-Stage Framework for High-Fidelity Facial Shadow Removal",
    "summary": "Facial shadows often degrade image quality and the performance of vision algorithms. Existing methods struggle to remove shadows while preserving texture, especially under complex lighting conditions, and they lack real-world paired datasets for training. We present the Augmented Shadow Face in the Wild (ASFW) dataset, the first large-scale real-world dataset for facial shadow removal, containing 1,081 paired shadow and shadow-free images created via a professional Photoshop workflow. ASFW offers photorealistic shadow variations and accurate ground truths, bridging the gap between synthetic and real domains. Deep models trained on ASFW demonstrate improved shadow removal in real-world conditions. We also introduce the Face Shadow Eraser (FSE) method to showcase the effectiveness of the dataset. Experiments demonstrate that ASFW enhances the performance of facial shadow removal models, setting new standards for this task.",
    "published": "2026-01-27T07:48:31Z",
    "updated": "2026-01-27T07:48:31Z",
    "link": "http://arxiv.org/pdf/2601.19309v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tailong Luo",
      "Jiesong Bai",
      "Jinyang Huang",
      "Junyu Xia",
      "Wangyu Wu",
      "Xuhang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.12253v2",
    "title": "Federated Joint Learning for Domain and Class Generalization",
    "summary": "Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \\textbf{Fed}erated Joint Learning for \\textbf{D}omain and \\textbf{C}lass \\textbf{G}eneralization, termed \\textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \\textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.",
    "published": "2026-01-18T04:24:11Z",
    "updated": "2026-01-27T07:46:51Z",
    "link": "http://arxiv.org/pdf/2601.12253v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Haoran Xu",
      "Jiaze Li",
      "Jianzhong Ju",
      "Zhenbo Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19295v1",
    "title": "ProMist-5K: A Comprehensive Dataset for Digital Emulation of Cinematic Pro-Mist Filter Effects",
    "summary": "Pro-Mist filters are widely used in cinematography for their ability to create soft halation, lower contrast, and produce a distinctive, atmospheric style. These effects are difficult to reproduce digitally due to the complex behavior of light diffusion. We present ProMist-5K, a dataset designed to support cinematic style emulation. It is built using a physically inspired pipeline in a scene-referred linear space and includes 20,000 high-resolution image pairs across four configurations, covering two filter densities (1/2 and 1/8) and two focal lengths (20mm and 50mm). Unlike general style datasets, ProMist-5K focuses on realistic glow and highlight diffusion effects. Multiple blur layers and carefully tuned weighting are used to model the varying intensity and spread of optical diffusion. The dataset provides a consistent and controllable target domain that supports various image translation models and learning paradigms. Experiments show that the dataset works well across different training settings and helps capture both subtle and strong cinematic appearances. ProMist-5K offers a practical and physically grounded resource for film-inspired image transformation, bridging the gap between digital flexibility and traditional lens aesthetics. The dataset is available at https://www.kaggle.com/datasets/yingtielei/promist5k.",
    "published": "2026-01-27T07:37:00Z",
    "updated": "2026-01-27T07:37:00Z",
    "link": "http://arxiv.org/pdf/2601.19295v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yingtie Lei",
      "Zimeng Li",
      "Chi-Man Pun",
      "Wangyu Wu",
      "Junke Yang",
      "Xuhang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11003v3",
    "title": "ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization",
    "summary": "The field of Fake Image Detection and Localization (FIDL) is highly fragmented, encompassing four domains: deepfake detection (Deepfake), image manipulation detection and localization (IMDL), artificial intelligence-generated image detection (AIGC), and document image manipulation localization (Doc). Although individual benchmarks exist in some domains, a unified benchmark for all domains in FIDL remains blank. The absence of a unified benchmark results in significant domain silos, where each domain independently constructs its datasets, models, and evaluation protocols without interoperability, preventing cross-domain comparisons and hindering the development of the entire FIDL field. To close the domain silo barrier, we propose ForensicHub, the first unified benchmark & codebase for all-domain fake image detection and localization. Considering drastic variations on dataset, model, and evaluation configurations across all domains, as well as the scarcity of open-sourced baseline models and the lack of individual benchmarks in some domains, ForensicHub: i) proposes a modular and configuration-driven architecture that decomposes forensic pipelines into interchangeable components across datasets, transforms, models, and evaluators, allowing flexible composition across all domains; ii) fully implements 10 baseline models, 6 backbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing benchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii) conducts indepth analysis based on the ForensicHub, offering 8 key actionable insights into FIDL model architecture, dataset characteristics, and evaluation standards. ForensicHub represents a significant leap forward in breaking the domain silos in the FIDL field and inspiring future breakthroughs.",
    "published": "2025-05-16T08:49:59Z",
    "updated": "2026-01-27T07:09:01Z",
    "link": "http://arxiv.org/pdf/2505.11003v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bo Du",
      "Xuekang Zhu",
      "Xiaochen Ma",
      "Chenfan Qu",
      "Kaiwen Feng",
      "Zhe Yang",
      "Chi-Man Pun",
      "Jian Liu",
      "Ji-Zhe Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19266v1",
    "title": "A Multi-View Consistency Framework with Semi-Supervised Domain Adaptation",
    "summary": "Semi-Supervised Domain Adaptation (SSDA) leverages knowledge from a fully labeled source domain to classify data in a partially labeled target domain. Due to the limited number of labeled samples in the target domain, there can be intrinsic similarity of classes in the feature space, which may result in biased predictions, even when the model is trained on a balanced dataset. To overcome this limitation, we introduce a multi-view consistency framework, which includes two views for training strongly augmented data. One is a debiasing strategy for correcting class-wise prediction probabilities according to the prediction performance of the model. The other involves leveraging pseudo-negative labels derived from the model predictions. Furthermore, we introduce a cross-domain affinity learning aimed at aligning features of the same class across different domains, thereby enhancing overall performance. Experimental results demonstrate that our method outperforms the competing methods on two standard domain adaptation datasets, DomainNet and Office-Home. Combining unsupervised domain adaptation and semi-supervised learning offers indispensable contributions to the industrial sector by enhancing model adaptability, reducing annotation costs, and improving performance.",
    "published": "2026-01-27T06:54:13Z",
    "updated": "2026-01-27T06:54:13Z",
    "link": "http://arxiv.org/pdf/2601.19266v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuting Hong",
      "Li Dong",
      "Xiaojie Qiu",
      "Hui Xiao",
      "Baochen Yao",
      "Siming Zheng",
      "Chengbin Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.14233v3",
    "title": "Enhancing Descriptive Captions with Visual Attributes for Multimodal Perception",
    "summary": "Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods for generating such captions often rely on distilling the captions from pretrained LMMs, constructing them from publicly available internet images, or even generating them through human annotation. However, these strategies can fall short in terms of precision and granularity, particularly when dealing with complex visual reasoning tasks. In this paper, we propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption. Our approach, named EDC, explores object low-level and fine-grained attributes (e.g., depth, emotion and fine-grained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. By systematically integrating these rich attributes into the generated captions, EDC significantly improves the descriptive quality of the captions, providing a deeper and more nuanced understanding of the visual content. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. The complete source code of EDC pipeline and datasets will be available at https://github.com/syp2ysy/DCE.",
    "published": "2024-12-18T18:45:43Z",
    "updated": "2026-01-27T06:48:00Z",
    "link": "http://arxiv.org/pdf/2412.14233v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yanpeng Sun",
      "Jing Hao",
      "Ke Zhu",
      "Jiang-Jiang Liu",
      "Yuxiang Zhao",
      "Xiaofan Li",
      "Na Zhao",
      "Zechao Li",
      "Jingdong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19262v1",
    "title": "Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images",
    "summary": "The rapid progress of generative models has enabled the creation of highly realistic synthetic images, raising concerns about authenticity and trust in digital media. Detecting such fake content reliably is an urgent challenge. While deep learning approaches dominate current literature, handcrafted features remain attractive for their interpretability, efficiency, and generalizability. In this paper, we conduct a systematic evaluation of handcrafted descriptors, including raw pixels, color histograms, Discrete Cosine Transform (DCT), Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gray-Level Co-occurrence Matrix (GLCM), and wavelet features, on the CIFAKE dataset of real versus synthetic images. Using 50,000 training and 10,000 test samples, we benchmark seven classifiers ranging from Logistic Regression to advanced gradient-boosted ensembles (LightGBM, XGBoost, CatBoost). Results demonstrate that LightGBM consistently outperforms alternatives, achieving PR-AUC 0.9879, ROC-AUC 0.9878, F1 0.9447, and a Brier score of 0.0414 with mixed features, representing strong gains in calibration and discrimination over simpler descriptors. Across three configurations (baseline, advanced, mixed), performance improves monotonically, confirming that combining diverse handcrafted features yields substantial benefit. These findings highlight the continued relevance of carefully engineered features and ensemble learning for detecting synthetic images, particularly in contexts where interpretability and computational efficiency are critical.",
    "published": "2026-01-27T06:43:01Z",
    "updated": "2026-01-27T06:43:01Z",
    "link": "http://arxiv.org/pdf/2601.19262v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Syed Mehedi Hasan Nirob",
      "Moqsadur Rahman",
      "Shamim Ehsan",
      "Summit Haque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06647v3",
    "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction",
    "summary": "Safety constitutes a foundational imperative for autonomous driving systems, necessitating maximal incorporation of accessible prior information. This study establishes that temporal perception buffers and cost-efficient high-definition (HD) maps inherently form complementary prior sources for online vectorized HD map construction. We present Uni-PrevPredMap, a pioneering unified framework systematically integrating previous predictions with corrupted HD maps. Our framework introduces a tri-mode paradigm maintaining operational consistency across non-prior, temporal-prior, and temporal-map-fusion modes. This tri-mode paradigm simultaneously decouples the framework from ideal map assumptions while ensuring robust performance in both map-present and map-absent scenarios. Additionally, we develop a tile-indexed 3D vectorized global map processor enabling efficient 3D prior data refreshment, compact storage, and real-time retrieval. Uni-PrevPredMap achieves state-of-the-art map-absent performance across established online vectorized HD map construction benchmarks. When provided with corrupted HD maps, it exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between temporal predictions and imperfect map data. Code is available at https://github.com/pnnnnnnn/Uni-PrevPredMap.",
    "published": "2025-04-09T07:36:17Z",
    "updated": "2026-01-27T06:34:15Z",
    "link": "http://arxiv.org/pdf/2504.06647v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nan Peng",
      "Xun Zhou",
      "Mingming Wang",
      "Guisong Chen",
      "Wenqi Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19247v1",
    "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
    "summary": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.",
    "published": "2026-01-27T06:30:32Z",
    "updated": "2026-01-27T06:30:32Z",
    "link": "http://arxiv.org/pdf/2601.19247v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiarun Liu",
      "Qifeng Chen",
      "Yiru Zhao",
      "Minghua Liu",
      "Baorui Ma",
      "Sheng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19246v1",
    "title": "Magnetic Resonance Simulation of Effective Transverse Relaxation (T2*)",
    "summary": "Purpose: To simulate effective transverse relaxation ($T_2^*$) as a part of MR simulation. $T_2^*$ consists of reversible ($T_2^{\\prime}$) and irreversible ($T_2$) components. Whereas simulations of $T_2$ are easy, $T_2^{\\prime}$ is not easily simulated if only magnetizations of individual isochromats are simulated.\n  Theory and Methods: Efficient methods for simulating $T_2^{\\prime}$ were proposed. To approximate the Lorentzian function of $T_2^{\\prime}$ realistically, conventional simulators require 100+ isochromats. This approximation can be avoided by utilizing a linear phase model for simulating an entire Lorentzian function directly. To represent the linear phase model, the partial derivatives of the magnetizations with respect to the frequency axis were also simulated. To accelerate the simulations with these partial derivatives, the proposed methods introduced two techniques: analytic solutions, and combined transitions. For understanding the fundamental mechanism of the proposed method, a simple one-isochromat simulation was performed. For evaluating realistic cases, several pulse sequences were simulated using two phantoms with and without $T_2^{\\prime}$ simulations.\n  Results: The one-isochromat simulation demonstrated that $T_2^{\\prime}$ simulations were possible. In the realistic cases, $T_2^{\\prime}$ was recovered as expected without using 100+ isochromats for each point. The computational times with $T_2^{\\prime}$ simulations were only 2.0 to 2.7 times longer than those without $T_2^{\\prime}$ simulations. When the above-mentioned two techniques were utilized, the analytic solutions accelerated 19 times, and the combined transitions accelerated up to 17 times.\n  Conclusion: Both theory and results showed that the proposed methods simulated $T_2^{\\prime}$ efficiently by utilizing a linear model with a Lorentzian function, analytic solutions, and combined transitions.",
    "published": "2026-01-27T06:28:52Z",
    "updated": "2026-01-27T06:28:52Z",
    "link": "http://arxiv.org/pdf/2601.19246v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "authors": [
      "Hidenori Takeshima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.13126v2",
    "title": "Knowledge-enhanced Pretraining for Vision-language Pathology Foundation Model on Cancer Diagnosis",
    "summary": "Vision-language foundation models have shown great promise in computational pathology but remain primarily data-driven, lacking explicit integration of medical knowledge. We introduce KEEP (KnowledgE-Enhanced Pathology), a foundation model that systematically incorporates disease knowledge into pretraining for cancer diagnosis. KEEP leverages a comprehensive disease knowledge graph encompassing 11,454 diseases and 139,143 attributes to reorganize millions of pathology image-text pairs into 143,000 semantically structured groups aligned with disease ontology hierarchies. This knowledge-enhanced pretraining aligns visual and textual representations within hierarchical semantic spaces, enabling deeper understanding of disease relationships and morphological patterns. Across 18 public benchmarks (over 14,000 whole-slide images) and 4 institutional rare cancer datasets (926 cases), KEEP consistently outperformed existing foundation models, showing substantial gains for rare subtypes. These results establish knowledge-enhanced vision-language modeling as a powerful paradigm for advancing computational pathology.",
    "published": "2024-12-17T17:45:21Z",
    "updated": "2026-01-27T06:24:09Z",
    "link": "http://arxiv.org/pdf/2412.13126v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Xiao Zhou",
      "Luoyi Sun",
      "Dexuan He",
      "Wenbin Guan",
      "Ge Wang",
      "Ruifen Wang",
      "Lifeng Wang",
      "Xiaojun Yuan",
      "Xin Sun",
      "Ya Zhang",
      "Kun Sun",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19236v1",
    "title": "VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics",
    "summary": "While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: https://anonymous.4open.science/r/VC-Bench-1B67/.",
    "published": "2026-01-27T06:15:12Z",
    "updated": "2026-01-27T06:15:12Z",
    "link": "http://arxiv.org/pdf/2601.19236v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Zhiyu Yin",
      "Zhipeng Liu",
      "Kehai Chen",
      "Lemao Liu",
      "Jin Liu",
      "Hong-Dong Li",
      "Yang Xiang",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19228v1",
    "title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
    "summary": "We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/",
    "published": "2026-01-27T05:50:40Z",
    "updated": "2026-01-27T05:50:40Z",
    "link": "http://arxiv.org/pdf/2601.19228v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianhui Song",
      "Haoyu Lu",
      "Hao Yang",
      "Lin Sui",
      "Haoning Wu",
      "Zaida Zhou",
      "Zhiqi Huang",
      "Yiping Bao",
      "Y. Charles",
      "Xinyu Zhou",
      "Limin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11027v2",
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "summary": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
    "published": "2025-10-13T05:51:22Z",
    "updated": "2026-01-27T05:41:52Z",
    "link": "http://arxiv.org/pdf/2510.11027v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ganlin Yang",
      "Tianyi Zhang",
      "Haoran Hao",
      "Weiyun Wang",
      "Yibin Liu",
      "Dehui Wang",
      "Guanzhou Chen",
      "Zijian Cai",
      "Junting Chen",
      "Weijie Su",
      "Wengang Zhou",
      "Yu Qiao",
      "Jifeng Dai",
      "Jiangmiao Pang",
      "Gen Luo",
      "Wenhai Wang",
      "Yao Mu",
      "Zhi Hou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19210v1",
    "title": "Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP",
    "summary": "Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model's inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.",
    "published": "2026-01-27T05:24:45Z",
    "updated": "2026-01-27T05:24:45Z",
    "link": "http://arxiv.org/pdf/2601.19210v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sen Nie",
      "Jie Zhang",
      "Zhuo Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19897v1",
    "title": "Self-Distillation Enables Continual Learning",
    "summary": "Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.",
    "published": "2026-01-27T18:59:08Z",
    "updated": "2026-01-27T18:59:08Z",
    "link": "http://arxiv.org/pdf/2601.19897v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Idan Shenfeld",
      "Mehul Damani",
      "Jonas Hübotter",
      "Pulkit Agrawal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19876v1",
    "title": "RHSIA: Real-time Hemodynamics Surrogation for Non-idealized Intracranial Aneurysms",
    "summary": "Extensive studies suggested that fluid mechanical markers of intracranial aneurysms (IAs) derived from Computational Fluid Dynamics (CFD) can indicate disease progression risks, but to date this has not been translated clinically. This is because CFD requires specialized expertise and is time-consuming and low throughput, making it difficult to support clinical trials. A deep learning model that maps IA morphology to biomechanical markers can address this, enabling physicians to obtain these markers in real time without performing CFD. Here, we show that a Graph Transformer model that incorporates temporal information, which is supervised by large CFD data, can accurately predict Wall Shear Stress (WSS) across the cardiac cycle from IA surface meshes. The model effectively captures the temporal variations of the WSS pattern, achieving a Structural Similarity Index (SSIM) of up to 0.981 and a maximum-based relative L2 error of 2.8%. Ablation studies and SOTA comparison confirmed its optimality. Further, as pulsatile CFD data is computationally expensive to generate and sample sizes are limited, we engaged a strategy of injecting a large amount of steady-state CFD data, which are extremely low-cost to generate, as augmentation. This approach enhances network performance substantially when pulsatile CFD data sample size is small. Our study provides a proof of concept that temporal sequences cardiovascular fluid mechanical parameters can be computed in real time using a deep learning model from the geometric mesh, and this is achievable even with small pulsatile CFD sample size. Our approach is likely applicable to other cardiovascular scenarios.",
    "published": "2026-01-27T18:39:58Z",
    "updated": "2026-01-27T18:39:58Z",
    "link": "http://arxiv.org/pdf/2601.19876v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yiying Sheng",
      "Wenhao Ding",
      "Dylan Roi",
      "Leonard Leong Litt Yeo",
      "Hwa Liang Leo",
      "Choon Hwai Yap"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.02700v4",
    "title": "A simple algorithm for output range analysis for deep neural networks",
    "summary": "This paper presents a novel approach for the output range estimation problem in Deep Neural Networks (DNNs) by integrating a Simulated Annealing (SA) algorithm tailored to operate within constrained domains and ensure convergence towards global optima. The method effectively addresses the challenges posed by the lack of local geometric information and the high non-linearity inherent to DNNs, making it applicable to a wide variety of architectures, with a special focus on Residual Networks (ResNets) due to their practical importance. Unlike existing methods, our algorithm imposes minimal assumptions on the internal architecture of neural networks, thereby extending its usability to complex models. Theoretical analysis guarantees convergence, while extensive empirical evaluations-including optimization tests involving functions with multiple local minima-demonstrate the robustness of our algorithm in navigating non-convex response surfaces. The experimental results highlight the algorithm's efficiency in accurately estimating DNN output ranges, even in scenarios characterized by high non-linearity and complex constraints. For reproducibility, Python codes and datasets used in the experiments are publicly available through our GitHub repository.",
    "published": "2024-07-02T22:47:40Z",
    "updated": "2026-01-27T18:39:11Z",
    "link": "http://arxiv.org/pdf/2407.02700v4.pdf",
    "category": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "authors": [
      "Helder Rojas",
      "Nilton Rojas",
      "Espinoza J. B.",
      "Luis Huamanchumo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19867v1",
    "title": "Bandits in Flux: Adversarial Constraints in Dynamic Environments",
    "summary": "We investigate the challenging problem of adversarial multi-armed bandits operating under time-varying constraints, a scenario motivated by numerous real-world applications. To address this complex setting, we propose a novel primal-dual algorithm that extends online mirror descent through the incorporation of suitable gradient estimators and effective constraint handling. We provide theoretical guarantees establishing sublinear dynamic regret and sublinear constraint violation for our proposed policy. Our algorithm achieves state-of-the-art performance in terms of both regret and constraint violation. Empirical evaluations demonstrate the superiority of our approach.",
    "published": "2026-01-27T18:26:07Z",
    "updated": "2026-01-27T18:26:07Z",
    "link": "http://arxiv.org/pdf/2601.19867v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Tareq Si Salem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12748v2",
    "title": "Deep Semantic Inference over the Air: An Efficient Task-Oriented Communication System",
    "summary": "Empowered by deep learning, semantic communication marks a paradigm shift from transmitting raw data to conveying task-relevant meaning, enabling more efficient and intelligent wireless systems. In this study, we explore a deep learning-based task-oriented communication framework that jointly considers classification performance, computational latency, and communication cost. We evaluate ResNets-based models on the CIFAR-10 and CIFAR-100 datasets to simulate real-world classification tasks in wireless environments. We partition the model at various points to simulate split inference across a wireless channel. By varying the split location and the size of the transmitted semantic feature vector, we systematically analyze the trade-offs between task accuracy and resource efficiency. Experimental results show that, with appropriate model partitioning and semantic feature compression, the system can retain over 85\\% of baseline accuracy while significantly reducing both computational load and communication overhead.",
    "published": "2025-08-18T09:18:07Z",
    "updated": "2026-01-27T18:26:00Z",
    "link": "http://arxiv.org/pdf/2508.12748v2.pdf",
    "category": [
      "cs.IT",
      "cs.LG"
    ],
    "authors": [
      "Chenyang Wang",
      "Roger Olsson",
      "Stefan Forsström",
      "Qing He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19862v1",
    "title": "Calibration without Ground Truth",
    "summary": "Villalobos et al. [2024] predict that publicly available human text will be exhausted within the next decade. Thus, improving models without access to ground-truth labels becomes increasingly important. We propose a label-free post-processing framework that improves a strong but miscalibrated model using a weaker yet better-calibrated reference. Our framework guarantees a strict performance improvement under any proper loss. Our approach is based on a characterization of when strict improvement is possible: when the strong and reference models are not mutually calibrated. We formalize this condition, connect it to arbitrage and no-trade results from economics, and develop an efficient Bregman projection algorithm that guarantees worst-case loss reduction without labels. Experiments on representative LLMs across varying scales demonstrate that our label-free method significantly reduces proper losses and calibration errors, achieving performance competitive with supervised baselines.",
    "published": "2026-01-27T18:18:47Z",
    "updated": "2026-01-27T18:18:47Z",
    "link": "http://arxiv.org/pdf/2601.19862v1.pdf",
    "category": [
      "cs.LG",
      "cs.GT"
    ],
    "authors": [
      "Yuqing Kong",
      "Mingyu Song",
      "Yizhou Wang",
      "Yifan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.10809v3",
    "title": "MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents",
    "summary": "Recent advances in operating system (OS) agents have enabled vision-language models (VLMs) to directly control a user's computer. Unlike conventional VLMs that passively output text, OS agents autonomously perform computer-based tasks in response to a single user prompt. OS agents do so by capturing, parsing, and analysing screenshots and executing low-level actions via application programming interfaces (APIs), such as mouse clicks and keyboard inputs. This direct interaction with the OS significantly raises the stakes, as failures or manipulations can have immediate and tangible consequences. In this work, we uncover a novel attack vector against these OS agents: Malicious Image Patches (MIPs), adversarially perturbed screen regions that, when captured by an OS agent, induce it to perform harmful actions by exploiting specific APIs. For instance, a MIP can be embedded in a desktop wallpaper or shared on social media to cause an OS agent to exfiltrate sensitive user data. We show that MIPs generalise across user prompts and screen configurations, and that they can hijack multiple OS agents even during the execution of benign instructions. These findings expose critical security vulnerabilities in OS agents that have to be carefully addressed before their widespread deployment.",
    "published": "2025-03-13T18:59:12Z",
    "updated": "2026-01-27T18:10:17Z",
    "link": "http://arxiv.org/pdf/2503.10809v3.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Lukas Aichberger",
      "Alasdair Paren",
      "Guohao Li",
      "Philip Torr",
      "Yarin Gal",
      "Adel Bibi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19853v1",
    "title": "Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living",
    "summary": "In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range-Angle (RA) heatmaps. The latent space is softly aligned with two semantic anchors corresponding to \"empty room\" and \"person present\", and Grad-CAM is applied in this aligned latent space to visualize which spatial regions support each presence decision. On our mmWave radar dataset, we qualitatively observe that the \"person present\" class produces compact Grad-CAM blobs that coincide with strong RA returns, whereas \"empty room\" samples yield diffuse or no evidence. We also conduct an ablation study using unrelated text prompts, which degrades both reconstruction and localization, suggesting that radar-specific anchors are important for meaningful explanations in this setting.",
    "published": "2026-01-27T18:06:51Z",
    "updated": "2026-01-27T18:06:51Z",
    "link": "http://arxiv.org/pdf/2601.19853v1.pdf",
    "category": [
      "eess.SP",
      "cs.LG"
    ],
    "authors": [
      "Huy Trinh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2209.10166v5",
    "title": "Chaotic Hedging with Iterated Integrals and Neural Networks",
    "summary": "In this paper, we derive an $L^p$-chaos expansion based on iterated Stratonovich integrals with respect to a given exponentially integrable continuous semimartingale. By omitting the orthogonality of the expansion, we show that every $p$-integrable functional, $p \\in [1,\\infty)$, can be approximated by a finite sum of iterated Stratonovich integrals. Using (possibly random) neural networks as integrands, we therefere obtain universal approximation results for $p$-integrable financial derivatives in the $L^p$-sense. Moreover, we can approximately solve the $L^p$-hedging problem (coinciding for $p = 2$ with the quadratic hedging problem), where the approximating hedging strategy can be computed in closed form within short runtime.",
    "published": "2022-09-21T07:57:07Z",
    "updated": "2026-01-27T17:46:03Z",
    "link": "http://arxiv.org/pdf/2209.10166v5.pdf",
    "category": [
      "q-fin.MF",
      "cs.LG",
      "math.PR",
      "q-fin.CP",
      "stat.ML"
    ],
    "authors": [
      "Ariel Neufeld",
      "Philipp Schmocker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19833v1",
    "title": "A Multi-directional Meta-Learning Framework for Class-Generalizable Anomaly Detection",
    "summary": "In this paper, we address the problem of class-generalizable anomaly detection, where the objective is to develop a unified model by focusing our learning on the available normal data and a small amount of anomaly data in order to detect the completely unseen anomalies, also referred to as the out-of-distribution (OOD) classes. Adding to this challenge is the fact that the anomaly data is rare and costly to label. To achieve this, we propose a multidirectional meta-learning algorithm -- at the inner level, the model aims to learn the manifold of the normal data (representation); at the outer level, the model is meta-tuned with a few anomaly samples to maximize the softmax confidence margin between the normal and anomaly samples (decision surface calibration), treating normals as in-distribution (ID) and anomalies as out-of-distribution (OOD). By iteratively repeating this process over multiple episodes of predominantly normal and a small number of anomaly samples, we realize a multidirectional meta-learning framework. This two-level optimization, enhanced by multidirectional training, enables stronger generalization to unseen anomaly classes.",
    "published": "2026-01-27T17:39:11Z",
    "updated": "2026-01-27T17:39:11Z",
    "link": "http://arxiv.org/pdf/2601.19833v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Padmaksha Roy",
      "Lamine Mili",
      "Almuatazbellah Boker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19818v1",
    "title": "Learn and Verify: A Framework for Rigorous Verification of Physics-Informed Neural Networks",
    "summary": "The numerical solution of differential equations using neural networks has become a central topic in scientific computing, with Physics-Informed Neural Networks (PINNs) emerging as a powerful paradigm for both forward and inverse problems. However, unlike classical numerical methods that offer established convergence guarantees, neural network-based approximations typically lack rigorous error bounds. Furthermore, the non-deterministic nature of their optimization makes it difficult to mathematically certify their accuracy. To address these challenges, we propose a \"Learn and Verify\" framework that provides computable, mathematically rigorous error bounds for the solutions of differential equations. By combining a novel Doubly Smoothed Maximum (DSM) loss for training with interval arithmetic for verification, we compute rigorous a posteriori error bounds as machine-verifiable proofs. Numerical experiments on nonlinear Ordinary Differential Equations (ODEs), including problems with time-varying coefficients and finite-time blow-up, demonstrate that the proposed framework successfully constructs rigorous enclosures of the true solutions, establishing a foundation for trustworthy scientific machine learning.",
    "published": "2026-01-27T17:21:33Z",
    "updated": "2026-01-27T17:21:33Z",
    "link": "http://arxiv.org/pdf/2601.19818v1.pdf",
    "category": [
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Kazuaki Tanaka",
      "Kohei Yatabe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16568v2",
    "title": "Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach",
    "summary": "Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.",
    "published": "2026-01-23T09:08:52Z",
    "updated": "2026-01-27T17:16:47Z",
    "link": "http://arxiv.org/pdf/2601.16568v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Abdurahman Maarouf",
      "Alket Bakiaj",
      "Stefan Feuerriegel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13506v2",
    "title": "Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource",
    "summary": "Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility: the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.",
    "published": "2025-12-15T16:34:47Z",
    "updated": "2026-01-27T17:09:04Z",
    "link": "http://arxiv.org/pdf/2512.13506v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Sofiya Zaichyk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19794v1",
    "title": "Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation",
    "summary": "The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions.",
    "published": "2026-01-27T16:53:19Z",
    "updated": "2026-01-27T16:53:19Z",
    "link": "http://arxiv.org/pdf/2601.19794v1.pdf",
    "category": [
      "cs.LG",
      "eess.SY"
    ],
    "authors": [
      "Ganesh Sundaram",
      "Jonas Ulmen",
      "Daniel Görges"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19791v1",
    "title": "To Grok Grokking: Provable Grokking in Ridge Regression",
    "summary": "We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persists long after overfitting has manifested; and (iii) the generalization error eventually becomes arbitrarily small. Moreover, we show, both theoretically and empirically, that grokking can be amplified or eliminated in a principled manner through proper hyperparameter tuning. To the best of our knowledge, these are the first rigorous quantitative bounds on the generalization delay (which we refer to as the \"grokking time\") in terms of training hyperparameters. Lastly, going beyond the linear setting, we empirically demonstrate that our quantitative bounds also capture the behavior of grokking on non-linear neural networks. Our results suggest that grokking is not an inherent failure mode of deep learning, but rather a consequence of specific training conditions, and thus does not require fundamental changes to the model architecture or learning algorithm to avoid.",
    "published": "2026-01-27T16:52:04Z",
    "updated": "2026-01-27T16:52:04Z",
    "link": "http://arxiv.org/pdf/2601.19791v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Mingyue Xu",
      "Gal Vardi",
      "Itay Safran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19788v1",
    "title": "Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers",
    "summary": "Federated Continual Learning (FCL) leverages inter-client collaboration to balance new knowledge acquisition and prior knowledge retention in non-stationary data. However, existing batch-based FCL methods lack adaptability to streaming scenarios featuring category overlap between old and new data and absent task identifiers, leading to indistinguishability of old and new knowledge, uncertain task assignments for samples, and knowledge confusion.To address this, we propose streaming federated continual learning setting: per federated learning (FL) round, clients process streaming data with disjoint samples and potentially overlapping categories without task identifiers, necessitating sustained inference capability for all prior categories after each FL round.Next, we introduce FedKACE: 1) an adaptive inference model switching mechanism that enables unidirectional switching from local model to global model to achieve a trade-off between personalization and generalization; 2) a adaptive gradient-balanced replay scheme that reconciles new knowledge learning and old knowledge retention under overlapping-class scenarios; 3) a kernel spectral boundary buffer maintenance that preserves high-information and high-boundary-influence samples to optimize cross-round knowledge retention. Experiments across multiple scenarios and regret analysis demonstrate the effectiveness of FedKACE.",
    "published": "2026-01-27T16:50:48Z",
    "updated": "2026-01-27T16:50:48Z",
    "link": "http://arxiv.org/pdf/2601.19788v1.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Sixing Tan",
      "Xianmin Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19766v1",
    "title": "The Effect of Architecture During Continual Learning",
    "summary": "Continual learning is a challenge for models with static architecture, as they fail to adapt to when data distributions evolve across tasks. We introduce a mathematical framework that jointly models architecture and weights in a Sobolev space, enabling a rigorous investigation into the role of neural network architecture in continual learning and its effect on the forgetting loss. We derive necessary conditions for the continual learning solution and prove that learning only model weights is insufficient to mitigate catastrophic forgetting under distribution shifts. Consequently, we prove that by learning the architecture and weights simultaneously at each task, we can reduce catastrophic forgetting.\n  To learn weights and architecture simultaneously, we formulate continual learning as a bilevel optimization problem: the upper level selects an optimal architecture for a given task, while the lower level computes optimal weights via dynamic programming over all tasks. To solve the upper level problem, we introduce a derivative-free direct search algorithm to determine the optimal architecture. Once found, we must transfer knowledge from the current architecture to the optimal one. However, the optimal architecture will result in a weights parameter space different from the current architecture (i.e., dimensions of weights matrices will not match). To bridge the dimensionality gap, we develop a low-rank transfer mechanism to map knowledge across architectures of mismatched dimensions. Empirical studies across regression and classification problems, including feedforward, convolutional, and graph neural networks, demonstrate that learning the optimal architecture and weights simultaneously yields substantially improved performance (up to two orders of magnitude), reduced forgetting, and enhanced robustness to noise compared with static architecture approaches.",
    "published": "2026-01-27T16:29:42Z",
    "updated": "2026-01-27T16:29:42Z",
    "link": "http://arxiv.org/pdf/2601.19766v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Allyson Hahn",
      "Krishnan Raghavan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19756v1",
    "title": "Provable Learning of Random Hierarchy Models and Hierarchical Shallow-to-Deep Chaining",
    "summary": "The empirical success of deep learning is often attributed to deep networks' ability to exploit hierarchical structure in data, constructing increasingly complex features across layers. Yet despite substantial progress in deep learning theory, most optimization results sill focus on networks with only two or three layers, leaving the theoretical understanding of hierarchical learning in genuinely deep models limited. This leads to a natural question: can we prove that deep networks, trained by gradient-based methods, can efficiently exploit hierarchical structure?\n  In this work, we consider Random Hierarchy Models -- a hierarchical context-free grammar introduced by arXiv:2307.02129 and conjectured to separate deep and shallow networks. We prove that, under mild conditions, a deep convolutional network can be efficiently trained to learn this function class. Our proof builds on a general observation: if intermediate layers can receive clean signal from the labels and the relevant features are weakly identifiable, then layerwise training each individual layer suffices to hierarchically learn the target function.",
    "published": "2026-01-27T16:19:54Z",
    "updated": "2026-01-27T16:19:54Z",
    "link": "http://arxiv.org/pdf/2601.19756v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Yunwei Ren",
      "Yatin Dandi",
      "Florent Krzakala",
      "Jason D. Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19755v1",
    "title": "Regularized $f$-Divergence Kernel Tests",
    "summary": "We propose a framework to construct practical kernel-based two-sample tests from the family of $f$-divergences. The test statistic is computed from the witness function of a regularized variational representation of the divergence, which we estimate using kernel methods. The proposed test is adaptive over hyperparameters such as the kernel bandwidth and the regularization parameter. We provide theoretical guarantees for statistical test power across our family of $f$-divergence estimates. While our test covers a variety of $f$-divergences, we bring particular focus to the Hockey-Stick divergence, motivated by its applications to differential privacy auditing and machine unlearning evaluation. For two-sample testing, experiments demonstrate that different $f$-divergences are sensitive to different localized differences, illustrating the importance of leveraging diverse statistics. For machine unlearning, we propose a relative test that distinguishes true unlearning failures from safe distributional variations.",
    "published": "2026-01-27T16:15:48Z",
    "updated": "2026-01-27T16:15:48Z",
    "link": "http://arxiv.org/pdf/2601.19755v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Mónica Ribero",
      "Antonin Schrab",
      "Arthur Gretton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.16858v2",
    "title": "Investigating Test Overfitting on SWE-bench",
    "summary": "Tests can be useful towards resolving issues on code repositories. However, relying too much on tests for issue resolution can lead to code that technically passes observed tests but actually misses important cases or even breaks functionality. This problem, called test overfitting, is exacerbated by the fact that issues usually lack readily executable tests. Instead, several issue resolution systems use tests auto-generated from issues, which may be imperfect. Some systems even iteratively refine code and tests jointly. This paper presents the first empirical study of test overfitting in this setting.",
    "published": "2025-11-20T23:55:56Z",
    "updated": "2026-01-27T16:12:38Z",
    "link": "http://arxiv.org/pdf/2511.16858v2.pdf",
    "category": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "Toufique Ahmed",
      "Jatin Ganhotra",
      "Avraham Shinnar",
      "Martin Hirzel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19745v1",
    "title": "GraphDLG: Exploring Deep Leakage from Gradients in Federated Graph Learning",
    "summary": "Federated graph learning (FGL) has recently emerged as a promising privacy-preserving paradigm that enables distributed graph learning across multiple data owners. A critical privacy concern in federated learning is whether an adversary can recover raw data from shared gradients, a vulnerability known as deep leakage from gradients (DLG). However, most prior studies on the DLG problem focused on image or text data, and it remains an open question whether graphs can be effectively recovered, particularly when the graph structure and node features are uniquely entangled in GNNs. In this work, we first theoretically analyze the components in FGL and derive a crucial insight: once the graph structure is recovered, node features can be obtained through a closed-form recursive rule. Building on this analysis, we propose GraphDLG, a novel approach to recover raw training graphs from shared gradients in FGL, which can utilize randomly generated graphs or client-side training graphs as auxiliaries to enhance recovery. Extensive experiments demonstrate that GraphDLG outperforms existing solutions by successfully decoupling the graph structure and node features, achieving improvements of over 5.46% (by MSE) for node feature reconstruction and over 25.04% (by AUC) for graph structure reconstruction.",
    "published": "2026-01-27T16:06:48Z",
    "updated": "2026-01-27T16:06:48Z",
    "link": "http://arxiv.org/pdf/2601.19745v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shuyue Wei",
      "Wantong Chen",
      "Tongyu Wei",
      "Chen Gong",
      "Yongxin Tong",
      "Lizhen Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20829v2",
    "title": "Explaining Grokking and Information Bottleneck through Neural Collapse Emergence",
    "summary": "The training dynamics of deep neural networks often defy expectations, even as these models form the foundation of modern machine learning. Two prominent examples are grokking, where test performance improves abruptly long after the training loss has plateaued, and the information bottleneck principle, where models progressively discard input information irrelevant to the prediction task as training proceeds. However, the mechanisms underlying these phenomena and their relations remain poorly understood. In this work, we present a unified explanation of such late-phase phenomena through the lens of neural collapse, which characterizes the geometry of learned representations. We show that the contraction of population within-class variance is a key factor underlying both grokking and information bottleneck, and relate this measure to the neural collapse measure defined on the training set. By analyzing the dynamics of neural collapse, we show that distinct time scales between fitting the training set and the progression of neural collapse account for the behavior of the late-phase phenomena. Finally, we validate our theoretical findings on multiple datasets and architectures.",
    "published": "2025-09-25T07:17:41Z",
    "updated": "2026-01-27T15:59:48Z",
    "link": "http://arxiv.org/pdf/2509.20829v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Keitaro Sakamoto",
      "Issei Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19730v1",
    "title": "Stability and Generalization of Nonconvex Optimization with Heavy-Tailed Noise",
    "summary": "The empirical evidence indicates that stochastic optimization with heavy-tailed gradient noise is more appropriate to characterize the training of machine learning models than that with standard bounded gradient variance noise. Most existing works on this phenomenon focus on the convergence of optimization errors, while the analysis for generalization bounds under the heavy-tailed gradient noise remains limited. In this paper, we develop a general framework for establishing generalization bounds under heavy-tailed noise. Specifically, we introduce a truncation argument to achieve the generalization error bound based on the algorithmic stability under the assumption of bounded $p$th centered moment with $p\\in(1,2]$. Building on this framework, we further provide the stability and generalization analysis for several popular stochastic algorithms under heavy-tailed noise, including clipped and normalized stochastic gradient descent, as well as their mini-batch and momentum variants.",
    "published": "2026-01-27T15:50:57Z",
    "updated": "2026-01-27T15:50:57Z",
    "link": "http://arxiv.org/pdf/2601.19730v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hongxu Chen",
      "Ke Wei",
      "Xiaoming Yuan",
      "Luo Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19720v1",
    "title": "Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action",
    "summary": "Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.",
    "published": "2026-01-27T15:43:02Z",
    "updated": "2026-01-27T15:43:02Z",
    "link": "http://arxiv.org/pdf/2601.19720v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Gong Gao",
      "Weidong Zhao",
      "Xianhui Liu",
      "Ning Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19718v1",
    "title": "Rethinking Divisive Hierarchical Clustering from a Distributional Perspective",
    "summary": "We uncover that current objective-based Divisive Hierarchical Clustering (DHC) methods produce a dendrogram that does not have three desired properties i.e., no unwarranted splitting, group similar clusters into a same subset, ground-truth correspondence. This shortcoming has their root cause in using a set-oriented bisecting assessment criterion. We show that this shortcoming can be addressed by using a distributional kernel, instead of the set-oriented criterion; and the resultant clusters achieve a new distribution-oriented objective to maximize the total similarity of all clusters (TSC). Our theoretical analysis shows that the resultant dendrogram guarantees a lower bound of TSC. The empirical evaluation shows the effectiveness of our proposed method on artificial and Spatial Transcriptomics (bioinformatics) datasets. Our proposed method successfully creates a dendrogram that is consistent with the biological regions in a Spatial Transcriptomics dataset, whereas other contenders fail.",
    "published": "2026-01-27T15:41:56Z",
    "updated": "2026-01-27T15:41:56Z",
    "link": "http://arxiv.org/pdf/2601.19718v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kaifeng Zhang",
      "Kai Ming Ting",
      "Tianrun Liang",
      "Qiuran Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.11428v3",
    "title": "Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families",
    "summary": "Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests - including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts - to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1,000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.",
    "published": "2026-01-16T16:47:44Z",
    "updated": "2026-01-27T15:32:48Z",
    "link": "http://arxiv.org/pdf/2601.11428v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Lennon Shikhman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19707v1",
    "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
    "summary": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale.",
    "published": "2026-01-27T15:30:10Z",
    "updated": "2026-01-27T15:30:10Z",
    "link": "http://arxiv.org/pdf/2601.19707v1.pdf",
    "category": [
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Yunyue Wei",
      "Chenhui Zuo",
      "Yanan Sui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18604v2",
    "title": "LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation",
    "summary": "Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.\n  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.\n  Availability and implementation: https://github.com/willyzzz/LaCoGSEA",
    "published": "2026-01-26T15:45:33Z",
    "updated": "2026-01-27T15:08:11Z",
    "link": "http://arxiv.org/pdf/2601.18604v2.pdf",
    "category": [
      "cs.LG",
      "q-bio.GN"
    ],
    "authors": [
      "Zhiwei Zheng",
      "Kevin Bryson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.12760v2",
    "title": "Conformal Online Learning of Deep Koopman Linear Embeddings",
    "summary": "We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.",
    "published": "2025-11-16T20:08:48Z",
    "updated": "2026-01-27T15:07:34Z",
    "link": "http://arxiv.org/pdf/2511.12760v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Ben Gao",
      "Jordan Patracone",
      "Stéphane Chrétien",
      "Olivier Alata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18640v2",
    "title": "TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning",
    "summary": "Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.\n  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as \"background\" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.\n  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.",
    "published": "2026-01-26T16:11:34Z",
    "updated": "2026-01-27T15:04:22Z",
    "link": "http://arxiv.org/pdf/2601.18640v2.pdf",
    "category": [
      "cs.LG",
      "q-bio.MN"
    ],
    "authors": [
      "Zhiwei Zheng",
      "Kevin Bryson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22124v2",
    "title": "Incorporating priors in learning: a random matrix study under a teacher-student framework",
    "summary": "Regularized linear regression is central to machine learning, yet its high-dimensional behavior with informative priors remains poorly understood. We provide the first exact asymptotic characterization of training and test risks for maximum a posteriori (MAP) regression with Gaussian priors centered at a domain-informed initialization. Our framework unifies ridge regression, least squares, and prior-informed estimators, and -- using random matrix theory -- yields closed-form risk formulas that expose the bias-variance-prior tradeoff, explain double descent, and quantify prior mismatch. We also identify a closed-form minimizer of test risk, enabling a simple estimator of the optimal regularization parameter. Simulations confirm the theory with high accuracy. By connecting Bayesian priors, classical regularization, and modern asymptotics, our results provide both conceptual clarity and practical guidance for learning with structured prior knowledge.",
    "published": "2025-09-26T09:47:15Z",
    "updated": "2026-01-27T15:02:26Z",
    "link": "http://arxiv.org/pdf/2509.22124v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Malik Tiomoko",
      "Ekkehard Schnoor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19675v1",
    "title": "LoPRo: Enhancing Low-Rank Quantization via Permuted Block-Wise Rotation",
    "summary": "Post-training quantization (PTQ) enables effective model compression while preserving relatively high accuracy. Current weight-only PTQ methods primarily focus on the challenging sub-3-bit regime, where approaches often suffer significant accuracy degradation, typically requiring fine-tuning to achieve competitive performance. In this work, we revisit the fundamental characteristics of weight quantization and analyze the challenges in quantizing the residual matrix under low-rank approximation. We propose LoPRo, a novel fine-tuning-free PTQ algorithm that enhances residual matrix quantization by applying block-wise permutation and Walsh-Hadamard transformations to rotate columns of similar importance, while explicitly preserving the quantization accuracy of the most salient column blocks. Furthermore, we introduce a mixed-precision fast low-rank decomposition based on rank-1 sketch (R1SVD) to further minimize quantization costs. Experiments demonstrate that LoPRo outperforms existing fine-tuning-free PTQ methods at both 2-bit and 3-bit quantization, achieving accuracy comparable to fine-tuning baselines. Specifically, LoPRo achieves state-of-the-art quantization accuracy on LLaMA-2 and LLaMA-3 series models while delivering up to a 4$\\times$ speedup. In the MoE model Mixtral-8x7B, LoPRo completes quantization within 2.5 hours, simultaneously reducing perplexity by 0.4$\\downarrow$ and improving accuracy by 8\\%$\\uparrow$. Moreover, compared to other low-rank quantization methods, LoPRo achieves superior accuracy with a significantly lower rank, while maintaining high inference efficiency and minimal additional latency.",
    "published": "2026-01-27T14:56:04Z",
    "updated": "2026-01-27T14:56:04Z",
    "link": "http://arxiv.org/pdf/2601.19675v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hongyaoxing Gu",
      "Lijuan Hu",
      "Liye Yu",
      "Haowei Li",
      "Fangfang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19668v1",
    "title": "Grasynda: Graph-based Synthetic Time Series Generation",
    "summary": "Data augmentation is a crucial tool in time series forecasting, especially for deep learning architectures that require a large training sample size to generalize effectively. However, extensive datasets are not always available in real-world scenarios. Although many data augmentation methods exist, their limitations include the use of transformations that do not adequately preserve data properties. This paper introduces Grasynda, a novel graph-based approach for synthetic time series generation that: (1) converts univariate time series into a network structure using a graph representation, where each state is a node and each transition is represented as a directed edge; and (2) encodes their temporal dynamics in a transition probability matrix. We performed an extensive evaluation of Grasynda as a data augmentation method for time series forecasting. We use three neural network variations on six benchmark datasets. The results indicate that Grasynda consistently outperforms other time series data augmentation methods, including ones used in state-of-the-art time series foundation models. The method and all experiments are publicly available.",
    "published": "2026-01-27T14:47:41Z",
    "updated": "2026-01-27T14:47:41Z",
    "link": "http://arxiv.org/pdf/2601.19668v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Luis Amorim",
      "Moises Santos",
      "Paulo J. Azevedo",
      "Carlos Soares",
      "Vitor Cerqueira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07580v3",
    "title": "Generative Modeling with Bayesian Sample Inference",
    "summary": "We derive a novel generative model from iterative Gaussian posterior inference. By treating the generated sample as an unknown variable, we can formulate the sampling process in the language of Bayesian probability. Our model uses a sequence of prediction and posterior update steps to iteratively narrow down the unknown sample starting from a broad initial belief. In addition to a rigorous theoretical analysis, we establish a connection between our model and diffusion models and show that it includes Bayesian Flow Networks (BFNs) as a special case. In our experiments, we demonstrate that our model improves sample quality on ImageNet32 over both BFNs and the closely related Variational Diffusion Models, while achieving equivalent log-likelihoods on ImageNet32 and ImageNet64. Find our code at https://github.com/martenlienen/bsi.",
    "published": "2025-02-11T14:27:10Z",
    "updated": "2026-01-27T14:42:47Z",
    "link": "http://arxiv.org/pdf/2502.07580v3.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Marten Lienen",
      "Marcel Kollovieh",
      "Stephan Günnemann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.04056v3",
    "title": "Problem Space Transformations for Out-of-Distribution Generalisation in Behavioural Cloning",
    "summary": "The combination of behavioural cloning and neural networks has driven significant progress in robotic manipulation. As these algorithms may require a large number of demonstrations for each task of interest, they remain fundamentally inefficient in complex scenarios, in which finite datasets can hardly cover the state space. One of the remaining challenges is thus out-of-distribution (OOD) generalisation, i.e. the ability to predict correct actions for states with a low likelihood with respect to the state occupancy induced by the dataset. This issue is aggravated when the system to control is treated as a black-box, ignoring its physical properties. This work highlights widespread properties of robotic manipulation, specifically pose equivariance and locality. We investigate the effect of the choice of problem space on OOD performance of BC policies and how transformations arising from characteristic properties of manipulation can be employed for its improvement. Through controlled, simulated and real-world experiments, we empirically demonstrate that these transformations allow behaviour cloning policies, using either standard MLP-based one-step action prediction or diffusion-based action-sequence prediction, to generalise better to certain OOD problem instances. Code is available at https://github.com/kirandoshi/pst_ood_gen.",
    "published": "2024-11-06T17:05:58Z",
    "updated": "2026-01-27T14:39:42Z",
    "link": "http://arxiv.org/pdf/2411.04056v3.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Kiran Doshi",
      "Marco Bagatella",
      "Stelian Coros"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23581v2",
    "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning",
    "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness in enhancing the reasoning abilities of LLMs by leveraging graph structures for knowledge representation and modeling complex real-world relationships. However, existing GraphRAG methods still face significant bottlenecks when handling complex problems that require multi-hop reasoning, as their query and retrieval phases are largely based on pre-defined heuristics and do not fully utilize the reasoning potentials of LLMs. To address this problem, we propose GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with process-constrained outcome-based reinforcement learning (RL) to enhance the multi-hop reasoning ability. Our method can decompose complex problems, autonomously invoke retrieval tools to acquire necessary information, and perform effective reasoning. Specifically, we utilize a modified version of Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking capability. Next, we design two process-constrained reward functions. To handle the shallow retrieval problem, we design a Progressive Retrieval Attenuation (PRA) reward to encourage essential retrievals. Then, to handle the over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the model performance with computational costs. We further design a phase-dependent training strategy, containing three training stages corresponding to cold start and these two rewards. Lastly, our method adopts a hybrid graph-textual retrieval to improve the reasoning capacity. Extensive experimental results demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets. Furthermore, our framework can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements.",
    "published": "2025-07-31T14:11:16Z",
    "updated": "2026-01-27T14:24:16Z",
    "link": "http://arxiv.org/pdf/2507.23581v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chuanyue Yu",
      "Kuo Zhao",
      "Yuhan Li",
      "Heng Chang",
      "Mingjian Feng",
      "Xiangzhe Jiang",
      "Yufei Sun",
      "Jia Li",
      "Yuzhi Zhang",
      "Jianxin Li",
      "Ziwei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22755v2",
    "title": "Concept activation vectors: a unifying view and adversarial attacks",
    "summary": "Concept Activation Vectors (CAVs) are a tool from explainable AI, offering a promising approach for understanding how human-understandable concepts are encoded in a model's latent spaces. They are computed from hidden-layer activations of inputs belonging either to a concept class or to non-concept examples. Adopting a probabilistic perspective, the distribution of the (non-)concept inputs induces a distribution over the CAV, making it a random vector in the latent space. This enables us to derive mean and covariance for different types of CAVs, leading to a unified theoretical view. This probabilistic perspective also reveals a potential vulnerability: CAVs can strongly depend on the rather arbitrary non-concept distribution, a factor largely overlooked in prior work. We illustrate this with a simple yet effective adversarial attack, underscoring the need for a more systematic study.",
    "published": "2025-09-26T09:22:31Z",
    "updated": "2026-01-27T14:22:14Z",
    "link": "http://arxiv.org/pdf/2509.22755v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.PR"
    ],
    "authors": [
      "Ekkehard Schnoor",
      "Malik Tiomoko",
      "Jawher Said",
      "Alex Jung",
      "Wojciech Samek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.11217v2",
    "title": "Model-free policy gradient for discrete-time mean-field control",
    "summary": "We study model-free policy learning for discrete-time mean-field control (MFC) problems with finite state space and compact action space. In contrast to the extensive literature on value-based methods for MFC, policy-based approaches remain largely unexplored due to the intrinsic dependence of transition kernels and rewards on the evolving population state distribution, which prevents the direct use of likelihood-ratio estimators of policy gradients from classical single-agent reinforcement learning. We introduce a novel perturbation scheme on the state-distribution flow and prove that the gradient of the resulting perturbed value function converges to the true policy gradient as the perturbation magnitude vanishes. This construction yields a fully model-free estimator based solely on simulated trajectories and an auxiliary estimate of the sensitivity of the state distribution. Building on this framework, we develop MF-REINFORCE, a model-free policy gradient algorithm for MFC, and establish explicit quantitative bounds on its bias and mean-squared error. Numerical experiments on representative mean-field control tasks demonstrate the effectiveness of the proposed approach.",
    "published": "2026-01-16T11:49:25Z",
    "updated": "2026-01-27T13:47:58Z",
    "link": "http://arxiv.org/pdf/2601.11217v2.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Matthieu Meunier",
      "Huyên Pham",
      "Christoph Reisinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19597v1",
    "title": "The Geometric Mechanics of Contrastive Representation Learning: Alignment Potentials, Entropic Dispersion, and Cross-Modal Divergence",
    "summary": "While InfoNCE powers modern contrastive learning, its geometric mechanisms remain under-characterized beyond the canonical alignment--uniformity decomposition. We present a measure-theoretic framework that models learning as the evolution of representation measures on a fixed embedding manifold. By establishing value and gradient consistency in the large-batch limit, we bridge the stochastic objective to explicit deterministic energy landscapes, uncovering a fundamental geometric bifurcation between the unimodal and multimodal regimes. In the unimodal setting, the intrinsic landscape is strictly convex with a unique Gibbs equilibrium; here, entropy acts merely as a tie-breaker, clarifying \"uniformity\" as a constrained expansion within the alignment basin. In contrast, the symmetric multimodal objective contains a persistent negative symmetric divergence term that remains even after kernel sharpening. We show that this term induces barrier-driven co-adaptation, enforcing a population-level modality gap as a structural geometric necessity rather than an initialization artifact. Our results shift the analytical lens from pointwise discrimination to population geometry, offering a principled basis for diagnosing and controlling distributional misalignment.",
    "published": "2026-01-27T13:33:03Z",
    "updated": "2026-01-27T13:33:03Z",
    "link": "http://arxiv.org/pdf/2601.19597v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yichao Cai",
      "Zhen Zhang",
      "Yuhang Liu",
      "Javen Qinfeng Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.06917v4",
    "title": "Sample-Efficient Optimization over Generative Priors via Coarse Learnability",
    "summary": "In zeroth-order optimization, we seek to minimize a function $d(\\cdot)$, which may encode combinatorial feasibility, using only function evaluations. We focus on the setting where solutions must also satisfy qualitative constraints or conform to a complex prior distribution. To address this, we introduce a new framework in which such constraints are represented by an initial generative prior $Ł(\\cdot)$, for example, a Large Language Model (LLM). The objective is to find solutions $s$ that minimize $d(s)$ while having high probability under $Ł(s)$, effectively sampling from a target distribution proportional to $Ł(s) \\cdot e^{-T \\cdot d(s)}$ for a temperature parameter $T$.\n  While this framework aligns with classical Model-Based Optimization (e.g., the Cross-Entropy method), existing theory is ill-suited for deriving sample complexity bounds in black-box deep generative models. We therefore propose a novel learning assumption, which we term \\emph{coarse learnability}, where an agent with access to a polynomial number of samples can learn a model whose point-wise density approximates the target within a polynomial factor. Leveraging this assumption, we design an iterative algorithm that employs a Metropolis-Hastings correction to provably approximate the target distribution using a polynomial number of samples. To the best of our knowledge, this is one of the first works to establish such sample-complexity guarantees for model-based optimization with deep generative priors.\n  We provide two lines of evidence supporting the coarse learnability assumption. Theoretically, we show that maximum likelihood estimation naturally induces the required coverage properties, holding for both standard exponential families and for misspecified models. Empirically, we demonstrate that LLMs can adapt their learned distributions to zeroth-order feedback to solve combinatorial optimization problems.",
    "published": "2025-03-10T04:58:18Z",
    "updated": "2026-01-27T13:28:38Z",
    "link": "http://arxiv.org/pdf/2503.06917v4.pdf",
    "category": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "authors": [
      "Pranjal Awasthi",
      "Sreenivas Gollapudi",
      "Ravi Kumar",
      "Kamesh Munagala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14610v5",
    "title": "Imputation-free Learning of Tabular Data with Missing Values using Incremental Feature Partitions in Transformer",
    "summary": "Tabular data sets with varying missing values are prepared for machine learning using an arbitrary imputation strategy. Synthetic values generated by imputation models often raise concerns regarding data quality and the reliability of data-driven outcomes. To address these concerns, this article proposes an imputation-free incremental attention learning (IFIAL) method for tabular data with missing values. A pair of attention masks is derived and retrofitted to a transformer to directly streamline tabular data without imputing or initializing missing values. The proposed method incrementally learns partitions of overlapping and fixed-size feature sets to enhance the performance of the transformer. The average classification performance rank order across 17 diverse tabular data sets highlights the superiority of IFIAL over 11 state-of-the-art learning methods with or without missing value imputations. Additional experiments corroborate the robustness of IFIAL to varying types and proportions of missing data, demonstrating its superiority over methods that rely on explicit imputations. A feature partition size equal to one-half the original feature space yields the best trade-off between computational efficiency and predictive performance. IFIAL is one of the first solutions that enables deep attention models to learn directly from tabular data, eliminating the need to impute missing values. %without the need for imputing missing values. The source code for this paper is publicly available.",
    "published": "2025-04-20T13:31:49Z",
    "updated": "2026-01-27T13:26:33Z",
    "link": "http://arxiv.org/pdf/2504.14610v5.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Manar D. Samad",
      "Kazi Fuad B. Akhter",
      "Shourav B. Rabbani",
      "Ibna Kowsar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17543v5",
    "title": "Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality",
    "summary": "Existing distribution compression methods reduce the number of observations in a dataset by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which we introduce to quantify the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that BDC can achieve comparable or superior downstream task performance to ambient-space compression at substantially lower cost and with significantly higher rates of compression.",
    "published": "2025-09-22T09:01:52Z",
    "updated": "2026-01-27T13:06:41Z",
    "link": "http://arxiv.org/pdf/2509.17543v5.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Dominic Broadbent",
      "Nick Whiteley",
      "Robert Allison",
      "Tom Lovett"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19567v1",
    "title": "Learning the Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou Trajectories: A Nonlinear Approach using a Deep Autoencoder Model",
    "summary": "We address the intrinsic dimensionality (ID) of high-dimensional trajectories, comprising $n_s = 4\\,000\\,000$ data points, of the Fermi-Pasta-Ulam-Tsingou (FPUT) $β$ model with $N = 32$ oscillators. To this end, a deep autoencoder (DAE) model is employed to infer the ID in the weakly nonlinear regime ($β\\lesssim 1$). We find that the trajectories lie on a nonlinear manifold of dimension $m^{\\ast} = 2$ embedded in a $64$-dimensional phase space. The DAE further reveals that this dimensionality increases to $m^{\\ast} = 3$ at $β= 1.1$, coinciding with a symmetry breaking transition, in which additional energy modes with even wave numbers $k = 2, 4$ become excited. Finally, we discuss the limitations of the linear approach based on principal component analysis (PCA), which fails to capture the underlying structure of the data and therefore yields unreliable results in most cases.",
    "published": "2026-01-27T12:59:29Z",
    "updated": "2026-01-27T12:59:29Z",
    "link": "http://arxiv.org/pdf/2601.19567v1.pdf",
    "category": [
      "cond-mat.stat-mech",
      "cs.LG"
    ],
    "authors": [
      "Gionni Marchetti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19552v1",
    "title": "Generalizable Equivariant Diffusion Models for Non-Abelian Lattice Gauge Theory",
    "summary": "We demonstrate that gauge equivariant diffusion models can accurately model the physics of non-Abelian lattice gauge theory using the Metropolis-adjusted annealed Langevin algorithm (MAALA), as exemplified by computations in two-dimensional U(2) and SU(2) gauge theories. Our network architecture is based on lattice gauge equivariant convolutional neural networks (L-CNNs), which respect local and global symmetries on the lattice. Models are trained on a single ensemble generated using a traditional Monte Carlo method. By studying Wilson loops of various size as well as the topological susceptibility, we find that the diffusion approach generalizes remarkably well to larger inverse couplings and lattice sizes with negligible loss of accuracy while retaining moderately high acceptance rates.",
    "published": "2026-01-27T12:44:44Z",
    "updated": "2026-01-27T12:44:44Z",
    "link": "http://arxiv.org/pdf/2601.19552v1.pdf",
    "category": [
      "hep-lat",
      "cs.LG"
    ],
    "authors": [
      "Gert Aarts",
      "Diaa E. Habibi",
      "Andreas Ipp",
      "David I. Müller",
      "Thomas R. Ranner",
      "Lingxiao Wang",
      "Wei Wang",
      "Qianteng Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19541v1",
    "title": "GenCP: Towards Generative Modeling Paradigm of Coupled Physics",
    "summary": "Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging. Many mainstream approaches face challenges when dealing with decoupled data. Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems. Here we propose GenCP, a novel and elegant generative paradigm for coupled multiphysics simulation. By formulating coupled-physics modeling as a probability modeling problem, our key innovation is to integrate probability density evolution in generative modeling with iterative multiphysics coupling, thereby enabling training on data from decoupled simulation and inferring coupled physics during sampling. We also utilize operator-splitting theory in the space of probability evolution to establish error controllability guarantees for this \"conditional-to-joint\" sampling scheme. We evaluate our paradigm on a synthetic setting and three challenging multi-physics scenarios to demonstrate both principled insight and superior application performance of GenCP. Code is available at this repo: github.com/AI4Science-WestlakeU/GenCP.",
    "published": "2026-01-27T12:31:49Z",
    "updated": "2026-01-27T12:31:49Z",
    "link": "http://arxiv.org/pdf/2601.19541v1.pdf",
    "category": [
      "cs.LG",
      "cs.CE"
    ],
    "authors": [
      "Tianrun Gao",
      "Haoren Zheng",
      "Wenhao Deng",
      "Haodong Feng",
      "Tao Zhang",
      "Ruiqi Feng",
      "Qianyi Chen",
      "Tailin Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15105v2",
    "title": "Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting",
    "summary": "Time series forecasting (TSF) is critical in domains like energy, finance, healthcare, and logistics, requiring models that generalize across diverse datasets. Large pre-trained models such as Chronos and Time-MoE show strong zero-shot (ZS) performance but suffer from high computational costs. In this work, we introduce Super-Linear, a lightweight and scalable mixture-of-experts (MoE) model for general forecasting. It replaces deep architectures with simple frequency-specialized linear experts, trained on resampled data across multiple frequency regimes. A lightweight spectral gating mechanism dynamically selects relevant experts, enabling efficient, accurate forecasting. Despite its simplicity, Super-Linear demonstrates strong performance across benchmarks, while substantially improving efficiency, robustness to sampling rates, and interpretability. The implementation of Super-Linear is available at: \\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}.",
    "published": "2025-09-18T16:11:31Z",
    "updated": "2026-01-27T12:31:03Z",
    "link": "http://arxiv.org/pdf/2509.15105v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Liran Nochumsohn",
      "Raz Marshanski",
      "Hedi Zisling",
      "Omri Azencot"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.17727v2",
    "title": "Automated Interpretability Metrics Do Not Distinguish Trained and Random Transformers",
    "summary": "Sparse autoencoders (SAEs) are widely used to extract sparse, interpretable latents from transformer activations. We test whether commonly used SAE quality metrics and automatic explanation pipelines can distinguish trained transformers from randomly initialized ones (e.g., where parameters are sampled i.i.d. from a Gaussian). Over a wide range of Pythia model sizes and multiple randomization schemes, we find that, in many settings, SAEs trained on randomly initialized transformers produce auto-interpretability scores and reconstruction metrics that are similar to those from trained models. These results show that high aggregate auto-interpretability scores do not, by themselves, guarantee that learned, computationally relevant features have been recovered. We therefore recommend treating common SAE metrics as useful but insufficient proxies for mechanistic interpretability and argue for routine randomized baselines and targeted measures of feature 'abstractness'.",
    "published": "2025-01-29T16:11:12Z",
    "updated": "2026-01-27T12:21:36Z",
    "link": "http://arxiv.org/pdf/2501.17727v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Thomas Heap",
      "Tim Lawson",
      "Lucy Farnik",
      "Laurence Aitchison"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18500v2",
    "title": "Nearly Optimal Bayesian Inference for Structural Missingness",
    "summary": "Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.",
    "published": "2026-01-26T14:03:11Z",
    "updated": "2026-01-27T12:05:37Z",
    "link": "http://arxiv.org/pdf/2601.18500v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chen Liang",
      "Donghua Yang",
      "Yutong Zhao",
      "Tianle Zhang",
      "Shenghang Zhou",
      "Zhiyu Liang",
      "Hengtong Zhang",
      "Hongzhi Wang",
      "Ziqi Li",
      "Xiyang Zhang",
      "Zheng Liang",
      "Yifei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19481v1",
    "title": "Posterior Distribution-assisted Evolutionary Dynamic Optimization as an Online Calibrator for Complex Social Simulations",
    "summary": "The calibration of simulators for complex social systems aims to identify the optimal parameter that drives the output of the simulator best matching the target data observed from the system. As many social systems may change internally over time, calibration naturally becomes an online task, requiring parameters to be updated continuously to maintain the simulator's fidelity. In this work, the online setting is first formulated as a dynamic optimization problem (DOP), requiring the search for a sequence of optimal parameters that fit the simulator to real system changes. However, in contrast to traditional DOP formulations, online calibration explicitly incorporates the observational data as the driver of environmental dynamics. Due to this fundamental difference, existing Evolutionary Dynamic Optimization (EDO) methods, despite being extensively studied for black-box DOPs, are ill-equipped to handle such a scenario. As a result, online calibration problems constitute a new set of challenging DOPs. Here, we propose to explicitly learn the posterior distributions of the parameters and the observational data, thereby facilitating both change detection and environmental adaptation of existing EDOs for this scenario. We thus present a pretrained posterior model for implementation, and fine-tune it during the optimization. Extensive tests on both economic and financial simulators verify that the posterior distribution strongly promotes EDOs in such DOPs widely existed in social science.",
    "published": "2026-01-27T11:15:06Z",
    "updated": "2026-01-27T11:15:06Z",
    "link": "http://arxiv.org/pdf/2601.19481v1.pdf",
    "category": [
      "cs.NE",
      "cs.LG"
    ],
    "authors": [
      "Peng Yang",
      "Zhenhua Yang",
      "Boquan Jiang",
      "Chenkai Wang",
      "Ke Tang",
      "Xin Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09084v2",
    "title": "GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions",
    "summary": "The Kolmogorov-Arnold representation theorem offers a theoretical alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate functions on edges rather than nodes. While recent implementations such as Kolmogorov-Arnold Networks (KANs) demonstrate high approximation capabilities, they suffer from significant parameter inefficiency due to the requirement of maintaining unique parameterizations for every network edge. In this work, we propose GS-KAN (Generalized Sprecher-KAN), a lightweight architecture inspired by David Sprecher's refinement of the superposition theorem. GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer. We evaluate GS-KAN against existing KAN architectures and MLPs across synthetic function approximation, tabular data regression and image classification tasks. Our results demonstrate that GS-KAN outperforms both MLPs and standard KAN baselines on continuous function approximation tasks while maintaining superior parameter efficiency. Additionally, GS-KAN achieves competitive performance with existing KAN architectures on tabular regression and outperforms MLPs on high-dimensional classification tasks. Crucially, the proposed architecture enables the deployment of KAN-based architectures in high-dimensional regimes under strict parameter constraints, a setting where standard implementations are typically infeasible due to parameter explosion. The source code is available at https://github.com/rambamn48/gs-impl.",
    "published": "2025-12-09T19:56:36Z",
    "updated": "2026-01-27T11:10:44Z",
    "link": "http://arxiv.org/pdf/2512.09084v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Oscar Eliasson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19467v1",
    "title": "On the Expressiveness of State Space Models via Temporal Logics",
    "summary": "We investigate the expressive power of state space models (SSM), which have recently emerged as a potential alternative to transformer architectures in large language models. Building on recent work, we analyse SSM expressiveness through fragments and extensions of linear temporal logic over finite traces. Our results show that the expressive capabilities of SSM vary substantially depending on the underlying gating mechanism. We further distinguish between SSM operating over fixed-width arithmetic (quantised models), whose expressive power remains within regular languages, and SSM with unbounded precision, which can capture counting properties and non-regular languages. In addition, we provide a systematic comparison between these different SSM variants and known results on transformers, thereby clarifying how the two architectures relate in terms of expressive power.",
    "published": "2026-01-27T10:49:24Z",
    "updated": "2026-01-27T10:49:24Z",
    "link": "http://arxiv.org/pdf/2601.19467v1.pdf",
    "category": [
      "cs.LO",
      "cs.FL",
      "cs.LG"
    ],
    "authors": [
      "Eric Alsmann",
      "Lowejatan Noori",
      "Martin Lange"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19449v1",
    "title": "Fixed Aggregation Features Can Rival GNNs",
    "summary": "Graph neural networks (GNNs) are widely believed to excel at node representation learning through trainable neighborhood aggregations. We challenge this view by introducing Fixed Aggregation Features (FAFs), a training-free approach that transforms graph learning tasks into tabular problems. This simple shift enables the use of well-established tabular methods, offering strong interpretability and the flexibility to deploy diverse classifiers. Across 14 benchmarks, well-tuned multilayer perceptrons trained on FAFs rival or outperform state-of-the-art GNNs and graph transformers on 12 tasks -- often using only mean aggregation. The only exceptions are the Roman Empire and Minesweeper datasets, which typically require unusually deep GNNs. To explain the theoretical possibility of non-trainable aggregations, we connect our findings to Kolmogorov-Arnold representations and discuss when mean aggregation can be sufficient. In conclusion, our results call for (i) richer benchmarks benefiting from learning diverse neighborhood aggregations, (ii) strong tabular baselines as standard, and (iii) employing and advancing tabular models for graph data to gain new insights into related tasks.",
    "published": "2026-01-27T10:36:31Z",
    "updated": "2026-01-27T10:36:31Z",
    "link": "http://arxiv.org/pdf/2601.19449v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Celia Rubio-Madrigal",
      "Rebekka Burkholz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19448v1",
    "title": "From Internal Diagnosis to External Auditing: A VLM-Driven Paradigm for Online Test-Time Backdoor Defense",
    "summary": "Deep Neural Networks remain inherently vulnerable to backdoor attacks. Traditional test-time defenses largely operate under the paradigm of internal diagnosis methods like model repairing or input robustness, yet these approaches are often fragile under advanced attacks as they remain entangled with the victim model's corrupted parameters. We propose a paradigm shift from Internal Diagnosis to External Semantic Auditing, arguing that effective defense requires decoupling safety from the victim model via an independent, semantically grounded auditor. To this end, we present a framework harnessing Universal Vision-Language Models (VLMs) as evolving semantic gatekeepers. We introduce PRISM (Prototype Refinement & Inspection via Statistical Monitoring), which overcomes the domain gap of general VLMs through two key mechanisms: a Hybrid VLM Teacher that dynamically refines visual prototypes online, and an Adaptive Router powered by statistical margin monitoring to calibrate gating thresholds in real-time. Extensive evaluation across 17 datasets and 11 attack types demonstrates that PRISM achieves state-of-the-art performance, suppressing Attack Success Rate to <1% on CIFAR-10 while improving clean accuracy, establishing a new standard for model-agnostic, externalized security.",
    "published": "2026-01-27T10:34:06Z",
    "updated": "2026-01-27T10:34:06Z",
    "link": "http://arxiv.org/pdf/2601.19448v1.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Binyan Xu",
      "Fan Yang",
      "Xilin Dai",
      "Di Tang",
      "Kehuan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19439v1",
    "title": "OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation",
    "summary": "The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture and optimize using conventional design methodologies. Although recent advances in machine learning (ML) have shown promise in automating specific stages of the analog design flow, the development of holistic, end-to-end frameworks that integrate these stages and iteratively refine layouts using post-layout, parasitic-aware performance feedback is still in its early stages. Furthermore, progress in this direction is hindered by the limited availability of open, high-quality datasets tailored to the analog domain, restricting both the benchmarking and the generalizability of ML-based techniques. To address these limitations, we present OSIRIS, a scalable dataset generation pipeline for analog IC design. OSIRIS systematically explores the design space of analog circuits while producing comprehensive performance metrics and metadata, thereby enabling ML-driven research in electronic design automation (EDA). In addition, we release a dataset consisting of 87,100 circuit variations generated with OSIRIS, accompanied by a reinforcement learning (RL)-based baseline method that exploits OSIRIS for analog design optimization.",
    "published": "2026-01-27T10:18:46Z",
    "updated": "2026-01-27T10:18:46Z",
    "link": "http://arxiv.org/pdf/2601.19439v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Giuseppe Chiari",
      "Michele Piccoli",
      "Davide Zoni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.18273v2",
    "title": "Synchronization on circles and spheres with nonlinear interactions",
    "summary": "We consider the dynamics of $n$ points on a sphere in $\\mathbb{R}^d$ ($d \\geq 2$) which attract each other according to a function $\\varphi$ of their inner products. When $\\varphi$ is linear ($\\varphi(t) = t$), the points converge to a common value (i.e., synchronize) in various connectivity scenarios: this is part of classical work on Kuramoto oscillator networks. When $\\varphi$ is exponential ($\\varphi(t) = e^{βt}$), these dynamics correspond to a limit of how idealized transformers process data, as described by Geshkovski et al. (2025). Accordingly, they ask whether synchronization occurs for exponential $\\varphi$.\n  The answer depends on the dimension $d$. In the context of consensus for multi-agent control, Markdahl et al. (2018) show that for $d \\geq 3$ (spheres), if the interaction graph is connected and $\\varphi$ is increasing and convex, then the system synchronizes. We give a separate proof of this result.\n  What is the situation on circles ($d=2$)? First, we show that $\\varphi$ being increasing and convex is no longer sufficient (even for complete graphs). Then we identify a new condition under which we do have synchronization on the circle (namely, if the Taylor coefficients of $\\varphi'$ are decreasing). As a corollary, this provide synchronization for exponential $\\varphi$ with $β\\in (0, 1]$. The proofs are based on nonconvex landscape analysis.",
    "published": "2024-05-28T15:24:30Z",
    "updated": "2026-01-27T10:06:47Z",
    "link": "http://arxiv.org/pdf/2405.18273v2.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "math.DS"
    ],
    "authors": [
      "Christopher Criscitiello",
      "Quentin Rebjock",
      "Andrew D. McRae",
      "Nicolas Boumal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.20712v3",
    "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
    "summary": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.",
    "published": "2025-12-23T19:19:45Z",
    "updated": "2026-01-27T09:50:38Z",
    "link": "http://arxiv.org/pdf/2512.20712v3.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Omer Gazit",
      "Yael Itzhakev",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19411v1",
    "title": "Task-Centric Policy Optimization from Misaligned Motion Priors",
    "summary": "Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing naïve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \\emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.",
    "published": "2026-01-27T09:46:34Z",
    "updated": "2026-01-27T09:46:34Z",
    "link": "http://arxiv.org/pdf/2601.19411v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Ziang Zheng",
      "Kai Feng",
      "Yi Nie",
      "Shentao Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19400v1",
    "title": "Improved Convergence Rates of Muon Optimizer for Nonconvex Optimization",
    "summary": "The Muon optimizer has recently attracted attention due to its orthogonalized first-order updates, and a deeper theoretical understanding of its convergence behavior is essential for guiding practical applications; however, existing convergence guarantees are either coarse or obtained under restrictive analytical settings. In this work, we establish sharper convergence guarantees for the Muon optimizer through a direct and simplified analysis that does not rely on restrictive assumptions on the update rule. Our results improve upon existing bounds by achieving faster convergence rates while covering a broader class of problem settings. These findings provide a more accurate theoretical characterization of Muon and offer insights applicable to a broader class of orthogonalized first-order methods.",
    "published": "2026-01-27T09:32:46Z",
    "updated": "2026-01-27T09:32:46Z",
    "link": "http://arxiv.org/pdf/2601.19400v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Shuntaro Nagashima",
      "Hideaki Iiduka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19395v1",
    "title": "SEAFormer: A Spatial Proximity and Edge-Aware Transformer for Real-World Vehicle Routing Problems",
    "summary": "Real-world Vehicle Routing Problems (RWVRPs) require solving complex, sequence-dependent challenges at scale with constraints such as delivery time window, replenishment or recharging stops, asymmetric travel cost, etc. While recent neural methods achieve strong results on large-scale classical VRP benchmarks, they struggle to address RWVRPs because their strategies overlook sequence dependencies and underutilize edge-level information, which are precisely the characteristics that define the complexity of RWVRPs. We present SEAFormer, a novel transformer that incorporates both node-level and edge-level information in decision-making through two key innovations. First, our Clustered Proximity Attention (CPA) exploits locality-aware clustering to reduce the complexity of attention from $O(n^2)$ to $O(n)$ while preserving global perspective, allowing SEAFormer to efficiently train on large instances. Second, our lightweight edge-aware module captures pairwise features through residual fusion, enabling effective incorporation of edge-based information and faster convergence. Extensive experiments across four RWVRP variants with various scales demonstrate that SEAFormer achieves superior results over state-of-the-art methods. Notably, SEAFormer is the first neural method to solve 1,000+ node RWVRPs effectively, while also achieving superior performance on classic VRPs, making it a versatile solution for both research benchmarks and real-world applications.",
    "published": "2026-01-27T09:27:02Z",
    "updated": "2026-01-27T09:27:02Z",
    "link": "http://arxiv.org/pdf/2601.19395v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Saeed Nasehi Basharzad",
      "Farhana Choudhury",
      "Egemen Tanin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19394v1",
    "title": "DSP-Reg: Domain-Sensitive Parameter Regularization for Robust Domain Generalization",
    "summary": "Domain Generalization (DG) is a critical area that focuses on developing models capable of performing well on data from unseen distributions, which is essential for real-world applications. Existing approaches primarily concentrate on learning domain-invariant features, which assume that a model robust to variations in the source domains will generalize well to unseen target domains. However, these approaches neglect a deeper analysis at the parameter level, which makes the model hard to explicitly differentiate between parameters sensitive to domain shifts and those robust, potentially hindering its overall ability to generalize. In order to address these limitations, we first build a covariance-based parameter sensitivity analysis framework to quantify the sensitivity of each parameter in a model to domain shifts. By computing the covariance of parameter gradients across multiple source domains, we can identify parameters that are more susceptible to domain variations, which serves as our theoretical foundation. Based on this, we propose Domain-Sensitive Parameter Regularization (DSP-Reg), a principled framework that guides model optimization by a soft regularization technique that encourages the model to rely more on domain-invariant parameters while suppressing those that are domain-specific. This approach provides a more granular control over the model's learning process, leading to improved robustness and generalization to unseen domains. Extensive experiments on benchmarks, such as PACS, VLCS, OfficeHome, and DomainNet, demonstrate that DSP-Reg outperforms state-of-the-art approaches, achieving an average accuracy of 66.7\\% and surpassing all baselines.",
    "published": "2026-01-27T09:24:51Z",
    "updated": "2026-01-27T09:24:51Z",
    "link": "http://arxiv.org/pdf/2601.19394v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xudong Han",
      "Senkang Hu",
      "Yihang Tao",
      "Yu Guo",
      "Philip Birch",
      "Sam Tak Wu Kwong",
      "Yuguang Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19383v1",
    "title": "High-quality data augmentation for code comment classification",
    "summary": "Code comments serve a crucial role in software development for documenting functionality, clarifying design choices, and assisting with issue tracking. They capture developers' insights about the surrounding source code, serving as an essential resource for both human comprehension and automated analysis. Nevertheless, since comments are in natural language, they present challenges for machine-based code understanding. To address this, recent studies have applied natural language processing (NLP) and deep learning techniques to classify comments according to developers' intentions. However, existing datasets for this task suffer from size limitations and class imbalance, as they rely on manual annotations and may not accurately represent the distribution of comments in real-world codebases. To overcome this issue, we introduce new synthetic oversampling and augmentation techniques based on high-quality data generation to enhance the NLBSE'26 challenge datasets. Our Synthetic Quality Oversampling Technique and Augmentation Technique (Q-SYNTH) yield promising results, improving the base classifier by $2.56\\%$.",
    "published": "2026-01-27T09:14:56Z",
    "updated": "2026-01-27T09:14:56Z",
    "link": "http://arxiv.org/pdf/2601.19383v1.pdf",
    "category": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "Thomas Borsani",
      "Andrea Rosani",
      "Giuseppe Di Fatta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.14485v5",
    "title": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally Constrained Predictions",
    "summary": "Artificial Neural Networks (ANNs), including fully-connected networks and transformers, are highly flexible and powerful function approximators, widely applied in fields like computer vision and natural language processing. However, their inability to inherently respect causal structures can limit their robustness, making them vulnerable to covariate shift and difficult to interpret/explain. This poses significant challenges for their reliability in real-world applications. In this paper, we introduce Causal Transformers (CaTs), a general model class designed to operate under predefined causal constraints, as specified by a Directed Acyclic Graph (DAG). CaTs retain the powerful function approximation abilities of traditional neural networks while adhering to the underlying structural constraints, improving robustness, reliability, and interpretability at inference time. This approach opens new avenues for deploying neural networks in more demanding, real-world scenarios where robustness and explainability is critical.",
    "published": "2024-10-18T14:10:16Z",
    "updated": "2026-01-27T09:11:54Z",
    "link": "http://arxiv.org/pdf/2410.14485v5.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Matthew J. Vowels",
      "Mathieu Rochat",
      "Sina Akbari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19379v1",
    "title": "Optimal Asynchronous Stochastic Nonconvex Optimization under Heavy-Tailed Noise",
    "summary": "This paper considers the problem of asynchronous stochastic nonconvex optimization with heavy-tailed gradient noise and arbitrarily heterogeneous computation times across workers. We propose an asynchronous normalized stochastic gradient descent algorithm with momentum. The analysis show that our method achieves the optimal time complexity under the assumption of bounded $p$th-order central moment with $p\\in(1,2]$. We also provide numerical experiments to show the effectiveness of proposed method.",
    "published": "2026-01-27T09:04:35Z",
    "updated": "2026-01-27T09:04:35Z",
    "link": "http://arxiv.org/pdf/2601.19379v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Yidong Wu",
      "Luo Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.00349v2",
    "title": "Convergence of energy-based learning in linear resistive networks",
    "summary": "Energy-based learning algorithms are alternatives to backpropagation and are well-suited to distributed implementations in analog electronic devices. However, a rigorous theory of convergence is lacking. We make a first step in this direction by analysing a particular energybased learning algorithm, Contrastive Learning, applied to a network of linear adjustable resistors. It is shown that, in this setup, Contrastive Learning is equivalent to projected gradient descent on a convex function with Lipschitz continuous gradient, giving a guarantee of convergence of the algorithm for a range of stepsizes. This convergence result is then extended to a stochastic variant of Contrastive Learning.",
    "published": "2025-03-01T04:47:02Z",
    "updated": "2026-01-27T08:59:09Z",
    "link": "http://arxiv.org/pdf/2503.00349v2.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "cs.NE",
      "eess.SY"
    ],
    "authors": [
      "Anne-Men Huijzer",
      "Thomas Chaffey",
      "Bart Besselink",
      "Henk J. van Waarde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19367v1",
    "title": "CHEHAB RL: Learning to Optimize Fully Homomorphic Encryption Computations",
    "summary": "Fully Homomorphic Encryption (FHE) enables computations directly on encrypted data, but its high computational cost remains a significant barrier. Writing efficient FHE code is a complex task requiring cryptographic expertise, and finding the optimal sequence of program transformations is often intractable. In this paper, we propose CHEHAB RL, a novel framework that leverages deep reinforcement learning (RL) to automate FHE code optimization. Instead of relying on predefined heuristics or combinatorial search, our method trains an RL agent to learn an effective policy for applying a sequence of rewriting rules to automatically vectorize scalar FHE code while reducing instruction latency and noise growth. The proposed approach supports the optimization of both structured and unstructured code. To train the agent, we synthesize a diverse dataset of computations using a large language model (LLM). We integrate our proposed approach into the CHEHAB FHE compiler and evaluate it on a suite of benchmarks, comparing its performance against Coyote, a state-of-the-art vectorizing FHE compiler. The results show that our approach generates code that is $5.3\\times$ faster in execution, accumulates $2.54\\times$ less noise, while the compilation process itself is $27.9\\times$ faster than Coyote (geometric means).",
    "published": "2026-01-27T08:49:09Z",
    "updated": "2026-01-27T08:49:09Z",
    "link": "http://arxiv.org/pdf/2601.19367v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Bilel Sefsaf",
      "Abderraouf Dandani",
      "Abdessamed Seddiki",
      "Arab Mohammed",
      "Eduardo Chielle",
      "Michail Maniatakos",
      "Riyadh Baghdadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.08045v2",
    "title": "Beyond Classical Attention: Quantum Attention for Scalable Computation",
    "summary": "As large language models (LLMs) demonstrate outstanding performance across various tasks, attention-driven models have profoundly transformed the field of machine learning. Since attention computations account for the primary computational overhead in both model inference and training, efficiently computing attention matrices has become one of the core challenges in accelerating large language models. It is well-known that quantum machines possess computational advantages over classical machines, and the role of quantum computing in LLMs remains largely unexplored. In this work, we focus on leveraging the Grover search algorithm to efficiently compute a sparse attention matrix. Through comparisons with classical algorithms, we demonstrate that our method achieves quantum acceleration in polynomial time. Additionally, we observe that the generated quantum attention matrices naturally exhibit low-rank structures, providing further theoretical support for efficient modeling. Moreover, within the specific context of attention matrix computation, we conduct a systematic and detailed analysis of the error and time complexity of the proposed algorithm.",
    "published": "2023-07-16T14:00:42Z",
    "updated": "2026-01-27T08:45:06Z",
    "link": "http://arxiv.org/pdf/2307.08045v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Xuyang Guo",
      "Zhao Song",
      "Xin Yang",
      "Ruizhe Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19352v1",
    "title": "GraphSB: Boosting Imbalanced Node Classification on Graphs through Structural Balance",
    "summary": "Imbalanced node classification is a critical challenge in graph learning, where most existing methods typically utilize Graph Neural Networks (GNNs) to learn node representations. These methods can be broadly categorized into the data-level and the algorithm-level. The former aims to synthesize minority-class nodes to mitigate quantity imbalance, while the latter tries to optimize the learning process to highlight minority classes. However, neither of them addresses the inherently imbalanced graph structure, which is a fundamental factor that incurs majority-class dominance and minority-class assimilation in GNNs. Our theoretical analysis further supports this critical insight. Therefore, we propose GraphSB (Graph Structural Balance), a novel framework that incorporates Structural Balance as a key strategy to address the underlying imbalanced graph structure before node synthesis. Structural Balance performs a two-stage structure optimization: Structure Enhancement that mines hard samples near decision boundaries through dual-view analysis and enhances connectivity for minority classes through adaptive augmentation, and Relation Diffusion that propagates the enhanced minority context while simultaneously capturing higher-order structural dependencies. Thus, GraphSB balances structural distribution before node synthesis, enabling more effective learning in GNNs. Extensive experiments demonstrate that GraphSB significantly outperforms the state-of-the-art methods. More importantly, the proposed Structural Balance can be seamlessly integrated into state-of-the-art methods as a simple plug-and-play module, increasing their accuracy by an average of 4.57%.",
    "published": "2026-01-27T08:34:06Z",
    "updated": "2026-01-27T08:34:06Z",
    "link": "http://arxiv.org/pdf/2601.19352v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhixiao Wang",
      "Chaofan Zhu",
      "Qihan Feng",
      "Jian Zhang",
      "Xiaobin Rui",
      "Philip S Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.04775v3",
    "title": "Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards",
    "summary": "We study stochastic linear bandits with heavy-tailed rewards, where the rewards have a finite $(1+ε)$-absolute central moment bounded by $\\upsilon$ for some $ε\\in (0,1]$. We improve both upper and lower bounds on the minimax regret compared to prior work. When $\\upsilon = \\mathcal{O}(1)$, the best prior known regret upper bound is $\\tilde{\\mathcal{O}}(d T^{\\frac{1}{1+ε}})$. While a lower with the same scaling has been given, it relies on a construction using $\\upsilon = \\mathcal{O}(d)$, and adapting the construction to the bounded-moment regime with $\\upsilon = \\mathcal{O}(1)$ yields only a $Ω(d^{\\fracε{1+ε}} T^{\\frac{1}{1+ε}})$ lower bound. This matches the known rate for multi-armed bandits and is generally loose for linear bandits, in particular being $\\sqrt{d}$ below the optimal rate in the finite-variance case ($ε= 1$). We propose a new elimination-based algorithm guided by experimental design, which achieves regret $\\tilde{\\mathcal{O}}(d^{\\frac{1+3ε}{2(1+ε)}} T^{\\frac{1}{1+ε}})$, thus improving the dependence on $d$ for all $ε\\in (0,1)$ and recovering a known optimal result for $ε= 1$. We also establish a lower bound of $Ω(d^{\\frac{2ε}{1+ε}} T^{\\frac{1}{1+ε}})$, which strictly improves upon the multi-armed bandit rate and highlights the hardness of heavy-tailed linear bandit problems. For finite action sets, we derive similarly improved upper and lower bounds for regret. Finally, we provide action set dependent regret upper bounds showing that for some geometries, such as $l_p$-norm balls for $p \\le 1 + ε$, we can further reduce the dependence on $d$, and we can handle infinite-dimensional settings via the kernel trick, in particular establishing new regret bounds for the Matérn kernel that are the first to be sublinear for all $ε\\in (0, 1]$.",
    "published": "2025-06-05T09:07:26Z",
    "updated": "2026-01-27T08:25:16Z",
    "link": "http://arxiv.org/pdf/2506.04775v3.pdf",
    "category": [
      "cs.LG",
      "cs.IT",
      "stat.ML"
    ],
    "authors": [
      "Artin Tajdini",
      "Jonathan Scarlett",
      "Kevin Jamieson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19333v1",
    "title": "Metric $k$-clustering using only Weak Comparison Oracles",
    "summary": "Clustering is a fundamental primitive in unsupervised learning. However, classical algorithms for $k$-clustering (such as $k$-median and $k$-means) assume access to exact pairwise distances -- an unrealistic requirement in many modern applications. We study clustering in the \\emph{Rank-model (R-model)}, where access to distances is entirely replaced by a \\emph{quadruplet oracle} that provides only relative distance comparisons. In practice, such an oracle can represent learned models or human feedback, and is expected to be noisy and entail an access cost.\n  Given a metric space with $n$ input items, we design randomized algorithms that, using only a noisy quadruplet oracle, compute a set of $O(k \\cdot \\mathsf{polylog}(n))$ centers along with a mapping from the input items to the centers such that the clustering cost of the mapping is at most constant times the optimum $k$-clustering cost. Our method achieves a query complexity of $O(n\\cdot k \\cdot \\mathsf{polylog}(n))$ for arbitrary metric spaces and improves to $O((n+k^2) \\cdot \\mathsf{polylog}(n))$ when the underlying metric has bounded doubling dimension. When the metric has bounded doubling dimension we can further improve the approximation from constant to $1+\\varepsilon$, for any arbitrarily small constant $\\varepsilon\\in(0,1)$, while preserving the same asymptotic query complexity. Our framework demonstrates how noisy, low-cost oracles, such as those derived from large language models, can be systematically integrated into scalable clustering algorithms.",
    "published": "2026-01-27T08:17:22Z",
    "updated": "2026-01-27T08:17:22Z",
    "link": "http://arxiv.org/pdf/2601.19333v1.pdf",
    "category": [
      "cs.LG",
      "cs.DS"
    ],
    "authors": [
      "Rahul Raychaudhury",
      "Aryan Esmailpour",
      "Sainyam Galhotra",
      "Stavros Sintos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05059v3",
    "title": "NIMO: a Nonlinear Interpretable MOdel",
    "summary": "Deep learning has achieved remarkable success across many domains, but it has also created a growing demand for interpretability in model predictions. Although many explainable machine learning methods have been proposed, post-hoc explanations lack guaranteed fidelity and are sensitive to hyperparameter choices, highlighting the appeal of inherently interpretable models. For example, linear regression provides clear feature effects through its coefficients. However, such models are often outperformed by more complex neural networks (NNs) that usually lack inherent interpretability. To address this dilemma, we introduce NIMO, a framework that combines inherent interpretability with the expressive power of neural networks. Building on the simple linear regression, NIMO is able to provide flexible and intelligible feature effects. Relevantly, we develop an optimization method based on parameter elimination, that allows for optimizing the NN parameters and linear coefficients effectively and efficiently. By relying on adaptive ridge regression we can easily incorporate sparsity as well. We show empirically that our model can provide faithful and intelligible feature effects while maintaining good predictive performance.",
    "published": "2025-06-05T14:02:55Z",
    "updated": "2026-01-27T08:11:55Z",
    "link": "http://arxiv.org/pdf/2506.05059v3.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Shijian Xu",
      "Marcello Massimo Negri",
      "Volker Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19315v1",
    "title": "Generalizable IoT Traffic Representations for Cross-Network Device Identification",
    "summary": "Machine learning models have demonstrated strong performance in classifying network traffic and identifying Internet-of-Things (IoT) devices, enabling operators to discover and manage IoT assets at scale. However, many existing approaches rely on end-to-end supervised pipelines or task-specific fine-tuning, resulting in traffic representations that are tightly coupled to labeled datasets and deployment environments, which can limit generalizability. In this paper, we study the problem of learning generalizable traffic representations for IoT device identification. We design compact encoder architectures that learn per-flow embeddings from unlabeled IoT traffic and evaluate them using a frozen-encoder protocol with a simple supervised classifier. Our specific contributions are threefold. (1) We develop unsupervised encoder--decoder models that learn compact traffic representations from unlabeled IoT network flows and assess their quality through reconstruction-based analysis. (2) We show that these learned representations can be used effectively for IoT device-type classification using simple, lightweight classifiers trained on frozen embeddings. (3) We provide a systematic benchmarking study against the state-of-the-art pretrained traffic encoders, showing that larger models do not necessarily yield more robust representations for IoT traffic. Using more than 18 million real IoT traffic flows collected across multiple years and deployment environments, we learn traffic representations from unlabeled data and evaluate device-type classification on disjoint labeled subsets, achieving macro F1-scores exceeding 0.9 for device-type classification and demonstrating robustness under cross-environment deployment.",
    "published": "2026-01-27T07:56:31Z",
    "updated": "2026-01-27T07:56:31Z",
    "link": "http://arxiv.org/pdf/2601.19315v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Arunan Sivanathan",
      "David Warren",
      "Deepak Mishra",
      "Sushmita Ruj",
      "Natasha Fernandes",
      "Quan Z. Sheng",
      "Minh Tran",
      "Ben Luo",
      "Daniel Coscia",
      "Gustavo Batista",
      "Hassan Habibi Gharakaheili"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.17109v2",
    "title": "Bridging Training and Merging Through Momentum-Aware Optimization",
    "summary": "Training large neural networks and merging task specific models both exploit low rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry aware model composition. The proposed method incurs modest memory overhead (approximately 30% over AdamW) to accumulate task saliency scores that enable curvature aware merging. These scores, computed as a byproduct of optimization, provide importance estimates comparable to post hoc Fisher computation while producing merge-ready models directly from training. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature aware parameter selection outperforms magnitude only baselines across all sparsity levels, with multi-task merging improving 1.6% over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach demonstrates that training-time curvature information suffices for effective model composition, enabling a unified training merging pipeline.",
    "published": "2025-12-18T22:37:33Z",
    "updated": "2026-01-27T07:56:00Z",
    "link": "http://arxiv.org/pdf/2512.17109v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Alireza Moayedikia",
      "Alicia Troncoso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19312v1",
    "title": "LightSBB-M: Bridging Schrödinger and Bass for Generative Diffusion Modeling",
    "summary": "The Schrodinger Bridge and Bass (SBB) formulation, which jointly controls drift and volatility, is an established extension of the classical Schrodinger Bridge (SB). Building on this framework, we introduce LightSBB-M, an algorithm that computes the optimal SBB transport plan in only a few iterations. The method exploits a dual representation of the SBB objective to obtain analytic expressions for the optimal drift and volatility, and it incorporates a tunable parameter beta greater than zero that interpolates between pure drift (the Schrodinger Bridge) and pure volatility (Bass martingale transport). We show that LightSBB-M achieves the lowest 2-Wasserstein distance on synthetic datasets against state-of-the-art SB and diffusion baselines with up to 32 percent improvement. We also illustrate the generative capability of the framework on an unpaired image-to-image translation task (adult to child faces in FFHQ). These findings demonstrate that LightSBB-M provides a scalable, high-fidelity SBB solver that outperforms existing SB and diffusion baselines across both synthetic and real-world generative tasks. The code is available at https://github.com/alexouadi/LightSBB-M.",
    "published": "2026-01-27T07:50:59Z",
    "updated": "2026-01-27T07:50:59Z",
    "link": "http://arxiv.org/pdf/2601.19312v1.pdf",
    "category": [
      "cs.LG",
      "eess.SY",
      "stat.CO",
      "stat.ML"
    ],
    "authors": [
      "Alexandre Alouadi",
      "Pierre Henry-Labordère",
      "Grégoire Loeper",
      "Othmane Mazhar",
      "Huyên Pham",
      "Nizar Touzi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19300v1",
    "title": "Queue Length Regret Bounds for Contextual Queueing Bandits",
    "summary": "We introduce contextual queueing bandits, a new context-aware framework for scheduling while simultaneously learning unknown service rates. Individual jobs carry heterogeneous contextual features, based on which the agent chooses a job and matches it with a server to maximize the departure rate. The service/departure rate is governed by a logistic model of the contextual feature with an unknown server-specific parameter. To evaluate the performance of a policy, we consider queue length regret, defined as the difference in queue length between the policy and the optimal policy. The main challenge in the analysis is that the lists of remaining job features in the queue may differ under our policy versus the optimal policy for a given time step, since they may process jobs in different orders. To address this, we propose the idea of policy-switching queues equipped with a sophisticated coupling argument. This leads to a novel queue length regret decomposition framework, allowing us to understand the short-term effect of choosing a suboptimal job-server pair and its long-term effect on queue state differences. We show that our algorithm, CQB-$\\varepsilon$, achieves a regret upper bound of $\\widetilde{\\mathcal{O}}(T^{-1/4})$. We also consider the setting of adversarially chosen contexts, for which our second algorithm, CQB-Opt, achieves a regret upper bound of $\\mathcal{O}(\\log^2 T)$. Lastly, we provide experimental results that validate our theoretical findings.",
    "published": "2026-01-27T07:40:23Z",
    "updated": "2026-01-27T07:40:23Z",
    "link": "http://arxiv.org/pdf/2601.19300v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Seoungbin Bae",
      "Garyeong Kang",
      "Dabeen Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19296v1",
    "title": "Process-Aware Procurement Lead Time Prediction for Shipyard Delay Mitigation",
    "summary": "Accurately predicting procurement lead time (PLT) remains a challenge in engineered-to-order industries such as shipbuilding and plant construction, where delays in a single key component can disrupt project timelines. In shipyards, pipe spools are critical components; installed deep within hull blocks soon after steel erection, any delay in their procurement can halt all downstream tasks. Recognizing their importance, existing studies predict PLT using the static physical attributes of pipe spools. However, procurement is inherently a dynamic, multi-stakeholder business process involving a continuous sequence of internal and external events at the shipyard, factors often overlooked in traditional approaches. To address this issue, this paper proposes a novel framework that combines event logs, dataset records of the procurement events, with static attributes to predict PLT. The temporal attributes of each event are extracted to reflect the continuity and temporal context of the process. Subsequently, a deep sequential neural network combined with a multi-layered perceptron is employed to integrate these static and dynamic features, enabling the model to capture both structural and contextual information in procurement. Comparative experiments are conducted using real-world pipe spool procurement data from a globally renowned South Korean shipbuilding corporation. Three tasks are evaluated, which are production, post-processing, and procurement lead time prediction. The results show a 22.6% to 50.4% improvement in prediction performance in terms of mean absolute error over the best-performing existing approaches across the three tasks. These findings indicate the value of considering procurement process information for more accurate PLT prediction.",
    "published": "2026-01-27T07:37:11Z",
    "updated": "2026-01-27T07:37:11Z",
    "link": "http://arxiv.org/pdf/2601.19296v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yongjae Lee",
      "Eunhee Park",
      "Daesan Park",
      "Dongho Kim",
      "Jongho Choi",
      "Hyerim Bae"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.04243v2",
    "title": "Twirlator: A Pipeline for Analyzing Subgroup Symmetry Effects in Quantum Machine Learning Ansatzes",
    "summary": "Symmetry is a strong inductive bias in geometric deep learning and its quantum counterpart, and has attracted increasing attention for improving the trainability of QML models. Yet incorporating symmetries into quantum machine learning (QML) ansatzes is not free: symmetrization often adds gates and constrains the circuits. To understand these effects, we present Twirlator, which is an automated pipeline that symmetrizes parameterized QML ansatzes and quantifies the trade-offs as the amount of symmetry increases. Twirlator models partial symmetries by the size of a subgroup of the symmetric group, enabling analysis between the ``no symmetry'' and ``full symmetry'' extremes. Across 19 common ansatz patterns, Twirlator symmetrizes circuits with respect to any subgroup of $S_n$ and measures (1) generator drift, (2) circuit overhead (depth and size), and (3) expressibility and entangling capability. The experimental evaluation focuses on subgroups of $S_4$ and $S_5$. Twirlator reveals that larger subgroups typically increase circuit overhead, reduce expressibility, and often increase entangling capability. The pipeline and results provide practical guidance for selecting ansatz patterns and symmetry levels that balance hardware cost and model performance in symmetry-aware QML applications.",
    "published": "2025-11-06T10:29:24Z",
    "updated": "2026-01-27T07:35:14Z",
    "link": "http://arxiv.org/pdf/2511.04243v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Valter Uotila",
      "Väinö Mehtola",
      "Ilmo Salmenperä",
      "Bo Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19285v1",
    "title": "Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework",
    "summary": "Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.",
    "published": "2026-01-27T07:16:44Z",
    "updated": "2026-01-27T07:16:44Z",
    "link": "http://arxiv.org/pdf/2601.19285v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xinyu Zhou",
      "Jiawei Zhang",
      "Stephen J. Wright"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19284v1",
    "title": "Output Feedback Stabilization of Linear Systems via Policy Gradient Methods",
    "summary": "Stabilizing a dynamical system is a fundamental problem that serves as a cornerstone for many complex tasks in the field of control systems. The problem becomes challenging when the system model is unknown. Among the Reinforcement Learning (RL) algorithms that have been successfully applied to solve problems pertaining to unknown linear dynamical systems, the policy gradient (PG) method stands out due to its ease of implementation and can solve the problem in a model-free manner. However, most of the existing works on PG methods for unknown linear dynamical systems assume full-state feedback. In this paper, we take a step towards model-free learning for partially observable linear dynamical systems with output feedback and focus on the fundamental stabilization problem of the system. We propose an algorithmic framework that stretches the boundary of PG methods to the problem without global convergence guarantees. We show that by leveraging zeroth-order PG update based on system trajectories and its convergence to stationary points, the proposed algorithms return a stabilizing output feedback policy for discrete-time linear dynamical systems. We also explicitly characterize the sample complexity of our algorithm and verify the effectiveness of the algorithm using numerical examples.",
    "published": "2026-01-27T07:15:59Z",
    "updated": "2026-01-27T07:15:59Z",
    "link": "http://arxiv.org/pdf/2601.19284v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Ankang Zhang",
      "Ming Chi",
      "Xiaoling Wang",
      "Lintao Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19264v1",
    "title": "Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code",
    "summary": "Large language models (LLMs) have made it remarkably easy to synthesize plausible source code from natural language prompts. While this accelerates software development and supports learning, it also raises new risks for academic integrity, authorship attribution, and responsible AI use. This paper investigates the problem of distinguishing human-written from machine-generated code by comparing two complementary approaches: feature-based detectors built from lightweight, interpretable stylometric and structural properties of code, and embedding-based detectors leveraging pretrained code encoders. Using a recent large-scale benchmark dataset of 600k human-written and AI-generated code samples, we find that feature-based models achieve strong performance (ROC-AUC 0.995, PR-AUC 0.995, F1 0.971), while embedding-based models with CodeBERT embeddings are also very competitive (ROC-AUC 0.994, PR-AUC 0.994, F1 0.965). Analysis shows that features tied to indentation and whitespace provide particularly discriminative cues, whereas embeddings capture deeper semantic patterns and yield slightly higher precision. These findings underscore the trade-offs between interpretability and generalization, offering practical guidance for deploying robust code-origin detection in academic and industrial contexts.",
    "published": "2026-01-27T06:43:51Z",
    "updated": "2026-01-27T06:43:51Z",
    "link": "http://arxiv.org/pdf/2601.19264v1.pdf",
    "category": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "Syed Mehedi Hasan Nirob",
      "Shamim Ehsan",
      "Moqsadur Rahman",
      "Summit Haque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19256v1",
    "title": "E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification",
    "summary": "Covariate-dependent uncertainty quantification in simulation-based inference is crucial for high-stakes decision-making but remains challenging due to the limitations of existing methods such as conformal prediction and classical bootstrap, which struggle with covariate-specific conditioning. We propose Efficient Quantile-Regression-Based Generative Metamodeling (E-QRGMM), a novel framework that accelerates the quantile-regression-based generative metamodeling (QRGMM) approach by integrating cubic Hermite interpolation with gradient estimation. Theoretically, we show that E-QRGMM preserves the convergence rate of the original QRGMM while reducing grid complexity from $O(n^{1/2})$ to $O(n^{1/5})$ for the majority of quantile levels, thereby substantially improving computational efficiency. Empirically, E-QRGMM achieves a superior trade-off between distributional accuracy and training speed compared to both QRGMM and other advanced deep generative models on synthetic and practical datasets. Moreover, by enabling bootstrap-based construction of confidence intervals for arbitrary estimands of interest, E-QRGMM provides a practical solution for covariate-dependent uncertainty quantification.",
    "published": "2026-01-27T06:39:24Z",
    "updated": "2026-01-27T06:39:24Z",
    "link": "http://arxiv.org/pdf/2601.19256v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Zhiyang Liang",
      "Qingkai Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19243v1",
    "title": "Contrast-Source-Based Physics-Driven Neural Network for Inverse Scattering Problems",
    "summary": "Deep neural networks (DNNs) have recently been applied to inverse scattering problems (ISPs) due to their strong nonlinear mapping capabilities. However, supervised DNN solvers require large-scale datasets, which limits their generalization in practical applications. Untrained neural networks (UNNs) address this issue by updating weights from measured electric fields and prior physical knowledge, but existing UNN solvers suffer from long inference time. To overcome these limitations, this paper proposes a contrast-source-based physics-driven neural network (CSPDNN), which predicts the induced current distribution to improve efficiency and incorporates an adaptive total variation loss for robust reconstruction under varying contrast and noise conditions. The improved imaging performance is validated through comprehensive numerical simulations and experimental data.",
    "published": "2026-01-27T06:27:56Z",
    "updated": "2026-01-27T06:27:56Z",
    "link": "http://arxiv.org/pdf/2601.19243v1.pdf",
    "category": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Yutong Du",
      "Zicheng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06374v2",
    "title": "Adaptive Regularization for Large-Scale Sparse Feature Embedding Models",
    "summary": "The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, the fundamental cause of this phenomenon remains unclear. In this work, we present a theoretical explanation grounded in Rademacher complexity, supported by empirical experiments, to explain why overfitting occurs in models with large-scale sparse categorical features. Based on this analysis, we propose a regularization method that constrains the norm budget of embedding layers adaptively. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.",
    "published": "2025-11-09T13:24:14Z",
    "updated": "2026-01-27T06:07:34Z",
    "link": "http://arxiv.org/pdf/2511.06374v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Mang Li",
      "Wei Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19220v1",
    "title": "Accelerated Multiple Wasserstein Gradient Flows for Multi-objective Distributional Optimization",
    "summary": "We study multi-objective optimization over probability distributions in Wasserstein space. Recently, Nguyen et al. (2025) introduced Multiple Wasserstein Gradient Descent (MWGraD) algorithm, which exploits the geometric structure of Wasserstein space to jointly optimize multiple objectives. Building on this approach, we propose an accelerated variant, A-MWGraD, inspired by Nesterov's acceleration. We analyze the continuous-time dynamics and establish convergence to weakly Pareto optimal points in probability space. Our theoretical results show that A-MWGraD achieves a convergence rate of O(1/t^2) for geodesically convex objectives and O(e^{-\\sqrtβt}) for $β$-strongly geodesically convex objectives, improving upon the O(1/t) rate of MWGraD in the geodesically convex setting. We further introduce a practical kernel-based discretization for A-MWGraD and demonstrate through numerical experiments that it consistently outperforms MWGraD in convergence speed and sampling efficiency on multi-target sampling tasks.",
    "published": "2026-01-27T05:41:36Z",
    "updated": "2026-01-27T05:41:36Z",
    "link": "http://arxiv.org/pdf/2601.19220v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Dai Hai Nguyen",
      "Duc Dung Nguyen",
      "Atsuyoshi Nakamura",
      "Hiroshi Mamitsuka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12751v2",
    "title": "Single Index Bandits: Generalized Linear Contextual Bandits with Unknown Reward Functions",
    "summary": "Generalized linear bandits have been extensively studied due to their broad applicability in real-world online decision-making problems. However, these methods typically assume that the expected reward function is known to the users, an assumption that is often unrealistic in practice. Misspecification of this link function can lead to the failure of all existing algorithms. In this work, we address this critical limitation by introducing a new problem of generalized linear bandits with unknown reward functions, also known as single index bandits. We first consider the case where the unknown reward function is monotonically increasing, and propose two novel and efficient algorithms, STOR and ESTOR, that achieve decent regrets under standard assumptions. Notably, our ESTOR can obtain the nearly optimal regret bound $\\tilde{O}_T(\\sqrt{T})$ in terms of the time horizon $T$. We then extend our methods to the high-dimensional sparse setting and show that the same regret rate can be attained with the sparsity index. Next, we introduce GSTOR, an algorithm that is agnostic to general reward functions, and establish regret bounds under a Gaussian design assumption. Finally, we validate the efficiency and effectiveness of our algorithms through experiments on both synthetic and real-world datasets.",
    "published": "2025-06-15T07:19:00Z",
    "updated": "2026-01-27T05:29:10Z",
    "link": "http://arxiv.org/pdf/2506.12751v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Yue Kang",
      "Mingshuo Liu",
      "Bongsoo Yi",
      "Jing Lyu",
      "Zhi Zhang",
      "Doudou Zhou",
      "Yao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06297v2",
    "title": "LoaQ: Layer-wise Output Approximation Quantization",
    "summary": "A natural and intuitive idea in model quantization is to approximate each component's quantized output to match its original. Motivated by this idea, most layer-wise post-training quantization (PTQ) methods focus on weight approximation at the linear-layer level. As a result, this local objective often yields insufficient approximations and practical deviations from the guiding intuition. Recent work has improved the approximation of linear-layer outputs within the layer-wise PTQ framework, but such refinements remain inadequate for achieving alignment with the full-model output. Based on a deeper understanding of the structure of mainstream LLMs, we propose LoaQ, which incorporates output-matching factors when quantizing linear layers within the layer-wise PTQ framework. It better aligns with this intuition and can feature a simple closed-form solution, making it orthogonal to existing techniques and readily integrable into existing quantization pipelines. Experiments on the LLaMA and Qwen model families demonstrate that LoaQ performs effectively in both weight-only and weight-activation quantization. By integrating seamlessly with existing quantization strategies, it further enhances overall quantization quality and shows strong potential to advance the frontier of post-training quantization.",
    "published": "2025-09-08T02:50:11Z",
    "updated": "2026-01-27T05:28:26Z",
    "link": "http://arxiv.org/pdf/2509.06297v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Li Lin",
      "Xiaojun Wan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19776v1",
    "title": "Subjective Evaluation of Frame Rate in Bitrate-Constrained Live Streaming",
    "summary": "Bandwidth constraints in live streaming require video codecs to balance compression strength and frame rate, yet the perceptual consequences of this trade-off remain underexplored. We present the high frame rate live streaming (HFR-LS) dataset, comprising 384 subject-rated 1080p videos encoded at multiple target bitrates by systematically varying compression strength and frame rate. A single-stimulus, hidden-reference subjective study shows that frame rate has a noticeable effect on perceived quality, and interacts with both bitrate and source content. The HFR-LS dataset is available at https://github.com/real-hjq/HFR-LS to facilitate research on bitrate-constrained live streaming.",
    "published": "2026-01-27T16:40:38Z",
    "updated": "2026-01-27T16:40:38Z",
    "link": "http://arxiv.org/pdf/2601.19776v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Jiaqi He",
      "Zhengfang Duanmu",
      "Kede Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19712v1",
    "title": "Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling",
    "summary": "Spatial audio is essential for immersive experiences, yet novel-view acoustic synthesis (NVAS) remains challenging due to complex physical phenomena such as reflection, diffraction, and material absorption. Existing methods based on single-view or panoramic inputs improve spatial fidelity but fail to capture global geometry and semantic cues such as object layout and material properties. To address this, we propose Phys-NVAS, the first physics-aware NVAS framework that integrates spatial geometry modeling with vision-language semantic priors. A global 3D acoustic environment is reconstructed from multi-view images and depth maps to estimate room size and shape, enhancing spatial awareness of sound propagation. Meanwhile, a vision-language model extracts physics-aware priors of objects, layouts, and materials, capturing absorption and reflection beyond geometry. An acoustic feature fusion adapter unifies these cues into a physics-aware representation for binaural generation. Experiments on RWAVS demonstrate that Phys-NVAS yields binaural audio with improved realism and physical consistency.",
    "published": "2026-01-27T15:36:04Z",
    "updated": "2026-01-27T15:36:04Z",
    "link": "http://arxiv.org/pdf/2601.19712v1.pdf",
    "category": [
      "cs.SD",
      "cs.MM"
    ],
    "authors": [
      "Congyi Fan",
      "Jian Guan",
      "Youtian Lin",
      "Dongli Xu",
      "Tong Ye",
      "Qiaoxi Zhu",
      "Pengming Feng",
      "Wenwu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19634v1",
    "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
    "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.",
    "published": "2026-01-27T14:10:39Z",
    "updated": "2026-01-27T14:10:39Z",
    "link": "http://arxiv.org/pdf/2601.19634v1.pdf",
    "category": [
      "cs.RO",
      "cs.MM"
    ],
    "authors": [
      "Wenda Yu",
      "Tianshi Wang",
      "Fengling Li",
      "Jingjing Li",
      "Lei Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19303v1",
    "title": "A Collaborative Extended Reality Prototype for 3D Surgical Planning and Visualization",
    "summary": "We present a collaborative extended reality (XR) prototype for 3D surgical planning and visualization. Our system consists of three key modules: XR-based immersive surgical planning, cloud-based data management, and coordinated stereoscopic 3D displays for interactive visualization. We describe the overall workflow, core functionalities, implementations and setups. By conducting user studies on a liver resection surgical planning case, we demonstrate the effectiveness of our prototype and provide practical insights to inspire future advances in medical XR collaboration.",
    "published": "2026-01-27T07:42:51Z",
    "updated": "2026-01-27T07:42:51Z",
    "link": "http://arxiv.org/pdf/2601.19303v1.pdf",
    "category": [
      "cs.HC",
      "cs.GR",
      "cs.MM"
    ],
    "authors": [
      "Shi Qiu",
      "Ruiyang Li",
      "Qixuan Liu",
      "Yuqi Tong",
      "Yue Qiu",
      "Yinqiao Wang",
      "Yan Li",
      "Chi-Wing Fu",
      "Pheng-Ann Heng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19843v1",
    "title": "Graphical X Splatting (GraphiXS): A Graphical Model for 4D Gaussian Splatting under Uncertainty",
    "summary": "We propose a new framework to systematically incorporate data uncertainty in Gaussian Splatting. Being the new paradigm of neural rendering, Gaussian Splatting has been investigated in many applications, with the main effort in extending its representation, improving its optimization process, and accelerating its speed. However, one orthogonal, much needed, but under-explored area is data uncertainty. In standard 4D Gaussian Splatting, data uncertainty can manifest as view sparsity, missing frames, camera asynchronization, etc. So far, there has been little research to holistically incorporating various types of data uncertainty under a single framework. To this end, we propose Graphical X Splatting, or GraphiXS, a new probabilistic framework that considers multiple types of data uncertainty, aiming for a fundamental augmentation of the current 4D Gaussian Splatting paradigm into a probabilistic setting. GraphiXS is general and can be instantiated with a range of primitives, e.g. Gaussians, Student's-t. Furthermore, GraphiXS can be used to `upgrade' existing methods to accommodate data uncertainty. Through exhaustive evaluation and comparison, we demonstrate that GraphiXS can systematically model various uncertainties in data, outperform existing methods in many settings where data are missing or polluted in space and time, and therefore is a major generalization of the current 4D Gaussian Splatting research.",
    "published": "2026-01-27T17:50:07Z",
    "updated": "2026-01-27T17:50:07Z",
    "link": "http://arxiv.org/pdf/2601.19843v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Doga Yilmaz",
      "Jialin Zhu",
      "Deshan Gong",
      "He Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19425v1",
    "title": "It's Not Just a Phase: Creating Phase-Aligned Peripheral Metamers",
    "summary": "Novel display technologies can deliver high-quality images across a wide field of view, creating immersive experiences. While rendering for such devices is expensive, most of the content falls into peripheral vision, where human perception differs from that in the fovea. Consequently, it is critical to understand and leverage the limitations of visual perception to enable efficient rendering. A standard approach is to exploit the reduced sensitivity to spatial details in the periphery by reducing rendering resolution, so-called foveated rendering. While this strategy avoids rendering part of the content altogether, an alternative promising direction is to replace accurate and expensive rendering with inexpensive synthesis of content that is perceptually indistinguishable from the ground-truth image. In this paper, we propose such a method for the efficient generation of an image signal that substitutes the rendering of high-frequency details. The method is grounded in findings from image statistics, which show that preserving appropriate local statistics is critical for perceived image quality. Based on this insight, we extrapolate several local image statistics from foveated content into higher spatial frequency ranges that are attenuated or omitted in the rendering process. This rich set of statistics is later used to synthesize a signal that is added to the initial rendering, boosting its perceived quality. We focus on phase information, demonstrating the importance of its alignment across space and frequencies. We calibrate and compare our method with state-of-the-art strategies, showing a significant reduction in the content that must be accurately rendered at a relatively small extra cost for synthesizing the additional signal.",
    "published": "2026-01-27T10:03:45Z",
    "updated": "2026-01-27T10:03:45Z",
    "link": "http://arxiv.org/pdf/2601.19425v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Sophie Kergaßner",
      "Piotr Didyk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18867v3",
    "title": "Efficient B-Spline Finite Elements for Cloth Simulation",
    "summary": "We present an efficient B-spline finite element method (FEM) for cloth simulation. While higher-order FEM has long promised higher accuracy, its adoption in cloth simulators has been limited by its larger computational costs while generating results with similar visual quality. Our contribution is a full algorithmic pipeline that makes cloth simulation using quadratic B-spline surfaces faster than standard linear FEM in practice while consistently improving accuracy and visual fidelity. Using quadratic B-spline basis functions, we obtain a globally $C^1$-continuous displacement field that supports consistent discretization of both membrane and bending energies, effectively reducing locking artifacts and mesh dependence common to linear elements. To close the performance gap, we introduce a reduced integration scheme that separately optimizes quadrature rules for membrane and bending energies, an accelerated Hessian assembly procedure tailored to the spline structure, and an optimized linear solver based on partial factorization. Together, these optimizations make high-order, smooth cloth simulation competitive at scale, yielding an average $2\\times$ speedup over highly-optimized linear FEM in our tests. Extensive experiments demonstrate improved accuracy, wrinkle detail, and robustness, including contact-rich scenarios, relative to linear FEM and recent higher-order approaches. Our method enables realistic wrinkling dynamics across a wide range of material parameters and supports practical garment animation, providing a new promising spatial discretization for high-quality cloth simulation.",
    "published": "2025-06-23T17:36:11Z",
    "updated": "2026-01-27T09:07:23Z",
    "link": "http://arxiv.org/pdf/2506.18867v3.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Yuqi Meng",
      "Yihao Shi",
      "Kemeng Huang",
      "Zixuan Lu",
      "Ning Guo",
      "Taku Komura",
      "Yin Yang",
      "Minchen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19310v1",
    "title": "ClipGS-VR: Immersive and Interactive Cinematic Visualization of Volumetric Medical Data in Mobile Virtual Reality",
    "summary": "High-fidelity cinematic medical visualization on mobile virtual reality (VR) remains challenging. Although ClipGS enables cross-sectional exploration via 3D Gaussian Splatting, it lacks arbitrary-angle slicing on consumer-grade VR headsets. To achieve real-time interactive performance, we introduce ClipGS-VR and restructure ClipGS's neural inference into a consolidated dataset, integrating high-fidelity layers from multiple pre-computed slicing states into a unified rendering structure. Our framework further supports arbitrary-angle slicing via gradient-based opacity modulation for smooth, visually coherent rendering. Evaluations confirm our approach maintains visual fidelity comparable to offline results while offering superior usability and interaction efficiency.",
    "published": "2026-01-27T07:48:59Z",
    "updated": "2026-01-27T07:48:59Z",
    "link": "http://arxiv.org/pdf/2601.19310v1.pdf",
    "category": [
      "cs.GR",
      "cs.HC"
    ],
    "authors": [
      "Yuqi Tong",
      "Ruiyang Li",
      "Chengkun Li",
      "Qixuan Liu",
      "Shi Qiu",
      "Pheng-Ann Heng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19294v1",
    "title": "Words have Weight: Comparing the use of pressure and weight as a metaphor in a User Interface in Virtual Reality",
    "summary": "This work investigates how weight and pressure can function as haptic metaphors to support user interface notifications in Virtual Reality (VR). While prior research has explored ungrounded weight simulation and pneumatic feedback, their combined role in conveying information through UI elements remains underexplored. We developed a wearable haptic device that transfers liquid and air into flexible containers mounted on the back of the user's hand, allowing us to independently manipulate weight and pressure. Through an initial evaluation using three conditions-no feedback, weight only, and weight combined with pressure-we examined how these signals affect perceived heaviness, coherence with visual cues, and the perceived urgency of notifications. Our results validate that pressure amplifies the perception of weight, but this increased heaviness does not translate into higher perceived urgency. These findings suggest that while pressure___enhanced weight can enrich haptic rendering of UI elements in VR, its contribution to communicating urgency may require further investigation, alternative pressure profiles, or different types of notifications.",
    "published": "2026-01-27T07:36:37Z",
    "updated": "2026-01-27T07:36:37Z",
    "link": "http://arxiv.org/pdf/2601.19294v1.pdf",
    "category": [
      "cs.GR",
      "cs.HC"
    ],
    "authors": [
      "Joffrey Guilmet",
      "Suzanne Sorli",
      "Diego Vilela Monteiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19233v1",
    "title": "UniMGS: Unifying Mesh and 3D Gaussian Splatting with Single-Pass Rasterization and Proxy-Based Deformation",
    "summary": "Joint rendering and deformation of mesh and 3D Gaussian Splatting (3DGS) have significant value as both representa tions offer complementary advantages for graphics applica tions. However, due to differences in representation and ren dering pipelines, existing studies render meshes and 3DGS separately, making it difficult to accurately handle occlusions and transparency. Moreover, the deformed 3DGS still suffers from visual artifacts due to the sensitivity to the topology quality of the proxy mesh. These issues pose serious obsta cles to the joint use of 3DGS and meshes, making it diffi cult to adapt 3DGS to conventional mesh-oriented graphics pipelines. We propose UniMGS, the first unified framework for rasterizing mesh and 3DGS in a single-pass anti-aliased manner, with a novel binding strategy for 3DGS deformation based on proxy mesh. Our key insight is to blend the col ors of both triangle and Gaussian fragments by anti-aliased α-blending in a single pass, achieving visually coherent re sults with precise handling of occlusion and transparency. To improve the visual appearance of the deformed 3DGS, our Gaussian-centric binding strategy employs a proxy mesh and spatially associates Gaussians with the mesh faces, signifi cantly reducing rendering artifacts. With these two compo nents, UniMGS enables the visualization and manipulation of 3D objects represented by mesh or 3DGS within a unified framework, opening up new possibilities in embodied AI, vir tual reality, and gaming. We will release our source code to facilitate future research.",
    "published": "2026-01-27T06:05:14Z",
    "updated": "2026-01-27T06:05:14Z",
    "link": "http://arxiv.org/pdf/2601.19233v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Zeyu Xiao",
      "Mingyang Sun",
      "Yimin Cong",
      "Lintao Wang",
      "Dongliang Kou",
      "Zhenyi Wu",
      "Dingkang Yang",
      "Peng Zhai",
      "Zeyu Wang",
      "Lihua Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19856v1",
    "title": "Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability",
    "summary": "Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\\% accuracy, with the Voting Classifier achieving 84.07\\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.",
    "published": "2026-01-27T18:12:44Z",
    "updated": "2026-01-27T18:12:44Z",
    "link": "http://arxiv.org/pdf/2601.19856v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Giulio Campagna",
      "Marta Lagomarsino",
      "Marta Lorenzini",
      "Dimitrios Chrysostomou",
      "Matthias Rehm",
      "Arash Ajoudani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19851v1",
    "title": "How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People",
    "summary": "Autonomy and independent navigation are vital to daily life but remain challenging for individuals with blindness. Robotic systems can enhance mobility and confidence by providing intelligent navigation assistance. However, fully autonomous systems may reduce users' sense of control, even when they wish to remain actively involved. Although collaboration between user and robot has been recognized as important, little is known about how perceptions of this relationship change with repeated use. We present a repeated exposure study with six blind participants who interacted with a navigation-assistive robot in a real-world museum. Participants completed tasks such as navigating crowds, approaching lines, and encountering obstacles. Findings show that participants refined their strategies over time, developing clearer preferences about when to rely on the robot versus act independently. This work provides insights into how strategies and preferences evolve with repeated interaction and offers design implications for robots that adapt to user needs over time.",
    "published": "2026-01-27T18:00:00Z",
    "updated": "2026-01-27T18:00:00Z",
    "link": "http://arxiv.org/pdf/2601.19851v1.pdf",
    "category": [
      "cs.HC",
      "cs.RO"
    ],
    "authors": [
      "Rayna Hata",
      "Masaki Kuribayashi",
      "Allan Wang",
      "Hironobu Takagi",
      "Chieko Asakawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19832v1",
    "title": "Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation",
    "summary": "Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.",
    "published": "2026-01-27T17:38:58Z",
    "updated": "2026-01-27T17:38:58Z",
    "link": "http://arxiv.org/pdf/2601.19832v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Elena Merlo",
      "Marta Lagomarsino",
      "Arash Ajoudani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19826v1",
    "title": "Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse",
    "summary": "As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.",
    "published": "2026-01-27T17:34:31Z",
    "updated": "2026-01-27T17:34:31Z",
    "link": "http://arxiv.org/pdf/2601.19826v1.pdf",
    "category": [
      "cs.RO",
      "cs.HC"
    ],
    "authors": [
      "Fan Yang",
      "Renkai Ma",
      "Yaxin Hu",
      "Lingyao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19761v1",
    "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
    "summary": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.",
    "published": "2026-01-27T16:25:56Z",
    "updated": "2026-01-27T16:25:56Z",
    "link": "http://arxiv.org/pdf/2601.19761v1.pdf",
    "category": [
      "cs.RO",
      "cs.IR"
    ],
    "authors": [
      "Jin Huang",
      "Fethiye Irmak Doğan",
      "Hatice Gunes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19742v1",
    "title": "SCOPE: Smooth Convex Optimization for Planned Evolution of Deformable Linear Objects",
    "summary": "We present SCOPE, a fast and efficient framework for modeling and manipulating deformable linear objects (DLOs). Unlike conventional energy-based approaches, SCOPE leverages convex approximations to significantly reduce computational cost while maintaining smooth and physically plausible deformations. This trade-off between speed and accuracy makes the method particularly suitable for applications requiring real-time or near-real-time response. The effectiveness of the proposed framework is demonstrated through comprehensive simulation experiments, highlighting its ability to generate smooth shape trajectories under geometric and length constraints.",
    "published": "2026-01-27T16:01:56Z",
    "updated": "2026-01-27T16:01:56Z",
    "link": "http://arxiv.org/pdf/2601.19742v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Ali Jnadi",
      "Hadi Salloum",
      "Yaroslav Kholodov",
      "Alexander Gasnikov",
      "Karam Almaghout"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19643v1",
    "title": "Enhancing Worker Safety in Harbors Using Quadruped Robots",
    "summary": "Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers' safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.",
    "published": "2026-01-27T14:19:55Z",
    "updated": "2026-01-27T14:19:55Z",
    "link": "http://arxiv.org/pdf/2601.19643v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zoe Betta",
      "Davide Corongiu",
      "Carmine Tommaso Recchiuto",
      "Antonio Sgorbissa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.04375v2",
    "title": "Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories",
    "summary": "Effectively capturing the joint distribution of all agents in a scene is relevant for predicting the true evolution of the scene and in turn providing more accurate information to the decision processes of autonomous vehicles. While new models have been developed for this purpose in recent years, it remains unclear how to best represent the joint distributions particularly from the perspective of the interactions between agents. Thus far there is no clear consensus on how best to represent interactions between agents; whether they should be learned implicitly from data by neural networks, or explicitly modeled using the spatial and temporal relations that are more grounded in human decision-making. This paper aims to study various means of describing interactions within the same network structure and their effect on the final learned joint distributions. Our findings show that more often than not, simply allowing a network to establish interactive connections between agents based on data has a detrimental effect on performance. Instead, having well defined interactions (such as which agent of an agent pair passes first at an intersection) can often bring about a clear boost in performance.",
    "published": "2025-11-06T14:01:47Z",
    "updated": "2026-01-27T14:02:56Z",
    "link": "http://arxiv.org/pdf/2511.04375v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Anna Mészáros",
      "Javier Alonso-Mora",
      "Jens Kober"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18639v2",
    "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
    "summary": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.",
    "published": "2026-01-26T16:11:05Z",
    "updated": "2026-01-27T13:24:25Z",
    "link": "http://arxiv.org/pdf/2601.18639v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ojasva Mishra",
      "Xiaolong Wu",
      "Min Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19536v1",
    "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
    "summary": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.",
    "published": "2026-01-27T12:27:27Z",
    "updated": "2026-01-27T12:27:27Z",
    "link": "http://arxiv.org/pdf/2601.19536v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hongji Liu",
      "Linwei Zheng",
      "Yongjian Li",
      "Mingkai Tang",
      "Xiaoyang Yan",
      "Ming Liu",
      "Jun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19529v1",
    "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
    "summary": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module's stable reconfiguration ability, as well as its positional and docking accuracy.",
    "published": "2026-01-27T12:15:10Z",
    "updated": "2026-01-27T12:15:10Z",
    "link": "http://arxiv.org/pdf/2601.19529v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jie Gu",
      "Yirui Sun",
      "Zhihao Xia",
      "Tin Lun Lam",
      "Chunxu Tian",
      "Dan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19514v1",
    "title": "PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment",
    "summary": "Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.",
    "published": "2026-01-27T11:55:23Z",
    "updated": "2026-01-27T11:55:23Z",
    "link": "http://arxiv.org/pdf/2601.19514v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ruiyu Wang",
      "Zheyu Zhuang",
      "Danica Kragic",
      "Florian T. Pokorny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19509v1",
    "title": "A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation",
    "summary": "In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.",
    "published": "2026-01-27T11:53:33Z",
    "updated": "2026-01-27T11:53:33Z",
    "link": "http://arxiv.org/pdf/2601.19509v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jin Huang",
      "Zichen Liu",
      "Haoda Li",
      "Zhikun Wang",
      "Ying Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19499v1",
    "title": "Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots",
    "summary": "Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.",
    "published": "2026-01-27T11:35:00Z",
    "updated": "2026-01-27T11:35:00Z",
    "link": "http://arxiv.org/pdf/2601.19499v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mehdi Heydari Shahna",
      "Seyed Adel Alizadeh Kolagar",
      "Jouni Mattila"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19496v1",
    "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
    "summary": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.",
    "published": "2026-01-27T11:32:04Z",
    "updated": "2026-01-27T11:32:04Z",
    "link": "http://arxiv.org/pdf/2601.19496v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jie Gu",
      "Hongrun Gao",
      "Zhihao Xia",
      "Yirun Sun",
      "Chunxu Tian",
      "Dan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.05197v3",
    "title": "A Riemannian Take on Distance Fields and Geodesic Flows in Robotics",
    "summary": "Distance functions are crucial in robotics for representing spatial relationships between a robot and its environment. They provide an implicit, continuous, and differentiable representation that integrates seamlessly with control, optimization, and learning. While standard distance fields rely on the Euclidean metric, many robotic tasks inherently involve non-Euclidean structures. To this end, we generalize Euclidean distance fields to more general metric spaces by solving the Riemannian eikonal equation, a first-order partial differential equation whose solution defines a distance field and its associated gradient flow on the manifold, enabling the computation of geodesics and globally length-minimizing paths. We demonstrate that geodesic distance fields, the classical Riemannian distance function represented as a global, continuous, and queryable field, are effective for a broad class of robotic problems where Riemannian geometry naturally arises. To realize this, we present a neural Riemannian eikonal solver (NES) that solves the equation as a mesh-free implicit representation without grid discretization, scaling to high-dimensional robot manipulators. Training leverages a physics-informed neural network (PINN) objective that constrains spatial derivatives via the PDE residual and boundary and metric conditions, so the model is supervised by the governing equation and requires no labeled distances or geodesics. We propose two NES variants, conditioned on boundary data and on spatially varying Riemannian metrics, underscoring the flexibility of the neural parameterization. We validate the effectiveness of our approach through extensive examples, yielding minimal-length geodesics across diverse robot tasks involving Riemannian geometry.",
    "published": "2024-12-06T17:22:18Z",
    "updated": "2026-01-27T10:58:17Z",
    "link": "http://arxiv.org/pdf/2412.05197v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yiming Li",
      "Jiacheng Qiu",
      "Sylvain Calinon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19462v1",
    "title": "Physical Human-Robot Interaction: A Critical Review of Safety Constraints",
    "summary": "This paper aims to provide a clear and rigorous understanding of commonly recognized safety constraints in physical human-robot interaction, i.e. ISO/TS 15066, by examining how they are obtained and which assumptions support them. We clarify the interpretation and practical impact of key simplifying assumptions, show how these modeling choices affect both safety and performance across the system, and indicate specific design parameters that can be adjusted in safety-critical control implementations. Numerical examples are provided to quantify performance degradation induced by common approximations and simplifying design choices. Furthermore, the fundamental role of energy in safety assessment is emphasized, and focused insights are offered on the existing body of work concerning energy-based safety methodologies.",
    "published": "2026-01-27T10:45:50Z",
    "updated": "2026-01-27T10:45:50Z",
    "link": "http://arxiv.org/pdf/2601.19462v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO"
    ],
    "authors": [
      "Riccardo Zanella",
      "Federico Califano",
      "Stefano Stramigioli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.04109v3",
    "title": "CBMC-V3: A CNS-inspired Control Framework Towards Agile Manipulation with SNN",
    "summary": "As robotic arm applications expand beyond traditional industrial settings into service-oriented domains such as catering, household and retail, existing control algorithms struggle to achieve the level of agile manipulation required in unstructured environments characterized by dynamic trajectories, unpredictable interactions, and diverse objects. This paper presents a biomimetic control framework based on Spiking Neural Network (SNN), inspired by the human Central Nervous System (CNS), to address these challenges. The proposed framework comprises five control modules-cerebral cortex, cerebellum, thalamus, brainstem, and spinal cord-organized into three hierarchical control levels (first-order, second-order, and third-order) and two information pathways (ascending and descending). All modules are fully implemented using SNN. The framework is validated through both simulation and experiments on a commercial robotic arm platform across a range of control tasks. The results demonstrate that the proposed method outperforms the baseline in terms of agile motion control capability, offering a practical and effective solution for achieving agile manipulation.",
    "published": "2025-11-06T06:48:29Z",
    "updated": "2026-01-27T10:28:26Z",
    "link": "http://arxiv.org/pdf/2511.04109v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yanbo Pang",
      "Qingkai Li",
      "Mingguo Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.04257v2",
    "title": "Automating Box Folding: Sequence Extraction and Ranking Methodologies",
    "summary": "Box folding represents a crucial challenge for automated packaging systems. This work bridges the gap between existing methods for folding sequence extraction and approaches focused on the adaptability of automated systems to specific box types. An innovative method is proposed to identify and rank folding sequences, enabling the transformation of a box from an initial state to a desired final configuration. The system evaluates and ranks these sequences based on their feasibility and compatibility with available hardware, providing recommendations for real-world implementations. Finally, an illustrative use case is presented, where a robot performs the folding of a box.",
    "published": "2025-05-07T09:02:09Z",
    "updated": "2026-01-27T09:53:30Z",
    "link": "http://arxiv.org/pdf/2505.04257v2.pdf",
    "category": [
      "cs.RO",
      "cs.SE"
    ],
    "authors": [
      "Giuseppe Fabio Preziosa",
      "Davide Ferloni",
      "Andrea Maria Zanchettin",
      "Marco Faroni",
      "Paolo Rocco"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19388v1",
    "title": "Judgelight: Trajectory-Level Post-Optimization for Multi-Agent Path Finding via Closed-Subwalk Collapsing",
    "summary": "Multi-Agent Path Finding (MAPF) is an NP-hard problem with applications in warehouse automation and multi-robot coordination. Learning-based MAPF solvers offer fast and scalable planning but often produce feasible trajectories that contain unnecessary or oscillatory movements. We propose Judgelight, a post-optimization method that improves trajectory quality after a MAPF solver generates a feasible schedule. Judgelight collapses closed subwalks in agents' trajectories to remove redundant movements while preserving all feasibility constraints. We formalize this process as MAPF-Collapse, prove that it is NP-hard, and present an exact optimization approach by formulating it as integer linear programming (ILP) problem. Experimental results show Judgelight consistently reduces solution cost by around 20%, particularly for learning-based solvers, producing trajectories that are better suited for real-world deployment.",
    "published": "2026-01-27T09:20:14Z",
    "updated": "2026-01-27T09:20:14Z",
    "link": "http://arxiv.org/pdf/2601.19388v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yimin Tang",
      "Sven Koenig",
      "Erdem Bıyık"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.19571v2",
    "title": "xFLIE: Leveraging Actionable Hierarchical Scene Representations for Autonomous Semantic-Aware Inspection Missions",
    "summary": "We present a novel architecture aimed towards incremental construction and exploitation of a hierarchical 3D scene graph representation during semantic-aware inspection missions. Inspection planning, particularly of distributed targets in previously unseen environments, presents an opportunity to exploit the semantic structure of the scene during reasoning, navigation and scene understanding. Motivated by this, we propose the 3D Layered Semantic Graph (3DLSG), a hierarchical inspection scene graph constructed in an incremental manner and organized into abstraction layers that support planning demands in real-time. To address the task of semantic-aware inspection, a mission framework, termed as Enhanced First-Look Inspect Explore (xFLIE), that tightly couples the 3DLSG with an inspection planner is proposed. We assess the performance through simulations and experimental trials, evaluating target-selection, path-planning and semantic navigation tasks over the 3DLSG model. The scenarios presented are diverse, ranging from city-scale distributed to solitary infrastructure targets in simulated worlds and subsequent outdoor and subterranean environment deployments onboard a quadrupedal robot. The proposed method successfully demonstrates incremental construction and planning over the 3DLSG representation to meet the objectives of the missions. Furthermore, the framework demonstrates successful semantic navigation tasks over the structured interface at the end of the inspection missions. Finally, we report multiple orders of magnitude reduction in path-planning time compared to conventional volumetric-map-based methods over various environment scale, demonstrating the planning efficiency and scalability of the proposed approach.",
    "published": "2024-12-27T10:26:59Z",
    "updated": "2026-01-27T08:48:11Z",
    "link": "http://arxiv.org/pdf/2412.19571v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Vignesh Kottayam Viswanathan",
      "Mario A. V. Saucedo",
      "Sumeet Gajanan Satpute",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19354v1",
    "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
    "summary": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: https://github.com/wzq-13/SSHC.git.",
    "published": "2026-01-27T08:37:21Z",
    "updated": "2026-01-27T08:37:21Z",
    "link": "http://arxiv.org/pdf/2601.19354v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Ziqian Wang",
      "Chenxi Fang",
      "Zhen Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.18548v2",
    "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
    "summary": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.",
    "published": "2026-01-26T14:55:26Z",
    "updated": "2026-01-27T06:36:00Z",
    "link": "http://arxiv.org/pdf/2601.18548v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yulin Li",
      "Zhiyuan Song",
      "Yiming Li",
      "Zhicheng Song",
      "Kai Chen",
      "Chunxin Zheng",
      "Zhihai Bi",
      "Jiahang Cao",
      "Sylvain Calinon",
      "Fan Shi",
      "Jun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19234v1",
    "title": "iFAN Ecosystem: A Unified AI, Digital Twin, Cyber-Physical Security, and Robotics Environment for Advanced Nuclear Simulation and Operations",
    "summary": "As nuclear facilities experience digital transformation and advanced reactor development, AI integration, cyber-physical security, and other emerging technologies such as autonomous robot operations are increasingly developed. However, evaluation and deployment is challenged by the lack of dedicated virtual testbeds. The Immersive Framework for Advanced Nuclear (iFAN) ecosystem is developed, a comprehensive digital twin framework with a realistic 3D environment with physics-based simulations. The iFAN ecosystem serves as a high-fidelity virtual testbed for plant operation, cybersecurity, physical security, and robotic operation, as it provides real-time data exchange for pre-deployment verification. Core features include virtual reality, reinforcement learning, radiation simulation, and cyber-physical security. In addition, the paper investigates various applications through potential operational scenarios. The iFAN ecosystem provides a versatile and secure architecture for validating the next generation of autonomous and cyber-resilient nuclear operations.",
    "published": "2026-01-27T06:08:55Z",
    "updated": "2026-01-27T06:08:55Z",
    "link": "http://arxiv.org/pdf/2601.19234v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Youndo Do",
      "Chad Meece",
      "Marc Zebrowitz",
      "Spencer Banks",
      "Myeongjun Choi",
      "Xiaoxu Diao",
      "Kai Tan",
      "Michael Doran",
      "Jason Reed",
      "Fan Zhang"
    ]
  }
]