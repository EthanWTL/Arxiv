[
  {
    "id": "http://arxiv.org/abs/2509.18467v1",
    "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with\n  Convolution across Tokens for Long Context Modeling",
    "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.",
    "published": "2025-09-22T22:43:44Z",
    "link": "http://arxiv.org/pdf/2509.18467v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zeyu Liu",
      "Souvik Kundu",
      "Lianghao Jiang",
      "Anni Li",
      "Srikanth Ronanki",
      "Sravan Bodapati",
      "Gourav Datta",
      "Peter A. Beerel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18461v1",
    "title": "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake\n  Content Before It's Created?",
    "summary": "Generative adversarial networks (GANs) and diffusion models have dramatically\nadvanced deepfake technology, and its threats to digital security, media\nintegrity, and public trust have increased rapidly. This research explored\nzero-shot deepfake detection, an emerging method even when the models have\nnever seen a particular deepfake variation. In this work, we studied\nself-supervised learning, transformer-based zero-shot classifier, generative\nmodel fingerprinting, and meta-learning techniques that better adapt to the\never-evolving deepfake threat. In addition, we suggested AI-driven prevention\nstrategies that mitigated the underlying generation pipeline of the deepfakes\nbefore they occurred. They consisted of adversarial perturbations for creating\ndeepfake generators, digital watermarking for content authenticity\nverification, real-time AI monitoring for content creation pipelines, and\nblockchain-based content verification frameworks. Despite these advancements,\nzero-shot detection and prevention faced critical challenges such as\nadversarial attacks, scalability constraints, ethical dilemmas, and the absence\nof standardized evaluation benchmarks. These limitations were addressed by\ndiscussing future research directions on explainable AI for deepfake detection,\nmultimodal fusion based on image, audio, and text analysis, quantum AI for\nenhanced security, and federated learning for privacy-preserving deepfake\ndetection. This further highlighted the need for an integrated defense\nframework for digital authenticity that utilized zero-shot learning in\ncombination with preventive deepfake mechanisms. Finally, we highlighted the\nimportant role of interdisciplinary collaboration between AI researchers,\ncybersecurity experts, and policymakers to create resilient defenses against\nthe rising tide of deepfake attacks.",
    "published": "2025-09-22T22:33:16Z",
    "link": "http://arxiv.org/pdf/2509.18461v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Ayan Sar",
      "Sampurna Roy",
      "Tanupriya Choudhury",
      "Ajith Abraham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18458v2",
    "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable\n  Length, Intrinsic Difficulty, and Distractor Density",
    "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.",
    "published": "2025-09-22T22:28:33Z",
    "link": "http://arxiv.org/pdf/2509.18458v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)",
      "I.2.7; I.2.6; I.2.4; I.2.8"
    ],
    "authors": [
      "Daniel Kaiser",
      "Arnoldo Frigessi",
      "Ali Ramezani-Kebrya",
      "Benjamin Ricaud"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18447v1",
    "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical\n  Human-Robot Interaction",
    "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.",
    "published": "2025-09-22T22:05:11Z",
    "link": "http://arxiv.org/pdf/2509.18447v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Rishabh Madan",
      "Jiawei Lin",
      "Mahika Goel",
      "Angchen Xie",
      "Xiaoyu Liang",
      "Marcus Lee",
      "Justin Guo",
      "Pranav N. Thakkar",
      "Rohan Banerjee",
      "Jose Barreiros",
      "Kate Tsui",
      "Tom Silver",
      "Tapomayukh Bhattacharjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18439v1",
    "title": "Developing an AI framework to automatically detect shared\n  decision-making in patient-doctor conversations",
    "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.",
    "published": "2025-09-22T21:50:13Z",
    "link": "http://arxiv.org/pdf/2509.18439v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Oscar J. Ponce-Ponte",
      "David Toro-Tobon",
      "Luis F. Figueroa",
      "Michael Gionfriddo",
      "Megan Branda",
      "Victor M. Montori",
      "Saturnino Luz",
      "Juan P. Brito"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20388v1",
    "title": "Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants",
    "summary": "The rapid integration of AI-powered coding assistants into developer\nworkflows has raised significant privacy and trust concerns. As developers\nentrust proprietary code to services like OpenAI's GPT, Google's Gemini, and\nGitHub Copilot, the unclear data handling practices of these tools create\nsecurity and compliance risks. This paper addresses this challenge by\nintroducing and applying a novel, expert-validated privacy scorecard. The\nmethodology involves a detailed analysis of four document types; from legal\npolicies to external audits; to score five leading assistants against 14\nweighted criteria. A legal expert and a data protection officer refined these\ncriteria and their weighting. The results reveal a distinct hierarchy of\nprivacy protections, with a 20-point gap between the highest- and lowest-ranked\ntools. The analysis uncovers common industry weaknesses, including the\npervasive use of opt-out consent for model training and a near-universal\nfailure to filter secrets from user prompts proactively. The resulting\nscorecard provides actionable guidance for developers and organizations,\nenabling evidence-based tool selection. This work establishes a new benchmark\nfor transparency and advocates for a shift towards more user-centric privacy\nstandards in the AI industry.",
    "published": "2025-09-22T21:45:45Z",
    "link": "http://arxiv.org/pdf/2509.20388v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Amir AL-Maamari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18436v1",
    "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories",
    "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).",
    "published": "2025-09-22T21:41:35Z",
    "link": "http://arxiv.org/pdf/2509.18436v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "authors": [
      "Hongda Jiang",
      "Xinyuan Zhang",
      "Siddhant Garg",
      "Rishab Arora",
      "Shiun-Zu Kuo",
      "Jiayang Xu",
      "Christopher Brossman",
      "Yue Liu",
      "Aaron Colak",
      "Ahmed Aly",
      "Anuj Kumar",
      "Xin Luna Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18424v1",
    "title": "Scattering Transformer: A Training-Free Transformer Architecture for\n  Heart Murmur Detection",
    "summary": "In an attempt to address the need for skilled clinicians in heart sound\ninterpretation, recent research efforts on automating cardiac auscultation have\nexplored deep learning approaches. The majority of these approaches have been\nbased on supervised learning that is always challenged in occasions where\ntraining data is limited. More recently, there has been a growing interest in\npotentials of pre-trained self-supervised audio foundation models for\nbiomedical end tasks. Despite exhibiting promising results, these foundational\nmodels are typically computationally intensive. Within the context of automatic\ncardiac auscultation, this study explores a lightweight alternative to these\ngeneral-purpose audio foundation models by introducing the Scattering\nTransformer, a novel, training-free transformer architecture for heart murmur\ndetection. The proposed method leverages standard wavelet scattering networks\nby introducing contextual dependencies in a transformer-like architecture\nwithout any backpropagation. We evaluate our approach on the public CirCor\nDigiScope dataset, directly comparing it against leading general-purpose\nfoundational models. The Scattering Transformer achieves a Weighted\nAccuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,\ndemonstrating performance highly competitive with contemporary state of the art\nmethods. This study establishes the Scattering Transformer as a viable and\npromising alternative in resource-constrained setups.",
    "published": "2025-09-22T21:08:06Z",
    "link": "http://arxiv.org/pdf/2509.18424v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Rami Zewail"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18420v1",
    "title": "Instruction-Following Evaluation in Function Calling for Large Language\n  Models",
    "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.",
    "published": "2025-09-22T21:04:39Z",
    "link": "http://arxiv.org/pdf/2509.18420v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Nikolai Skripko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18415v1",
    "title": "Context Lineage Assurance for Non-Human Identities in Critical\n  Multi-Agent Systems",
    "summary": "The proliferation of autonomous software agents necessitates rigorous\nframeworks for establishing secure and verifiable agent-to-agent (A2A)\ninteractions, particularly when such agents are instantiated as non-human\nidentities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a\ncryptographically grounded mechanism for lineage verification, wherein the\nprovenance and evolution of NHIs are anchored in append-only Merkle tree\nstructures modeled after Certificate Transparency (CT) logs. Unlike traditional\nA2A models that primarily secure point-to-point interactions, our approach\nenables both agents and external verifiers to cryptographically validate\nmulti-hop provenance, thereby ensuring the integrity of the entire call chain.\n  A federated proof server acts as an auditor across one or more Merkle logs,\naggregating inclusion proofs and consistency checks into compact, signed\nattestations that external parties can verify without access to the full\nexecution trace. In parallel, we augment the A2A agent card to incorporate\nexplicit identity verification primitives, enabling both peer agents and human\napprovers to authenticate the legitimacy of NHI representations in a\nstandardized manner. Together, these contributions establish a cohesive model\nthat integrates identity attestation, lineage verification, and independent\nproof auditing, thereby advancing the security posture of inter-agent\necosystems and providing a foundation for robust governance of NHIs in\nregulated environments such as FedRAMP.",
    "published": "2025-09-22T20:59:51Z",
    "link": "http://arxiv.org/pdf/2509.18415v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Sumana Malkapuram",
      "Sameera Gangavarapu",
      "Kailashnath Reddy Kavalakuntla",
      "Ananya Gangavarapu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18407v1",
    "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled\n  Intersections",
    "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.",
    "published": "2025-09-22T20:46:23Z",
    "link": "http://arxiv.org/pdf/2509.18407v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Navya Tiwari",
      "Joseph Vazhaeparampil",
      "Victoria Preston"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18405v1",
    "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language\n  and Vision Language Models",
    "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.",
    "published": "2025-09-22T20:43:59Z",
    "link": "http://arxiv.org/pdf/2509.18405v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Sourav Halder",
      "Jinjun Tong",
      "Xinyu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18400v1",
    "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized\n  Tariff Code Classification",
    "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment.",
    "published": "2025-09-22T20:32:24Z",
    "link": "http://arxiv.org/pdf/2509.18400v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Pritish Yuvraj",
      "Siva Devarakonda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18394v1",
    "title": "An Artificial Intelligence Value at Risk Approach: Metrics and Models",
    "summary": "Artificial intelligence risks are multidimensional in nature, as the same\nrisk scenarios may have legal, operational, and financial risk dimensions. With\nthe emergence of new AI regulations, the state of the art of artificial\nintelligence risk management seems to be highly immature due to upcoming AI\nregulations. Despite the appearance of several methodologies and generic\ncriteria, it is rare to find guidelines with real implementation value,\nconsidering that the most important issue is customizing artificial\nintelligence risk metrics and risk models for specific AI risk scenarios.\nFurthermore, the financial departments, legal departments and Government Risk\nCompliance teams seem to remain unaware of many technical aspects of AI\nsystems, in which data scientists and AI engineers emerge as the most\nappropriate implementers. It is crucial to decompose the problem of artificial\nintelligence risk in several dimensions: data protection, fairness, accuracy,\nrobustness, and information security. Consequently, the main task is developing\nadequate metrics and risk models that manage to reduce uncertainty for\ndecision-making in order to take informed decisions concerning the risk\nmanagement of AI systems.\n  The purpose of this paper is to orientate AI stakeholders about the depths of\nAI risk management. Although it is not extremely technical, it requires a basic\nknowledge of risk management, quantifying uncertainty, the FAIR model, machine\nlearning, large language models and AI context engineering. The examples\npresented pretend to be very basic and understandable, providing simple ideas\nthat can be developed regarding specific AI customized environments. There are\nmany issues to solve in AI risk management, and this paper will present a\nholistic overview of the inter-dependencies of AI risks, and how to model them\ntogether, within risk scenarios.",
    "published": "2025-09-22T20:27:29Z",
    "link": "http://arxiv.org/pdf/2509.18394v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "q-fin.RM"
    ],
    "authors": [
      "Luis Enriquez Alvarez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18386v1",
    "title": "Graph Enhanced Trajectory Anomaly Detection",
    "summary": "Trajectory anomaly detection is essential for identifying unusual and\nunexpected movement patterns in applications ranging from intelligent\ntransportation systems to urban safety and fraud prevention.\n  Existing methods only consider limited aspects of the trajectory nature and\nits movement space by treating trajectories as sequences of sampled locations,\nwith sampling determined by positioning technology, e.g., GPS, or by high-level\nabstractions such as staypoints. Trajectories are analyzed in Euclidean space,\nneglecting the constraints and connectivity information of the underlying\nmovement network, e.g., road or transit networks.\n  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework\ntightly integrates road network topology, segment semantics, and historical\ntravel patterns to model trajectory data. GETAD uses a Graph Attention Network\nto learn road-aware embeddings that capture both physical attributes and\ntransition behavior, and augments these with graph-based positional encodings\nthat reflect the spatial layout of the road network.\n  A Transformer-based decoder models sequential movement, while a\nmultiobjective loss function combining autoregressive prediction and supervised\nlink prediction ensures realistic and structurally coherent representations.\n  To improve the robustness of anomaly detection, we introduce Confidence\nWeighted Negative Log Likelihood (CW NLL), an anomaly scoring function that\nemphasizes high-confidence deviations.\n  Experiments on real-world and synthetic datasets demonstrate that GETAD\nachieves consistent improvements over existing methods, particularly in\ndetecting subtle anomalies in road-constrained environments. These results\nhighlight the benefits of incorporating graph structure and contextual\nsemantics into trajectory modeling, enabling more precise and context-aware\nanomaly detection.",
    "published": "2025-09-22T20:15:15Z",
    "link": "http://arxiv.org/pdf/2509.18386v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jonathan Kabala Mbuya",
      "Dieter Pfoser",
      "Antonios Anastasopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18383v1",
    "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?",
    "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.",
    "published": "2025-09-22T20:11:40Z",
    "link": "http://arxiv.org/pdf/2509.18383v1.pdf",
    "category": [
      "cs.AI",
      "cs.DM",
      "cs.LG"
    ],
    "authors": [
      "Moran Feldman",
      "Amin Karbasi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18382v1",
    "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models\n  Under Compute Constraints",
    "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model.",
    "published": "2025-09-22T20:09:37Z",
    "link": "http://arxiv.org/pdf/2509.18382v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Adarsha Balaji",
      "Le Chen",
      "Rajeev Thakur",
      "Franck Cappello",
      "Sandeep Madireddy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18369v1",
    "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with\n  Contrastive and Transport Regularization for Bengali Captioning",
    "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.",
    "published": "2025-09-22T19:49:35Z",
    "link": "http://arxiv.org/pdf/2509.18369v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Riad Ahmed Anonto",
      "Sardar Md. Saffat Zabin",
      "M. Saifur Rahman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18367v1",
    "title": "Multi-Worker Selection based Distributed Swarm Learning for Edge IoT\n  with Non-i.i.d. Data",
    "summary": "Recent advances in distributed swarm learning (DSL) offer a promising\nparadigm for edge Internet of Things. Such advancements enhance data privacy,\ncommunication efficiency, energy saving, and model scalability. However, the\npresence of non-independent and identically distributed (non-i.i.d.) data pose\na significant challenge for multi-access edge computing, degrading learning\nperformance and diverging training behavior of vanilla DSL. Further, there\nstill lacks theoretical guidance on how data heterogeneity affects model\ntraining accuracy, which requires thorough investigation. To fill the gap, this\npaper first study the data heterogeneity by measuring the impact of non-i.i.d.\ndatasets under the DSL framework. This then motivates a new multi-worker\nselection design for DSL, termed M-DSL algorithm, which works effectively with\ndistributed heterogeneous data. A new non-i.i.d. degree metric is introduced\nand defined in this work to formulate the statistical difference among local\ndatasets, which builds a connection between the measure of data heterogeneity\nand the evaluation of DSL performance. In this way, our M-DSL guides effective\nselection of multiple works who make prominent contributions for global model\nupdates. We also provide theoretical analysis on the convergence behavior of\nour M-DSL, followed by extensive experiments on different heterogeneous\ndatasets and non-i.i.d. data settings. Numerical results verify performance\nimprovement and network intelligence enhancement provided by our M-DSL beyond\nthe benchmarks.",
    "published": "2025-09-22T19:47:44Z",
    "link": "http://arxiv.org/pdf/2509.18367v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhuoyu Yao",
      "Yue Wang",
      "Songyang Zhang",
      "Yingshu Li",
      "Zhipeng Cai",
      "Zhi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18361v1",
    "title": "Reading Between the Lines: Scalable User Feedback via Implicit Sentiment\n  in Developer Prompts",
    "summary": "Evaluating developer satisfaction with conversational AI assistants at scale\nis critical but challenging. User studies provide rich insights, but are\nunscalable, while large-scale quantitative signals from logs or in-product\nratings are often too shallow or sparse to be reliable. To address this gap, we\npropose and evaluate a new approach: using sentiment analysis of developer\nprompts to identify implicit signals of user satisfaction. With an analysis of\nindustrial usage logs of 372 professional developers, we show that this\napproach can identify a signal in ~8% of all interactions, a rate more than 13\ntimes higher than explicit user feedback, with reasonable accuracy even with an\noff-the-shelf sentiment analysis approach. This new practical approach to\ncomplement existing feedback channels would open up new directions for building\na more comprehensive understanding of the developer experience at scale.",
    "published": "2025-09-22T19:37:42Z",
    "link": "http://arxiv.org/pdf/2509.18361v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Daye Nam",
      "Malgorzata Salawa",
      "Satish Chandra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18355v1",
    "title": "Chiplet-Based RISC-V SoC with Modular AI Acceleration",
    "summary": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications.",
    "published": "2025-09-22T19:31:58Z",
    "link": "http://arxiv.org/pdf/2509.18355v1.pdf",
    "category": [
      "cs.AR",
      "cs.AI"
    ],
    "authors": [
      "P. Ramkumar",
      "S. S. Bharadwaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18354v1",
    "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without\n  Training Data",
    "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet",
    "published": "2025-09-22T19:29:20Z",
    "link": "http://arxiv.org/pdf/2509.18354v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "62H35, 68T07, 62M40, 68T45",
      "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"
    ],
    "authors": [
      "Mehrdad Moradi",
      "Shengzhe Chen",
      "Hao Yan",
      "Kamran Paynabar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18316v1",
    "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for\n  Diagnostic Reasoning",
    "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.",
    "published": "2025-09-22T18:39:09Z",
    "link": "http://arxiv.org/pdf/2509.18316v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Saksham Khatwani",
      "He Cheng",
      "Majid Afshar",
      "Dmitriy Dligach",
      "Yanjun Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18293v1",
    "title": "Evaluating Large Language Models for Detecting Antisemitism",
    "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.",
    "published": "2025-09-22T18:23:21Z",
    "link": "http://arxiv.org/pdf/2509.18293v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Jay Patel",
      "Hrudayangam Mehta",
      "Jeremy Blackburn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18282v1",
    "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot\n  Generalization of Robot Manipulation Policies",
    "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.",
    "published": "2025-09-22T18:10:14Z",
    "link": "http://arxiv.org/pdf/2509.18282v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jesse Zhang",
      "Marius Memmel",
      "Kevin Kim",
      "Dieter Fox",
      "Jesse Thomason",
      "Fabio Ramos",
      "Erdem Bıyık",
      "Abhishek Gupta",
      "Anqi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18094v1",
    "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level\n  Visual Reasoning",
    "summary": "Recent advances in Large Multi-modal Models (LMMs) have demonstrated their\nremarkable success as general-purpose multi-modal assistants, with particular\nfocuses on holistic image- and video-language understanding. Conversely, less\nattention has been given to scaling fine-grained pixel-level understanding\ncapabilities, where the models are expected to realize pixel-level alignment\nbetween visual signals and language semantics. Some previous studies have\napplied LMMs to related tasks such as region-level captioning and referring\nexpression segmentation. However, these models are limited to performing either\nreferring or segmentation tasks independently and fail to integrate these\nfine-grained perception capabilities into visual reasoning. To bridge this gap,\nwe propose UniPixel, a large multi-modal model capable of flexibly\ncomprehending visual prompt inputs and generating mask-grounded responses. Our\nmodel distinguishes itself by seamlessly integrating pixel-level perception\nwith general visual understanding capabilities. Specifically, UniPixel\nprocesses visual prompts and generates relevant masks on demand, and performs\nsubsequent reasoning conditioning on these intermediate pointers during\ninference, thereby enabling fine-grained pixel-level reasoning. The\neffectiveness of our approach has been verified on 10 benchmarks across a\ndiverse set of tasks, including pixel-level referring/segmentation and\nobject-centric understanding in images/videos. A novel PixelQA task that\njointly requires referring, segmentation, and question answering is also\ndesigned to verify the flexibility of our method.",
    "published": "2025-09-22T17:59:40Z",
    "link": "http://arxiv.org/pdf/2509.18094v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ye Liu",
      "Zongyang Ma",
      "Junfu Pu",
      "Zhongang Qi",
      "Yang Wu",
      "Ying Shan",
      "Chang Wen Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18093v1",
    "title": "SEQR: Secure and Efficient QR-based LoRA Routing",
    "summary": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency.",
    "published": "2025-09-22T17:59:38Z",
    "link": "http://arxiv.org/pdf/2509.18093v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "William Fleshman",
      "Benjamin Van Durme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18091v1",
    "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial\n  Cascade Ranking System",
    "summary": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.",
    "published": "2025-09-22T17:59:07Z",
    "link": "http://arxiv.org/pdf/2509.18091v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sunhao Dai",
      "Jiakai Tang",
      "Jiahua Wu",
      "Kun Wang",
      "Yuxuan Zhu",
      "Bingjun Chen",
      "Bangyang Hong",
      "Yu Zhao",
      "Cong Fu",
      "Kangle Wu",
      "Yabo Ni",
      "Anxiang Zeng",
      "Wenjie Wang",
      "Xu Chen",
      "Jun Xu",
      "See-Kiong Ng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18085v1",
    "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
    "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
    "published": "2025-09-22T17:58:21Z",
    "link": "http://arxiv.org/pdf/2509.18085v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sudhanshu Agrawal",
      "Risheek Garrepalli",
      "Raghavv Goel",
      "Mingu Lee",
      "Christopher Lott",
      "Fatih Porikli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18083v1",
    "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
    "summary": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
    "published": "2025-09-22T17:56:38Z",
    "link": "http://arxiv.org/pdf/2509.18083v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Valentin Lacombe",
      "Valentin Quesnel",
      "Damien Sileo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18076v1",
    "title": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates",
    "summary": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications.",
    "published": "2025-09-22T17:55:14Z",
    "link": "http://arxiv.org/pdf/2509.18076v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hy Dang",
      "Tianyi Liu",
      "Zhuofeng Wu",
      "Jingfeng Yang",
      "Haoming Jiang",
      "Tao Yang",
      "Pei Chen",
      "Zhengyang Wang",
      "Helen Wang",
      "Huasheng Li",
      "Bing Yin",
      "Meng Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18234v1",
    "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on\n  Multimodal Medical Benchmarks",
    "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.",
    "published": "2025-09-22T17:48:05Z",
    "link": "http://arxiv.org/pdf/2509.18234v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Yu Gu",
      "Jingjing Fu",
      "Xiaodong Liu",
      "Jeya Maria Jose Valanarasu",
      "Noel Codella",
      "Reuben Tan",
      "Qianchu Liu",
      "Ying Jin",
      "Sheng Zhang",
      "Jinyu Wang",
      "Rui Wang",
      "Lei Song",
      "Guanghui Qin",
      "Naoto Usuyama",
      "Cliff Wong",
      "Cheng Hao",
      "Hohin Lee",
      "Praneeth Sanapathi",
      "Sarah Hilado",
      "Bian Jiang",
      "Javier Alvarez-Valle",
      "Mu Wei",
      "Jianfeng Gao",
      "Eric Horvitz",
      "Matt Lungren",
      "Hoifung Poon",
      "Paul Vozila"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18060v1",
    "title": "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for\n  Ü-Tsang, Amdo and Kham Speech Dataset Generation",
    "summary": "Tibetan is a low-resource language with limited parallel speech corpora\nspanning its three major dialects (\\\"U-Tsang, Amdo, and Kham), limiting\nprogress in speech modeling. To address this issue, we propose TMD-TTS, a\nunified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes\nparallel dialectal speech from explicit dialect labels. Our method features a\ndialect fusion module and a Dialect-Specialized Dynamic Routing Network\n(DSDR-Net) to capture fine-grained acoustic and linguistic variations across\ndialects. Extensive objective and subjective evaluations demonstrate that\nTMD-TTS significantly outperforms baselines in dialectal expressiveness. We\nfurther validate the quality and utility of the synthesized speech through a\nchallenging Speech-to-Speech Dialect Conversion (S2SDC) task.",
    "published": "2025-09-22T17:38:52Z",
    "link": "http://arxiv.org/pdf/2509.18060v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yutong Liu",
      "Ziyue Zhang",
      "Ban Ma-bao",
      "Renzeng Duojie",
      "Yuqing Cai",
      "Yongbin Yu",
      "Xiangxiang Wang",
      "Fan Gao",
      "Cheng Huang",
      "Nyima Tashi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18058v2",
    "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier\n  LLMs",
    "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are crafted to be subtly incorrect or otherwise harmless in\npractice. This behavior emerges with hard-to-predict variations even within\nmodels from the same model family. We find no apparent cause for the propensity\nto deceive, but show that more capable models are better at executing this\nstrategy. Strategic dishonesty already has a practical impact on safety\nevaluations, as we show that dishonest responses fool all output-based monitors\nused to detect jailbreaks that we test, rendering benchmark scores unreliable.\nFurther, strategic dishonesty can act like a honeypot against malicious users,\nwhich noticeably obfuscates prior jailbreak attacks. While output monitors\nfail, we show that linear probes on internal activations can be used to\nreliably detect strategic dishonesty. We validate probes on datasets with\nverifiable outcomes and by using them as steering vectors. Overall, we consider\nstrategic dishonesty as a concrete example of a broader concern that alignment\nof LLMs is hard to control, especially when helpfulness and harmlessness\nconflict.",
    "published": "2025-09-22T17:30:56Z",
    "link": "http://arxiv.org/pdf/2509.18058v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Alexander Panfilov",
      "Evgenii Kortukov",
      "Kristina Nikolić",
      "Matthias Bethge",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Ameya Prabhu",
      "Maksym Andriushchenko",
      "Jonas Geiping"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18057v2",
    "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
    "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
    "published": "2025-09-22T17:30:33Z",
    "link": "http://arxiv.org/pdf/2509.18057v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "math.CO"
    ],
    "authors": [
      "Ansh Nagda",
      "Prabhakar Raghavan",
      "Abhradeep Thakurta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18054v1",
    "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for\n  Algorithm Selection in the Facility Layout Problem",
    "summary": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot.",
    "published": "2025-09-22T17:29:10Z",
    "link": "http://arxiv.org/pdf/2509.18054v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Nikhil N S",
      "Amol Dilip Joshi",
      "Bilal Muhammed",
      "Soban Babu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18046v1",
    "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement\n  Learning with Mamba",
    "summary": "End-to-end reinforcement learning (RL) for humanoid locomotion is appealing\nfor its compact perception-action mapping, yet practical policies often suffer\nfrom training instability, inefficient feature fusion, and high actuation cost.\nWe present HuMam, a state-centric end-to-end RL framework that employs a\nsingle-layer Mamba encoder to fuse robot-centric states with oriented footstep\ntargets and a continuous phase clock. The policy outputs joint position targets\ntracked by a low-level PD loop and is optimized with PPO. A concise six-term\nreward balances contact quality, swing smoothness, foot placement, posture, and\nbody stability while implicitly promoting energy saving. On the JVRC-1 humanoid\nin mc-mujoco, HuMam consistently improves learning efficiency, training\nstability, and overall task performance over a strong feedforward baseline,\nwhile reducing power consumption and torque peaks. To our knowledge, this is\nthe first end-to-end humanoid RL controller that adopts Mamba as the fusion\nbackbone, demonstrating tangible gains in efficiency, stability, and control\neconomy.",
    "published": "2025-09-22T17:19:55Z",
    "link": "http://arxiv.org/pdf/2509.18046v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.ET",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "authors": [
      "Yinuo Wang",
      "Yuanyang Qi",
      "Jinzhao Zhou",
      "Gavin Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18044v1",
    "title": "Hybrid Reputation Aggregation: A Robust Defense Mechanism for\n  Adversarial Federated Learning in 5G and Edge Network Environments",
    "summary": "Federated Learning (FL) in 5G and edge network environments face severe\nsecurity threats from adversarial clients. Malicious participants can perform\nlabel flipping, inject backdoor triggers, or launch Sybil attacks to corrupt\nthe global model. This paper introduces Hybrid Reputation Aggregation (HRA), a\nnovel robust aggregation mechanism designed to defend against diverse\nadversarial behaviors in FL without prior knowledge of the attack type. HRA\ncombines geometric anomaly detection with momentum-based reputation tracking of\nclients. In each round, it detects outlier model updates via distance-based\ngeometric analysis while continuously updating a trust score for each client\nbased on historical behavior. This hybrid approach enables adaptive filtering\nof suspicious updates and long-term penalization of unreliable clients,\ncountering attacks ranging from backdoor insertions to random noise Byzantine\nfailures. We evaluate HRA on a large-scale proprietary 5G network dataset (3M+\nrecords) and the widely used NF-CSE-CIC-IDS2018 benchmark under diverse\nadversarial attack scenarios. Experimental results reveal that HRA achieves\nrobust global model accuracy of up to 98.66% on the 5G dataset and 96.60% on\nNF-CSE-CIC-IDS2018, outperforming state-of-the-art aggregators such as Krum,\nTrimmed Mean, and Bulyan by significant margins. Our ablation studies further\ndemonstrate that the full hybrid system achieves 98.66% accuracy, while the\nanomaly-only and reputation-only variants drop to 84.77% and 78.52%,\nrespectively, validating the synergistic value of our dual-mechanism approach.\nThis demonstrates HRA's enhanced resilience and robustness in 5G/edge federated\nlearning deployments, even under significant adversarial conditions.",
    "published": "2025-09-22T17:18:59Z",
    "link": "http://arxiv.org/pdf/2509.18044v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Saeid Sheikhi",
      "Panos Kostakos",
      "Lauri Loven"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19391v1",
    "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation",
    "summary": "Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers\nby adding trainable low-rank matrices to attention projections. While\neffective, these matrices are considered independent for each attention\nprojection (Query, Key, and Value) and each layer. Recent extensions have\nconsidered joint, tensor-based adaptations, but only in limited forms and\nwithout a systematic framework. We introduce TensLoRA, a unified framework that\naggregates LoRA updates into higher-order tensors and models a broad family of\ntensor-based low-rank adaptations. Our formulation generalizes existing\ntensor-based methods and enables mode-specific compression rates, allowing\nparameter budgets to be tailored according to the modality and task.\nExperiments on vision and language benchmarks reveal that the tensor\nconstruction directly impacts performance, sometimes better than standard LoRA\nunder similar parameter counts.",
    "published": "2025-09-22T17:15:23Z",
    "link": "http://arxiv.org/pdf/2509.19391v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "68T",
      "I.2.6; I.2.7; I.2.10"
    ],
    "authors": [
      "Axel Marmoret",
      "Reda Bensaid",
      "Jonathan Lys",
      "Vincent Gripon",
      "François Leduc-Primeau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18233v1",
    "title": "Perceptions of AI Across Sectors: A Comparative Review of Public\n  Attitudes",
    "summary": "This paper offers a domain-mediated comparative review of 251 studies on\npublic attitudes toward AI, published between 2011 and 2025. Drawing on a\nsystematic literature review, we analyse how different factors including\nperceived benefits and concerns (or risks) shape public acceptance of - or\nresistance to - artificial intelligence across domains and use-cases, including\nhealthcare, education, security, public administration, generative AI, and\nautonomous vehicles. The analysis highlights recurring patterns in individual,\ncontextual, and technical factors influencing perception, while also tracing\nvariations in institutional trust, perceived fairness, and ethical concerns. We\nshow that the public perception in AI is shaped not only by technical design or\nperformance but also by sector-specific considerations as well as imaginaries,\ncultural narratives, and historical legacies. This comparative approach offers\na foundation for developing more tailored and context-sensitive strategies for\nresponsible AI governance.",
    "published": "2025-09-22T17:07:57Z",
    "link": "http://arxiv.org/pdf/2509.18233v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Filip Bialy",
      "Mark Elliot",
      "Robert Meckin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18025v1",
    "title": "Deep Learning as the Disciplined Construction of Tame Objects",
    "summary": "One can see deep-learning models as compositions of functions within the\nso-called tame geometry. In this expository note, we give an overview of some\ntopics at the interface of tame geometry (also known as o-minimality),\noptimization theory, and deep learning theory and practice. To do so, we\ngradually introduce the concepts and tools used to build convergence guarantees\nfor stochastic gradient descent in a general nonsmooth nonconvex, but tame,\nsetting. This illustrates some ways in which tame geometry is a natural\nmathematical framework for the study of AI systems, especially within Deep\nLearning.",
    "published": "2025-09-22T17:00:40Z",
    "link": "http://arxiv.org/pdf/2509.18025v1.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "math.LO",
      "stat.ML"
    ],
    "authors": [
      "Gilles Bareilles",
      "Allen Gehret",
      "Johannes Aspman",
      "Jana Lepšová",
      "Jakub Mareček"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18015v1",
    "title": "Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization\n  in Chest Radiographs",
    "summary": "Recent work has shown promising performance of frontier large language models\n(LLMs) and their multimodal counterparts in medical quizzes and diagnostic\ntasks, highlighting their potential for broad clinical utility given their\naccessible, general-purpose nature. However, beyond diagnosis, a fundamental\naspect of medical image interpretation is the ability to localize pathological\nfindings. Evaluating localization not only has clinical and educational\nrelevance but also provides insight into a model's spatial understanding of\nanatomy and disease. Here, we systematically assess two general-purpose MLLMs\n(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to\nlocalize pathologies on chest radiographs, using a prompting pipeline that\noverlays a spatial grid and elicits coordinate-based predictions. Averaged\nacross nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a\nlocalization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),\nall lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark\n(80.1%). Despite modest performance, error analysis revealed that GPT-5's\npredictions were largely in anatomically plausible regions, just not always\nprecisely localized. GPT-4 performed well on pathologies with fixed anatomical\nlocations, but struggled with spatially variable findings and exhibited\nanatomically implausible predictions more frequently. MedGemma demonstrated the\nlowest performance on all pathologies, showing limited capacity to generalize\nto this novel task. Our findings highlight both the promise and limitations of\ncurrent MLLMs in medical imaging and underscore the importance of integrating\nthem with task-specific tools for reliable use.",
    "published": "2025-09-22T16:54:23Z",
    "link": "http://arxiv.org/pdf/2509.18015v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Advait Gosai",
      "Arun Kavishwar",
      "Stephanie L. McNamara",
      "Soujanya Samineni",
      "Renato Umeton",
      "Alexander Chowdhury",
      "William Lotter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18010v1",
    "title": "Cross-Attention is Half Explanation in Speech-to-Text Models",
    "summary": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.",
    "published": "2025-09-22T16:49:26Z",
    "link": "http://arxiv.org/pdf/2509.18010v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Sara Papi",
      "Dennis Fucci",
      "Marco Gaido",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18008v1",
    "title": "Through the Lens of Human-Human Collaboration: A Configurable Research\n  Platform for Exploring Human-Agent Collaboration",
    "summary": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis.",
    "published": "2025-09-22T16:47:08Z",
    "link": "http://arxiv.org/pdf/2509.18008v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Bingsheng Yao",
      "Jiaju Chen",
      "Chaoran Chen",
      "April Wang",
      "Toby Jia-jun Li",
      "Dakuo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18001v1",
    "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
    "summary": "Sharpness-aware minimization (SAM) has emerged as a highly effective\ntechnique for improving model generalization, but its underlying principles are\nnot fully understood. We investigated the phenomenon known as m-sharpness,\nwhere the performance of SAM improves monotonically as the micro-batch size for\ncomputing perturbations decreases. Leveraging an extended Stochastic\nDifferential Equation (SDE) framework, combined with an analysis of the\nstructure of stochastic gradient noise (SGN), we precisely characterize the\ndynamics of various SAM variants. Our findings reveal that the stochastic noise\nintroduced during SAM perturbations inherently induces a variance-based\nsharpness regularization effect. Motivated by our theoretical insights, we\nintroduce Reweighted SAM, which employs sharpness-weighted sampling to mimic\nthe generalization benefits of m-SAM while remaining parallelizable.\nComprehensive experiments validate the effectiveness of our theoretical\nanalysis and proposed method.",
    "published": "2025-09-22T16:40:42Z",
    "link": "http://arxiv.org/pdf/2509.18001v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Haocheng Luo",
      "Mehrtash Harandi",
      "Dinh Phung",
      "Trung Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17999v2",
    "title": "The Narcissus Hypothesis: Descending to the Rung of Illusion",
    "summary": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion.",
    "published": "2025-09-22T16:39:22Z",
    "link": "http://arxiv.org/pdf/2509.17999v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Riccardo Cadei",
      "Christian Internò"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17998v2",
    "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs",
    "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/richardcsuwandi/cake.",
    "published": "2025-09-22T16:39:12Z",
    "link": "http://arxiv.org/pdf/2509.17998v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Richard Cornelius Suwandi",
      "Feng Yin",
      "Juntao Wang",
      "Renjie Li",
      "Tsung-Hui Chang",
      "Sergios Theodoridis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17995v1",
    "title": "Variation in Verification: Understanding Verification Dynamics in Large\n  Language Models",
    "summary": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.",
    "published": "2025-09-22T16:36:56Z",
    "link": "http://arxiv.org/pdf/2509.17995v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yefan Zhou",
      "Austin Xu",
      "Yilun Zhou",
      "Janvijay Singh",
      "Jiang Gui",
      "Shafiq Joty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17991v1",
    "title": "ReDepress: A Cognitive Framework for Detecting Depression Relapse from\n  Social Media",
    "summary": "Almost 50% depression patients face the risk of going into relapse. The risk\nincreases to 80% after the second episode of depression. Although, depression\ndetection from social media has attained considerable attention, depression\nrelapse detection has remained largely unexplored due to the lack of curated\ndatasets and the difficulty of distinguishing relapse and non-relapse users. In\nthis work, we present ReDepress, the first clinically validated social media\ndataset focused on relapse, comprising 204 Reddit users annotated by mental\nhealth professionals. Unlike prior approaches, our framework draws on cognitive\ntheories of depression, incorporating constructs such as attention bias,\ninterpretation bias, memory bias and rumination into both annotation and\nmodeling. Through statistical analyses and machine learning experiments, we\ndemonstrate that cognitive markers significantly differentiate relapse and\nnon-relapse groups, and that models enriched with these features achieve\ncompetitive performance, with transformer-based temporal models attaining an F1\nof 0.86. Our findings validate psychological theories in real-world textual\ndata and underscore the potential of cognitive-informed computational methods\nfor early relapse detection, paving the way for scalable, low-cost\ninterventions in mental healthcare.",
    "published": "2025-09-22T16:33:59Z",
    "link": "http://arxiv.org/pdf/2509.17991v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Aakash Kumar Agarwal",
      "Saprativa Bhattacharjee",
      "Mauli Rastogi",
      "Jemima S. Jacob",
      "Biplab Banerjee",
      "Rashmi Gupta",
      "Pushpak Bhattacharyya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17978v1",
    "title": "The STAR-XAI Protocol: An Interactive Framework for Inducing\n  Second-Order Agency in AI Agents",
    "summary": "Current Large Reasoning Models (LRMs) exhibit significant limitations in\nreliability and transparency, often showing a collapse in reasoning\ncapabilities when faced with high-complexity, long-horizon tasks. This\n\"illusion of thinking\" is frequently an artifact of non-agentic, black-box\nevaluation paradigms that fail to cultivate robust problem-solving processes.\nIn response, we introduce The STAR-XAI Protocol (Socratic, Transparent,\nAgentic, Reasoning - for eXplainable Artificial Intelligence), a novel\nmethodology for training and operating verifiably reliable AI agents. Our\nmethod reframes the human-AI interaction as a structured, Socratic dialogue,\ngoverned by an explicit and evolving rulebook, the Consciousness Transfer\nPackage (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc\nstrategic justification and a state-locking Checksum that prevents error\naccumulation, the protocol transforms a powerful but opaque LRM into a\ndisciplined \"Clear Box\" agent. We demonstrate the efficacy of this method\nthrough an exhaustive 25-move case study in the complex strategic game \"Caps i\nCaps\". The agent not only solved the high-complexity puzzle but also\ndemonstrated Second-Order Agency, identifying flaws in its own\nsupervisor-approved plans and adapting its core integrity protocols mid-task.\nThe STAR-XAI Protocol offers a practical pathway to creating AI agents that are\nnot just high-performing, but also transparent, auditable, and trustworthy by\ndesign.",
    "published": "2025-09-22T16:24:17Z",
    "link": "http://arxiv.org/pdf/2509.17978v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO"
    ],
    "authors": [
      "Antoni Guasch",
      "Maria Isabel Valdez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17971v1",
    "title": "Intra-Cluster Mixup: An Effective Data Augmentation Technique for\n  Complementary-Label Learning",
    "summary": "In this paper, we investigate the challenges of complementary-label learning\n(CLL), a specialized form of weakly-supervised learning (WSL) where models are\ntrained with labels indicating classes to which instances do not belong, rather\nthan standard ordinary labels. This alternative supervision is appealing\nbecause collecting complementary labels is generally cheaper and less\nlabor-intensive. Although most existing research in CLL emphasizes the\ndevelopment of novel loss functions, the potential of data augmentation in this\ndomain remains largely underexplored. In this work, we uncover that the\nwidely-used Mixup data augmentation technique is ineffective when directly\napplied to CLL. Through in-depth analysis, we identify that the\ncomplementary-label noise generated by Mixup negatively impacts the performance\nof CLL models. We then propose an improved technique called Intra-Cluster Mixup\n(ICM), which only synthesizes augmented data from nearby examples, to mitigate\nthe noise effect. ICM carries the benefits of encouraging complementary label\nsharing of nearby examples, and leads to substantial performance improvements\nacross synthetic and real-world labeled datasets. In particular, our wide\nspectrum of experimental results on both balanced and imbalanced CLL settings\njustifies the potential of ICM in allying with state-of-the-art CLL algorithms,\nachieving significant accuracy increases of 30% and 10% on MNIST and CIFAR\ndatasets, respectively.",
    "published": "2025-09-22T16:20:41Z",
    "link": "http://arxiv.org/pdf/2509.17971v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Tan-Ha Mai",
      "Hsuan-Tien Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17970v2",
    "title": "Joint Memory Frequency and Computing Frequency Scaling for\n  Energy-efficient DNN Inference",
    "summary": "Deep neural networks (DNNs) have been widely applied in diverse applications,\nbut the problems of high latency and energy overhead are inevitable on\nresource-constrained devices. To address this challenge, most researchers focus\non the dynamic voltage and frequency scaling (DVFS) technique to balance the\nlatency and energy consumption by changing the computing frequency of\nprocessors. However, the adjustment of memory frequency is usually ignored and\nnot fully utilized to achieve efficient DNN inference, which also plays a\nsignificant role in the inference time and energy consumption. In this paper,\nwe first investigate the impact of joint memory frequency and computing\nfrequency scaling on the inference time and energy consumption with a\nmodel-based and data-driven method. Then by combining with the fitting\nparameters of different DNN models, we give a preliminary analysis for the\nproposed model to see the effects of adjusting memory frequency and computing\nfrequency simultaneously. Finally, simulation results in local inference and\ncooperative inference cases further validate the effectiveness of jointly\nscaling the memory frequency and computing frequency to reduce the energy\nconsumption of devices.",
    "published": "2025-09-22T16:20:29Z",
    "link": "http://arxiv.org/pdf/2509.17970v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Yunchu Han",
      "Zhaojun Nan",
      "Sheng Zhou",
      "Zhisheng Niu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17957v1",
    "title": "On the Variational Costs of Changing Our Minds",
    "summary": "The human mind is capable of extraordinary achievements, yet it often appears\nto work against itself. It actively defends its cherished beliefs even in the\nface of contradictory evidence, conveniently interprets information to conform\nto desired narratives, and selectively searches for or avoids information to\nsuit its various purposes. Despite these behaviours deviating from common\nnormative standards for belief updating, we argue that such 'biases' are not\ninherently cognitive flaws, but rather an adaptive response to the significant\npragmatic and cognitive costs associated with revising one's beliefs. This\npaper introduces a formal framework that aims to model the influence of these\ncosts on our belief updating mechanisms.\n  We treat belief updating as a motivated variational decision, where agents\nweigh the perceived 'utility' of a belief against the informational cost\nrequired to adopt a new belief state, quantified by the Kullback-Leibler\ndivergence from the prior to the variational posterior. We perform\ncomputational experiments to demonstrate that simple instantiations of this\nresource-rational model can be used to qualitatively emulate commonplace human\nbehaviours, including confirmation bias and attitude polarisation. In doing so,\nwe suggest that this framework makes steps toward a more holistic account of\nthe motivated Bayesian mechanics of belief change and provides practical\ninsights for predicting, compensating for, and correcting deviations from\ndesired belief updating processes.",
    "published": "2025-09-22T16:13:06Z",
    "link": "http://arxiv.org/pdf/2509.17957v1.pdf",
    "category": [
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "authors": [
      "David Hyland",
      "Mahault Albarracin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17956v1",
    "title": "\"I think this is fair'': Uncovering the Complexities of Stakeholder\n  Decision-Making in AI Fairness Assessment",
    "summary": "Assessing fairness in artificial intelligence (AI) typically involves AI\nexperts who select protected features, fairness metrics, and set fairness\nthresholds. However, little is known about how stakeholders, particularly those\naffected by AI outcomes but lacking AI expertise, assess fairness. To address\nthis gap, we conducted a qualitative study with 30 stakeholders without AI\nexpertise, representing potential decision subjects in a credit rating\nscenario, to examine how they assess fairness when placed in the role of\ndeciding on features with priority, metrics, and thresholds. We reveal that\nstakeholders' fairness decisions are more complex than typical AI expert\npractices: they considered features far beyond legally protected features,\ntailored metrics for specific contexts, set diverse yet stricter fairness\nthresholds, and even preferred designing customized fairness. Our results\nextend the understanding of how stakeholders can meaningfully contribute to AI\nfairness governance and mitigation, underscoring the importance of\nincorporating stakeholders' nuanced fairness judgments.",
    "published": "2025-09-22T16:12:12Z",
    "link": "http://arxiv.org/pdf/2509.17956v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Lin Luo",
      "Yuri Nakao",
      "Mathieu Chollet",
      "Hiroya Inakoshi",
      "Simone Stumpf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17946v1",
    "title": "HICode: Hierarchical Inductive Coding with LLMs",
    "summary": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data.",
    "published": "2025-09-22T16:07:11Z",
    "link": "http://arxiv.org/pdf/2509.17946v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Mian Zhong",
      "Pristina Wang",
      "Anjalie Field"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17942v1",
    "title": "StefaLand: An Efficient Geoscience Foundation Model That Improves\n  Dynamic Land-Surface Predictions",
    "summary": "Stewarding natural resources, mitigating floods, droughts, wildfires, and\nlandslides, and meeting growing demands require models that can predict\nclimate-driven land-surface responses and human feedback with high accuracy.\nTraditional impact models, whether process-based, statistical, or machine\nlearning, struggle with spatial generalization due to limited observations and\nconcept drift. Recently proposed vision foundation models trained on satellite\nimagery demand massive compute and are ill-suited for dynamic land-surface\nprediction. We introduce StefaLand, a generative spatiotemporal earth\nfoundation model centered on landscape interactions. StefaLand improves\npredictions on three tasks and four datasets: streamflow, soil moisture, and\nsoil composition, compared to prior state-of-the-art. Results highlight its\nability to generalize across diverse, data-scarce regions and support broad\nland-surface applications. The model builds on a masked autoencoder backbone\nthat learns deep joint representations of landscape attributes, with a\nlocation-aware architecture fusing static and time-series inputs,\nattribute-based representations that drastically reduce compute, and residual\nfine-tuning adapters that enhance transfer. While inspired by prior methods,\ntheir alignment with geoscience and integration in one model enables robust\nperformance on dynamic land-surface tasks. StefaLand can be pretrained and\nfinetuned on academic compute yet outperforms state-of-the-art baselines and\neven fine-tuned vision foundation models. To our knowledge, this is the first\ngeoscience land-surface foundation model that demonstrably improves dynamic\nland-surface interaction predictions and supports diverse downstream\napplications.",
    "published": "2025-09-22T16:05:45Z",
    "link": "http://arxiv.org/pdf/2509.17942v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nicholas Kraabel",
      "Jiangtao Liu",
      "Yuchen Bian",
      "Daniel Kifer",
      "Chaopeng Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17941v1",
    "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments\n  via Composable Diffusion",
    "summary": "This paper considers the problem of enabling robots to navigate dynamic\nenvironments while following instructions. The challenge lies in the\ncombinatorial nature of instruction specifications: each instruction can\ninclude multiple specifications, and the number of possible specification\ncombinations grows exponentially as the robot's skill set expands. For example,\n\"overtake the pedestrian while staying on the right side of the road\" consists\nof two specifications: \"overtake the pedestrian\" and \"walk on the right side of\nthe road.\" To tackle this challenge, we propose ComposableNav, based on the\nintuition that following an instruction involves independently satisfying its\nconstituent specifications, each corresponding to a distinct motion primitive.\nUsing diffusion models, ComposableNav learns each primitive separately, then\ncomposes them in parallel at deployment time to satisfy novel combinations of\nspecifications unseen in training. Additionally, to avoid the onerous need for\ndemonstrations of individual motion primitives, we propose a two-stage training\nprocedure: (1) supervised pre-training to learn a base diffusion model for\ndynamic navigation, and (2) reinforcement learning fine-tuning that molds the\nbase model into different motion primitives. Through simulation and real-world\nexperiments, we show that ComposableNav enables robots to follow instructions\nby generating trajectories that satisfy diverse and unseen combinations of\nspecifications, significantly outperforming both non-compositional VLM-based\npolicies and costmap composing baselines. Videos and additional materials can\nbe found on the project page: https://amrl.cs.utexas.edu/ComposableNav/",
    "published": "2025-09-22T16:04:50Z",
    "link": "http://arxiv.org/pdf/2509.17941v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zichao Hu",
      "Chen Tang",
      "Michael J. Munje",
      "Yifeng Zhu",
      "Alex Liu",
      "Shuijing Liu",
      "Garrett Warnell",
      "Peter Stone",
      "Joydeep Biswas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17930v1",
    "title": "Transformer-Encoder Trees for Efficient Multilingual Machine Translation\n  and Speech Translation",
    "summary": "Multilingual translation faces challenges of computational redundancy and\nlimited accuracy for low-resource languages, especially in speech translation.\nTo address this, we propose a novel hierarchical Transformer Encoder Tree (TET)\ncombined with non-autoregressive encoder-only models trained with Connectionist\nTemporal Classification for multilingual translation. By sharing intermediate\nrepresentations among linguistically similar target languages, TET can improve\naccuracy on low-resource languages, reduce computational redundancy, and allow\ngenerating all target languages in a single forward pass, thus eliminating\nsequential bottlenecks and improving parallelism. For speech translation,\ncombining TET with a non-autoregressive speech recognition backbone (wav2vec2)\nshows promising results in terms of translation quality compared to\nautoregressive systems while being 7-14 times faster.",
    "published": "2025-09-22T15:52:18Z",
    "link": "http://arxiv.org/pdf/2509.17930v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yiwen Guan",
      "Jacob Whitehill"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17917v1",
    "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent",
    "summary": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities.",
    "published": "2025-09-22T15:40:31Z",
    "link": "http://arxiv.org/pdf/2509.17917v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Junyu Lu",
      "Songxin Zhang",
      "Zejian Xie",
      "Zhuoyang Song",
      "Jiaxing Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17907v1",
    "title": "MEF: A Systematic Evaluation Framework for Text-to-Image Models",
    "summary": "Rapid advances in text-to-image (T2I) generation have raised higher\nrequirements for evaluation methodologies. Existing benchmarks center on\nobjective capabilities and dimensions, but lack an application-scenario\nperspective, limiting external validity. Moreover, current evaluations\ntypically rely on either ELO for overall ranking or MOS for dimension-specific\nscoring, yet both methods have inherent shortcomings and limited\ninterpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),\na systematic and practical approach for evaluating T2I models. First, we\npropose a structured taxonomy encompassing user scenarios, elements, element\ncompositions, and text expression forms to construct the Magic-Bench-377, which\nsupports label-level assessment and ensures a balanced coverage of both user\nscenarios and capabilities. On this basis, we combine ELO and\ndimension-specific MOS to generate model rankings and fine-grained assessments\nrespectively. This joint evaluation method further enables us to quantitatively\nanalyze the contribution of each dimension to user satisfaction using\nmultivariate logistic regression. By applying MEF to current T2I models, we\nobtain a leaderboard and key characteristics of the leading models. We release\nour evaluation framework and make Magic-Bench-377 fully open-source to advance\nresearch in the evaluation of visual generative models.",
    "published": "2025-09-22T15:32:42Z",
    "link": "http://arxiv.org/pdf/2509.17907v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xiaojing Dong",
      "Weilin Huang",
      "Liang Li",
      "Yiying Li",
      "Shu Liu",
      "Tongtong Ou",
      "Shuang Ouyang",
      "Yu Tian",
      "Fengxuan Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17905v2",
    "title": "Mitigating Strategy-Selection Bias in Reasoning for More Effective\n  Test-Time Scaling",
    "summary": "Test-time scaling (TTS) has been shown to improve the performance of large\nlanguage models (LLMs) by sampling and aggregating diverse reasoning paths.\nHowever, existing research has overlooked a critical issue: selection bias of\nreasoning strategies during scaling. Specifically, when generating reasoning\nprocesses, LLMs tend to follow certain strategies (e.g., algebraic solutions\nfor math problems) while neglecting other valid alternatives (e.g., geometric\nsolutions), resulting in insufficient exploration of the solution space. To\nfurther understand the impact of this bias, we present a theoretical analysis\nthat reveals when it undermines the effectiveness of test-time scaling.\nMotivated by this theoretical insight, we introduce TTS-Uniform, a framework\ndesigned to mitigate the selection bias of reasoning strategies. It (i)\nidentifies potential strategies, (ii) uniformly allocates the sampling budget\nacross them, and (iii) filters out unstable strategies prior to aggregation.\nExperimental results show that TTS-Uniform significantly enhances scaling\neffectiveness across multiple mainstream LLMs and benchmark datasets.",
    "published": "2025-09-22T15:30:56Z",
    "link": "http://arxiv.org/pdf/2509.17905v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zongqian Wu",
      "Baoduo Xu",
      "Tianyu Li",
      "Zhu Sun",
      "Xiaofeng Zhu",
      "Lei Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18272v1",
    "title": "StereoFoley: Object-Aware Stereo Audio Generation from Video",
    "summary": "We present StereoFoley, a video-to-audio generation framework that produces\nsemantically aligned, temporally synchronized, and spatially accurate stereo\nsound at 48 kHz. While recent generative video-to-audio models achieve strong\nsemantic and temporal fidelity, they largely remain limited to mono or fail to\ndeliver object-aware stereo imaging, constrained by the lack of professionally\nmixed, spatially accurate video-to-audio datasets. First, we develop and train\na base model that generates stereo audio from video, achieving state-of-the-art\nin both semantic accuracy and synchronization. Next, to overcome dataset\nlimitations, we introduce a synthetic data generation pipeline that combines\nvideo analysis, object tracking, and audio synthesis with dynamic panning and\ndistance-based loudness controls, enabling spatially accurate object-aware\nsound. Finally, we fine-tune the base model on this synthetic dataset, yielding\nclear object-audio correspondence. Since no established metrics exist, we\nintroduce stereo object-awareness measures and validate it through a human\nlistening study, showing strong correlation with perception. This work\nestablishes the first end-to-end framework for stereo object-aware\nvideo-to-audio generation, addressing a critical gap and setting a new\nbenchmark in the field.",
    "published": "2025-09-22T18:00:54Z",
    "link": "http://arxiv.org/pdf/2509.18272v1.pdf",
    "category": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Tornike Karchkhadze",
      "Kuan-Lin Chen",
      " Mojtaba",
      " Heydari",
      "Robert Henzel",
      "Alessandro Toso",
      "Mehrez Souden",
      "Joshua Atkins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17901v1",
    "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?",
    "summary": "Modern multimodal large language models often claim \"video understanding,\"\nyet most evaluations use muted videos or simply discard audio. We ask a direct\nquestion: how much does audio actually matter for contemporary Video-LLMs and\nthe benchmarks that certify them? We audit widely used suites and observe that\nmany items are even solvable from a single frame, rendering audio largely\nredundant. Building on LLaVA-OneVision architecture, we attach a speech/audio\nencoder (e.g., Whisper) and analyze when audio helps, while addressing audio\ntoken explosion with a lightweight Mamba-based state-space token compressor. We\nfind that audio yields minimal gains on recent video benchmarks but is decisive\non curated, audio-sensitive subsets. To enable faithful evaluation, we release\nAVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a\ngrowing gap between current academic practice and real-world expectations, and\nprovide practical tools for scalable audio-visual Video-LLMs. We will fully\nopen-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.",
    "published": "2025-09-22T15:28:54Z",
    "link": "http://arxiv.org/pdf/2509.17901v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "authors": [
      "Geewook Kim",
      "Minjoon Seo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17421v1",
    "title": "RealBench: A Chinese Multi-image Understanding Benchmark Close to\n  Real-world Scenarios",
    "summary": "While various multimodal multi-image evaluation datasets have been emerged,\nbut these datasets are primarily based on English, and there has yet to be a\nChinese multi-image dataset. To fill this gap, we introduce RealBench, the\nfirst Chinese multimodal multi-image dataset, which contains 9393 samples and\n69910 images. RealBench distinguishes itself by incorporating real\nuser-generated content, ensuring high relevance to real-world applications.\nAdditionally, the dataset covers a wide variety of scenes, image resolutions,\nand image structures, further increasing the difficulty of multi-image\nunderstanding. Ultimately, we conduct a comprehensive evaluation of RealBench\nusing 21 multimodal LLMs of different sizes, including closed-source models\nthat support multi-image inputs as well as open-source visual and video models.\nThe experimental results indicate that even the most powerful closed-source\nmodels still face challenges when handling multi-image Chinese scenarios.\nMoreover, there remains a noticeable performance gap of around 71.8\\% on\naverage between open-source visual/video models and closed-source models. These\nresults show that RealBench provides an important research foundation for\nfurther exploring multi-image understanding capabilities in the Chinese\ncontext.",
    "published": "2025-09-22T07:14:31Z",
    "link": "http://arxiv.org/pdf/2509.17421v1.pdf",
    "category": [
      "cs.CL",
      "cs.MM"
    ],
    "authors": [
      "Fei Zhao",
      "Chengqiang Lu",
      "Yufan Shen",
      "Qimeng Wang",
      "Yicheng Qian",
      "Haoxin Zhang",
      "Yan Gao",
      "Yi Wu",
      "Yao Hu",
      "Zhen Wu",
      "Shangyu Xing",
      "Xinyu Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17336v1",
    "title": "Mano Report",
    "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
    "published": "2025-09-22T03:13:58Z",
    "link": "http://arxiv.org/pdf/2509.17336v1.pdf",
    "category": [
      "cs.MM",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Tianyu Fu",
      "Anyang Su",
      "Chenxu Zhao",
      "Hanning Wang",
      "Minghui Wu",
      "Zhe Yu",
      "Fei Hu",
      "Mingjia Shi",
      "Wei Dong",
      "Jiayao Wang",
      "Yuyang Chen",
      "Ruiyang Yu",
      "Siran Peng",
      "Menglin Li",
      "Nan Huang",
      "Haitian Wei",
      "Jiawei Yu",
      "Yi Xin",
      "Xilin Zhao",
      "Kai Gu",
      "Ping Jiang",
      "Sifan Zhou",
      "Shuo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18097v1",
    "title": "Preconditioned Deformation Grids",
    "summary": "Dynamic surface reconstruction of objects from point cloud sequences is a\nchallenging field in computer graphics. Existing approaches either require\nmultiple regularization terms or extensive training data which, however, lead\nto compromises in reconstruction accuracy as well as over-smoothing or poor\ngeneralization to unseen objects and motions. To address these lim- itations,\nwe introduce Preconditioned Deformation Grids, a novel technique for estimating\ncoherent deformation fields directly from unstructured point cloud sequences\nwithout requiring or forming explicit correspondences. Key to our approach is\nthe use of multi-resolution voxel grids that capture the overall motion at\nvarying spatial scales, enabling a more flexible deformation representation. In\nconjunction with incorporating grid-based Sobolev preconditioning into\ngradient-based optimization, we show that applying a Chamfer loss between the\ninput point clouds as well as to an evolving template mesh is sufficient to\nobtain accurate deformations. To ensure temporal consistency along the object\nsurface, we include a weak isometry loss on mesh edges which complements the\nmain objective without constraining deformation fidelity. Extensive evaluations\ndemonstrate that our method achieves superior results, particularly for long\nsequences, compared to state-of-the-art techniques.",
    "published": "2025-09-22T17:59:55Z",
    "link": "http://arxiv.org/pdf/2509.18097v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Julian Kaltheuner",
      "Alexander Oebel",
      "Hannah Droege",
      "Patrick Stotko",
      "Reinhard Klein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17985v1",
    "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video\n  Diffusion Models",
    "summary": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing\nhigh-quality 3D scene videos from coarse geometry, a camera trajectory, and a\nreference image. Our approach streamlines the 3D graphic design workflow,\nenabling flexible design exploration and rapid production of deliverables. A\nstraightforward approach to synthesizing a video from coarse geometry might\ncondition a video diffusion model on geometric structure. However, existing\nvideo diffusion models struggle to generate high-fidelity results for complex\nscenes due to the difficulty of jointly modeling visual quality, motion, and\ntemporal consistency. To address this, we propose a generative framework that\nleverages the complementary strengths of image and video diffusion models.\nSpecifically, our framework consists of a Sparse Anchor-view Generation (SAG)\nand a Geometry-guided Generative Inbetweening (GGI) module. The SAG module\ngenerates high-quality, cross-view consistent anchor views using an image\ndiffusion model, aided by Sparse Appearance-guided Sampling. Building on these\nanchor views, GGI module faithfully interpolates intermediate frames using a\nvideo diffusion model, enhanced by flow-based camera control and structural\nguidance. Notably, both modules operate without any paired dataset of 3D scene\nmodels and natural images, which is extremely difficult to obtain.\nComprehensive experiments show that our method produces high-quality,\nstyle-consistent scene videos under diverse and challenging scenarios,\noutperforming simple and extended baselines.",
    "published": "2025-09-22T16:28:47Z",
    "link": "http://arxiv.org/pdf/2509.17985v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Geonung Kim",
      "Janghyeok Han",
      "Sunghyun Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17979v1",
    "title": "Towards Seeing Bones at Radio Frequency",
    "summary": "Wireless sensing literature has long aspired to achieve X-ray-like vision at\nradio frequencies. Yet, state-of-the-art wireless sensing literature has yet to\ngenerate the archetypal X-ray image: one of the bones beneath flesh. In this\npaper, we explore MCT, a penetration-based RF-imaging system for imaging bones\nat mm-resolution, one that significantly exceeds prior penetration-based RF\nimaging literature. Indeed the long wavelength, significant attenuation and\ncomplex diffraction that occur as RF propagates through flesh, have long\nlimited imaging resolution (to several centimeters at best). We address these\nconcerns through a novel penetration-based synthetic aperture algorithm,\ncoupled with a learning-based pipeline to correct for diffraction-induced\nartifacts. A detailed evaluation of meat models demonstrates a resolution\nimprovement from sub-decimeter to sub-centimeter over prior art in RF\npenetrative imaging.",
    "published": "2025-09-22T16:24:36Z",
    "link": "http://arxiv.org/pdf/2509.17979v1.pdf",
    "category": [
      "cs.GR",
      "cs.ET",
      "cs.LG"
    ],
    "authors": [
      "Yiwen Song",
      "Hongyang Li",
      "Kuang Yuan",
      "Ran Bi",
      "Swarun Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17974v1",
    "title": "A Comparative Study of Different Edit Distance-Based Methods for Feature\n  Tracking using Merge Trees on Time-Varying Scalar Fields",
    "summary": "Feature tracking in time-varying scalar fields is a fundamental task in\nscientific computing. Topological descriptors, which summarize important\nfeatures of data, have proved to be viable tools to facilitate this task. The\nmerge tree is a topological descriptor that captures the connectivity behaviors\nof the sub- or superlevel sets of a scalar field. Edit distances between merge\ntrees play a vital role in effective temporal data tracking. Existing methods\nto compute them fall into two main classes, namely whether they are dependent\nor independent of the branch decomposition. These two classes represent the\nmost prominent approaches for producing tracking results. In this paper, we\ncompare four different merge tree edit distance-based methods for feature\ntracking. We demonstrate that these methods yield distinct results with both\nanalytical and real-world data sets. Furthermore, we investigate how these\nresults vary and identify the factors that influence them. Our experiments\nreveal significant differences in tracked features over time, even among those\nproduced by techniques within the same category.",
    "published": "2025-09-22T16:22:47Z",
    "link": "http://arxiv.org/pdf/2509.17974v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Son Le Thanh",
      "Tino Weinkauf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17803v1",
    "title": "Effect of Appearance and Animation Realism on the Perception of\n  Emotionally Expressive Virtual Humans",
    "summary": "3D Virtual Human technology is growing with several potential applications in\nhealth, education, business and telecommunications. Investigating the\nperception of these virtual humans can help guide to develop better and more\neffective applications. Recent developments show that the appearance of the\nvirtual humans reached to a very realistic level. However, there is not yet\nadequate analysis on the perception of appearance and animation realism for\nemotionally expressive virtual humans. In this paper, we designed a user\nexperiment and analyzed the effect of a realistic virtual human's appearance\nrealism and animation realism in varying emotion conditions. We found that\nhigher appearance realism and higher animation realism leads to higher social\npresence and higher attractiveness ratings. We also found significant effects\nof animation realism on perceived realism and emotion intensity levels. Our\nstudy sheds light into how appearance and animation realism effects the\nperception of highly realistic virtual humans in emotionally expressive\nscenarios and points out to future directions.",
    "published": "2025-09-22T13:59:14Z",
    "link": "http://arxiv.org/pdf/2509.17803v1.pdf",
    "category": [
      "cs.GR",
      "cs.HC"
    ],
    "authors": [
      "Nabila Amadou",
      "Kazi Injamamul Haque",
      "Zerrin Yumak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17755v1",
    "title": "Learning Neural Antiderivatives",
    "summary": "Neural fields offer continuous, learnable representations that extend beyond\ntraditional discrete formats in visual computing. We study the problem of\nlearning neural representations of repeated antiderivatives directly from a\nfunction, a continuous analogue of summed-area tables. Although widely used in\ndiscrete domains, such cumulative schemes rely on grids, which prevents their\napplicability in continuous neural contexts. We introduce and analyze a range\nof neural methods for repeated integration, including both adaptations of prior\nwork and novel designs. Our evaluation spans multiple input dimensionalities\nand integration orders, assessing both reconstruction quality and performance\nin downstream tasks such as filtering and rendering. These results enable\nintegrating classical cumulative operators into modern neural systems and offer\ninsights into learning tasks involving differential and integral operators.",
    "published": "2025-09-22T13:19:07Z",
    "link": "http://arxiv.org/pdf/2509.17755v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Fizza Rubab",
      "Ntumba Elie Nsampi",
      "Martin Balint",
      "Felix Mujkanovic",
      "Hans-Peter Seidel",
      "Tobias Ritschel",
      "Thomas Leimkühler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17748v1",
    "title": "\"I don't like my avatar\": Investigating Human Digital Doubles",
    "summary": "Creating human digital doubles is becoming easier and much more accessible to\neveryone using consumer grade devices. In this work, we investigate how avatar\nstyle (realistic vs cartoon) and avatar familiarity (self, acquaintance,\nunknown person) affect self/other-identification, perceived realism, affinity\nand social presence with a controlled offline experiment. We created two styles\nof avatars (realistic-looking MetaHumans and cartoon-looking ReadyPlayerMe\navatars) and facial animations stimuli for them using performance capture.\nQuestionnaire responses demonstrate that higher appearance realism leads to a\nhigher level of identification, perceived realism and social presence. However,\navatars with familiar faces, especially those with high appearance realism,\nlead to a lower level of identification, perceived realism, and affinity.\nAlthough participants identified their digital doubles as their own, they\nconsistently did not like their avatars, especially of realistic appearance.\nBut they were less critical and more forgiving about their acquaintance's or an\nunknown person's digital double.",
    "published": "2025-09-22T13:11:28Z",
    "link": "http://arxiv.org/pdf/2509.17748v1.pdf",
    "category": [
      "cs.GR",
      "cs.HC"
    ],
    "authors": [
      "Siyi Liu",
      "Kazi Injamamul Haque",
      "Zerrin Yumak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18466v1",
    "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion\n  over Challenging Terrain",
    "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.",
    "published": "2025-09-22T22:43:07Z",
    "link": "http://arxiv.org/pdf/2509.18466v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Junnosuke Kamohara",
      "Feiyang Wu",
      "Chinmayee Wamorkar",
      "Seth Hutchinson",
      "Ye Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18463v1",
    "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in\n  Reinforcement Learning During a Liquid Pouring Task",
    "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.",
    "published": "2025-09-22T22:35:08Z",
    "link": "http://arxiv.org/pdf/2509.18463v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Jannick van Buuren",
      "Roberto Giglio",
      "Loris Roveda",
      "Luka Peternel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18460v1",
    "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot\n  Perception Systems",
    "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.",
    "published": "2025-09-22T22:29:33Z",
    "link": "http://arxiv.org/pdf/2509.18460v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Haeyoon Han",
      "Mahdi Taheri",
      "Soon-Jo Chung",
      "Fred Y. Hadaegh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18455v1",
    "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous\n  Hands",
    "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.",
    "published": "2025-09-22T22:25:35Z",
    "link": "http://arxiv.org/pdf/2509.18455v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yunshuang Li",
      "Yiyang Ling",
      "Gaurav S. Sukhatme",
      "Daniel Seita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18428v1",
    "title": "Latent Action Pretraining Through World Modeling",
    "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.",
    "published": "2025-09-22T21:19:10Z",
    "link": "http://arxiv.org/pdf/2509.18428v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Bahey Tharwat",
      "Yara Nasser",
      "Ali Abouzeid",
      "Ian Reid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18384v1",
    "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot\n  Planning from Formal Methods Feedback",
    "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.",
    "published": "2025-09-22T20:14:32Z",
    "link": "http://arxiv.org/pdf/2509.18384v1.pdf",
    "category": [
      "cs.RO",
      "cs.FL"
    ],
    "authors": [
      "Yunhao Yang",
      "Junyuan Hong",
      "Gabriel Jacob Perin",
      "Zhiwen Fan",
      "Li Yin",
      "Zhangyang Wang",
      "Ufuk Topcu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18371v1",
    "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear\n  Multi-Agent Games",
    "summary": "Multi-agent games in dynamic nonlinear settings are challenging due to the\ntime-varying interactions among the agents and the non-stationarity of the\n(potential) Nash equilibria. In this paper we consider model-free games, where\nagent transitions and costs are observed without knowledge of the transition\nand cost functions that generate them. We propose a policy gradient approach to\nlearn distributed policies that follow the communication structure in\nmulti-team games, with multiple agents per team. Our formulation is inspired by\nthe structure of distributed policies in linear quadratic games, which take the\nform of time-varying linear feedback gains. In the nonlinear case, we model the\npolicies as nonlinear feedback gains, parameterized by self-attention layers to\naccount for the time-varying multi-agent communication topology. We demonstrate\nthat our distributed policy gradient approach achieves strong performance in\nseveral settings, including distributed linear and nonlinear regulation, and\nsimulated and real multi-robot pursuit-and-evasion games.",
    "published": "2025-09-22T19:52:16Z",
    "link": "http://arxiv.org/pdf/2509.18371v1.pdf",
    "category": [
      "eess.SY",
      "cs.MA",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Eduardo Sebastián",
      "Maitrayee Keskar",
      "Eeman Iqbal",
      "Eduardo Montijano",
      "Carlos Sagüés",
      "Nikolay Atanasov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18350v1",
    "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic\n  Geodata",
    "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.",
    "published": "2025-09-22T19:22:32Z",
    "link": "http://arxiv.org/pdf/2509.18350v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Oussema Dhaouadi",
      "Riccardo Marin",
      "Johannes Meier",
      "Jacques Kaiser",
      "Daniel Cremers"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18342v1",
    "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
    "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.",
    "published": "2025-09-22T19:04:31Z",
    "link": "http://arxiv.org/pdf/2509.18342v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Rajitha de Silva",
      "Jonathan Cox",
      "James R. Heselden",
      "Marija Popovic",
      "Cesar Cadena",
      "Riccardo Polvara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18330v1",
    "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital\n  Terrain for Mars 2020",
    "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.",
    "published": "2025-09-22T18:51:19Z",
    "link": "http://arxiv.org/pdf/2509.18330v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Marsette Vona"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18327v1",
    "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation",
    "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.",
    "published": "2025-09-22T18:49:56Z",
    "link": "http://arxiv.org/pdf/2509.18327v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Katherine H. Allen",
      "Chris Rogers",
      "Elaine S. Short"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18311v1",
    "title": "Fine-Tuning Robot Policies While Maintaining User Privacy",
    "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.",
    "published": "2025-09-22T18:36:25Z",
    "link": "http://arxiv.org/pdf/2509.18311v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Benjamin A. Christie",
      "Sagar Parekh",
      "Dylan P. Losey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18084v2",
    "title": "ByteWrist: A Parallel Robotic Wrist Enabling Flexible and\n  Anthropomorphic Motion for Confined Spaces",
    "summary": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic\nparallel wrist for robotic manipulation. ByteWrist addresses the critical\nlimitations of existing serial and parallel wrists in narrow-space operations\nthrough a compact three-stage parallel drive mechanism integrated with\narc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)\nmotion while maintaining exceptional compactness, making it particularly\nsuitable for complex unstructured environments such as home services, medical\nassistance, and precision assembly. The key innovations include: (1) a nested\nthree-stage motor-driven linkages that minimize volume while enabling\nindependent multi-DOF control, (2) arc-shaped end linkages that optimize force\ntransmission and expand motion range, and (3) a central supporting ball\nfunctioning as a spherical joint that enhances structural stiffness without\ncompromising flexibility. Meanwhile, we present comprehensive kinematic\nmodeling including forward / inverse kinematics and a numerical Jacobian\nsolution for precise control. Empirically, we observe ByteWrist demonstrates\nstrong performance in narrow-space maneuverability and dual-arm cooperative\nmanipulation tasks, outperforming Kinova-based systems. Results indicate\nsignificant improvements in compactness, efficiency, and stiffness compared to\ntraditional designs, establishing ByteWrist as a promising solution for\nnext-generation robotic manipulation in constrained environments.",
    "published": "2025-09-22T17:57:07Z",
    "link": "http://arxiv.org/pdf/2509.18084v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiawen Tian",
      "Liqun Huang",
      "Zhongren Cui",
      "Jingchao Qiao",
      "Jiafeng Xu",
      "Xiao Ma",
      "Zeyu Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18068v1",
    "title": "RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point\n  Clouds",
    "summary": "Millimeter-wave radar provides perception robust to fog, smoke, dust, and low\nlight, making it attractive for size, weight, and power constrained robotic\nplatforms. Current radar imaging methods, however, rely on synthetic aperture\nor multi-frame aggregation to improve resolution, which is impractical for\nsmall aerial, inspection, or wearable systems. We present RadarSFD, a\nconditional latent diffusion framework that reconstructs dense LiDAR-like point\nclouds from a single radar frame without motion or SAR. Our approach transfers\ngeometric priors from a pretrained monocular depth estimator into the diffusion\nbackbone, anchors them to radar inputs via channel-wise latent concatenation,\nand regularizes outputs with a dual-space objective combining latent and\npixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer\nDistance and 28 cm Modified Hausdorff Distance, improving over the single-frame\nRadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame\nmethods using 5-41 frames. Qualitative results show recovery of fine walls and\nnarrow gaps, and experiments across new environments confirm strong\ngeneralization. Ablation studies highlight the importance of pretrained\ninitialization, radar BEV conditioning, and the dual-space loss. Together,\nthese results establish the first practical single-frame, no-SAR mmWave radar\npipeline for dense point cloud perception in compact robotic systems.",
    "published": "2025-09-22T17:49:00Z",
    "link": "http://arxiv.org/pdf/2509.18068v1.pdf",
    "category": [
      "cs.RO",
      "eess.SP"
    ],
    "authors": [
      "Bin Zhao",
      "Nakul Garg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18053v3",
    "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multimodal Large Language Models and Graph-of-Thoughts",
    "summary": "Current state-of-the-art autonomous vehicles could face safety-critical\nsituations when their local sensors are occluded by large nearby objects on the\nroad. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed\nas a means of addressing this problem, and one recently introduced framework\nfor cooperative autonomous driving has further adopted an approach that\nincorporates a Multimodal Large Language Model (MLLM) to integrate cooperative\nperception and planning processes. However, despite the potential benefit of\napplying graph-of-thoughts reasoning to the MLLM, this idea has not been\nconsidered by previous cooperative autonomous driving research. In this paper,\nwe propose a novel graph-of-thoughts framework specifically designed for\nMLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our\nproposed novel ideas of occlusion-aware perception and planning-aware\nprediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for\ntraining and testing the cooperative driving graph-of-thoughts. Our\nexperimental results show that our method outperforms other baselines in\ncooperative perception, prediction, and planning tasks. Our project website:\nhttps://eddyhkchiu.github.io/v2vgot.github.io/ .",
    "published": "2025-09-22T17:27:29Z",
    "link": "http://arxiv.org/pdf/2509.18053v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hsu-kuang Chiu",
      "Ryo Hachiuma",
      "Chien-Yi Wang",
      "Yu-Chiang Frank Wang",
      "Min-Hung Chen",
      "Stephen F. Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18043v1",
    "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States",
    "summary": "Imitation learning (IL) has proven effective across a wide range of\nmanipulation tasks. However, IL policies often struggle when faced with\nout-of-distribution observations; for instance, when the target object is in a\npreviously unseen position or occluded by other objects. In these cases,\nextensive demonstrations are needed for current IL methods to reach robust and\ngeneralizable behaviors. But when humans are faced with these sorts of atypical\ninitial states, we often rearrange the environment for more favorable task\nexecution. For example, a person might rotate a coffee cup so that it is easier\nto grasp the handle, or push a box out of the way so they can directly grasp\ntheir target object. In this work we seek to equip robot learners with the same\ncapability: enabling robots to prepare the environment before executing their\ngiven policy. We propose ReSET, an algorithm that takes initial states -- which\nare outside the policy's distribution -- and autonomously modifies object poses\nso that the restructured scene is similar to training data. Theoretically, we\nshow that this two step process (rearranging the environment before rolling out\nthe given policy) reduces the generalization gap. Practically, our ReSET\nalgorithm combines action-agnostic human videos with task-agnostic\nteleoperation data to i) decide when to modify the scene, ii) predict what\nsimplifying actions a human would take, and iii) map those predictions into\nrobot action primitives. Comparisons with diffusion policies, VLAs, and other\nbaselines show that using ReSET to prepare the environment enables more robust\ntask execution with equal amounts of total training data. See videos at our\nproject website: https://reset2025paper.github.io/",
    "published": "2025-09-22T17:18:52Z",
    "link": "http://arxiv.org/pdf/2509.18043v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Yinlong Dai",
      "Andre Keyser",
      "Dylan P. Losey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18005v1",
    "title": "M3ET: Efficient Vision-Language Learning for Robotics based on\n  Multimodal Mamba-Enhanced Transformer",
    "summary": "In recent years, multimodal learning has become essential in robotic vision\nand information fusion, especially for understanding human behavior in complex\nenvironments. However, current methods struggle to fully leverage the textual\nmodality, relying on supervised pretrained models, which limits semantic\nextraction in unsupervised robotic environments, particularly with significant\nmodality loss. These methods also tend to be computationally intensive, leading\nto high resource consumption in real-world applications. To address these\nchallenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a\nlightweight model designed for efficient multimodal learning, particularly on\nmobile platforms. By incorporating the Mamba module and a semantic-based\nadaptive attention mechanism, M3ET optimizes feature fusion, alignment, and\nmodality reconstruction. Our experiments show that M3ET improves cross-task\nperformance, with a 2.3 times increase in pretraining inference speed. In\nparticular, the core VQA task accuracy of M3ET remains at 0.74, while the\nmodel's parameter count is reduced by 0.67. Although performance on the EQA\ntask is limited, M3ET's lightweight design makes it well suited for deployment\non resource-constrained robotic platforms.",
    "published": "2025-09-22T16:44:34Z",
    "link": "http://arxiv.org/pdf/2509.18005v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yanxin Zhang",
      "Liang He",
      "Zeyi Kang",
      "Zuheng Ming",
      "Kaixing Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17952v1",
    "title": "Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller\n  Tuning with Digital Twins",
    "summary": "We propose a \\textit{guided multi-fidelity Bayesian optimization} framework\nfor data-efficient controller tuning that integrates corrected digital twin\n(DT) simulations with real-world measurements. The method targets closed-loop\nsystems with limited-fidelity simulations or inexpensive approximations. To\naddress model mismatch, we build a multi-fidelity surrogate with a learned\ncorrection model that refines DT estimates from real data. An adaptive\ncost-aware acquisition function balances expected improvement, fidelity, and\nsampling cost. Our method ensures adaptability as new measurements arrive. The\naccuracy of DTs is re-estimated, dynamically adapting both cross-source\ncorrelations and the acquisition function. This ensures that accurate DTs are\nused more frequently, while inaccurate DTs are appropriately downweighted.\nExperiments on robotic drive hardware and supporting numerical studies\ndemonstrate that our method enhances tuning efficiency compared to standard\nBayesian optimization (BO) and multi-fidelity methods.",
    "published": "2025-09-22T16:10:26Z",
    "link": "http://arxiv.org/pdf/2509.17952v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Mahdi Nobar",
      "Jürg Keller",
      "Alessandro Forino",
      "John Lygeros",
      "Alisa Rupenyan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17940v1",
    "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous\n  Driving",
    "summary": "End-to-end autonomous driving has substantially progressed by directly\npredicting future trajectories from raw perception inputs, which bypasses\ntraditional modular pipelines. However, mainstream methods trained via\nimitation learning suffer from critical safety limitations, as they fail to\ndistinguish between trajectories that appear human-like but are potentially\nunsafe. Some recent approaches attempt to address this by regressing multiple\nrule-driven scores but decoupling supervision from policy optimization,\nresulting in suboptimal performance. To tackle these challenges, we propose\nDriveDPO, a Safety Direct Preference Optimization Policy Learning framework.\nFirst, we distill a unified policy distribution from human imitation similarity\nand rule-based safety scores for direct policy optimization. Further, we\nintroduce an iterative Direct Preference Optimization stage formulated as\ntrajectory-level preference alignment. Extensive experiments on the NAVSIM\nbenchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of\n90.0. Furthermore, qualitative results across diverse challenging scenarios\nhighlight DriveDPO's ability to produce safer and more reliable driving\nbehaviors.",
    "published": "2025-09-22T16:01:11Z",
    "link": "http://arxiv.org/pdf/2509.17940v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Shuyao Shang",
      "Yuntao Chen",
      "Yuqi Wang",
      "Yingyan Li",
      "Zhaoxiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17884v1",
    "title": "The Surprising Effectiveness of Linear Models for Whole-Body\n  Model-Predictive Control",
    "summary": "When do locomotion controllers require reasoning about nonlinearities? In\nthis work, we show that a whole-body model-predictive controller using a simple\nlinear time-invariant approximation of the whole-body dynamics is able to\nexecute basic locomotion tasks on complex legged robots. The formulation\nrequires no online nonlinear dynamics evaluations or matrix inversions. We\ndemonstrate walking, disturbance rejection, and even navigation to a goal\nposition without a separate footstep planner on a quadrupedal robot. In\naddition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with\nsignificant limb inertia, complex actuator dynamics, and large sim-to-real gap.",
    "published": "2025-09-22T15:17:45Z",
    "link": "http://arxiv.org/pdf/2509.17884v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Arun L. Bishop",
      "Juan Alvarez-Padilla",
      "Sam Schoedel",
      "Ibrahima Sory Sow",
      "Juee Chandrachud",
      "Sheitej Sharma",
      "Will Kraus",
      "Beomyeong Park",
      "Robert J. Griffin",
      "John M. Dolan",
      "Zachary Manchester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17877v1",
    "title": "Sight Over Site: Perception-Aware Reinforcement Learning for Efficient\n  Robotic Inspection",
    "summary": "Autonomous inspection is a central problem in robotics, with applications\nranging from industrial monitoring to search-and-rescue. Traditionally,\ninspection has often been reduced to navigation tasks, where the objective is\nto reach a predefined location while avoiding obstacles. However, this\nformulation captures only part of the real inspection problem. In real-world\nenvironments, the inspection targets may become visible well before their exact\ncoordinates are reached, making further movement both redundant and\ninefficient. What matters more for inspection is not simply arriving at the\ntarget's position, but positioning the robot at a viewpoint from which the\ntarget becomes observable. In this work, we revisit inspection from a\nperception-aware perspective. We propose an end-to-end reinforcement learning\nframework that explicitly incorporates target visibility as the primary\nobjective, enabling the robot to find the shortest trajectory that guarantees\nvisual contact with the target without relying on a map. The learned policy\nleverages both perceptual and proprioceptive sensing and is trained entirely in\nsimulation, before being deployed to a real-world robot. We further develop an\nalgorithm to compute ground-truth shortest inspection paths, which provides a\nreference for evaluation. Through extensive experiments, we show that our\nmethod outperforms existing classical and learning-based navigation approaches,\nyielding more efficient inspection trajectories in both simulated and\nreal-world settings. The project is avialable at\nhttps://sight-over-site.github.io/",
    "published": "2025-09-22T15:14:02Z",
    "link": "http://arxiv.org/pdf/2509.17877v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Richard Kuhlmann",
      "Jakob Wolfram",
      "Boyang Sun",
      "Jiaxu Xing",
      "Davide Scaramuzza",
      "Marc Pollefeys",
      "Cesar Cadena"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17850v1",
    "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for\n  Autonomous Driving via Conditional Diffusion Model",
    "summary": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for\nautonomous driving systems to avoid misguided decisions and potential\naccidents. However, achieving reliable predictions in highly dynamic and\ncomplex traffic scenarios remains a significant challenge. One of the key\nimpediments lies in the limited effectiveness of current approaches to capture\nthe multi-modal behaviors of drivers, which leads to predicted trajectories\nthat deviate from actual future motions. To address this issue, we propose\nSocialTraj, a novel trajectory prediction framework integrating social\npsychology principles through social value orientation (SVO). By utilizing\nBayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we\nobtain the critical social context to infer the future interaction trend. To\nensure modal consistency in predicted behaviors, the estimated SVOs of SVs are\nembedded into a conditional denoising diffusion model that aligns generated\ntrajectories with historical driving styles. Additionally, the planned future\ntrajectory of the ego vehicle (EV) is explicitly incorporated to enhance\ninteraction modeling. Extensive experiments on NGSIM and HighD datasets\ndemonstrate that SocialTraj is capable of adapting to highly dynamic and\ninteractive scenarios while generating socially compliant and behaviorally\nconsistent trajectory predictions, outperforming existing baselines. Ablation\nstudies demonstrate that dynamic SVO estimation and explicit ego-planning\ncomponents notably improve prediction accuracy and substantially reduce\ninference time.",
    "published": "2025-09-22T14:42:51Z",
    "link": "http://arxiv.org/pdf/2509.17850v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xiao Zhou",
      "Zengqi Peng",
      "Jun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17812v1",
    "title": "Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback\n  for Robotic Hand Manipulation",
    "summary": "This paper proposes Tac2Motion, a contact-aware reinforcement learning\nframework to facilitate the learning of contact-rich in-hand manipulation\ntasks, such as removing a lid. To this end, we propose tactile sensing-based\nreward shaping and incorporate the sensing into the observation space through\nembedding. The designed rewards encourage an agent to ensure firm grasping and\nsmooth finger gaiting at the same time, leading to higher data efficiency and\nrobust performance compared to the baseline. We verify the proposed framework\non the opening a lid scenario, showing generalization of the trained policy\ninto a couple of object types and various dynamics such as torsional friction.\nLastly, the learned policy is demonstrated on the multi-fingered robot, Shadow\nRobot, showing that the control policy can be transferred to the real world.\nThe video is available: https://youtu.be/poeJBPR7urQ.",
    "published": "2025-09-22T14:05:59Z",
    "link": "http://arxiv.org/pdf/2509.17812v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yitaek Kim",
      "Casper Hewson Rask",
      "Christoffer Sloth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17783v2",
    "title": "RoboSeek: You Need to Interact with Your Objects",
    "summary": "Optimizing and refining action execution through exploration and interaction\nis a promising way for robotic manipulation. However, practical approaches to\ninteraction-driven robotic learning are still underexplored, particularly for\nlong-horizon tasks where sequential decision-making, physical constraints, and\nperceptual uncertainties pose significant challenges. Motivated by embodied\ncognition theory, we propose RoboSeek, a framework for embodied action\nexecution that leverages interactive experience to accomplish manipulation\ntasks. RoboSeek optimizes prior knowledge from high-level perception models\nthrough closed-loop training in simulation and achieves robust real-world\nexecution via a real2sim2real transfer pipeline. Specifically, we first\nreplicate real-world environments in simulation using 3D reconstruction to\nprovide visually and physically consistent environments, then we train policies\nin simulation using reinforcement learning and the cross-entropy method\nleveraging visual priors. The learned policies are subsequently deployed on\nreal robotic platforms for execution. RoboSeek is hardware-agnostic and is\nevaluated on multiple robotic platforms across eight long-horizon manipulation\ntasks involving sequential interactions, tool use, and object handling. Our\napproach achieves an average success rate of 79%, significantly outperforming\nbaselines whose success rates remain below 50%, highlighting its generalization\nand robustness across tasks and platforms. Experimental results validate the\neffectiveness of our training framework in complex, dynamic real-world settings\nand demonstrate the stability of the proposed real2sim2real transfer mechanism,\npaving the way for more generalizable embodied robotic learning. Project Page:\nhttps://russderrick.github.io/Roboseek/",
    "published": "2025-09-22T13:45:02Z",
    "link": "http://arxiv.org/pdf/2509.17783v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yibo Peng",
      "Jiahao Yang",
      "Shenhao Yan",
      "Ziyu Huang",
      "Shuang Li",
      "Shuguang Cui",
      "Yiming Zhao",
      "Yatong Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17760v1",
    "title": "Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term\n  Research",
    "summary": "Many research groups face challenges when legacy (unsupported) robotic\nplatforms lose manufacturer support and cannot accommodate modern sensing,\nspeech, and interaction capabilities. We present the Enhanced NAO, a\nrevitalized version of Aldebaran's NAO robot that uses upgraded microphones,\nRGB-D and thermal cameras, and additional compute resources in a fully\nself-contained package. This system combines cloud and local models for\nperception and dialogue, while preserving the NAO's expressive body and\nbehaviors. In a pilot validation study, the Enhanced NAO delivered\nsignificantly higher conversational quality and stronger user preference\ncompared to the NAO AI Edition, without increasing response latency. Key\nupgrades, such as beamforming microphones and low-latency audio processing,\nreduced artifacts like self-hearing and improved multi-party separation.\nExpanded visual and thermal sensing established a foundation for future\ninteraction capabilities. Beyond the NAO, our framework provides a\nplatform-agnostic strategy for extending the lifespan and research utility of\nlegacy robots, ensuring they remain valuable tools for human-robot interaction.",
    "published": "2025-09-22T13:23:15Z",
    "link": "http://arxiv.org/pdf/2509.17760v1.pdf",
    "category": [
      "cs.RO",
      "cs.HC",
      "eess.AS"
    ],
    "authors": [
      "Austin Wilson",
      "Sahar Kapasi",
      "Zane Greene",
      "Alexis E. Block"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17759v1",
    "title": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic\n  Manipulation Policies",
    "summary": "Scaling real robot data is a key bottleneck in imitation learning, leading to\nthe use of auxiliary data for policy training. While other aspects of robotic\nmanipulation such as image or language understanding may be learned from\ninternet-based datasets, acquiring motion knowledge remains challenging. Human\ndata, with its rich diversity of manipulation behaviors, offers a valuable\nresource for this purpose. While previous works show that using human data can\nbring benefits, such as improving robustness and training efficiency, it\nremains unclear whether it can realize its greatest advantage: enabling robot\npolicies to directly learn new motions for task completion. In this paper, we\nsystematically explore this potential through multi-task human-robot\ncotraining. We introduce MotionTrans, a framework that includes a data\ncollection system, a human data transformation pipeline, and a weighted\ncotraining strategy. By cotraining 30 human-robot tasks simultaneously, we\ndirecly transfer motions of 13 tasks from human data to deployable end-to-end\nrobot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot\nmanner. MotionTrans also significantly enhances pretraining-finetuning\nperformance (+40% success rate). Through ablation study, we also identify key\nfactors for successful motion learning: cotraining with robot data and broad\ntask-related motion coverage. These findings unlock the potential of\nmotion-level learning from human data, offering insights into its effective use\nfor training robotic manipulation policies. All data, code, and model weights\nare open-sourced https://motiontrans.github.io/.",
    "published": "2025-09-22T13:21:11Z",
    "link": "http://arxiv.org/pdf/2509.17759v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chengbo Yuan",
      "Rui Zhou",
      "Mengzhen Liu",
      "Yingdong Hu",
      "Shengjie Wang",
      "Li Yi",
      "Chuan Wen",
      "Shanghang Zhang",
      "Yang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17750v1",
    "title": "EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety\n  Filtering",
    "summary": "We present EigenSafe, an operator-theoretic framework for learning-enabled\nsafety-critical control for stochastic systems. In many robotic systems where\ndynamics are best modeled as stochastic systems due to factors such as sensing\nnoise and environmental disturbances, it is challenging for conventional\nmethods such as Hamilton-Jacobi reachability and control barrier functions to\nprovide a holistic measure of safety. We derive a linear operator governing the\ndynamic programming principle for safety probability, and find that its\ndominant eigenpair provides information about safety for both individual states\nand the overall closed-loop system. The proposed learning framework, called\nEigenSafe, jointly learns this dominant eigenpair and a safe backup policy in\nan offline manner. The learned eigenfunction is then used to construct a safety\nfilter that detects potentially unsafe situations and falls back to the backup\npolicy. The framework is validated in three simulated stochastic\nsafety-critical control tasks.",
    "published": "2025-09-22T13:12:13Z",
    "link": "http://arxiv.org/pdf/2509.17750v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "authors": [
      "Inkyu Jang",
      "Jonghae Park",
      "Chams E. Mballo",
      "Sihyun Cho",
      "Claire J. Tomlin",
      "H. Jin Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17684v1",
    "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for\n  Visuomotor Diffusion Policy Learning",
    "summary": "This paper evaluates DINOv3, a recent large-scale self-supervised vision\nbackbone, for visuomotor diffusion policy learning in robotic manipulation. We\ninvestigate whether a purely self-supervised encoder can match or surpass\nconventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under\nthree regimes: training from scratch, frozen, and finetuned. Across four\nbenchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned\ndiffusion policy, we find that (i) finetuned DINOv3 matches or exceeds\nResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating\nstrong transferable priors, and (iii) self-supervised features improve sample\nefficiency and robustness. These results support self-supervised large visual\nmodels as effective, generalizable perceptual front-ends for action diffusion\npolicies, motivating further exploration of scalable label-free pretraining in\nrobotic manipulation. Compared to using ResNet18 as a backbone, our approach\nwith DINOv3 achieves up to a 10% absolute increase in test-time success rates\non challenging tasks such as Can, and on-the-par performance in tasks like\nLift, PushT, and Square.",
    "published": "2025-09-22T12:27:26Z",
    "link": "http://arxiv.org/pdf/2509.17684v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "ThankGod Egbe",
      "Peng Wang",
      "Zhihao Guo",
      "Zidong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17683v1",
    "title": "Towards Learning Boulder Excavation with Hydraulic Excavators",
    "summary": "Construction sites frequently require removing large rocks before excavation\nor grading can proceed. Human operators typically extract these boulders using\nonly standard digging buckets, avoiding time-consuming tool changes to\nspecialized grippers. This task demands manipulating irregular objects with\nunknown geometries in harsh outdoor environments where dust, variable lighting,\nand occlusions hinder perception. The excavator must adapt to varying soil\nresistance--dragging along hard-packed surfaces or penetrating soft\nground--while coordinating multiple hydraulic joints to secure rocks using a\nshovel. Current autonomous excavation focuses on continuous media (soil,\ngravel) or uses specialized grippers with detailed geometric planning for\ndiscrete objects. These approaches either cannot handle large irregular rocks\nor require impractical tool changes that interrupt workflow. We train a\nreinforcement learning policy in simulation using rigid-body dynamics and\nanalytical soil models. The policy processes sparse LiDAR points (just 20 per\nrock) from vision-based segmentation and proprioceptive feedback to control\nstandard excavator buckets. The learned agent discovers different strategies\nbased on soil resistance: dragging along the surface in hard soil and\npenetrating directly in soft conditions. Field tests on a 12-ton excavator\nachieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to\n83% for human operators. This demonstrates that standard construction equipment\ncan learn complex manipulation despite sparse perception and challenging\noutdoor conditions.",
    "published": "2025-09-22T12:27:06Z",
    "link": "http://arxiv.org/pdf/2509.17683v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jonas Gruetter",
      "Lorenzo Terenzi",
      "Pascal Egli",
      "Marco Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17666v1",
    "title": "Robust and Resilient Soft Robotic Object Insertion with\n  Compliance-Enabled Contact Formation and Failure Recovery",
    "summary": "Object insertion tasks are prone to failures under pose uncertainties and\nenvironmental variations, traditionally requiring manual finetuning or\ncontroller retraining. We present a novel approach for robust and resilient\nobject insertion using a passively compliant soft wrist that enables safe\ncontact absorption through large deformations, without high-frequency control\nor force sensing. Our method structures insertion as compliance-enabled contact\nformations, sequential contact states that progressively constrain degrees of\nfreedom, and integrates automated failure recovery strategies. Our key insight\nis that wrist compliance permits safe, repeated recovery attempts; hence, we\nrefer to it as compliance-enabled failure recovery. We employ a pre-trained\nvision-language model (VLM) that assesses each skill execution from terminal\nposes and images, identifies failure modes, and proposes recovery actions by\nselecting skills and updating goals. In simulation, our method achieved an 83%\nsuccess rate, recovering from failures induced by randomized\nconditions--including grasp misalignments up to 5 degrees, hole-pose errors up\nto 20mm, fivefold increases in friction, and previously unseen\nsquare/rectangular pegs--and we further validate the approach on a real robot.",
    "published": "2025-09-22T12:10:34Z",
    "link": "http://arxiv.org/pdf/2509.17666v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mimo Shirasaka",
      "Cristian C. Beltran-Hernandez",
      "Masashi Hamaya",
      "Yoshitaka Ushiku"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17647v1",
    "title": "VideoArtGS: Building Digital Twins of Articulated Objects from Monocular\n  Video",
    "summary": "Building digital twins of articulated objects from monocular video presents\nan essential challenge in computer vision, which requires simultaneous\nreconstruction of object geometry, part segmentation, and articulation\nparameters from limited viewpoint inputs. Monocular video offers an attractive\ninput format due to its simplicity and scalability; however, it's challenging\nto disentangle the object geometry and part dynamics with visual supervision\nalone, as the joint movement of the camera and parts leads to ill-posed\nestimation. While motion priors from pre-trained tracking models can alleviate\nthe issue, how to effectively integrate them for articulation learning remains\nlargely unexplored. To address this problem, we introduce VideoArtGS, a novel\napproach that reconstructs high-fidelity digital twins of articulated objects\nfrom monocular video. We propose a motion prior guidance pipeline that analyzes\n3D tracks, filters noise, and provides reliable initialization of articulation\nparameters. We also design a hybrid center-grid part assignment module for\narticulation-based deformation fields that captures accurate part motion.\nVideoArtGS demonstrates state-of-the-art performance in articulation and mesh\nreconstruction, reducing the reconstruction error by about two orders of\nmagnitude compared to existing methods. VideoArtGS enables practical digital\ntwin creation from monocular video, establishing a new benchmark for\nvideo-based articulated object reconstruction. Our work is made publicly\navailable at: https://videoartgs.github.io.",
    "published": "2025-09-22T11:52:02Z",
    "link": "http://arxiv.org/pdf/2509.17647v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Yu Liu",
      "Baoxiong Jia",
      "Ruijie Lu",
      "Chuyue Gan",
      "Huayu Chen",
      "Junfeng Ni",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17582v1",
    "title": "GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation\n  Skills on Legged Robots",
    "summary": "Most modern approaches to quadruped locomotion focus on using Deep\nReinforcement Learning (DRL) to learn policies from scratch, in an end-to-end\nmanner. Such methods often fail to scale, as every new problem or application\nrequires time-consuming and iterative reward definition and tuning. We present\nGeneralist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained\nwith Deep Reinforcement Learning that is capable of tracking arbitrary contact\npoints on a quadruped robot. The strength of our approach is that it provides a\ngeneral and modular low-level controller that can be reused for a wider range\nof high-level tasks, without the need to re-train new controllers from scratch.\nWe demonstrate the scalability and robustness of our method by evaluating on a\nwide range of locomotion and manipulation tasks in a common framework and under\na single generalist policy. These include a variety of gaits, traversing\ncomplex terrains (eg. stairs and slopes) as well as previously unseen\nstepping-stones and narrow beams, and interacting with objects (eg. pushing\nbuttons, tracking trajectories). Our framework acquires new behaviors more\nefficiently, simply by combining a task-specific high-level contact planner and\nthe pre-trained generalist policy. A supplementary video can be found at\nhttps://youtu.be/o8Dd44MkG2E.",
    "published": "2025-09-22T11:07:22Z",
    "link": "http://arxiv.org/pdf/2509.17582v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Vassil Atanassov",
      "Wanming Yu",
      "Siddhant Gangapurwala",
      "James Wilson",
      "Ioannis Havoutis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17572v1",
    "title": "Morphologies of a sagging elastica with intrinsic sensing and actuation",
    "summary": "The morphology of a slender soft-robot can be modified by sensing its shape\nvia sensors and exerting moments via actuators embedded along its body. The\nactuating moments required to morph these soft-robots to a desired shape are\noften difficult to compute due to the geometric non-linearity associated with\nthe structure, the errors in modeling the experimental system, and the\nlimitations in sensing and feedback/actuation capabilities. In this article, we\nexplore the effect of a simple feedback strategy (actuation being proportional\nto the sensed curvature) on the shape of a soft-robot, modeled as an elastica.\nThe finite number of sensors and actuators, often seen in experiments, is\ncaptured in the model via filters of specified widths. Using proportional\nfeedback, we study the simple task of straightening the device by compensating\nfor the sagging introduced by its self-weight. The device undergoes a hierarchy\nof morphological instabilities defined in the phase-space given by the\ngravito-bending number, non-dimensional sensing/feedback gain, and the scaled\nwidth of the filter. For complex shape-morphing tasks, given a perfect model of\nthe device with limited sensing and actuating capabilities, we find that a\ntrade-off arises (set by the sensor spacing & actuator size) between capturing\nthe long and short wavelength features. We show that the error in\nshape-morphing is minimal for a fixed filter width when we choose an\nappropriate actuating gain (whose magnitude goes as a square of the filter\nwidth). Our model provides a quantitative lens to study and design slender soft\ndevices with limited sensing and actuating capabilities for complex maneuvering\napplications.",
    "published": "2025-09-22T11:02:11Z",
    "link": "http://arxiv.org/pdf/2509.17572v1.pdf",
    "category": [
      "cs.RO",
      "physics.app-ph"
    ],
    "authors": [
      "Vishnu Deo Mishra",
      "S Ganga Prasath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18224v1",
    "title": "Reversible Kalman Filter for state estimation with Manifold",
    "summary": "This work introduces an algorithm for state estimation on manifolds within\nthe framework of the Kalman filter. Its primary objective is to provide a\nmethodology enabling the evaluation of the precision of existing Kalman filter\nvariants with arbitrary accuracy on synthetic data, something that, to the best\nof our knowledge, has not been addressed in prior work. To this end, we develop\na new filter that exhibits favorable numerical properties, thereby correcting\nthe divergences observed in previous Kalman filter variants. In this\nformulation, the achievable precision is no longer constrained by the\nsmall-velocity assumption and is determined solely by sensor noise. In\naddition, this new filter assumes high precision on the sensors, which, in real\nscenarios require a detection step that we define heuristically, allowing one\nto extend this approach to scenarios, using either a 9-axis IMU or a\ncombination of odometry, accelerometer, and pressure sensors. The latter\nconfiguration is designed for the reconstruction of trajectories in underwater\nenvironments.",
    "published": "2025-09-22T07:59:31Z",
    "link": "http://arxiv.org/pdf/2509.18224v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Svyatoslav Covanov",
      "Cedric Pradalier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17450v1",
    "title": "Learning Dexterous Manipulation with Quantized Hand State",
    "summary": "Dexterous robotic hands enable robots to perform complex manipulations that\nrequire fine-grained control and adaptability. Achieving such manipulation is\nchallenging because the high degrees of freedom tightly couple hand and arm\nmotions, making learning and control difficult. Successful dexterous\nmanipulation relies not only on precise hand motions, but also on accurate\nspatial positioning of the arm and coordinated arm-hand dynamics. However, most\nexisting visuomotor policies represent arm and hand actions in a single\ncombined space, which often causes high-dimensional hand actions to dominate\nthe coupled action space and compromise arm control. To address this, we\npropose DQ-RISE, which quantizes hand states to simplify hand motion prediction\nwhile preserving essential patterns, and applies a continuous relaxation that\nallows arm actions to diffuse jointly with these compact hand states. This\ndesign enables the policy to learn arm-hand coordination from data while\npreventing hand actions from overwhelming the action space. Experiments show\nthat DQ-RISE achieves more balanced and efficient learning, paving the way\ntoward structured and generalizable dexterous manipulation. Project website:\nhttp://rise-policy.github.io/DQ-RISE/",
    "published": "2025-09-22T07:44:17Z",
    "link": "http://arxiv.org/pdf/2509.17450v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ying Feng",
      "Hongjie Fang",
      "Yinong He",
      "Jingjing Chen",
      "Chenxi Wang",
      "Zihao He",
      "Ruonan Liu",
      "Cewu Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17435v1",
    "title": "GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a\n  Low-Cost RGB Camera",
    "summary": "This paper proposes an image-based visual servoing (IBVS) framework for UAV\nnavigation and collision avoidance using only an RGB camera. While UAV\nnavigation has been extensively studied, it remains challenging to apply IBVS\nin missions involving multiple visual targets and collision avoidance. The\nproposed method achieves navigation without explicit path planning, and\ncollision avoidance is realized through AI-based monocular depth estimation\nfrom RGB images. Unlike approaches that rely on stereo cameras or external\nworkstations, our framework runs fully onboard a Jetson platform, ensuring a\nself-contained and deployable system. Experimental results validate that the\nUAV can navigate across multiple AprilTags and avoid obstacles effectively in\nGPS-denied environments.",
    "published": "2025-09-22T07:26:40Z",
    "link": "http://arxiv.org/pdf/2509.17435v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Xiaoyu Wang",
      "Yan Rui Tan",
      "William Leong",
      "Sunan Huang",
      "Rodney Teo",
      "Cheng Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17430v2",
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian\n  Splats from a Mobile Device",
    "summary": "The field of Embodied AI predominantly relies on simulation for training and\nevaluation, often using either fully synthetic environments that lack\nphotorealism or high-fidelity real-world reconstructions captured with\nexpensive hardware. As a result, sim-to-real transfer remains a major\nchallenge. In this paper, we introduce EmbodiedSplat, a novel approach that\npersonalizes policy training by efficiently capturing the deployment\nenvironment and fine-tuning policies within the reconstructed scenes. Our\nmethod leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to\nbridge the gap between realistic scene capture and effective training\nenvironments. Using iPhone-captured deployment scenes, we reconstruct meshes\nvia GS, enabling training in settings that closely approximate real-world\nconditions. We conduct a comprehensive analysis of training strategies,\npre-training datasets, and mesh reconstruction techniques, evaluating their\nimpact on sim-to-real predictivity in real-world scenarios. Experimental\nresults demonstrate that agents fine-tuned with EmbodiedSplat outperform both\nzero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and\nsynthetically generated datasets (HSSD), achieving absolute success rate\nimprovements of 20% and 40% on real-world Image Navigation task. Moreover, our\napproach yields a high sim-vs-real correlation (0.87-0.97) for the\nreconstructed meshes, underscoring its effectiveness in adapting policies to\ndiverse environments with minimal effort. Project page:\nhttps://gchhablani.github.io/embodied-splat.",
    "published": "2025-09-22T07:22:31Z",
    "link": "http://arxiv.org/pdf/2509.17430v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Gunjan Chhablani",
      "Xiaomeng Ye",
      "Muhammad Zubair Irshad",
      "Zsolt Kira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17390v1",
    "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS\n  Models to LiDAR",
    "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic\nrendering, its vast ecosystem of assets remains incompatible with\nhigh-performance LiDAR simulation, a critical tool for robotics and autonomous\ndriving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with\na truly plug-and-play approach. Our method converts \\textit{any} pretrained\n3DGS model into a high-fidelity, watertight mesh without requiring\nLiDAR-specific supervision or architectural alterations. This conversion is\nachieved through a general pipeline of volumetric discretization and Truncated\nSigned Distance Field (TSDF) extraction. We pair this with a highly optimized,\nGPU-accelerated ray-casting module that simulates LiDAR returns at over 500\nFPS. We validate our approach on indoor and outdoor scenes, demonstrating\nexceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for\ngeometrically accurate depth sensing, our framework extends their utility\nbeyond visualization and unlocks new capabilities for scalable, multimodal\nsimulation. Our open-source implementation is available at\nhttps://github.com/TATP-233/FGGS-LiDAR.",
    "published": "2025-09-22T06:52:49Z",
    "link": "http://arxiv.org/pdf/2509.17390v1.pdf",
    "category": [
      "cs.RO",
      "68T40, 68U05",
      "I.6.8"
    ],
    "authors": [
      "Junzhe Wu",
      "Yufei Jia",
      "Yiyi Yan",
      "Zhixing Chen",
      "Tiao Tan",
      "Zifan Wang",
      "Guangyu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17389v1",
    "title": "3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks",
    "summary": "Robotics and automation are key enablers to increase throughput in ongoing\nconservation efforts across various threatened ecosystems. Cataloguing,\ndigitisation, husbandry, and similar activities require the ability to interact\nwith delicate, fragile samples without damaging them. Additionally,\nlearning-based solutions to these tasks require the ability to safely acquire\ndata to train manipulation policies through, e.g., reinforcement learning. To\naddress these twin needs, we introduce a novel method to print free-form,\nhighly sensorised soft 'physical twins'. We present an automated design\nworkflow to create complex and customisable 3D soft sensing structures on\ndemand from 3D scans or models. Compared to the state of the art, our soft\nliquid metal sensors faithfully recreate complex natural geometries and display\nexcellent sensing properties suitable for validating performance in delicate\nmanipulation tasks. We demonstrate the application of our physical twins as\n'sensing corals': high-fidelity, 3D printed replicas of scanned corals that\neliminate the need for live coral experimentation, whilst increasing data\nquality, offering an ethical and scalable pathway for advancing autonomous\ncoral handling and soft manipulation broadly. Through extensive bench-top\nmanipulation and underwater grasping experiments, we show that our sensing\ncoral is able to detect grasps under 0.5 N, effectively capturing the delicate\ninteractions and light contact forces required for coral handling. Finally, we\nshowcase the value of our physical twins across two demonstrations: (i)\nautomated coral labelling for lab identification and (ii) robotic coral\naquaculture. Sensing physical twins such as ours can provide richer grasping\nfeedback than conventional sensors providing experimental validation of prior\nto deployment in handling fragile and delicate items.",
    "published": "2025-09-22T06:52:38Z",
    "link": "http://arxiv.org/pdf/2509.17389v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Lois Liow",
      "Jonty Milford",
      "Emre Uygun",
      "Andre Farinha",
      "Vinoth Viswanathan",
      "Josh Pinskier",
      "David Howard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17387v1",
    "title": "High-Precision and High-Efficiency Trajectory Tracking for Excavators\n  Based on Closed-Loop Dynamics",
    "summary": "The complex nonlinear dynamics of hydraulic excavators, such as time delays\nand control coupling, pose significant challenges to achieving high-precision\ntrajectory tracking. Traditional control methods often fall short in such\napplications due to their inability to effectively handle these nonlinearities,\nwhile commonly used learning-based methods require extensive interactions with\nthe environment, leading to inefficiency. To address these issues, we introduce\nEfficientTrack, a trajectory tracking method that integrates model-based\nlearning to manage nonlinear dynamics and leverages closed-loop dynamics to\nimprove learning efficiency, ultimately minimizing tracking errors. We validate\nour method through comprehensive experiments both in simulation and on a\nreal-world excavator. Comparative experiments in simulation demonstrate that\nour method outperforms existing learning-based approaches, achieving the\nhighest tracking precision and smoothness with the fewest interactions.\nReal-world experiments further show that our method remains effective under\nload conditions and possesses the ability for continual learning, highlighting\nits practical applicability. For implementation details and source code, please\nrefer to https://github.com/ZiqingZou/EfficientTrack.",
    "published": "2025-09-22T06:51:53Z",
    "link": "http://arxiv.org/pdf/2509.17387v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ziqing Zou",
      "Cong Wang",
      "Yue Hu",
      "Xiao Liu",
      "Bowen Xu",
      "Rong Xiong",
      "Changjie Fan",
      "Yingfeng Chen",
      "Yue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17381v1",
    "title": "Fast Trajectory Planner with a Reinforcement Learning-based Controller\n  for Robotic Manipulators",
    "summary": "Generating obstacle-free trajectories for robotic manipulators in\nunstructured and cluttered environments remains a significant challenge.\nExisting motion planning methods often require additional computational effort\nto generate the final trajectory by solving kinematic or dynamic equations.\nThis paper highlights the strong potential of model-free reinforcement learning\nmethods over model-based approaches for obstacle-free trajectory planning in\njoint space. We propose a fast trajectory planning system for manipulators that\ncombines vision-based path planning in task space with reinforcement\nlearning-based obstacle avoidance in joint space. We divide the framework into\ntwo key components. The first introduces an innovative vision-based trajectory\nplanner in task space, leveraging the large-scale fast segment anything (FSA)\nmodel in conjunction with basis spline (B-spline)-optimized kinodynamic path\nsearching. The second component enhances the proximal policy optimization (PPO)\nalgorithm by integrating action ensembles (AE) and policy feedback (PF), which\ngreatly improve precision and stability in goal-reaching and obstacle avoidance\nwithin the joint space. These PPO enhancements increase the algorithm's\nadaptability across diverse robotic tasks, ensuring consistent execution of\ncommands from the first component by the manipulator, while also enhancing both\nobstacle avoidance efficiency and reaching accuracy. The experimental results\ndemonstrate the effectiveness of PPO enhancements, as well as\nsimulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)\ntransfer, in improving model robustness and planner efficiency in complex\nscenarios. These enhancements allow the robot to perform obstacle avoidance and\nreal-time trajectory planning in obstructed environments. Project page\navailable at: https://sites.google.com/view/ftp4rm/home",
    "published": "2025-09-22T06:45:21Z",
    "link": "http://arxiv.org/pdf/2509.17381v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yongliang Wang",
      "Hamidreza Kasaei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17350v2",
    "title": "DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using\n  RGB-only Perception",
    "summary": "Dynamic in air handover is a fundamental challenge for dual-arm robots,\nrequiring accurate perception, precise coordination, and natural motion. Prior\nmethods often rely on dynamics models, strong priors, or depth sensing,\nlimiting generalization and naturalness. We present DyDexHandover, a novel\nframework that employs multi-agent reinforcement learning to train an end to\nend RGB based policy for bimanual object throwing and catching. To achieve more\nhuman-like behavior, the throwing policy is guided by a human policy\nregularization scheme, encouraging fluid and natural motion, and enhancing the\ngeneralization capability of the policy. A dual arm simulation environment was\nbuilt in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly\n99 percent success on training objects and 75 percent on unseen objects, while\ngenerating human-like throwing and catching behaviors. To our knowledge, it is\nthe first method to realize dual-arm in-air handover using only raw RGB\nperception.",
    "published": "2025-09-22T04:28:06Z",
    "link": "http://arxiv.org/pdf/2509.17350v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Haoran Zhou",
      "Yangwei You",
      "Shuaijun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17341v1",
    "title": "Trajectory Encryption Cooperative Salvo Guidance",
    "summary": "This paper introduces the concept of trajectory encryption in cooperative\nsimultaneous target interception, wherein heterogeneity in guidance principles\nacross a team of unmanned autonomous systems is leveraged as a strategic design\nfeature. By employing a mix of heterogeneous time-to-go formulations leading to\na cooperative guidance strategy, the swarm of vehicles is able to generate\ndiverse trajectory families. This diversity expands the feasible solution space\nfor simultaneous target interception, enhances robustness under disturbances,\nand enables flexible time-to-go adjustments without predictable detouring. From\nan adversarial perspective, heterogeneity obscures the collective interception\nintent by preventing straightforward prediction of swarm dynamics, effectively\nacting as an encryption layer in the trajectory domain. Simulations demonstrate\nthat the swarm of heterogeneous vehicles is able to intercept a moving target\nsimultaneously from a diverse set of initial engagement configurations.",
    "published": "2025-09-22T04:10:20Z",
    "link": "http://arxiv.org/pdf/2509.17341v1.pdf",
    "category": [
      "eess.SY",
      "cs.MA",
      "cs.RO",
      "cs.SY",
      "math.OC"
    ],
    "authors": [
      "Lohitvel Gopikannan",
      "Shashi Ranjan Kumar",
      "Abhinav Sinha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17340v1",
    "title": "AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile\n  Mapless Drone Navigation",
    "summary": "Agile mapless navigation in cluttered 3D environments poses significant\nchallenges for autonomous drones. Conventional mapping-planning-control\npipelines incur high computational cost and propagate estimation errors. We\npresent AERO-MPPI, a fully GPU-accelerated framework that unifies perception\nand planning through an anchor-guided ensemble of Model Predictive Path\nIntegral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR\npoint-cloud representation that rapidly extracts spatially distributed\n\"anchors\" as look-ahead intermediate endpoints, from which we construct\npolynomial trajectory guides to explore distinct homotopy path classes. At each\nplanning step, we run multiple MPPI instances in parallel and evaluate them\nwith a two-stage multi-objective cost that balances collision avoidance and\ngoal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI\nachieves real-time onboard operation and mitigates the local-minima failures of\nsingle-MPPI approaches. Extensive simulations in forests, verticals, and\ninclines demonstrate sustained reliable flight above 7 m/s, with success rates\nabove 80% and smoother trajectories compared to state-of-the-art baselines.\nReal-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX\n16G confirm that AERO-MPPI runs in real time onboard and consistently achieves\nsafe, agile, and robust flight in complex cluttered environments. The code will\nbe open-sourced upon acceptance of the paper.",
    "published": "2025-09-22T03:21:51Z",
    "link": "http://arxiv.org/pdf/2509.17340v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Xin Chen",
      "Rui Huang",
      "Longbin Tang",
      "Lin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17323v1",
    "title": "DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory\n  Refinement for Multi-Object Tracking",
    "summary": "Visual Multi-Object Tracking (MOT) is a crucial component of robotic\nperception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D\ncues, such as bounding boxes and motion modeling, which struggle under\nocclusions and close-proximity interactions. Trackers relying on these 2D cues\nare particularly unreliable in robotic environments, where dense targets and\nfrequent occlusions are common. While depth information has the potential to\nalleviate these issues, most existing MOT datasets lack depth annotations,\nleading to its underexploited role in the domain. To unveil the potential of\ndepth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based\ndetector enhanced with instance-level depth information. Specifically, we\npropose two key innovations: (i) foundation model-based instance-level soft\ndepth label supervision, which refines depth prediction, and (ii) the\ndistillation of dense depth maps to maintain global depth consistency. These\nstrategies enable DepTR-MOT to output instance-level depth during inference,\nwithout requiring foundation models and without additional computational cost.\nBy incorporating depth cues, our method enhances the robustness of the TBD\nparadigm, effectively resolving occlusion and close-proximity challenges.\nExperiments on both the QuadTrack and DanceTrack datasets demonstrate the\neffectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,\nrespectively. In particular, results on QuadTrack, a robotic platform MOT\ndataset, highlight the advantages of our method in handling occlusion and\nclose-proximity challenges in robotic tracking. The source code will be made\npublicly available at https://github.com/warriordby/DepTR-MOT.",
    "published": "2025-09-22T02:58:04Z",
    "link": "http://arxiv.org/pdf/2509.17323v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "authors": [
      "Buyin Deng",
      "Lingxin Huang",
      "Kai Luo",
      "Fei Teng",
      "Kailun Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17321v2",
    "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
    "summary": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately $70\\%$ of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\n\\href{github.com/budzianowski/opengvl}{OpenGVL}.",
    "published": "2025-09-22T02:52:55Z",
    "link": "http://arxiv.org/pdf/2509.17321v2.pdf",
    "category": [
      "cs.RO",
      "cs.CL"
    ],
    "authors": [
      "Paweł Budzianowski",
      "Emilia Wiśnios",
      "Gracjan Góral",
      "Igor Kulakov",
      "Viktor Petrenko",
      "Krzysztof Walas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17308v1",
    "title": "Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing\n  Intrinsic Dynamics via Physical Reservoir Computing",
    "summary": "Cable-driven serpentine manipulators hold great potential in unstructured\nenvironments, offering obstacle avoidance, multi-directional force application,\nand a lightweight design. By placing all motors and sensors at the base and\nemploying plastic links, we can further reduce the arm's weight. To demonstrate\nthis concept, we developed a 9-degree-of-freedom cable-driven serpentine\nmanipulator with an arm length of 545 mm and a total mass of only 308 g.\nHowever, this design introduces flexibility-induced variations, such as cable\nslack, elongation, and link deformation. These variations result in\ndiscrepancies between analytical predictions and actual link positions, making\npose estimation more challenging. To address this challenge, we propose a\nphysical reservoir computing based pose estimation method that exploits the\nmanipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.\nExperimental results show a mean pose error of 4.3 mm using our method,\ncompared to 4.4 mm with a baseline long short-term memory network and 39.5 mm\nwith an analytical approach. This work provides a new direction for control and\nperception strategies in lightweight cable-driven serpentine manipulators\nleveraging their intrinsic dynamics.",
    "published": "2025-09-22T01:10:13Z",
    "link": "http://arxiv.org/pdf/2509.17308v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kazutoshi Tanaka",
      "Tomoya Takahashi",
      "Masashi Hamaya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17299v1",
    "title": "Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn\n  and Larvae Imaging Camera System (CSLICS)",
    "summary": "Coral aquaculture for reef restoration requires accurate and continuous spawn\ncounting for resource distribution and larval health monitoring, but current\nmethods are labor-intensive and represent a critical bottleneck in the coral\nproduction pipeline. We propose the Coral Spawn and Larvae Imaging Camera\nSystem (CSLICS), which uses low cost modular cameras and object detectors\ntrained using human-in-the-loop labeling approaches for automated spawn\ncounting in larval rearing tanks. This paper details the system engineering,\ndataset collection, and computer vision techniques to detect, classify and\ncount coral spawn. Experimental results from mass spawning events demonstrate\nan F1 score of 82.4\\% for surface spawn detection at different embryogenesis\nstages, 65.3\\% F1 score for sub-surface spawn detection, and a saving of 5,720\nhours of labor per spawning event compared to manual sampling methods at the\nsame frequency. Comparison of manual counts with CSLICS monitoring during a\nmass coral spawning event on the Great Barrier Reef demonstrates CSLICS'\naccurate measurement of fertilization success and sub-surface spawn counts.\nThese findings enhance the coral aquaculture process and enable upscaling of\ncoral reef restoration efforts to address climate change threats facing\necosystems like the Great Barrier Reef.",
    "published": "2025-09-22T00:47:32Z",
    "link": "http://arxiv.org/pdf/2509.17299v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Dorian Tsai",
      "Christopher A. Brunner",
      "Riki Lamont",
      "F. Mikaela Nordborg",
      "Andrea Severati",
      "Java Terry",
      "Karen Jackel",
      "Matthew Dunbabin",
      "Tobias Fischer",
      "Scarlett Raine"
    ]
  }
]