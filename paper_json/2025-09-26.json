[
  {
    "id": "http://arxiv.org/abs/cs/9308101v1",
    "title": "Dynamic Backtracking",
    "summary": "Because of their occasional need to return to shallow points in a search\ntree, existing backtracking methods can sometimes erase meaningful progress\ntoward solving a search problem. In this paper, we present a method by which\nbacktrack points can be moved deeper in the search space, thereby avoiding this\ndifficulty. The technique developed is a variant of dependency-directed\nbacktracking that uses only polynomial space while still providing useful\ncontrol information and retaining the completeness guarantees provided by\nearlier approaches.",
    "published": "1993-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9308101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. L. Ginsberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9308102v1",
    "title": "A Market-Oriented Programming Environment and its Application to\n  Distributed Multicommodity Flow Problems",
    "summary": "Market price systems constitute a well-understood class of mechanisms that\nunder certain conditions provide effective decentralization of decision making\nwith minimal communication overhead. In a market-oriented programming approach\nto distributed problem solving, we derive the activities and resource\nallocations for a set of computational agents by computing the competitive\nequilibrium of an artificial economy. WALRAS provides basic constructs for\ndefining computational market structures, and protocols for deriving their\ncorresponding price equilibria. In a particular realization of this approach\nfor a form of multicommodity flow problem, we see that careful construction of\nthe decision process according to economic principles can lead to efficient\ndistributed resource allocation, and that the behavior of the system can be\nmeaningfully analyzed in economic terms.",
    "published": "1993-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9308102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. P. Wellman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9309101v1",
    "title": "An Empirical Analysis of Search in GSAT",
    "summary": "We describe an extensive study of search in GSAT, an approximation procedure\nfor propositional satisfiability. GSAT performs greedy hill-climbing on the\nnumber of satisfied clauses in a truth assignment. Our experiments provide a\nmore complete picture of GSAT's search than previous accounts. We describe in\ndetail the two phases of search: rapid hill-climbing followed by a long plateau\nsearch. We demonstrate that when applied to randomly generated 3SAT problems,\nthere is a very simple scaling with problem size for both the mean number of\nsatisfied clauses and the mean branching rate. Our results allow us to make\ndetailed numerical conjectures about the length of the hill-climbing phase, the\naverage gradient of this phase, and to conjecture that both the average score\nand average branching rate decay exponentially during plateau search. We end by\nshowing how these results can be used to direct future theoretical analysis.\nThis work provides a case study of how computer experiments can be used to\nimprove understanding of the theoretical properties of algorithms.",
    "published": "1993-09-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9309101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "I. P. Gent",
      "T. Walsh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9311101v1",
    "title": "The Difficulties of Learning Logic Programs with Cut",
    "summary": "As real logic programmers normally use cut (!), an effective learning\nprocedure for logic programs should be able to deal with it. Because the cut\npredicate has only a procedural meaning, clauses containing cut cannot be\nlearned using an extensional evaluation method, as is done in most learning\nsystems. On the other hand, searching a space of possible programs (instead of\na space of independent clauses) is unfeasible. An alternative solution is to\ngenerate first a candidate base program which covers the positive examples, and\nthen make it consistent by inserting cut where appropriate. The problem of\nlearning programs with cut has not been investigated before and this seems to\nbe a natural and reasonable approach. We generalize this scheme and investigate\nthe difficulties that arise. Some of the major shortcomings are actually\ncaused, in general, by the need for intensional evaluation. As a conclusion,\nthe analysis of this paper suggests, on precise and technical grounds, that\nlearning cut is difficult, and current induction techniques should probably be\nrestricted to purely declarative logic languages.",
    "published": "1993-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9311101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "F. Bergadano",
      "D. Gunetti",
      "U. Trinchero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9311102v1",
    "title": "Software Agents: Completing Patterns and Constructing User Interfaces",
    "summary": "To support the goal of allowing users to record and retrieve information,\nthis paper describes an interactive note-taking system for pen-based computers\nwith two distinctive features. First, it actively predicts what the user is\ngoing to write. Second, it automatically constructs a custom, button-box user\ninterface on request. The system is an example of a learning-apprentice\nsoftware- agent. A machine learning component characterizes the syntax and\nsemantics of the user's information. A performance system uses this learned\ninformation to generate completion strings and construct a user interface.\nDescription of Online Appendix: People like to record information. Doing this\non paper is initially efficient, but lacks flexibility. Recording information\non a computer is less efficient but more powerful. In our new note taking\nsoftwre, the user records information directly on a computer. Behind the\ninterface, an agent acts for the user. To help, it provides defaults and\nconstructs a custom user interface. The demonstration is a QuickTime movie of\nthe note taking agent in action. The file is a binhexed self-extracting\narchive. Macintosh utilities for binhex are available from\nmac.archive.umich.edu. QuickTime is available from ftp.apple.com in the\ndts/mac/sys.soft/quicktime.",
    "published": "1993-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9311102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. C. Schlimmer",
      "L. A. Hermens"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9312101v1",
    "title": "Decidable Reasoning in Terminological Knowledge Representation Systems",
    "summary": "Terminological knowledge representation systems (TKRSs) are tools for\ndesigning and using knowledge bases that make use of terminological languages\n(or concept languages). We analyze from a theoretical point of view a TKRS\nwhose capabilities go beyond the ones of presently available TKRSs. The new\nfeatures studied, often required in practical applications, can be summarized\nin three main points. First, we consider a highly expressive terminological\nlanguage, called ALCNR, including general complements of concepts, number\nrestrictions and role conjunction. Second, we allow to express inclusion\nstatements between general concepts, and terminological cycles as a particular\ncase. Third, we prove the decidability of a number of desirable TKRS-deduction\nservices (like satisfiability, subsumption and instance checking) through a\nsound, complete and terminating calculus for reasoning in ALCNR-knowledge\nbases. Our calculus extends the general technique of constraint systems. As a\nbyproduct of the proof, we get also the result that inclusion statements in\nALCNR can be simulated by terminological cycles, if descriptive semantics is\nadopted.",
    "published": "1993-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9312101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. Buchheit",
      "F. M. Donini",
      "A. Schaerf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9401101v1",
    "title": "Teleo-Reactive Programs for Agent Control",
    "summary": "A formalism is presented for computing and organizing actions for autonomous\nagents in dynamic environments. We introduce the notion of teleo-reactive (T-R)\nprograms whose execution entails the construction of circuitry for the\ncontinuous computation of the parameters and conditions on which agent action\nis based. In addition to continuous feedback, T-R programs support parameter\nbinding and recursion. A primary difference between T-R programs and many other\ncircuit-based systems is that the circuitry of T-R programs is more compact; it\nis constructed at run time and thus does not have to anticipate all the\ncontingencies that might arise over all possible runs. In addition, T-R\nprograms are intuitive and easy to write and are written in a form that is\ncompatible with automatic planning and learning methods. We briefly describe\nsome experimental applications of T-R programs in the control of simulated and\nactual mobile robots.",
    "published": "1994-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9401101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "N. Nilsson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9402101v1",
    "title": "Learning the Past Tense of English Verbs: The Symbolic Pattern\n  Associator vs. Connectionist Models",
    "summary": "Learning the past tense of English verbs - a seemingly minor aspect of\nlanguage acquisition - has generated heated debates since 1986, and has become\na landmark task for testing the adequacy of cognitive modeling. Several\nartificial neural networks (ANNs) have been implemented, and a challenge for\nbetter symbolic models has been posed. In this paper, we present a\ngeneral-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree\nlearning algorithm ID3. We conduct extensive head-to-head comparisons on the\ngeneralization ability between ANN models and the SPA under different\nrepresentations. We conclude that the SPA generalizes the past tense of unseen\nverbs better than ANN models by a wide margin, and we offer insights as to why\nthis should be the case. We also discuss a new default strategy for\ndecision-tree learning algorithms.",
    "published": "1994-02-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9402101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "C. X. Ling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9402102v1",
    "title": "Substructure Discovery Using Minimum Description Length and Background\n  Knowledge",
    "summary": "The ability to identify interesting and repetitive substructures is an\nessential component to discovering knowledge in structural data. We describe a\nnew version of our SUBDUE substructure discovery system based on the minimum\ndescription length principle. The SUBDUE system discovers substructures that\ncompress the original data and represent structural concepts in the data. By\nreplacing previously-discovered substructures in the data, multiple passes of\nSUBDUE produce a hierarchical description of the structural regularities in the\ndata. SUBDUE uses a computationally-bounded inexact graph match that identifies\nsimilar, but not identical, instances of a substructure and finds an\napproximate measure of closeness of two substructures when under computational\nconstraints. In addition to the minimum description length principle, other\nbackground knowledge can be used by SUBDUE to guide the search towards more\nappropriate substructures. Experiments in a variety of domains demonstrate\nSUBDUE's ability to find substructures capable of compressing the original data\nand to discover structural concepts important to the domain. Description of\nOnline Appendix: This is a compressed tar file containing the SUBDUE discovery\nsystem, written in C. The program accepts as input databases represented in\ngraph form, and will output discovered substructures with their corresponding\nvalue.",
    "published": "1994-02-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9402102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. J. Cook",
      "L. B. Holder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9402103v1",
    "title": "Bias-Driven Revision of Logical Domain Theories",
    "summary": "The theory revision problem is the problem of how best to go about revising a\ndeficient domain theory using information contained in examples that expose\ninaccuracies. In this paper we present our approach to the theory revision\nproblem for propositional domain theories. The approach described here, called\nPTR, uses probabilities associated with domain theory elements to numerically\ntrack the ``flow'' of proof through the theory. This allows us to measure the\nprecise role of a clause or literal in allowing or preventing a (desired or\nundesired) derivation for a given example. This information is used to\nefficiently locate and repair flawed elements of the theory. PTR is proved to\nconverge to a theory which correctly classifies all examples, and shown\nexperimentally to be fast and accurate even for deep theories.",
    "published": "1994-02-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9402103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. Koppel",
      "R. Feldman",
      "A. M. Segre"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9403101v1",
    "title": "Exploring the Decision Forest: An Empirical Investigation of Occam's\n  Razor in Decision Tree Induction",
    "summary": "We report on a series of experiments in which all decision trees consistent\nwith the training data are constructed. These experiments were run to gain an\nunderstanding of the properties of the set of consistent decision trees and the\nfactors that affect the accuracy of individual trees. In particular, we\ninvestigated the relationship between the size of a decision tree consistent\nwith some training data and the accuracy of the tree on test data. The\nexperiments were performed on a massively parallel Maspar computer. The results\nof the experiments on several artificial and two real world problems indicate\nthat, for many of the problems investigated, smaller consistent decision trees\nare on average less accurate than the average accuracy of slightly larger\ntrees.",
    "published": "1994-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9403101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. M. Murphy",
      "M. J. Pazzani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9406101v1",
    "title": "A Semantics and Complete Algorithm for Subsumption in the CLASSIC\n  Description Logic",
    "summary": "This paper analyzes the correctness of the subsumption algorithm used in\nCLASSIC, a description logic-based knowledge representation system that is\nbeing used in practical applications. In order to deal efficiently with\nindividuals in CLASSIC descriptions, the developers have had to use an\nalgorithm that is incomplete with respect to the standard, model-theoretic\nsemantics for description logics. We provide a variant semantics for\ndescriptions with respect to which the current implementation is complete, and\nwhich can be independently motivated. The soundness and completeness of the\npolynomial-time subsumption algorithm is established using description graphs,\nwhich are an abstracted version of the implementation structures used in\nCLASSIC, and are of independent interest.",
    "published": "1994-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9406101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Borgida",
      "P. F. Patel-Schneider"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9406102v1",
    "title": "Applying GSAT to Non-Clausal Formulas",
    "summary": "In this paper we describe how to modify GSAT so that it can be applied to\nnon-clausal formulas. The idea is to use a particular ``score'' function which\ngives the number of clauses of the CNF conversion of a formula which are false\nunder a given truth assignment. Its value is computed in linear time, without\nconstructing the CNF conversion itself. The proposed methodology applies to\nmost of the variants of GSAT proposed so far.",
    "published": "1994-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9406102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. Sebastiani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9408101v1",
    "title": "Random Worlds and Maximum Entropy",
    "summary": "Given a knowledge base KB containing first-order and statistical facts, we\nconsider a principled method, called the random-worlds method, for computing a\ndegree of belief that some formula Phi holds given KB. If we are reasoning\nabout a world or system consisting of N individuals, then we can consider all\npossible worlds, or first-order models, with domain {1,...,N} that satisfy KB,\nand compute the fraction of them in which Phi is true. We define the degree of\nbelief to be the asymptotic value of this fraction as N grows large. We show\nthat when the vocabulary underlying Phi and KB uses constants and unary\npredicates only, we can naturally associate an entropy with each world. As N\ngrows larger, there are many more worlds with higher entropy. Therefore, we can\nuse a maximum-entropy computation to compute the degree of belief. This result\nis in a similar spirit to previous work in physics and artificial intelligence,\nbut is far more general. Of equal interest to the result itself are the\nlimitations on its scope. Most importantly, the restriction to unary predicates\nseems necessary. Although the random-worlds method makes sense in general, the\nconnection to maximum entropy seems to disappear in the non-unary case. These\nobservations suggest unexpected limitations to the applicability of\nmaximum-entropy methods.",
    "published": "1994-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9408101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. J. Grove",
      "J. Y. Halpern",
      "D. Koller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9408102v1",
    "title": "Pattern Matching and Discourse Processing in Information Extraction from\n  Japanese Text",
    "summary": "Information extraction is the task of automatically picking up information of\ninterest from an unconstrained text. Information of interest is usually\nextracted in two steps. First, sentence level processing locates relevant\npieces of information scattered throughout the text; second, discourse\nprocessing merges coreferential information to generate the output. In the\nfirst step, pieces of information are locally identified without recognizing\nany relationships among them. A key word search or simple pattern search can\nachieve this purpose. The second step requires deeper knowledge in order to\nunderstand relationships among separately identified pieces of information.\nPrevious information extraction systems focused on the first step, partly\nbecause they were not required to link up each piece of information with other\npieces. To link the extracted pieces of information and map them onto a\nstructured output format, complex discourse processing is essential. This paper\nreports on a Japanese information extraction system that merges information\nusing a pattern matcher and discourse processor. Evaluation results show a high\nlevel of system performance which approaches human performance.",
    "published": "1994-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9408102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "T. Kitani",
      "Y. Eriguchi",
      "M. Hara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9408103v1",
    "title": "A System for Induction of Oblique Decision Trees",
    "summary": "This article describes a new system for induction of oblique decision trees.\nThis system, OC1, combines deterministic hill-climbing with two forms of\nrandomization to find a good oblique split (in the form of a hyperplane) at\neach node of a decision tree. Oblique decision tree methods are tuned\nespecially for domains in which the attributes are numeric, although they can\nbe adapted to symbolic or mixed symbolic/numeric attributes. We present\nextensive empirical studies, using both real and artificial data, that analyze\nOC1's ability to construct oblique trees that are smaller and more accurate\nthan their axis-parallel counterparts. We also examine the benefits of\nrandomization for the construction of oblique decision trees.",
    "published": "1994-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9408103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. K. Murthy",
      "S. Kasif",
      "S. Salzberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9409101v1",
    "title": "On Planning while Learning",
    "summary": "This paper introduces a framework for Planning while Learning where an agent\nis given a goal to achieve in an environment whose behavior is only partially\nknown to the agent. We discuss the tractability of various plan-design\nprocesses. We show that for a large natural class of Planning while Learning\nsystems, a plan can be presented and verified in a reasonable time. However,\ncoming up algorithmically with a plan, even for simple classes of systems is\napparently intractable. We emphasize the role of off-line plan-design\nprocesses, and show that, in most natural cases, the verification (projection)\npart can be carried out in an efficient algorithmic manner.",
    "published": "1994-09-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9409101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Safra",
      "M. Tennenholtz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9412101v1",
    "title": "Wrap-Up: a Trainable Discourse Module for Information Extraction",
    "summary": "The vast amounts of on-line text now available have led to renewed interest\nin information extraction (IE) systems that analyze unrestricted text,\nproducing a structured representation of selected information from the text.\nThis paper presents a novel approach that uses machine learning to acquire\nknowledge for some of the higher level IE processing. Wrap-Up is a trainable IE\ndiscourse component that makes intersentential inferences and identifies\nlogical relations among information extracted from the text. Previous\ncorpus-based approaches were limited to lower level processing such as\npart-of-speech tagging, lexical disambiguation, and dictionary construction.\nWrap-Up is fully trainable, and not only automatically decides what classifiers\nare needed, but even derives the feature set for each classifier automatically.\nPerformance equals that of a partially trainable discourse module requiring\nmanual customization for each domain.",
    "published": "1994-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9412101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Soderland",
      "Lehnert. W"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9412102v1",
    "title": "Operations for Learning with Graphical Models",
    "summary": "This paper is a multidisciplinary review of empirical, statistical learning\nfrom a graphical model perspective. Well-known examples of graphical models\ninclude Bayesian networks, directed graphs representing a Markov chain, and\nundirected networks representing a Markov field. These graphical models are\nextended to model data analysis and empirical learning using the notation of\nplates. Graphical operations for simplifying and manipulating a problem are\nprovided including decomposition, differentiation, and the manipulation of\nprobability models from the exponential family. Two standard algorithm schemas\nfor learning are reviewed in a graphical framework: Gibbs sampling and the\nexpectation maximization algorithm. Using these operations and schemas, some\npopular algorithms can be synthesized from their graphical specification. This\nincludes versions of linear regression, techniques for feed-forward networks,\nand learning Gaussian and discrete Bayesian networks from data. The paper\nconcludes by sketching some implications for data analysis and summarizing how\nsome popular algorithms fall within the framework presented. The main original\ncontributions here are the decomposition techniques and the demonstration that\ngraphical models provide a framework for understanding and developing complex\nlearning algorithms.",
    "published": "1994-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9412102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "W. L. Buntine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9412103v1",
    "title": "Total-Order and Partial-Order Planning: A Comparative Analysis",
    "summary": "For many years, the intuitions underlying partial-order planning were largely\ntaken for granted. Only in the past few years has there been renewed interest\nin the fundamental principles underlying this paradigm. In this paper, we\npresent a rigorous comparative analysis of partial-order and total-order\nplanning by focusing on two specific planners that can be directly compared. We\nshow that there are some subtle assumptions that underly the wide-spread\nintuitions regarding the supposed efficiency of partial-order planning. For\ninstance, the superiority of partial-order planning can depend critically upon\nthe search strategy and the structure of the search space. Understanding the\nunderlying assumptions is crucial for constructing efficient planners.",
    "published": "1994-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9412103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Minton",
      "J. Bresina",
      "M. Drummond"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9501101v1",
    "title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes",
    "summary": "Multiclass learning problems involve finding a definition for an unknown\nfunction f(x) whose range is a discrete set containing k &gt 2 values (i.e., k\n``classes''). The definition is acquired by studying collections of training\nexamples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning\nproblems include direct application of multiclass algorithms such as the\ndecision-tree algorithms C4.5 and CART, application of binary concept learning\nalgorithms to learn individual binary functions for each of the k classes, and\napplication of binary concept learning algorithms with distributed output\nrepresentations. This paper compares these three approaches to a new technique\nin which error-correcting codes are employed as a distributed output\nrepresentation. We show that these output representations improve the\ngeneralization performance of both C4.5 and backpropagation on a wide range of\nmulticlass learning tasks. We also demonstrate that this approach is robust\nwith respect to changes in the size of the training sample, the assignment of\ndistributed representations to particular classes, and the application of\noverfitting avoidance techniques such as decision-tree pruning. Finally, we\nshow that---like the other methods---the error-correcting code technique can\nprovide reliable class probability estimates. Taken together, these results\ndemonstrate that error-correcting output codes provide a general-purpose method\nfor improving the performance of inductive learning programs on multiclass\nproblems.",
    "published": "1995-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9501101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "T. G. Dietterich",
      "G. Bakiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9501102v1",
    "title": "A Domain-Independent Algorithm for Plan Adaptation",
    "summary": "The paradigms of transformational planning, case-based planning, and plan\ndebugging all involve a process known as plan adaptation - modifying or\nrepairing an old plan so it solves a new problem. In this paper we provide a\ndomain-independent algorithm for plan adaptation, demonstrate that it is sound,\ncomplete, and systematic, and compare it to other adaptation algorithms in the\nliterature. Our approach is based on a view of planning as searching a graph of\npartial plans. Generative planning starts at the graph's root and moves from\nnode to node using plan-refinement operators. In planning by adaptation, a\nlibrary plan - an arbitrary node in the plan graph - is the starting point for\nthe search, and the plan-adaptation algorithm can apply both the same\nrefinement operators available to a generative planner and can also retract\nconstraints and steps from the plan. Our algorithm's completeness ensures that\nthe adaptation algorithm will eventually search the entire graph and its\nsystematicity ensures that it will do so without redundantly searching any\nparts of the graph.",
    "published": "1995-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9501102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Hanks",
      "D. S. Weld"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9501103v1",
    "title": "Truncating Temporal Differences: On the Efficient Implementation of\n  TD(lambda) for Reinforcement Learning",
    "summary": "Temporal difference (TD) methods constitute a class of methods for learning\npredictions in multi-step prediction problems, parameterized by a recency\nfactor lambda. Currently the most important application of these methods is to\ntemporal credit assignment in reinforcement learning. Well known reinforcement\nlearning algorithms, such as AHC or Q-learning, may be viewed as instances of\nTD learning. This paper examines the issues of the efficient and general\nimplementation of TD(lambda) for arbitrary lambda, for use with reinforcement\nlearning algorithms optimizing the discounted sum of rewards. The traditional\napproach, based on eligibility traces, is argued to suffer from both\ninefficiency and lack of generality. The TTD (Truncated Temporal Differences)\nprocedure is proposed as an alternative, that indeed only approximates\nTD(lambda), but requires very little computation per action and can be used\nwith arbitrary function representation methods. The idea from which it is\nderived is fairly simple and not new, but probably unexplored so far.\nEncouraging experimental results are presented, suggesting that using lambda\n&gt 0 with the TTD procedure allows one to obtain a significant learning\nspeedup at essentially the same cost as usual TD(0) learning.",
    "published": "1995-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9501103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. Cichosz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9503101v1",
    "title": "On the Informativeness of the DNA Promoter Sequences Domain Theory",
    "summary": "The DNA promoter sequences domain theory and database have become popular for\ntesting systems that integrate empirical and analytical learning. This note\nreports a simple change and reinterpretation of the domain theory in terms of\nM-of-N concepts, involving no learning, that results in an accuracy of 93.4% on\nthe 106 items of the database. Moreover, an exhaustive search of the space of\nM-of-N domain theory interpretations indicates that the expected accuracy of a\nrandomly chosen interpretation is 76.5%, and that a maximum accuracy of 97.2%\nis achieved in 12 cases. This demonstrates the informativeness of the domain\ntheory, without the complications of understanding the interactions between\nvarious learning algorithms and the theory. In addition, our results help\ncharacterize the difficulty of learning using the DNA promoters theory.",
    "published": "1995-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9503101v1.pdf",
    "category": [
      "cs.AI",
      "q-bio"
    ],
    "authors": [
      "J. Ortega"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9503102v1",
    "title": "Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic\n  Decision Tree Induction Algorithm",
    "summary": "This paper introduces ICET, a new algorithm for cost-sensitive\nclassification. ICET uses a genetic algorithm to evolve a population of biases\nfor a decision tree induction algorithm. The fitness function of the genetic\nalgorithm is the average cost of classification when using the decision tree,\nincluding both the costs of tests (features, measurements) and the costs of\nclassification errors. ICET is compared here with three other algorithms for\ncost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,\nwhich classifies without regard to cost. The five algorithms are evaluated\nempirically on five real-world medical datasets. Three sets of experiments are\nperformed. The first set examines the baseline performance of the five\nalgorithms on the five datasets and establishes that ICET performs\nsignificantly better than its competitors. The second set tests the robustness\nof ICET under a variety of conditions and shows that ICET maintains its\nadvantage. The third set looks at ICET's search in bias space and discovers a\nway to improve the search.",
    "published": "1995-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9503102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. D. Turney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9504101v1",
    "title": "Rerepresenting and Restructuring Domain Theories: A Constructive\n  Induction Approach",
    "summary": "Theory revision integrates inductive learning and background knowledge by\ncombining training examples with a coarse domain theory to produce a more\naccurate theory. There are two challenges that theory revision and other\ntheory-guided systems face. First, a representation language appropriate for\nthe initial theory may be inappropriate for an improved theory. While the\noriginal representation may concisely express the initial theory, a more\naccurate theory forced to use that same representation may be bulky,\ncumbersome, and difficult to reach. Second, a theory structure suitable for a\ncoarse domain theory may be insufficient for a fine-tuned theory. Systems that\nproduce only small, local changes to a theory have limited value for\naccomplishing complex structural alterations that may be required.\nConsequently, advanced theory-guided learning systems require flexible\nrepresentation and flexible structure. An analysis of various theory revision\nsystems and theory-guided learning systems reveals specific strengths and\nweaknesses in terms of these two desired properties. Designed to capture the\nunderlying qualities of each system, a new system uses theory-guided\nconstructive induction. Experiments in three domains show improvement over\nprevious theory-guided systems. This leads to a study of the behavior,\nlimitations, and potential of theory-guided constructive induction.",
    "published": "1995-04-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9504101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. K. Donoho",
      "L. A. Rendell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9505101v1",
    "title": "Using Pivot Consistency to Decompose and Solve Functional CSPs",
    "summary": "Many studies have been carried out in order to increase the search efficiency\nof constraint satisfaction problems; among them, some make use of structural\nproperties of the constraint network; others take into account semantic\nproperties of the constraints, generally assuming that all the constraints\npossess the given property. In this paper, we propose a new decomposition\nmethod benefiting from both semantic properties of functional constraints (not\nbijective constraints) and structural properties of the network; furthermore,\nnot all the constraints need to be functional. We show that under some\nconditions, the existence of solutions can be guaranteed. We first characterize\na particular subset of the variables, which we name a root set. We then\nintroduce pivot consistency, a new local consistency which is a weak form of\npath consistency and can be achieved in O(n^2d^2) complexity (instead of\nO(n^3d^3) for path consistency), and we present associated properties; in\nparticular, we show that any consistent instantiation of the root set can be\nlinearly extended to a solution, which leads to the presentation of the\naforementioned new method for solving by decomposing functional CSPs.",
    "published": "1995-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9505101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. David"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9505102v1",
    "title": "Adaptive Load Balancing: A Study in Multi-Agent Learning",
    "summary": "We study the process of multi-agent reinforcement learning in the context of\nload balancing in a distributed system, without use of either central\ncoordination or explicit communication. We first define a precise framework in\nwhich to study adaptive load balancing, important features of which are its\nstochastic nature and the purely local information available to individual\nagents. Given this framework, we show illuminating results on the interplay\nbetween basic adaptive behavior parameters and their effect on system\nefficiency. We then investigate the properties of adaptive load balancing in\nheterogeneous populations, and address the issue of exploration vs.\nexploitation in that context. Finally, we show that naive use of communication\nmay not improve, and might even harm system efficiency.",
    "published": "1995-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9505102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Schaerf",
      "Y. Shoham",
      "M. Tennenholtz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9505103v1",
    "title": "Provably Bounded-Optimal Agents",
    "summary": "Since its inception, artificial intelligence has relied upon a theoretical\nfoundation centered around perfect rationality as the desired property of\nintelligent systems. We argue, as others have done, that this foundation is\ninadequate because it imposes fundamentally unsatisfiable requirements. As a\nresult, there has arisen a wide gap between theory and practice in AI,\nhindering progress in the field. We propose instead a property called bounded\noptimality. Roughly speaking, an agent is bounded-optimal if its program is a\nsolution to the constrained optimization problem presented by its architecture\nand the task environment. We show how to construct agents with this property\nfor a simple class of machine architectures in a broad class of real-time\nenvironments. We illustrate these results using a simple model of an automated\nmail sorting facility. We also define a weaker property, asymptotic bounded\noptimality (ABO), that generalizes the notion of optimality in classical\ncomplexity theory. We then construct universal ABO programs, i.e., programs\nthat are ABO no matter what real-time constraints are applied. Universal ABO\nprograms can be used as building blocks for more complex systems. We conclude\nwith a discussion of the prospects for bounded optimality as a theoretical\nbasis for AI, and relate it to similar trends in philosophy, economics, and\ngame theory.",
    "published": "1995-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9505103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. J. Russell",
      "D. Subramanian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9505104v1",
    "title": "Pac-Learning Recursive Logic Programs: Efficient Algorithms",
    "summary": "We present algorithms that learn certain classes of function-free recursive\nlogic programs in polynomial time from equivalence queries. In particular, we\nshow that a single k-ary recursive constant-depth determinate clause is\nlearnable. Two-clause programs consisting of one learnable recursive clause and\none constant-depth determinate non-recursive clause are also learnable, if an\nadditional ``basecase'' oracle is assumed. These results immediately imply the\npac-learnability of these classes. Although these classes of learnable\nrecursive programs are very constrained, it is shown in a companion paper that\nthey are maximally general, in that generalizing either class in any natural\nway leads to a computationally difficult learning problem. Thus, taken together\nwith its companion paper, this paper establishes a boundary of efficient\nlearnability for recursive logic programs.",
    "published": "1995-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9505104v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "W. W. Cohen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9505105v1",
    "title": "Pac-learning Recursive Logic Programs: Negative Results",
    "summary": "In a companion paper it was shown that the class of constant-depth\ndeterminate k-ary recursive clauses is efficiently learnable. In this paper we\npresent negative results showing that any natural generalization of this class\nis hard to learn in Valiant's model of pac-learnability. In particular, we show\nthat the following program classes are cryptographically hard to learn:\nprograms with an unbounded number of constant-depth linear recursive clauses;\nprograms with one constant-depth determinate clause containing an unbounded\nnumber of recursive calls; and programs with one linear recursive clause of\nconstant locality. These results immediately imply the non-learnability of any\nmore general class of programs. We also show that learning a constant-depth\ndeterminate program with either two linear recursive clauses or one linear\nrecursive clause and one non-recursive clause is as hard as learning boolean\nDNF. Together with positive results from the companion paper, these negative\nresults establish a boundary of efficient learnability for recursive\nfunction-free clauses.",
    "published": "1995-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9505105v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "W. W. Cohen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9506101v1",
    "title": "FLECS: Planning with a Flexible Commitment Strategy",
    "summary": "There has been evidence that least-commitment planners can efficiently handle\nplanning problems that involve difficult goal interactions. This evidence has\nled to the common belief that delayed-commitment is the \"best\" possible\nplanning strategy. However, we recently found evidence that eager-commitment\nplanners can handle a variety of planning problems more efficiently, in\nparticular those with difficult operator choices. Resigned to the futility of\ntrying to find a universally successful planning strategy, we devised a planner\nthat can be used to study which domains and problems are best for which\nplanning strategies. In this article we introduce this new planning algorithm,\nFLECS, which uses a FLExible Commitment Strategy with respect to plan-step\norderings. It is able to use any strategy from delayed-commitment to\neager-commitment. The combination of delayed and eager operator-ordering\ncommitments allows FLECS to take advantage of the benefits of explicitly using\na simulated execution state and reasoning about planning constraints. FLECS can\nvary its commitment strategy across different problems and domains, and also\nduring the course of a single planning problem. FLECS represents a novel\ncontribution to planning in that it explicitly provides the choice of which\ncommitment strategy to use while planning. FLECS provides a framework to\ninvestigate the mapping from planning domains and problems to efficient\nplanning strategies.",
    "published": "1995-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9506101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. Veloso",
      "P. Stone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9506102v1",
    "title": "Induction of First-Order Decision Lists: Results on Learning the Past\n  Tense of English Verbs",
    "summary": "This paper presents a method for inducing logic programs from examples that\nlearns a new class of concepts called first-order decision lists, defined as\nordered lists of clauses each ending in a cut. The method, called FOIDL, is\nbased on FOIL (Quinlan, 1990) but employs intensional background knowledge and\navoids the need for explicit negative examples. It is particularly useful for\nproblems that involve rules with specific exceptions, such as learning the\npast-tense of English verbs, a task widely studied in the context of the\nsymbolic/connectionist debate. FOIDL is able to learn concise, accurate\nprograms for this problem from significantly fewer examples than previous\nmethods (both connectionist and symbolic).",
    "published": "1995-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9506102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. J. Mooney",
      "M. E. Califf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9507101v1",
    "title": "Building and Refining Abstract Planning Cases by Change of\n  Representation Language",
    "summary": "ion is one of the most promising approaches to improve the performance of\nproblem solvers. In several domains abstraction by dropping sentences of a\ndomain description -- as used in most hierarchical planners -- has proven\nuseful. In this paper we present examples which illustrate significant\ndrawbacks of abstraction by dropping sentences. To overcome these drawbacks, we\npropose a more general view of abstraction involving the change of\nrepresentation language. We have developed a new abstraction methodology and a\nrelated sound and complete learning algorithm that allows the complete change\nof representation language of planning cases from concrete to abstract.\nHowever, to achieve a powerful change of the representation language, the\nabstract language itself as well as rules which describe admissible ways of\nabstracting states must be provided in the domain model. This new abstraction\napproach is the core of Paris (Plan Abstraction and Refinement in an Integrated\nSystem), a system in which abstract planning cases are automatically learned\nfrom given concrete cases. An empirical study in the domain of process planning\nin mechanical engineering shows significant advantages of the proposed\nreasoning from abstract cases over classical hierarchical planning.",
    "published": "1995-07-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9507101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. Bergmann",
      "W. Wilke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9508101v1",
    "title": "Using Qualitative Hypotheses to Identify Inaccurate Data",
    "summary": "Identifying inaccurate data has long been regarded as a significant and\ndifficult problem in AI. In this paper, we present a new method for identifying\ninaccurate data on the basis of qualitative correlations among related data.\nFirst, we introduce the definitions of related data and qualitative\ncorrelations among related data. Then we put forward a new concept called\nsupport coefficient function (SCF). SCF can be used to extract, represent, and\ncalculate qualitative correlations among related data within a dataset. We\npropose an approach to determining dynamic shift intervals of inaccurate data,\nand an approach to calculating possibility of identifying inaccurate data,\nrespectively. Both of the approaches are based on SCF. Finally we present an\nalgorithm for identifying inaccurate data by using qualitative correlations\namong related data as confirmatory or disconfirmatory evidence. We have\ndeveloped a practical system for interpreting infrared spectra by applying the\nmethod, and have fully tested the system against several hundred real spectra.\nThe experimental results show that the method is significantly better than the\nconventional methods used in many similar systems.",
    "published": "1995-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9508101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Q. Zhao",
      "T. Nishida"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9508102v1",
    "title": "An Integrated Framework for Learning and Reasoning",
    "summary": "Learning and reasoning are both aspects of what is considered to be\nintelligence. Their studies within AI have been separated historically,\nlearning being the topic of machine learning and neural networks, and reasoning\nfalling under classical (or symbolic) AI. However, learning and reasoning are\nin many ways interdependent. This paper discusses the nature of some of these\ninterdependencies and proposes a general framework called FLARE, that combines\ninductive learning using prior knowledge together with reasoning in a\npropositional setting. Several examples that test the framework are presented,\nincluding classical induction, many important reasoning protocols and two\nsimple expert systems.",
    "published": "1995-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9508102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "C. G. Giraud-Carrier",
      "T. R. Martinez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9510101v1",
    "title": "Diffusion of Context and Credit Information in Markovian Models",
    "summary": "This paper studies the problem of ergodicity of transition probability\nmatrices in Markovian models, such as hidden Markov models (HMMs), and how it\nmakes very difficult the task of learning to represent long-term context for\nsequential data. This phenomenon hurts the forward propagation of long-term\ncontext information, as well as learning a hidden state representation to\nrepresent long-term context, which depends on propagating credit information\nbackwards in time. Using results from Markov chain theory, we show that this\nproblem of diffusion of context and credit is reduced when the transition\nprobabilities approach 0 or 1, i.e., the transition probability matrices are\nsparse and the model essentially deterministic. The results found in this paper\napply to learning approaches based on continuous optimization, such as gradient\ndescent and the Baum-Welch algorithm.",
    "published": "1995-10-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9510101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Y. Bengio",
      "P. Frasconi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9510102v1",
    "title": "Improving Connectionist Energy Minimization",
    "summary": "Symmetric networks designed for energy minimization such as Boltzman machines\nand Hopfield nets are frequently investigated for use in optimization,\nconstraint satisfaction and approximation of NP-hard problems. Nevertheless,\nfinding a global solution (i.e., a global minimum for the energy function) is\nnot guaranteed and even a local solution may take an exponential number of\nsteps. We propose an improvement to the standard local activation function used\nfor such networks. The improved algorithm guarantees that a global minimum is\nfound in linear time for tree-like subnetworks. The algorithm, called activate,\nis uniform and does not assume that the network is tree-like. It can identify\ntree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid\nlocal minima along these trees. For acyclic networks, the algorithm is\nguaranteed to converge to a global minimum from any initial state of the system\n(self-stabilization) and remains correct under various types of schedulers. On\nthe negative side, we show that in the presence of cycles, no uniform algorithm\nexists that guarantees optimality even under a sequential asynchronous\nscheduler. An asynchronous scheduler can activate only one unit at a time while\na synchronous scheduler can activate any number of units in a single time step.\nIn addition, no uniform algorithm exists to optimize even acyclic networks when\nthe scheduler is synchronous. Finally, we show how the algorithm can be\nimproved using the cycle-cutset scheme. The general algorithm, called\nactivate-with-cutset, improves over activate and has some performance\nguarantees that are related to the size of the network's cycle-cutset.",
    "published": "1995-10-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9510102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. Pinkas",
      "R. Dechter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9510103v1",
    "title": "Learning Membership Functions in a Function-Based Object Recognition\n  System",
    "summary": "Functionality-based recognition systems recognize objects at the category\nlevel by reasoning about how well the objects support the expected function.\nSuch systems naturally associate a ``measure of goodness'' or ``membership\nvalue'' with a recognized object. This measure of goodness is the result of\ncombining individual measures, or membership values, from potentially many\nprimitive evaluations of different properties of the object's shape. A\nmembership function is used to compute the membership value when evaluating a\nprimitive of a particular physical property of an object. In previous versions\nof a recognition system known as Gruff, the membership function for each of the\nprimitive evaluations was hand-crafted by the system designer. In this paper,\nwe provide a learning component for the Gruff system, called Omlet, that\nautomatically learns membership functions given a set of example objects\nlabeled with their desired category measure. The learning algorithm is\ngenerally applicable to any problem in which low-level membership values are\ncombined through an and-or tree structure to give a final overall membership\nvalue.",
    "published": "1995-10-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9510103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "K. Woods",
      "D. Cook",
      "L. Hall",
      "K. Bowyer",
      "L. Stark"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9511101v1",
    "title": "Flexibly Instructable Agents",
    "summary": "This paper presents an approach to learning from situated, interactive\ntutorial instruction within an ongoing agent. Tutorial instruction is a\nflexible (and thus powerful) paradigm for teaching tasks because it allows an\ninstructor to communicate whatever types of knowledge an agent might need in\nwhatever situations might arise. To support this flexibility, however, the\nagent must be able to learn multiple kinds of knowledge from a broad range of\ninstructional interactions. Our approach, called situated explanation, achieves\nsuch learning through a combination of analytic and inductive techniques. It\ncombines a form of explanation-based learning that is situated for each\ninstruction with a full suite of contextually guided responses to incomplete\nexplanations. The approach is implemented in an agent called Instructo-Soar\nthat learns hierarchies of new tasks and other domain knowledge from\ninteractive natural language instructions. Instructo-Soar meets three key\nrequirements of flexible instructability that distinguish it from previous\nsystems: (1) it can take known or unknown commands at any instruction point;\n(2) it can handle instructions that apply to either its current situation or to\na hypothetical situation specified in language (as in, for instance,\nconditional instructions); and (3) it can learn, from instructions, each class\nof knowledge it uses to perform tasks.",
    "published": "1995-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9511101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. B. Huffman",
      "J. E. Laird"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512101v1",
    "title": "OPUS: An Efficient Admissible Algorithm for Unordered Search",
    "summary": "OPUS is a branch and bound search algorithm that enables efficient admissible\nsearch through spaces for which the order of search operator application is not\nsignificant. The algorithm's search efficiency is demonstrated with respect to\nvery large machine learning search spaces. The use of admissible search is of\npotential value to the machine learning community as it means that the exact\nlearning biases to be employed for complex learning tasks can be precisely\nspecified and manipulated. OPUS also has potential for application in other\nareas of artificial intelligence, notably, truth maintenance.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. I. Webb"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512102v1",
    "title": "Vision-Based Road Detection in Automotive Systems: A Real-Time\n  Expectation-Driven Approach",
    "summary": "The main aim of this work is the development of a vision-based road detection\nsystem fast enough to cope with the difficult real-time constraints imposed by\nmoving vehicle applications. The hardware platform, a special-purpose massively\nparallel system, has been chosen to minimize system production and operational\ncosts. This paper presents a novel approach to expectation-driven low-level\nimage segmentation, which can be mapped naturally onto mesh-connected massively\nparallel SIMD architectures capable of handling hierarchical data structures.\nThe input image is assumed to contain a distorted version of a given template;\na multiresolution stretching process is used to reshape the original template\nin accordance with the acquired image content, minimizing a potential function.\nThe distorted template is the process output.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Broggi",
      "S. Berte"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512103v1",
    "title": "Generalization of Clauses under Implication",
    "summary": "In the area of inductive learning, generalization is a main operation, and\nthe usual definition of induction is based on logical implication. Recently\nthere has been a rising interest in clausal representation of knowledge in\nmachine learning. Almost all inductive learning systems that perform\ngeneralization of clauses use the relation theta-subsumption instead of\nimplication. The main reason is that there is a well-known and simple technique\nto compute least general generalizations under theta-subsumption, but not under\nimplication. However generalization under theta-subsumption is inappropriate\nfor learning recursive clauses, which is a crucial problem since recursion is\nthe basic program structure of logic programs. We note that implication between\nclauses is undecidable, and we therefore introduce a stronger form of\nimplication, called T-implication, which is decidable between clauses. We show\nthat for every finite set of clauses there exists a least general\ngeneralization under T-implication. We describe a technique to reduce\ngeneralizations under implication of a clause to generalizations under\ntheta-subsumption of what we call an expansion of the original clause. Moreover\nwe show that for every non-tautological clause there exists a T-complete\nexpansion, which means that every generalization under T-implication of the\nclause is reduced to a generalization under theta-subsumption of the expansion.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. Idestam-Almquist"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512104v1",
    "title": "Decision-Theoretic Foundations for Causal Reasoning",
    "summary": "We present a definition of cause and effect in terms of decision-theoretic\nprimitives and thereby provide a principled foundation for causal reasoning.\nOur definition departs from the traditional view of causation in that causal\nassertions may vary with the set of decisions available. We argue that this\napproach provides added clarity to the notion of cause. Also in this paper, we\nexamine the encoding of causal relationships in directed acyclic graphs. We\ndescribe a special class of influence diagrams, those in canonical form, and\nshow its relationship to Pearl's representation of cause and effect. Finally,\nwe show how canonical form facilitates counterfactual reasoning.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512104v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. Heckerman",
      "R. Shachter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512105v1",
    "title": "Translating between Horn Representations and their Characteristic Models",
    "summary": "Characteristic models are an alternative, model based, representation for\nHorn expressions. It has been shown that these two representations are\nincomparable and each has its advantages over the other. It is therefore\nnatural to ask what is the cost of translating, back and forth, between these\nrepresentations. Interestingly, the same translation questions arise in\ndatabase theory, where it has applications to the design of relational\ndatabases. This paper studies the computational complexity of these problems.\nOur main result is that the two translation problems are equivalent under\npolynomial reductions, and that they are equivalent to the corresponding\ndecision problem. Namely, translating is equivalent to deciding whether a given\nset of models is the set of characteristic models for a given Horn expression.\nWe also relate these problems to the hypergraph transversal problem, a well\nknown problem which is related to other applications in AI and for which no\npolynomial time algorithm is known. It is shown that in general our translation\nproblems are at least as hard as the hypergraph transversal problem, and in a\nspecial case they are equivalent to it.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512105v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. Khardon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512106v1",
    "title": "Statistical Feature Combination for the Evaluation of Game Positions",
    "summary": "This article describes an application of three well-known statistical methods\nin the field of game-tree search: using a large number of classified Othello\npositions, feature weights for evaluation functions with a\ngame-phase-independent meaning are estimated by means of logistic regression,\nFisher's linear discriminant, and the quadratic discriminant function for\nnormally distributed features. Thereafter, the playing strengths are compared\nby means of tournaments between the resulting versions of a world-class Othello\nprogram. In this application, logistic regression - which is used here for the\nfirst time in the context of game playing - leads to better results than the\nother approaches.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512106v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. Buro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9512107v1",
    "title": "Rule-based Machine Learning Methods for Functional Prediction",
    "summary": "We describe a machine learning method for predicting the value of a\nreal-valued function, given the values of multiple input variables. The method\ninduces solutions from samples in the form of ordered disjunctive normal form\n(DNF) decision rules. A central objective of the method and representation is\nthe induction of compact, easily interpretable solutions. This rule-based\ndecision model can be extended to search efficiently for similar cases prior to\napproximating function values. Experimental results on real-world data\ndemonstrate that the new techniques are competitive with existing machine\nlearning and statistical methods and can sometimes yield superior regression\nperformance.",
    "published": "1995-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9512107v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. M. Weiss",
      "N. Indurkhya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9601101v1",
    "title": "The Design and Experimental Analysis of Algorithms for Temporal\n  Reasoning",
    "summary": "Many applications -- from planning and scheduling to problems in molecular\nbiology -- rely heavily on a temporal reasoning component. In this paper, we\ndiscuss the design and empirical analysis of algorithms for a temporal\nreasoning system based on Allen's influential interval-based framework for\nrepresenting temporal information. At the core of the system are algorithms for\ndetermining whether the temporal information is consistent, and, if so, finding\none or more scenarios that are consistent with the temporal information. Two\nimportant algorithms for these tasks are a path consistency algorithm and a\nbacktracking algorithm. For the path consistency algorithm, we develop\ntechniques that can result in up to a ten-fold speedup over an already highly\noptimized implementation. For the backtracking algorithm, we develop variable\nand value ordering heuristics that are shown empirically to dramatically\nimprove the performance of the algorithm. As well, we show that a previously\nsuggested reformulation of the backtracking search problem can reduce the time\nand space requirements of the backtracking search. Taken together, the\ntechniques we develop allow a temporal reasoning component to solve problems\nthat are of practical size.",
    "published": "1996-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9601101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. vanBeek",
      "D. W. Manchak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9602101v1",
    "title": "Well-Founded Semantics for Extended Logic Programs with Dynamic\n  Preferences",
    "summary": "The paper describes an extension of well-founded semantics for logic programs\nwith two types of negation. In this extension information about preferences\nbetween rules can be expressed in the logical language and derived dynamically.\nThis is achieved by using a reserved predicate symbol and a naming technique.\nConflicts among rules are resolved whenever possible on the basis of derived\npreference information. The well-founded conclusions of prioritized logic\nprograms can be computed in polynomial time. A legal reasoning example\nillustrates the usefulness of the approach.",
    "published": "1996-02-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9602101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. Brewka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9602102v1",
    "title": "Logarithmic-Time Updates and Queries in Probabilistic Networks",
    "summary": "Traditional databases commonly support efficient query and update procedures\nthat operate in time which is sublinear in the size of the database. Our goal\nin this paper is to take a first step toward dynamic reasoning in probabilistic\ndatabases with comparable efficiency. We propose a dynamic data structure that\nsupports efficient algorithms for updating and querying singly connected\nBayesian networks. In the conventional algorithm, new evidence is absorbed in\nO(1) time and queries are processed in time O(N), where N is the size of the\nnetwork. We propose an algorithm which, after a preprocessing phase, allows us\nto answer queries in time O(log N) at the expense of O(log N) time per evidence\nabsorption. The usefulness of sub-linear processing time manifests itself in\napplications requiring (near) real-time response over large probabilistic\ndatabases. We briefly discuss a potential application of dynamic probabilistic\nreasoning in computational biology.",
    "published": "1996-02-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9602102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. L. Delcher",
      "A. J. Grove",
      "S. Kasif",
      "J. Pearl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9603101v1",
    "title": "Quantum Computing and Phase Transitions in Combinatorial Search",
    "summary": "We introduce an algorithm for combinatorial search on quantum computers that\nis capable of significantly concentrating amplitude into solutions for some NP\nsearch problems, on average. This is done by exploiting the same aspects of\nproblem structure as used by classical backtrack methods to avoid unproductive\nsearch choices. This quantum algorithm is much more likely to find solutions\nthan the simple direct use of quantum parallelism. Furthermore, empirical\nevaluation on small problems shows this quantum algorithm displays the same\nphase transition behavior, and at the same location, as seen in many previously\nstudied classical search methods. Specifically, difficult problem instances are\nconcentrated near the abrupt change from underconstrained to overconstrained\nproblems.",
    "published": "1996-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9603101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "T. Hogg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9603102v1",
    "title": "Mean Field Theory for Sigmoid Belief Networks",
    "summary": "We develop a mean field theory for sigmoid belief networks based on ideas\nfrom statistical mechanics. Our mean field theory provides a tractable\napproximation to the true probability distribution in these networks; it also\nyields a lower bound on the likelihood of evidence. We demonstrate the utility\nof this framework on a benchmark problem in statistical pattern\nrecognition---the classification of handwritten digits.",
    "published": "1996-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9603102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "L. K. Saul",
      "T. Jaakkola",
      "M. I. Jordan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9603103v1",
    "title": "Improved Use of Continuous Attributes in C4.5",
    "summary": "A reported weakness of C4.5 in domains with continuous attributes is\naddressed by modifying the formation and evaluation of tests on continuous\nattributes. An MDL-inspired penalty is applied to such tests, eliminating some\nof them from consideration and altering the relative desirability of all tests.\nEmpirical trials show that the modifications lead to smaller decision trees\nwith higher predictive accuracies. Results also confirm that a new version of\nC4.5 incorporating these changes is superior to recent approaches that use\nglobal discretization and that construct small trees with multi-interval\nsplits.",
    "published": "1996-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9603103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. R. Quinlan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9603104v1",
    "title": "Active Learning with Statistical Models",
    "summary": "For many types of machine learning algorithms, one can compute the\nstatistically `optimal' way to select training data. In this paper, we review\nhow optimal data selection techniques have been used with feedforward neural\nnetworks. We then show how the same principles may be used to select data for\ntwo alternative, statistically-based learning architectures: mixtures of\nGaussians and locally weighted regression. While the techniques for neural\nnetworks are computationally expensive and approximate, the techniques for\nmixtures of Gaussians and locally weighted regression are both efficient and\naccurate. Empirically, we observe that the optimality criterion sharply\ndecreases the number of training examples the learner needs in order to achieve\ngood performance.",
    "published": "1996-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9603104v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. A. Cohn",
      "Z. Ghahramani",
      "M. I. Jordan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9604101v1",
    "title": "A Divergence Critic for Inductive Proof",
    "summary": "Inductive theorem provers often diverge. This paper describes a simple\ncritic, a computer program which monitors the construction of inductive proofs\nattempting to identify diverging proof attempts. Divergence is recognized by\nmeans of a ``difference matching'' procedure. The critic then proposes lemmas\nand generalizations which ``ripple'' these differences away so that the proof\ncan go through without divergence. The critic enables the theorem prover Spike\nto prove many theorems completely automatically from the definitions alone.",
    "published": "1996-04-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9604101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "T. Walsh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9604102v1",
    "title": "Practical Methods for Proving Termination of General Logic Programs",
    "summary": "Termination of logic programs with negated body atoms (here called general\nlogic programs) is an important topic. One reason is that many computational\nmechanisms used to process negated atoms, like Clark's negation as failure and\nChan's constructive negation, are based on termination conditions. This paper\nintroduces a methodology for proving termination of general logic programs\nw.r.t. the Prolog selection rule. The idea is to distinguish parts of the\nprogram depending on whether or not their termination depends on the selection\nrule. To this end, the notions of low-, weakly up-, and up-acceptable program\nare introduced. We use these notions to develop a methodology for proving\ntermination of general logic programs, and show how interesting problems in\nnon-monotonic reasoning can be formalized and implemented by means of\nterminating general logic programs.",
    "published": "1996-04-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9604102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "E. Marchiori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9604103v1",
    "title": "Iterative Optimization and Simplification of Hierarchical Clusterings",
    "summary": "Clustering is often used for discovering structure in data. Clustering\nsystems differ in the objective function used to evaluate clustering quality\nand the control strategy used to search the space of clusterings. Ideally, the\nsearch strategy should consistently construct clusterings of high quality, but\nbe computationally inexpensive as well. In general, we cannot have it both\nways, but we can partition the search so that a system inexpensively constructs\na `tentative' clustering for initial examination, followed by iterative\noptimization, which continues to search in background for improved clusterings.\nGiven this motivation, we evaluate an inexpensive strategy for creating initial\nclusterings, coupled with several control strategies for iterative\noptimization, each of which repeatedly modifies an initial clustering in search\nof a better one. One of these methods appears novel as an iterative\noptimization strategy in clustering contexts. Once a clustering has been\nconstructed it is judged by analysts -- often according to task-specific\ncriteria. Several authors have abstracted these criteria and posited a generic\nperformance task akin to pattern completion, where the error rate over\ncompleted patterns is used to `externally' judge clustering utility. Given this\nperformance task, we adapt resampling-based pruning strategies used by\nsupervised learning systems to the task of simplifying hierarchical\nclusterings, thus promising to ease post-clustering analysis. Finally, we\npropose a number of objective functions, based on attribute-selection measures\nfor decision-tree induction, that might perform well on the error rate and\nsimplicity dimensions.",
    "published": "1996-04-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9604103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. Fisher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9605101v1",
    "title": "Further Experimental Evidence against the Utility of Occam's Razor",
    "summary": "This paper presents new experimental evidence against the utility of Occam's\nrazor. A~systematic procedure is presented for post-processing decision trees\nproduced by C4.5. This procedure was derived by rejecting Occam's razor and\ninstead attending to the assumption that similar objects are likely to belong\nto the same class. It increases a decision tree's complexity without altering\nthe performance of that tree on the training data from which it is inferred.\nThe resulting more complex decision trees are demonstrated to have, on average,\nfor a variety of common learning tasks, higher predictive accuracy than the\nless complex original decision trees. This result raises considerable doubt\nabout the utility of Occam's razor as it is commonly applied in modern machine\nlearning.",
    "published": "1996-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9605101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. I. Webb"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9605102v1",
    "title": "Least Generalizations and Greatest Specializations of Sets of Clauses",
    "summary": "The main operations in Inductive Logic Programming (ILP) are generalization\nand specialization, which only make sense in a generality order. In ILP, the\nthree most important generality orders are subsumption, implication and\nimplication relative to background knowledge. The two languages used most often\nare languages of clauses and languages of only Horn clauses. This gives a total\nof six different ordered languages. In this paper, we give a systematic\ntreatment of the existence or non-existence of least generalizations and\ngreatest specializations of finite sets of clauses in each of these six ordered\nsets. We survey results already obtained by others and also contribute some\nanswers of our own. Our main new results are, firstly, the existence of a\ncomputable least generalization under implication of every finite set of\nclauses containing at least one non-tautologous function-free clause (among\nother, not necessarily function-free clauses). Secondly, we show that such a\nleast generalization need not exist under relative implication, not even if\nboth the set that is to be generalized and the background knowledge are\nfunction-free. Thirdly, we give a complete discussion of existence and\nnon-existence of greatest specializations in each of the six ordered languages.",
    "published": "1996-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9605102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. H. Nienhuys-Cheng",
      "R. deWolf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9605103v1",
    "title": "Reinforcement Learning: A Survey",
    "summary": "This paper surveys the field of reinforcement learning from a\ncomputer-science perspective. It is written to be accessible to researchers\nfamiliar with machine learning. Both the historical basis of the field and a\nbroad selection of current work are summarized. Reinforcement learning is the\nproblem faced by an agent that learns behavior through trial-and-error\ninteractions with a dynamic environment. The work described here has a\nresemblance to work in psychology, but differs considerably in the details and\nin the use of the word ``reinforcement.'' The paper discusses central issues of\nreinforcement learning, including trading off exploration and exploitation,\nestablishing the foundations of the field via Markov decision theory, learning\nfrom delayed reinforcement, constructing empirical models to accelerate\nlearning, making use of generalization and hierarchy, and coping with hidden\nstate. It concludes with a survey of some implemented systems and an assessment\nof the practical utility of current methods for reinforcement learning.",
    "published": "1996-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9605103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "L. P. Kaelbling",
      "M. L. Littman",
      "A. W. Moore"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9605104v1",
    "title": "Adaptive Problem-solving for Large-scale Scheduling Problems: A Case\n  Study",
    "summary": "Although most scheduling problems are NP-hard, domain specific techniques\nperform well in practice but are quite expensive to construct. In adaptive\nproblem-solving solving, domain specific knowledge is acquired automatically\nfor a general problem solver with a flexible control architecture. In this\napproach, a learning system explores a space of possible heuristic methods for\none well-suited to the eccentricities of the given domain and problem\ndistribution. In this article, we discuss an application of the approach to\nscheduling satellite communications. Using problem distributions based on\nactual mission requirements, our approach identifies strategies that not only\ndecrease the amount of CPU time required to produce schedules, but also\nincrease the percentage of problems that are solvable within computational\nresource limitations.",
    "published": "1996-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9605104v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. Gratch",
      "S. Chien"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9605105v1",
    "title": "A Formal Framework for Speedup Learning from Problems and Solutions",
    "summary": "Speedup learning seeks to improve the computational efficiency of problem\nsolving with experience. In this paper, we develop a formal framework for\nlearning efficient problem solving from random problems and their solutions. We\napply this framework to two different representations of learned knowledge,\nnamely control rules and macro-operators, and prove theorems that identify\nsufficient conditions for learning in each representation. Our proofs are\nconstructive in that they are accompanied with learning algorithms. Our\nframework captures both empirical and explanation-based speedup learning in a\nunified fashion. We illustrate our framework with implementations in two\ndomains: symbolic integration and Eight Puzzle. This work integrates many\nstrands of experimental and theoretical work in machine learning, including\nempirical learning of control rules, macro-operator learning, Explanation-Based\nLearning (EBL), and Probably Approximately Correct (PAC) Learning.",
    "published": "1996-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9605105v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. Tadepalli",
      "B. K. Natarajan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9605106v1",
    "title": "2Planning for Contingencies: A Decision-based Approach",
    "summary": "A fundamental assumption made by classical AI planners is that there is no\nuncertainty in the world: the planner has full knowledge of the conditions\nunder which the plan will be executed and the outcome of every action is fully\npredictable. These planners cannot therefore construct contingency plans, i.e.,\nplans in which different actions are performed in different circumstances. In\nthis paper we discuss some issues that arise in the representation and\nconstruction of contingency plans and describe Cassandra, a partial-order\ncontingency planner. Cassandra uses explicit decision-steps that enable the\nagent executing the plan to decide which plan branch to follow. The\ndecision-steps in a plan result in subgoals to acquire knowledge, which are\nplanned for in the same way as any other subgoals. Cassandra thus distinguishes\nthe process of gathering information from the process of making decisions. The\nexplicit representation of decisions in Cassandra allows a coherent approach to\nthe problems of contingent planning, and provides a solid base for extensions\nsuch as the use of different decision-making procedures.",
    "published": "1996-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9605106v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "L. Pryor",
      "G. Collins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9606101v1",
    "title": "A Principled Approach Towards Symbolic Geometric Constraint Satisfaction",
    "summary": "An important problem in geometric reasoning is to find the configuration of a\ncollection of geometric bodies so as to satisfy a set of given constraints.\nRecently, it has been suggested that this problem can be solved efficiently by\nsymbolically reasoning about geometry. This approach, called degrees of freedom\nanalysis, employs a set of specialized routines called plan fragments that\nspecify how to change the configuration of a set of bodies to satisfy a new\nconstraint while preserving existing constraints. A potential drawback, which\nlimits the scalability of this approach, is concerned with the difficulty of\nwriting plan fragments. In this paper we address this limitation by showing how\nthese plan fragments can be automatically synthesized using first principles\nabout geometric bodies, actions, and topology.",
    "published": "1996-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9606101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Bhansali",
      "G. A. Kramer",
      "T. J. Hoar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9606102v1",
    "title": "On Partially Controlled Multi-Agent Systems",
    "summary": "Motivated by the control theoretic distinction between controllable and\nuncontrollable events, we distinguish between two types of agents within a\nmulti-agent system: controllable agents, which are directly controlled by the\nsystem's designer, and uncontrollable agents, which are not under the\ndesigner's direct control. We refer to such systems as partially controlled\nmulti-agent systems, and we investigate how one might influence the behavior of\nthe uncontrolled agents through appropriate design of the controlled agents. In\nparticular, we wish to understand which problems are naturally described in\nthese terms, what methods can be applied to influence the uncontrollable\nagents, the effectiveness of such methods, and whether similar methods work\nacross different domains. Using a game-theoretic framework, this paper studies\nthe design of partially controlled multi-agent systems in two contexts: in one\ncontext, the uncontrollable agents are expected utility maximizers, while in\nthe other they are reinforcement learners. We suggest different techniques for\ncontrolling agents' behavior in each domain, assess their success, and examine\ntheir relationship.",
    "published": "1996-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9606102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. I. Brafman",
      "M. Tennenholtz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9608103v1",
    "title": "Spatial Aggregation: Theory and Applications",
    "summary": "Visual thinking plays an important role in scientific reasoning. Based on the\nresearch in automating diverse reasoning tasks about dynamical systems,\nnonlinear controllers, kinematic mechanisms, and fluid motion, we have\nidentified a style of visual thinking, imagistic reasoning. Imagistic reasoning\norganizes computations around image-like, analogue representations so that\nperceptual and symbolic operations can be brought to bear to infer structure\nand behavior. Programs incorporating imagistic reasoning have been shown to\nperform at an expert level in domains that defy current analytic or numerical\nmethods. We have developed a computational paradigm, spatial aggregation, to\nunify the description of a class of imagistic problem solvers. A program\nwritten in this paradigm has the following properties. It takes a continuous\nfield and optional objective functions as input, and produces high-level\ndescriptions of structure, behavior, or control actions. It computes a\nmulti-layer of intermediate representations, called spatial aggregates, by\nforming equivalence classes and adjacency relations. It employs a small set of\ngeneric operators such as aggregation, classification, and localization to\nperform bidirectional mapping between the information-rich field and\nsuccessively more abstract spatial aggregates. It uses a data structure, the\nneighborhood graph, as a common interface to modularize computations. To\nillustrate our theory, we describe the computational structure of three\nimplemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the\nspatial aggregation generic operators by mixing and matching a library of\ncommonly used routines.",
    "published": "1996-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9608103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "K. Yip",
      "F. Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9608104v1",
    "title": "A Hierarchy of Tractable Subsets for Computing Stable Models",
    "summary": "Finding the stable models of a knowledge base is a significant computational\nproblem in artificial intelligence. This task is at the computational heart of\ntruth maintenance systems, autoepistemic logic, and default logic.\nUnfortunately, it is NP-hard. In this paper we present a hierarchy of classes\nof knowledge bases, Omega_1,Omega_2,..., with the following properties: first,\nOmega_1 is the class of all stratified knowledge bases; second, if a knowledge\nbase Pi is in Omega_k, then Pi has at most k stable models, and all of them may\nbe found in time O(lnk), where l is the length of the knowledge base and n the\nnumber of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find\nthe minimum k such that Pi belongs to Omega_k in time polynomial in the size of\nPi; and, last, where K is the class of all knowledge bases, it is the case that\nunion{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some\nclass in the hierarchy.",
    "published": "1996-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9608104v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. Ben-Eliyahu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9609101v1",
    "title": "Accelerating Partial-Order Planners: Some Techniques for Effective\n  Search Control and Pruning",
    "summary": "We propose some domain-independent techniques for bringing well-founded\npartial-order planners closer to practicality. The first two techniques are\naimed at improving search control while keeping overhead costs low. One is\nbased on a simple adjustment to the default A* heuristic used by UCPOP to\nselect plans for refinement. The other is based on preferring ``zero\ncommitment'' (forced) plan refinements whenever possible, and using LIFO\nprioritization otherwise. A more radical technique is the use of operator\nparameter domains to prune search. These domains are initially computed from\nthe definitions of the operators and the initial and goal conditions, using a\npolynomial-time algorithm that propagates sets of constants through the\noperator graph, starting in the initial conditions. During planning, parameter\ndomains can be used to prune nonviable operator instances and to remove\nspurious clobbering threats. In experiments based on modifications of UCPOP,\nour improved plan and goal selection strategies gave speedups by factors\nranging from 5 to more than 1000 for a variety of problems that are nontrivial\nfor the unmodified version. Crucially, the hardest problems gave the greatest\nimprovements. The pruning technique based on parameter domains often gave\nspeedups by an order of magnitude or more for difficult problems, both with the\ndefault UCPOP search strategy and with our improved strategy. The Lisp code for\nour techniques and for the test problems is provided in on-line appendices.",
    "published": "1996-09-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9609101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Gerevini",
      "L. Schubert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9609102v1",
    "title": "Cue Phrase Classification Using Machine Learning",
    "summary": "Cue phrases may be used in a discourse sense to explicitly signal discourse\nstructure, but also in a sentential sense to convey semantic rather than\nstructural information. Correctly classifying cue phrases as discourse or\nsentential is critical in natural language processing systems that exploit\ndiscourse structure, e.g., for performing tasks such as anaphora resolution and\nplan recognition. This paper explores the use of machine learning for\nclassifying cue phrases as discourse or sentential. Two machine learning\nprograms (Cgrendel and C4.5) are used to induce classification models from sets\nof pre-classified cue phrases and their features in text and speech. Machine\nlearning is shown to be an effective technique for not only automating the\ngeneration of classification models, but also for improving upon previous\nresults. When compared to manually derived classification models already in the\nliterature, the learned models often perform with higher accuracy and contain\nnew linguistic insights into the data. In addition, the ability to\nautomatically construct classification models makes it easier to comparatively\nanalyze the utility of alternative feature representations of the data.\nFinally, the ease of retraining makes the learning approach more scalable and\nflexible than manual methods.",
    "published": "1996-09-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9609102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. J. Litman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9610101v1",
    "title": "Mechanisms for Automated Negotiation in State Oriented Domains",
    "summary": "This paper lays part of the groundwork for a domain theory of negotiation,\nthat is, a way of classifying interactions so that it is clear, given a domain,\nwhich negotiation mechanisms and strategies are appropriate. We define State\nOriented Domains, a general category of interaction. Necessary and sufficient\nconditions for cooperation are outlined. We use the notion of worth in an\naltered definition of utility, thus enabling agreements in a wider class of\njoint-goal reachable situations. An approach is offered for conflict\nresolution, and it is shown that even in a conflict situation, partial\ncooperative steps can be taken by interacting agents (that is, agents in\nfundamental conflict might still agree to cooperate up to a certain point). A\nUnified Negotiation Protocol (UNP) is developed that can be used in all types\nof encounters. It is shown that in certain borderline cooperative situations, a\npartial cooperative agreement (i.e., one that does not achieve all agents'\ngoals) might be preferred by all agents, even though there exists a rational\nagreement that would achieve all their goals. Finally, we analyze cases where\nagents have incomplete information on the goals and worth of other agents.\nFirst we consider the case where agents' goals are private information, and we\nanalyze what goal declaration strategies the agents might adopt to increase\ntheir utility. Then, we consider the situation where the agents' goals (and\ntherefore stand-alone costs) are common knowledge, but the worth they attach to\ntheir goals is private information. We introduce two mechanisms, one 'strict',\nthe other 'tolerant', and analyze their affects on the stability and efficiency\nof negotiation outcomes.",
    "published": "1996-10-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9610101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. Zlotkin",
      "J. S. Rosenschein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9610102v1",
    "title": "Learning First-Order Definitions of Functions",
    "summary": "First-order learning involves finding a clause-form definition of a relation\nfrom examples of the relation and relevant background information. In this\npaper, a particular first-order learning system is modified to customize it for\nfinding definitions of functional relations. This restriction leads to faster\nlearning times and, in some cases, to definitions that have higher predictive\naccuracy. Other first-order learning systems might benefit from similar\nspecialization.",
    "published": "1996-10-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9610102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. R. Quinlan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9611101v1",
    "title": "MUSE CSP: An Extension to the Constraint Satisfaction Problem",
    "summary": "This paper describes an extension to the constraint satisfaction problem\n(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).\nThis extension is especially useful for those problems which segment into\nmultiple sets of partially shared variables. Such problems arise naturally in\nsignal processing applications including computer vision, speech processing,\nand handwriting recognition. For these applications, it is often difficult to\nsegment the data in only one way given the low-level information utilized by\nthe segmentation algorithms. MUSE CSP can be used to compactly represent\nseveral similar instances of the constraint satisfaction problem. If multiple\ninstances of a CSP have some common variables which have the same domains and\nconstraints, then they can be combined into a single instance of a MUSE CSP,\nreducing the work required to apply the constraints. We introduce the concepts\nof MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We\nthen demonstrate how MUSE CSP can be used to compactly represent lexically\nambiguous sentences and the multiple sentence hypotheses that are often\ngenerated by speech recognition algorithms so that grammar constraints can be\nused to provide parses for all syntactically correct sentences. Algorithms for\nMUSE arc and path consistency are provided. Finally, we discuss how to create a\nMUSE CSP from a set of CSPs which are labeled to indicate when the same\nvariable is shared by more than a single CSP.",
    "published": "1996-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9611101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "R. A Helzerman",
      "M. P. Harper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9612101v1",
    "title": "Exploiting Causal Independence in Bayesian Network Inference",
    "summary": "A new method is proposed for exploiting causal independencies in exact\nBayesian network inference. A Bayesian network can be viewed as representing a\nfactorization of a joint probability into the multiplication of a set of\nconditional probabilities. We present a notion of causal independence that\nenables one to further factorize the conditional probabilities into a\ncombination of even smaller factors and consequently obtain a finer-grain\nfactorization of the joint probability. The new formulation of causal\nindependence lets us specify the conditional probability of a variable given\nits parents in terms of an associative and commutative operator, such as\n``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a\nsimple algorithm VE for Bayesian network inference that, given evidence and a\nquery variable, uses the factorization to find the posterior distribution of\nthe query. We show how this algorithm can be extended to exploit causal\nindependence. Empirical studies, based on the CPCS networks for medical\ndiagnosis, show that this method is more efficient than previous methods and\nallows for inference in larger networks than previous algorithms.",
    "published": "1996-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9612101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "N. L. Zhang",
      "D. Poole"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9612102v1",
    "title": "Quantitative Results Comparing Three Intelligent Interfaces for\n  Information Capture: A Case Study Adding Name Information into an Electronic\n  Personal Organizer",
    "summary": "Efficiently entering information into a computer is key to enjoying the\nbenefits of computing. This paper describes three intelligent user interfaces:\nhandwriting recognition, adaptive menus, and predictive fillin. In the context\nof adding a personUs name and address to an electronic organizer, tests show\nhandwriting recognition is slower than typing on an on-screen, soft keyboard,\nwhile adaptive menus and predictive fillin can be twice as fast. This paper\nalso presents strategies for applying these three interfaces to other\ninformation collection domains.",
    "published": "1996-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9612102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. C. Schlimmer",
      "P. C. Wells"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9612103v1",
    "title": "Characterizations of Decomposable Dependency Models",
    "summary": "Decomposable dependency models possess a number of interesting and useful\nproperties. This paper presents new characterizations of decomposable models in\nterms of independence relationships, which are obtained by adding a single\naxiom to the well-known set characterizing dependency models that are\nisomorphic to undirected graphs. We also briefly discuss a potential\napplication of our results to the problem of learning graphical models from\ndata.",
    "published": "1996-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9612103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "L. M. deCampos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9701101v1",
    "title": "Improved Heterogeneous Distance Functions",
    "summary": "Instance-based learning techniques typically handle continuous and linear\ninput values well, but often do not handle nominal input attributes\nappropriately. The Value Difference Metric (VDM) was designed to find\nreasonable distance values between nominal attribute values, but it largely\nignores continuous attributes, requiring discretization to map continuous\nvalues into nominal values. This paper proposes three new heterogeneous\ndistance functions, called the Heterogeneous Value Difference Metric (HVDM),\nthe Interpolated Value Difference Metric (IVDM), and the Windowed Value\nDifference Metric (WVDM). These new distance functions are designed to handle\napplications with nominal attributes, continuous attributes, or both. In\nexperiments on 48 applications the new distance metrics achieve higher\nclassification accuracy on average than three previous distance functions on\nthose datasets that have both nominal and continuous attributes.",
    "published": "1997-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9701101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. R. Wilson",
      "T. R. Martinez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9701102v1",
    "title": "SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis\n  Using Artificial Neural Networks",
    "summary": "Previous approaches of analyzing spontaneously spoken language often have\nbeen based on encoding syntactic and semantic knowledge manually and\nsymbolically. While there has been some progress using statistical or\nconnectionist language models, many current spoken- language systems still use\na relatively brittle, hand-coded symbolic grammar or symbolic semantic\ncomponent. In contrast, we describe a so-called screening approach for learning\nrobust processing of spontaneously spoken language. A screening approach is a\nflat analysis which uses shallow sequences of category representations for\nanalyzing an utterance at various syntactic, semantic and dialog levels. Rather\nthan using a deeply structured symbolic analysis, we use a flat connectionist\nanalysis. This screening approach aims at supporting speech and language\nprocessing by using (1) data-driven learning and (2) robustness of\nconnectionist networks. In order to test this approach, we have developed the\nSCREEN system which is based on this new robust, learned and flat analysis. In\nthis paper, we focus on a detailed description of SCREEN's architecture, the\nflat syntactic and semantic analysis, the interaction with a speech recognizer,\nand a detailed evaluation analysis of the robustness under the influence of\nnoisy or incomplete input. The main result of this paper is that flat\nrepresentations allow more robust processing of spontaneous spoken language\nthan deeply structured representations. In particular, we show how the\nfault-tolerance and learning capability of connectionist networks can support a\nflat analysis for providing more robust spoken-language processing within an\noverall hybrid symbolic/connectionist framework.",
    "published": "1997-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9701102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Wermter",
      "V. Weber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9703101v1",
    "title": "A Uniform Framework for Concept Definitions in Description Logics",
    "summary": "Most modern formalisms used in Databases and Artificial Intelligence for\ndescribing an application domain are based on the notions of class (or concept)\nand relationship among classes. One interesting feature of such formalisms is\nthe possibility of defining a class, i.e., providing a set of properties that\nprecisely characterize the instances of the class. Many recent articles point\nout that there are several ways of assigning a meaning to a class definition\ncontaining some sort of recursion. In this paper, we argue that, instead of\nchoosing a single style of semantics, we achieve better results by adopting a\nformalism that allows for different semantics to coexist. We demonstrate the\nfeasibility of our argument, by presenting a knowledge representation\nformalism, the description logic muALCQ, with the above characteristics. In\naddition to the constructs for conjunction, disjunction, negation, quantifiers,\nand qualified number restrictions, muALCQ includes special fixpoint constructs\nto express (suitably interpreted) recursive definitions. These constructs\nenable the usual frame-based descriptions to be combined with definitions of\nrecursive data structures such as directed acyclic graphs, lists, streams, etc.\nWe establish several properties of muALCQ, including the decidability and the\ncomputational complexity of reasoning, by formulating a correspondence with a\nparticular modal logic of programs called the modal mu-calculus.",
    "published": "1997-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9703101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. DeGiacomo",
      "M. Lenzerini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cond-mat/9703183v1",
    "title": "Finite size scaling of the bayesian perceptron",
    "summary": "We study numerically the properties of the bayesian perceptron through a\ngradient descent on the optimal cost function. The theoretical distribution of\nstabilities is deduced. It predicts that the optimal generalizer lies close to\nthe boundary of the space of (error-free) solutions. The numerical simulations\nare in good agreement with the theoretical distribution. The extrapolation of\nthe generalization error to infinite input space size agrees with the\ntheoretical results. Finite size corrections are negative and exhibit two\ndifferent scaling regimes, depending on the training set size. The variance of\nthe generalization error vanishes for $N \\rightarrow \\infty$ confirming the\nproperty of self-averaging.",
    "published": "1997-03-20T15:54:36Z",
    "link": "http://arxiv.org/pdf/cond-mat/9703183v1.pdf",
    "category": [
      "cond-mat.stat-mech",
      "cond-mat.dis-nn",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "A. Buhot",
      "J. -M. Torres Moreno",
      "M. B. Gordon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9704101v1",
    "title": "Lifeworld Analysis",
    "summary": "We argue that the analysis of agent/environment interactions should be\nextended to include the conventions and invariants maintained by agents\nthroughout their activity. We refer to this thicker notion of environment as a\nlifeworld and present a partial set of formal tools for describing structures\nof lifeworlds and the ways in which they computationally simplify activity. As\none specific example, we apply the tools to the analysis of the Toast system\nand show how versions of the system with very different control structures in\nfact implement a common control structure together with different conventions\nfor encoding task state in the positions or states of objects in the\nenvironment.",
    "published": "1997-04-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9704101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. Agre",
      "I. Horswill"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9705101v1",
    "title": "Query DAGs: A Practical Paradigm for Implementing Belief-Network\n  Inference",
    "summary": "We describe a new paradigm for implementing inference in belief networks,\nwhich consists of two steps: (1) compiling a belief network into an arithmetic\nexpression called a Query DAG (Q-DAG); and (2) answering queries using a simple\nevaluation algorithm. Each node of a Q-DAG represents a numeric operation, a\nnumber, or a symbol for evidence. Each leaf node of a Q-DAG represents the\nanswer to a network query, that is, the probability of some event of interest.\nIt appears that Q-DAGs can be generated using any of the standard algorithms\nfor exact inference in belief networks (we show how they can be generated using\nclustering and conditioning algorithms). The time and space complexity of a\nQ-DAG generation algorithm is no worse than the time complexity of the\ninference algorithm on which it is based. The complexity of a Q-DAG evaluation\nalgorithm is linear in the size of the Q-DAG, and such inference amounts to a\nstandard evaluation of the arithmetic expression it represents. The intended\nvalue of Q-DAGs is in reducing the software and hardware resources required to\nutilize belief networks in on-line, real-world applications. The proposed\nframework also facilitates the development of on-line inference on different\nsoftware and hardware platforms due to the simplicity of the Q-DAG evaluation\nalgorithm. Interestingly enough, Q-DAGs were found to serve other purposes:\nsimple techniques for reducing Q-DAGs tend to subsume relatively complex\noptimization techniques for belief-network inference, such as network-pruning\nand computation-caching.",
    "published": "1997-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9705101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Darwiche",
      "G. Provan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9705102v1",
    "title": "Connectionist Theory Refinement: Genetically Searching the Space of\n  Network Topologies",
    "summary": "An algorithm that learns from a set of examples should ideally be able to\nexploit the available resources of (a) abundant computing power and (b)\ndomain-specific knowledge to improve its ability to generalize. Connectionist\ntheory-refinement systems, which use background knowledge to select a neural\nnetwork's topology and initial weights, have proven to be effective at\nexploiting domain-specific knowledge; however, most do not exploit available\ncomputing power. This weakness occurs because they lack the ability to refine\nthe topology of the neural networks they produce, thereby limiting\ngeneralization, especially when given impoverished domain theories. We present\nthe REGENT algorithm which uses (a) domain-specific knowledge to help create an\ninitial population of knowledge-based neural networks and (b) genetic operators\nof crossover and mutation (specifically designed for knowledge-based networks)\nto continually search for better network topologies. Experiments on three\nreal-world domains indicate that our new algorithm is able to significantly\nincrease generalization compared to a standard connectionist theory-refinement\nsystem, as well as our previous algorithm for growing knowledge-based networks.",
    "published": "1997-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9705102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. W. Opitz",
      "J. W. Shavlik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9706101v1",
    "title": "Flaw Selection Strategies for Partial-Order Planning",
    "summary": "Several recent studies have compared the relative efficiency of alternative\nflaw selection strategies for partial-order causal link (POCL) planning. We\nreview this literature, and present new experimental results that generalize\nthe earlier work and explain some of the discrepancies in it. In particular, we\ndescribe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by\nJoslin and Pollack (1994), and compare it with other strategies, including\nGerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very\ndifferent, and apparently conflicting claims about the most effective way to\nreduce search-space size in POCL planning. We resolve this conflict, arguing\nthat much of the benefit that Gerevini and Schubert ascribe to the LIFO\ncomponent of their ZLIFO strategy is better attributed to other causes. We show\nthat for many problems, a strategy that combines least-cost flaw selection with\nthe delay of separable threats will be effective in reducing search-space size,\nand will do so without excessive computational overhead. Although such a\nstrategy thus provides a good default, we also show that certain domain\ncharacteristics may reduce its effectiveness.",
    "published": "1997-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9706101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. E. Pollack",
      "D. Joslin",
      "M. Paolucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9706102v1",
    "title": "A Complete Classification of Tractability in RCC-5",
    "summary": "We investigate the computational properties of the spatial algebra RCC-5\nwhich is a restricted version of the RCC framework for spatial reasoning. The\nsatisfiability problem for RCC-5 is known to be NP-complete but not much is\nknown about its approximately four billion subclasses. We provide a complete\nclassification of satisfiability for all these subclasses into polynomial and\nNP-complete respectively. In the process, we identify all maximal tractable\nsubalgebras which are four in total.",
    "published": "1997-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9706102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "P. Jonsson",
      "T. Drakengren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9707101v1",
    "title": "A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search\n  Difficulty",
    "summary": "The easy-hard-easy pattern in the difficulty of combinatorial search problems\nas constraints are added has been explained as due to a competition between the\ndecrease in number of solutions and increased pruning. We test the generality\nof this explanation by examining one of its predictions: if the number of\nsolutions is held fixed by the choice of problems, then increased pruning\nshould lead to a monotonic decrease in search cost. Instead, we find the\neasy-hard-easy pattern in median search cost even when the number of solutions\nis held constant, for some search methods. This generalizes previous\nobservations of this pattern and shows that the existing theory does not\nexplain the full range of the peak in search cost. In these cases the pattern\nappears to be due to changes in the size of the minimal unsolvable subproblems,\nrather than changing numbers of solutions.",
    "published": "1997-07-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9707101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. L. Mammen",
      "T. Hogg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9707102v1",
    "title": "Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time",
    "summary": "This paper combines two important directions of research in temporal\nresoning: that of finding maximal tractable subclasses of Allen's interval\nalgebra, and that of reasoning with metric temporal information. Eight new\nmaximal tractable subclasses of Allen's interval algebra are presented, some of\nthem subsuming previously reported tractable algebras. The algebras allow for\nmetric temporal constraints on interval starting or ending points, using the\nrecent framework of Horn DLRs. Two of the algebras can express the notion of\nsequentiality between intervals, being the first such algebras admitting both\nqualitative and metric time.",
    "published": "1997-07-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9707102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "T. Drakengren",
      "P. Jonsson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9707103v1",
    "title": "Defining Relative Likelihood in Partially-Ordered Preferential\n  Structures",
    "summary": "Starting with a likelihood or preference order on worlds, we extend it to a\nlikelihood ordering on sets of worlds in a natural way, and examine the\nresulting logic. Lewis earlier considered such a notion of relative likelihood\nin the context of studying counterfactuals, but he assumed a total preference\norder on worlds. Complications arise when examining partial orders that are not\npresent for total orders. There are subtleties involving the exact approach to\nlifting the order on worlds to an order on sets of worlds. In addition, the\naxiomatization of the logic of relative likelihood in the case of partial\norders gives insight into the connection between relative likelihood and\ndefault reasoning.",
    "published": "1997-07-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9707103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9709101v1",
    "title": "Towards Flexible Teamwork",
    "summary": "Many AI researchers are today striving to build agent teams for complex,\ndynamic multi-agent domains, with intended applications in arenas such as\neducation, training, entertainment, information integration, and collective\nrobotics. Unfortunately, uncertainties in these complex, dynamic domains\nobstruct coherent teamwork. In particular, team members often encounter\ndiffering, incomplete, and possibly inconsistent views of their environment.\nFurthermore, team members can unexpectedly fail in fulfilling responsibilities\nor discover unexpected opportunities. Highly flexible coordination and\ncommunication is key in addressing such uncertainties. Simply fitting\nindividual agents with precomputed coordination plans will not do, for their\ninflexibility can cause severe failures in teamwork, and their\ndomain-specificity hinders reusability. Our central hypothesis is that the key\nto such flexibility and reusability is providing agents with general models of\nteamwork. Agents exploit such models to autonomously reason about coordination\nand communication, providing requisite flexibility. Furthermore, the models\nenable reuse across domains, both saving implementation effort and enforcing\nconsistency. This article presents one general, implemented model of teamwork,\ncalled STEAM. The basic building block of teamwork in STEAM is joint intentions\n(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a\n(partial) hierarchy of joint intentions (this hierarchy is seen to parallel\nGrosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members\nmonitor the team's and individual members' performance, reorganizing the team\nas necessary. Finally, decision-theoretic communication selectivity in STEAM\nensures reduction in communication overheads of teamwork, with appropriate\nsensitivity to the environmental conditions. This article describes STEAM's\napplication in three different complex domains, and presents detailed empirical\nresults.",
    "published": "1997-09-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9709101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. Tambe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9709102v1",
    "title": "Identifying Hierarchical Structure in Sequences: A linear-time algorithm",
    "summary": "SEQUITUR is an algorithm that infers a hierarchical structure from a sequence\nof discrete symbols by replacing repeated phrases with a grammatical rule that\ngenerates the phrase, and continuing this process recursively. The result is a\nhierarchical representation of the original sequence, which offers insights\ninto its lexical structure. The algorithm is driven by two constraints that\nreduce the size of the grammar, and produce structure as a by-product. SEQUITUR\nbreaks new ground by operating incrementally. Moreover, the method's simple\nstructure permits a proof that it operates in space and time that is linear in\nthe size of the input. Our implementation can process 50,000 symbols per second\nand has been applied to an extensive range of real world sequences.",
    "published": "1997-09-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9709102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "C. G. Nevill-Manning",
      "I. H. Witten"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9710101v1",
    "title": "Analysis of Three-Dimensional Protein Images",
    "summary": "A fundamental goal of research in molecular biology is to understand protein\nstructure. Protein crystallography is currently the most successful method for\ndetermining the three-dimensional (3D) conformation of a protein, yet it\nremains labor intensive and relies on an expert's ability to derive and\nevaluate a protein scene model. In this paper, the problem of protein structure\ndetermination is formulated as an exercise in scene analysis. A computational\nmethodology is presented in which a 3D image of a protein is segmented into a\ngraph of critical points. Bayesian and certainty factor approaches are\ndescribed and used to analyze critical point graphs and identify meaningful\nsubstructures, such as alpha-helices and beta-sheets. Results of applying the\nmethodologies to protein images at low and medium resolution are reported. The\nresearch is related to approaches to representation, segmentation and\nclassification in vision, as well as to top-down approaches to protein\nstructure prediction.",
    "published": "1997-10-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9710101v1.pdf",
    "category": [
      "cs.AI",
      "q-bio"
    ],
    "authors": [
      "L. Leherte",
      "J. Glasgow",
      "K. Baxter",
      "E. Steeg",
      "S. Fortier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9711102v1",
    "title": "Storing and Indexing Plan Derivations through Explanation-based Analysis\n  of Retrieval Failures",
    "summary": "Case-Based Planning (CBP) provides a way of scaling up domain-independent\nplanning to solve large problems in complex domains. It replaces the detailed\nand lengthy search for a solution with the retrieval and adaptation of previous\nplanning experiences. In general, CBP has been demonstrated to improve\nperformance over generative (from-scratch) planning. However, the performance\nimprovements it provides are dependent on adequate judgements as to problem\nsimilarity. In particular, although CBP may substantially reduce planning\neffort overall, it is subject to a mis-retrieval problem. The success of CBP\ndepends on these retrieval errors being relatively rare. This paper describes\nthe design and implementation of a replay framework for the case-based planner\nDERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating\nexplanation-based learning techniques that allow it to explain and learn from\nthe retrieval failures it encounters. These techniques are used to refine\njudgements about case similarity in response to feedback when a wrong decision\nhas been made. The same failure analysis is used in building the case library,\nthrough the addition of repairing cases. Large problems are split and stored as\nsingle goal subproblems. Multi-goal problems are stored only when these smaller\ncases fail to be merged into a full solution. An empirical evaluation of this\napproach demonstrates the advantage of learning from experienced retrieval\nfailure.",
    "published": "1997-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9711102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "L. H. Ihrig",
      "S. Kambhampati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9711103v1",
    "title": "A Model Approximation Scheme for Planning in Partially Observable\n  Stochastic Domains",
    "summary": "Partially observable Markov decision processes (POMDPs) are a natural model\nfor planning problems where effects of actions are nondeterministic and the\nstate of the world is not completely observable. It is difficult to solve\nPOMDPs exactly. This paper proposes a new approximation scheme. The basic idea\nis to transform a POMDP into another one where additional information is\nprovided by an oracle. The oracle informs the planning agent that the current\nstate of the world is in a certain region. The transformed POMDP is\nconsequently said to be region observable. It is easier to solve than the\noriginal POMDP. We propose to solve the transformed POMDP and use its optimal\npolicy to construct an approximate policy for the original POMDP. By\ncontrolling the amount of additional information that the oracle provides, it\nis possible to find a proper tradeoff between computational time and\napproximation quality. In terms of algorithmic contributions, we study in\ndetails how to exploit region observability in solving the transformed POMDP.\nTo facilitate the study, we also propose a new exact algorithm for general\nPOMDPs. The algorithm is conceptually simple and yet is significantly more\nefficient than all previous exact algorithms.",
    "published": "1997-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9711103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "N. L. Zhang",
      "W. Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9711104v1",
    "title": "Dynamic Non-Bayesian Decision Making",
    "summary": "The model of a non-Bayesian agent who faces a repeated game with incomplete\ninformation against Nature is an appropriate tool for modeling general\nagent-environment interactions. In such a model the environment state\n(controlled by Nature) may change arbitrarily, and the feedback/reward function\nis initially unknown. The agent is not Bayesian, that is he does not form a\nprior probability neither on the state selection strategy of Nature, nor on his\nreward function. A policy for the agent is a function which assigns an action\nto every history of observations and actions. Two basic feedback structures are\nconsidered. In one of them -- the perfect monitoring case -- the agent is able\nto observe the previous environment state as part of his feedback, while in the\nother -- the imperfect monitoring case -- all that is available to the agent is\nthe reward obtained. Both of these settings refer to partially observable\nprocesses, where the current environment state is unknown. Our main result\nrefers to the competitive ratio criterion in the perfect monitoring case. We\nprove the existence of an efficient stochastic policy that ensures that the\ncompetitive ratio is obtained at almost all stages with an arbitrarily high\nprobability, where efficiency is measured in terms of rate of convergence. It\nis further shown that such an optimal policy does not exist in the imperfect\nmonitoring case. Moreover, it is proved that in the perfect monitoring case\nthere does not exist a deterministic policy that satisfies our long run\noptimality criterion. In addition, we discuss the maxmin criterion and prove\nthat a deterministic efficient optimal strategy does exist in the imperfect\nmonitoring case under this criterion. Finally we show that our approach to\nlong-run optimality can be viewed as qualitative, which distinguishes it from\nprevious work in this area.",
    "published": "1997-11-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9711104v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "D. Monderer",
      "M. Tennenholtz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9712101v1",
    "title": "When Gravity Fails: Local Search Topology",
    "summary": "Local search algorithms for combinatorial search problems frequently\nencounter a sequence of states in which it is impossible to improve the value\nof the objective function; moves through these regions, called plateau moves,\ndominate the time spent in local search. We analyze and characterize plateaus\nfor three different classes of randomly generated Boolean Satisfiability\nproblems. We identify several interesting features of plateaus that impact the\nperformance of local search algorithms. We show that local minima tend to be\nsmall but occasionally may be very large. We also show that local minima can be\nescaped without unsatisfying a large number of clauses, but that systematically\nsearching for an escape route may be computationally expensive if the local\nminimum is large. We show that plateaus with exits, called benches, tend to be\nmuch larger than minima, and that some benches have very few exit states which\nlocal search can use to escape. We show that the solutions (i.e., global\nminima) of randomly generated problem instances form clusters, which behave\nsimilarly to local minima. We revisit several enhancements of local search\nalgorithms and explain their performance in light of our results. Finally we\ndiscuss strategies for creating the next generation of local search algorithms.",
    "published": "1997-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9712101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. Frank",
      "P. Cheeseman",
      "J. Stutz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9712102v1",
    "title": "Bidirectional Heuristic Search Reconsidered",
    "summary": "The assessment of bidirectional heuristic search has been incorrect since it\nwas first published more than a quarter of a century ago. For quite a long\ntime, this search strategy did not achieve the expected results, and there was\na major misunderstanding about the reasons behind it. Although there is still\nwide-spread belief that bidirectional heuristic search is afflicted by the\nproblem of search frontiers passing each other, we demonstrate that this\nconjecture is wrong. Based on this finding, we present both a new generic\napproach to bidirectional heuristic search and a new approach to dynamically\nimproving heuristic values that is feasible in bidirectional search only. These\napproaches are put into perspective with both the traditional and more recently\nproposed approaches in order to facilitate a better overall understanding.\nEmpirical results of experiments with our new approaches show that\nbidirectional heuristic search can be performed very efficiently and also with\nlimited memory. These results suggest that bidirectional heuristic search\nappears to be better for solving certain difficult problems than corresponding\nunidirectional search. This provides some evidence for the usefulness of a\nsearch strategy that was long neglected. In summary, we show that bidirectional\nheuristic search is viable and consequently propose that it be reconsidered.",
    "published": "1997-12-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9712102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "H. Kaindl",
      "G. Kainz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9801101v1",
    "title": "Incremental Recompilation of Knowledge",
    "summary": "Approximating a general formula from above and below by Horn formulas (its\nHorn envelope and Horn core, respectively) was proposed by Selman and Kautz\n(1991, 1996) as a form of ``knowledge compilation,'' supporting rapid\napproximate reasoning; on the negative side, this scheme is static in that it\nsupports no updates, and has certain complexity drawbacks pointed out by\nKavvadias, Papadimitriou and Sideri (1993). On the other hand, the many\nframeworks and schemes proposed in the literature for theory update and\nrevision are plagued by serious complexity-theoretic impediments, even in the\nHorn case, as was pointed out by Eiter and Gottlob (1992), and is further\ndemonstrated in the present paper. More fundamentally, these schemes are not\ninductive, in that they may lose in a single update any positive properties of\nthe represented sets of formulas (small size, Horn structure, etc.). In this\npaper we propose a new scheme, incremental recompilation, which combines Horn\napproximation and model-based updates; this scheme is inductive and very\nefficient, free of the problems facing its constituents. A set of formulas is\nrepresented by an upper and lower Horn approximation. To update, we replace the\nupper Horn formula by the Horn envelope of its minimum-change update, and\nsimilarly the lower one by the Horn core of its update; the key fact which\nenables this scheme is that Horn envelopes and cores are easy to compute when\nthe underlying formula is the result of a minimum-change update of a Horn\nformula by a clause. We conjecture that efficient algorithms are possible for\nmore complex updates.",
    "published": "1998-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9801101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "G. Gogic",
      "C. H. Papadimitriou",
      "M. Sideri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9801102v1",
    "title": "Monotonicity and Persistence in Preferential Logics",
    "summary": "An important characteristic of many logics for Artificial Intelligence is\ntheir nonmonotonicity. This means that adding a formula to the premises can\ninvalidate some of the consequences. There may, however, exist formulae that\ncan always be safely added to the premises without destroying any of the\nconsequences: we say they respect monotonicity. Also, there may be formulae\nthat, when they are a consequence, can not be invalidated when adding any\nformula to the premises: we call them conservative. We study these two classes\nof formulae for preferential logics, and show that they are closely linked to\nthe formulae whose truth-value is preserved along the (preferential) ordering.\nWe will consider some preferential logics for illustration, and prove syntactic\ncharacterization results for them. The results in this paper may improve the\nefficiency of theorem provers for preferential logics.",
    "published": "1998-01-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9801102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. Engelfriet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9803101v1",
    "title": "Synthesizing Customized Planners from Specifications",
    "summary": "Existing plan synthesis approaches in artificial intelligence fall into two\ncategories -- domain independent and domain dependent. The domain independent\napproaches are applicable across a variety of domains, but may not be very\nefficient in any one given domain. The domain dependent approaches need to be\n(re)designed for each domain separately, but can be very efficient in the\ndomain for which they are designed. One enticing alternative to these\napproaches is to automatically synthesize domain independent planners given the\nknowledge about the domain and the theory of planning. In this paper, we\ninvestigate the feasibility of using existing automated software synthesis\ntools to support such synthesis. Specifically, we describe an architecture\ncalled CLAY in which the Kestrel Interactive Development System (KIDS) is used\nto derive a domain-customized planner through a semi-automatic combination of a\ndeclarative theory of planning, and the declarative control knowledge specific\nto a given domain, to semi-automatically combine them to derive\ndomain-customized planners. We discuss what it means to write a declarative\ntheory of planning and control knowledge for KIDS, and illustrate our approach\nby generating a class of domain-specific planners using state space\nrefinements. Our experiments show that the synthesized planners can outperform\nclassical refinement planners (implemented as instantiations of UCP,\nKambhampati & Srivastava, 1995), using the same control knowledge. We will\ncontrast the costs and benefits of the synthesis approach with conventional\nmethods for customizing domain independent planners.",
    "published": "1998-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9803101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "B. Srivastava",
      "S. Kambhampati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9803102v1",
    "title": "Cached Sufficient Statistics for Efficient Machine Learning with Large\n  Datasets",
    "summary": "This paper introduces new algorithms and data structures for quick counting\nfor machine learning datasets. We focus on the counting task of constructing\ncontingency tables, but our approach is also applicable to counting the number\nof records in a dataset that match conjunctive queries. Subject to certain\nassumptions, the costs of these operations can be shown to be independent of\nthe number of records in the dataset and loglinear in the number of non-zero\nentries in the contingency table. We provide a very sparse data structure, the\nADtree, to minimize memory use. We provide analytical worst-case bounds for\nthis structure for several models of data distribution. We empirically\ndemonstrate that tractably-sized data structures can be produced for large\nreal-world datasets by (a) using a sparse tree structure that never allocates\nmemory for counts of zero, (b) never allocating memory for counts that can be\ndeduced from other counts, and (c) not bothering to expand the tree fully near\nits leaves. We show how the ADtree can be used to accelerate Bayes net\nstructure finding algorithms, rule learning algorithms, and feature selection\nalgorithms, and we provide a number of empirical results comparing ADtree\nmethods against traditional direct counting approaches. We also discuss the\npossible uses of ADtrees in other machine learning methods, and discuss the\nmerits of ADtrees in comparison with alternative representations such as\nkd-trees, R-trees and Frequent Sets.",
    "published": "1998-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9803102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Moore",
      "M. S. Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9803103v1",
    "title": "Tractability of Theory Patching",
    "summary": "In this paper we consider the problem of `theory patching', in which we are\ngiven a domain theory, some of whose components are indicated to be possibly\nflawed, and a set of labeled training examples for the domain concept. The\ntheory patching problem is to revise only the indicated components of the\ntheory, such that the resulting theory correctly classifies all the training\nexamples. Theory patching is thus a type of theory revision in which revisions\nare made to individual components of the theory. Our concern in this paper is\nto determine for which classes of logical domain theories the theory patching\nproblem is tractable. We consider both propositional and first-order domain\ntheories, and show that the theory patching problem is equivalent to that of\ndetermining what information contained in a theory is `stable' regardless of\nwhat revisions might be performed to the theory. We show that determining\nstability is tractable if the input theory satisfies two conditions: that\nrevisions to each theory component have monotonic effects on the classification\nof examples, and that theory components act independently in the classification\nof examples in the theory. We also show how the concepts introduced can be used\nto determine the soundness and completeness of particular theory patching\nalgorithms.",
    "published": "1998-03-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9803103v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "S. Argamon-Engelson",
      "M. Koppel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9805101v1",
    "title": "Integrative Windowing",
    "summary": "In this paper we re-investigate windowing for rule learning algorithms. We\nshow that, contrary to previous results for decision tree learning, windowing\ncan in fact achieve significant run-time gains in noise-free domains and\nexplain the different behavior of rule learning algorithms by the fact that\nthey learn each rule independently. The main contribution of this paper is\nintegrative windowing, a new type of algorithm that further exploits this\nproperty by integrating good rules into the final theory right after they have\nbeen discovered. Thus it avoids re-learning these rules in subsequent\niterations of the windowing process. Experimental evidence in a variety of\nnoise-free domains shows that integrative windowing can in fact achieve\nsubstantial run-time gains. Furthermore, we discuss the problem of noise in\nwindowing and present an algorithm that is able to achieve run-time gains in a\nset of experiments in a simple domain with artificial noise.",
    "published": "1998-05-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9805101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "J. Fürnkranz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9806101v1",
    "title": "Model-Based Diagnosis using Structured System Descriptions",
    "summary": "This paper presents a comprehensive approach for model-based diagnosis which\nincludes proposals for characterizing and computing preferred diagnoses,\nassuming that the system description is augmented with a system structure (a\ndirected graph explicating the interconnections between system components).\nSpecifically, we first introduce the notion of a consequence, which is a\nsyntactically unconstrained propositional sentence that characterizes all\nconsistency-based diagnoses and show that standard characterizations of\ndiagnoses, such as minimal conflicts, correspond to syntactic variations on a\nconsequence. Second, we propose a new syntactic variation on the consequence\nknown as negation normal form (NNF) and discuss its merits compared to standard\nvariations. Third, we introduce a basic algorithm for computing consequences in\nNNF given a structured system description. We show that if the system structure\ndoes not contain cycles, then there is always a linear-size consequence in NNF\nwhich can be computed in linear time. For arbitrary system structures, we show\na precise connection between the complexity of computing consequences and the\ntopology of the underlying system structure. Finally, we present an algorithm\nthat enumerates the preferred diagnoses characterized by a consequence. The\nalgorithm is shown to take linear time in the size of the consequence if the\npreference criterion satisfies some general conditions.",
    "published": "1998-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9806101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "A. Darwiche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9806102v1",
    "title": "A Selective Macro-learning Algorithm and its Application to the NxN\n  Sliding-Tile Puzzle",
    "summary": "One of the most common mechanisms used for speeding up problem solvers is\nmacro-learning. Macros are sequences of basic operators acquired during problem\nsolving. Macros are used by the problem solver as if they were basic operators.\nThe major problem that macro-learning presents is the vast number of macros\nthat are available for acquisition. Macros increase the branching factor of the\nsearch space and can severely degrade problem-solving efficiency. To make macro\nlearning useful, a program must be selective in acquiring and utilizing macros.\nThis paper describes a general method for selective acquisition of macros.\nSolvable training problems are generated in increasing order of difficulty. The\nonly macros acquired are those that take the problem solver out of a local\nminimum to a better state. The utility of the method is demonstrated in several\ndomains, including the domain of NxN sliding-tile puzzles. After learning on\nsmall puzzles, the system is able to efficiently solve puzzles of any size.",
    "published": "1998-06-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9806102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "L. Finkelstein",
      "S. Markovitch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9808101v1",
    "title": "The Computational Complexity of Probabilistic Planning",
    "summary": "We examine the computational complexity of testing and finding small plans in\nprobabilistic planning domains with both flat and propositional\nrepresentations. The complexity of plan evaluation and existence varies with\nthe plan type sought; we examine totally ordered plans, acyclic plans, and\nlooping plans, and partially ordered plans under three natural definitions of\nplan value. We show that problems of interest are complete for a variety of\ncomplexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the\nprocess of proving that certain planning problems are complete for NP^PP, we\nintroduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the\nstandard Boolean satisfiability problem to computations involving probabilistic\nquantities; our results suggest that the development of good heuristics for\nE-MAJSAT could be important for the creation of efficient algorithms for a wide\nvariety of problems.",
    "published": "1998-08-01T00:00:00Z",
    "link": "http://arxiv.org/pdf/cs/9808101v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "M. L. Littman",
      "J. Goldsmith",
      "M. Mundhenk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9808001v1",
    "title": "Chess Pure Strategies are Probably Chaotic",
    "summary": "It is odd that chess grandmasters often disagree in their analysis of\npositions, sometimes even of simple ones, and that a grandmaster can hold his\nown against an powerful analytic machine such as Deep Blue. The fact that there\nmust exist pure winning strategies for chess is used to construct a control\nstrategy function. It is then shown that chess strategy is equivalent to an\nautonomous system of differential equations, and conjectured that the system is\nchaotic. If true the conjecture would explain the forenamed peculiarities and\nwould also imply that there cannot exist a static evaluator for chess.",
    "published": "1998-08-21T19:13:51Z",
    "link": "http://arxiv.org/pdf/cs/9808001v1.pdf",
    "category": [
      "cs.CC",
      "cs.AI",
      "F.2.0; I.2.0"
    ],
    "authors": [
      "M. Chaves"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9808005v1",
    "title": "First-Order Conditional Logic Revisited",
    "summary": "Conditional logics play an important role in recent attempts to formulate\ntheories of default reasoning. This paper investigates first-order conditional\nlogic. We show that, as for first-order probabilistic logic, it is important\nnot to confound statistical conditionals over the domain (such as ``most birds\nfly''), and subjective conditionals over possible worlds (such as ``I believe\nthat Tweety is unlikely to fly''). We then address the issue of ascribing\nsemantics to first-order conditional logic. As in the propositional case, there\nare many possible semantics. To study the problem in a coherent way, we use\nplausibility structures. These provide us with a general framework in which\nmany of the standard approaches can be embedded. We show that while these\nstandard approaches are all the same at the propositional level, they are\nsignificantly different in the context of a first-order language. Furthermore,\nwe show that plausibilities provide the most natural extension of conditional\nlogic to the first-order case: We provide a sound and complete axiomatization\nthat contains only the KLM properties and standard axioms of first-order modal\nlogic. We show that most of the other approaches have additional properties,\nwhich result in an inappropriate treatment of an infinitary version of the\nlottery paradox.",
    "published": "1998-08-28T00:16:49Z",
    "link": "http://arxiv.org/pdf/cs/9808005v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4; F.4.1"
    ],
    "authors": [
      "Nir Friedman",
      "Joseph Y. Halpern",
      "Daphne Koller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9808006v2",
    "title": "Set-Theoretic Completeness for Epistemic and Conditional Logic",
    "summary": "The standard approach to logic in the literature in philosophy and\nmathematics, which has also been adopted in computer science, is to define a\nlanguage (the syntax), an appropriate class of models together with an\ninterpretation of formulas in the language (the semantics), a collection of\naxioms and rules of inference characterizing reasoning (the proof theory), and\nthen relate the proof theory to the semantics via soundness and completeness\nresults. Here we consider an approach that is more common in the economics\nliterature, which works purely at the semantic, set-theoretic level. We provide\nset-theoretic completeness results for a number of epistemic and conditional\nlogics, and contrast the expressive power of the syntactic and set-theoretic\napproaches",
    "published": "1998-08-28T00:39:47Z",
    "link": "http://arxiv.org/pdf/cs/9808006v2.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4; F.4.1"
    ],
    "authors": [
      "Joseph Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9808007v1",
    "title": "Plausibility Measures and Default Reasoning",
    "summary": "We introduce a new approach to modeling uncertainty based on plausibility\nmeasures. This approach is easily seen to generalize other approaches to\nmodeling uncertainty, such as probability measures, belief functions, and\npossibility measures. We focus on one application of plausibility measures in\nthis paper: default reasoning. In recent years, a number of different semantics\nfor defaults have been proposed, such as preferential structures,\n$\\epsilon$-semantics, possibilistic structures, and $\\kappa$-rankings, that\nhave been shown to be characterized by the same set of axioms, known as the KLM\nproperties. While this was viewed as a surprise, we show here that it is almost\ninevitable. In the framework of plausibility measures, we can give a necessary\ncondition for the KLM axioms to be sound, and an additional condition necessary\nand sufficient to ensure that the KLM axioms are complete. This additional\ncondition is so weak that it is almost always met whenever the axioms are\nsound. In particular, it is easily seen to hold for all the proposals made in\nthe literature.",
    "published": "1998-08-29T00:12:30Z",
    "link": "http://arxiv.org/pdf/cs/9808007v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4; F.4.1"
    ],
    "authors": [
      "Nir Friedman",
      "Joseph Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9809013v1",
    "title": "Reasoning about Noisy Sensors and Effectors in the Situation Calculus",
    "summary": "Agents interacting with an incompletely known world need to be able to reason\nabout the effects of their actions, and to gain further information about that\nworld they need to use sensors of some sort. Unfortunately, both the effects of\nactions and the information returned from sensors are subject to error. To cope\nwith such uncertainties, the agent can maintain probabilistic beliefs about the\nstate of the world. With probabilistic beliefs the agent will be able to\nquantify the likelihood of the various outcomes of its actions and is better\nable to utilize the information gathered from its error-prone actions and\nsensors. In this paper, we present a model in which we can reason about an\nagent's probabilistic degrees of belief and the manner in which these beliefs\nchange as various actions are executed. We build on a general logical theory of\naction developed by Reiter and others, formalized in the situation calculus. We\npropose a simple axiomatization that captures an agent's state of belief and\nthe manner in which these beliefs change when actions are executed. Our model\ndisplays a number of intuitively reasonable properties.",
    "published": "1998-09-09T22:28:32Z",
    "link": "http://arxiv.org/pdf/cs/9809013v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4, F.4.1"
    ],
    "authors": [
      "Fahiem Bacchus",
      "Joseph Y. Halpern",
      "Hector J. Levesque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9809032v1",
    "title": "Stable models and an alternative logic programming paradigm",
    "summary": "In this paper we reexamine the place and role of stable model semantics in\nlogic programming and contrast it with a least Herbrand model approach to Horn\nprograms. We demonstrate that inherent features of stable model semantics\nnaturally lead to a logic programming system that offers an interesting\nalternative to more traditional logic programming styles of Horn logic\nprogramming, stratified logic programming and logic programming with\nwell-founded semantics. The proposed approach is based on the interpretation of\nprogram clauses as constraints. In this setting programs do not describe a\nsingle intended model, but a family of stable models. These stable models\nencode solutions to the constraint satisfaction problem described by the\nprogram. Our approach imposes restrictions on the syntax of logic programs. In\nparticular, function symbols are eliminated from the language. We argue that\nthe resulting logic programming system is well-attuned to problems in the class\nNP, has a well-defined domain of applications, and an emerging methodology of\nprogramming. We point out that what makes the whole approach viable is recent\nprogress in implementations of algorithms to compute stable models of\npropositional logic programs.",
    "published": "1998-09-18T20:34:59Z",
    "link": "http://arxiv.org/pdf/cs/9809032v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3, I.2.4"
    ],
    "authors": [
      "Victor W. Marek",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9809034v1",
    "title": "Semantics and Conversations for an Agent Communication Language",
    "summary": "We address the issues of semantics and conversations for agent communication\nlanguages and the Knowledge Query Manipulation Language (KQML) in particular.\nBased on ideas from speech act theory, we present a semantic description for\nKQML that associates ``cognitive'' states of the agent with the use of the\nlanguage's primitives (performatives). We have used this approach to describe\nthe semantics for the whole set of reserved KQML performatives. Building on the\nsemantics, we devise the conversation policies, i.e., a formal description of\nhow KQML performatives may be combined into KQML exchanges (conversations),\nusing a Definite Clause Grammar. Our research offers methods for a speech act\ntheory-based semantic description of a language of communication acts and for\nthe specification of the protocols associated with these acts. Languages of\ncommunication acts address the issue of communication among software\napplications at a level of abstraction that is useful to the emerging software\nagents paradigm.",
    "published": "1998-09-18T21:41:18Z",
    "link": "http://arxiv.org/pdf/cs/9809034v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "I.2.11"
    ],
    "authors": [
      "Yannis Labrou",
      "Tim Finin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9809108v1",
    "title": "Learning Nested Agent Models in an Information Economy",
    "summary": "We present our approach to the problem of how an agent, within an economic\nMulti-Agent System, can determine when it should behave strategically (i.e.\nlearn and use models of other agents), and when it should act as a simple\nprice-taker. We provide a framework for the incremental implementation of\nmodeling capabilities in agents, and a description of the forms of knowledge\nrequired. The agents were implemented and different populations simulated in\norder to learn more about their behavior and the merits of using and learning\nagent models. Our results show, among other lessons, how savvy buyers can avoid\nbeing ``cheated'' by sellers, how price volatility can be used to\nquantitatively predict the benefits of deeper models, and how specific types of\nagent populations influence system behavior.",
    "published": "1998-09-26T17:43:36Z",
    "link": "http://arxiv.org/pdf/cs/9809108v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "I 2.11"
    ],
    "authors": [
      "Jose M. Vidal",
      "Edmund H. Durfee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9809110v1",
    "title": "Similarity-Based Models of Word Cooccurrence Probabilities",
    "summary": "In many applications of natural language processing (NLP) it is necessary to\ndetermine the likelihood of a given word combination. For example, a speech\nrecognizer may need to determine which of the two word combinations ``eat a\npeach'' and ``eat a beach'' is more likely. Statistical NLP methods determine\nthe likelihood of a word combination from its frequency in a training corpus.\nHowever, the nature of language is such that many word combinations are\ninfrequent and do not occur in any given corpus. In this work we propose a\nmethod for estimating the probability of such previously unseen word\ncombinations using available information on ``most similar'' words.\n  We describe probabilistic word association models based on distributional\nword similarity, and apply them to two tasks, language modeling and pseudo-word\ndisambiguation. In the language modeling task, a similarity-based model is used\nto improve probability estimates for unseen bigrams in a back-off language\nmodel. The similarity-based method yields a 20% perplexity improvement in the\nprediction of unseen bigrams and statistically significant reductions in\nspeech-recognition error.\n  We also compare four similarity-based estimation methods against back-off and\nmaximum-likelihood estimation methods on a pseudo-word sense disambiguation\ntask in which we controlled for both unigram and bigram frequency to avoid\ngiving too much weight to easy-to-disambiguate high-frequency configurations.\nThe similarity-based methods perform up to 40% better on this particular task.",
    "published": "1998-09-27T18:42:51Z",
    "link": "http://arxiv.org/pdf/cs/9809110v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7;I.2.6"
    ],
    "authors": [
      "Ido Dagan",
      "Lillian Lee",
      "Fernando C. N. Pereira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9809121v1",
    "title": "Using Local Optimality Criteria for Efficient Information Retrieval with\n  Redundant Information Filters",
    "summary": "We consider information retrieval when the data, for instance multimedia, is\ncoputationally expensive to fetch. Our approach uses \"information filters\" to\nconsiderably narrow the universe of possiblities before retrieval. We are\nespecially interested in redundant information filters that save time over more\ngeneral but more costly filters. Efficient retrieval requires that decision\nmust be made about the necessity, order, and concurrent processing of proposed\nfilters (an \"execution plan\"). We develop simple polynomial-time local criteria\nfor optimal execution plans, and show that most forms of concurrency are\nsuboptimal with information filters. Although the general problem of finding an\noptimal execution plan is likely exponential in the number of filters, we show\nexperimentally that our local optimality criteria, used in a polynomial-time\nalgorithm, nearly always find the global optimum with 15 filters or less, a\nsufficient number of filters for most applications. Our methods do not require\nspecial hardware and avoid the high processor idleness that is characteristic\nof massive parallelism solutions to this problem. We apply our ideas to an\nimportant application, information retrieval of cpationed data using\nnatural-language understanding, a problem for which the natural-language\nprocessing can be the bottleneck if not implemented well.",
    "published": "1998-09-29T21:55:20Z",
    "link": "http://arxiv.org/pdf/cs/9809121v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "H.3.3"
    ],
    "authors": [
      "Neil C. Rowe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9810005v1",
    "title": "Anytime Coalition Structure Generation with Worst Case Guarantees",
    "summary": "Coalition formation is a key topic in multiagent systems. One would prefer a\ncoalition structure that maximizes the sum of the values of the coalitions, but\noften the number of coalition structures is too large to allow exhaustive\nsearch for the optimal one. But then, can the coalition structure found via a\npartial search be guaranteed to be within a bound from optimum? We show that\nnone of the previous coalition structure generation algorithms can establish\nany bound because they search fewer nodes than a threshold that we show\nnecessary for establishing a bound. We present an algorithm that establishes a\ntight bound within this minimal amount of search, and show that any other\nalgorithm would have to search strictly more. The fraction of nodes needed to\nbe searched approaches zero as the number of agents grows. If additional time\nremains, our anytime algorithm searches further, and establishes a\nprogressively lower tight bound. Surprisingly, just searching one more node\ndrops the bound in half. As desired, our algorithm lowers the bound rapidly\nearly on, and exhibits diminishing returns to computation. It also drastically\noutperforms its obvious contenders. Finally, we show how to distribute the\ndesired search across self-interested manipulative agents.",
    "published": "1998-10-05T16:08:41Z",
    "link": "http://arxiv.org/pdf/cs/9810005v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "I.2.11"
    ],
    "authors": [
      "Tuomas Sandholm",
      "Kate Larson",
      "Martin Andersson",
      "Onn Shehory",
      "Fernando Tohme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cond-mat/9810144v2",
    "title": "Relaxation in graph coloring and satisfiability problems",
    "summary": "Using T=0 Monte Carlo simulation, we study the relaxation of graph coloring\n(K-COL) and satisfiability (K-SAT), two hard problems that have recently been\nshown to possess a phase transition in solvability as a parameter is varied. A\nchange from exponentially fast to power law relaxation, and a transition to\nfreezing behavior are found. These changes take place for smaller values of the\nparameter than the solvability transition. Results for the coloring problem for\ncolorable and clustered graphs and for the fraction of persistent spins for\nsatisfiability are also presented.",
    "published": "1998-10-13T11:27:32Z",
    "link": "http://arxiv.org/pdf/cond-mat/9810144v2.pdf",
    "category": [
      "cond-mat.dis-nn",
      "cs.AI"
    ],
    "authors": [
      "Pontus Svenson",
      "Mats G. Nordahl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9810016v1",
    "title": "SYNERGY: A Linear Planner Based on Genetic Programming",
    "summary": "In this paper we describe SYNERGY, which is a highly parallelizable, linear\nplanning system that is based on the genetic programming paradigm. Rather than\nreasoning about the world it is planning for, SYNERGY uses artificial\nselection, recombination and fitness measure to generate linear plans that\nsolve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase\nproblem and a few variants of the robot navigation problem), and the\nexperimental results show that our planner is capable of handling problem\ninstances that are one to two orders of magnitude larger than the ones solved\nby UCPOP. In order to facilitate the search reduction and to enhance the\nexpressive power of SYNERGY, we also propose two major extensions to our\nplanning system: a formalism for using hierarchical planning operators, and a\nframework for planning in dynamic environments.",
    "published": "1998-10-16T22:11:35Z",
    "link": "http://arxiv.org/pdf/cs/9810016v1.pdf",
    "category": [
      "cs.AI",
      "I.2.8"
    ],
    "authors": [
      "Ion Muslea"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9810018v1",
    "title": "A Proof Theoretic View of Constraint Programming",
    "summary": "We provide here a proof theoretic account of constraint programming that\nattempts to capture the essential ingredients of this programming style. We\nexemplify it by presenting proof rules for linear constraints over interval\ndomains, and illustrate their use by analyzing the constraint propagation\nprocess for the {\\tt SEND + MORE = MONEY} puzzle. We also show how this\napproach allows one to build new constraint solvers.",
    "published": "1998-10-20T11:23:05Z",
    "link": "http://arxiv.org/pdf/cs/9810018v1.pdf",
    "category": [
      "cs.AI",
      "cs.PL",
      "F.4.1;I.2.3;D.1.0"
    ],
    "authors": [
      "Krzysztof R. Apt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9810020v1",
    "title": "Computational Geometry Column 33",
    "summary": "Several recent SIGGRAPH papers on surface simplification are described.",
    "published": "1998-10-22T20:44:35Z",
    "link": "http://arxiv.org/pdf/cs/9810020v1.pdf",
    "category": [
      "cs.CG",
      "cs.AI",
      "cs.GR",
      "F.2.2;I.3"
    ],
    "authors": [
      "Joseph O'Rourke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9811024v1",
    "title": "The Essence of Constraint Propagation",
    "summary": "We show that several constraint propagation algorithms (also called (local)\nconsistency, consistency enforcing, Waltz, filtering or narrowing algorithms)\nare instances of algorithms that deal with chaotic iteration. To this end we\npropose a simple abstract framework that allows us to classify and compare\nthese algorithms and to establish in a uniform way their basic properties.",
    "published": "1998-11-13T13:04:02Z",
    "link": "http://arxiv.org/pdf/cs/9811024v1.pdf",
    "category": [
      "cs.AI",
      "I.1.2; I.2.2"
    ],
    "authors": [
      "Krzysztof R. Apt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9811029v1",
    "title": "A Human - machine interface for teleoperation of arm manipulators in a\n  complex environment",
    "summary": "This paper discusses the feasibility of using configuration space (C-space)\nas a means of visualization and control in operator-guided real-time motion of\na robot arm manipulator. The motivation is to improve performance of the human\noperator in tasks involving the manipulator motion in an environment with\nobstacles. Unlike some other motion planning tasks, operators are known to make\nexpensive mistakes in such tasks, even in a simpler two-dimensional case. They\nhave difficulty learning better procedures and their performance improves very\nlittle with practice. Using an example of a two-dimensional arm manipulator, we\nshow that translating the problem into C-space improves the operator\nperformance rather remarkably, on the order of magnitude compared to the usual\nwork space control. An interface that makes the transfer possible is described,\nand an example of its use in a virtual environment is shown.",
    "published": "1998-11-20T21:06:07Z",
    "link": "http://arxiv.org/pdf/cs/9811029v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "I.2.9"
    ],
    "authors": [
      "I. Ivanisevic",
      "V. Lumelsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9812004v1",
    "title": "Name Strategy: Its Existence and Implications",
    "summary": "It is argued that colour name strategy, object name strategy, and chunking\nstrategy in memory are all aspects of the same general phenomena, called\nstereotyping. It is pointed out that the Berlin-Kay universal partial ordering\nof colours and the frequency of traffic accidents classified by colour are\nsurprisingly similar. Some consequences of the existence of a name strategy for\nthe philosophy of language and mathematics are discussed. It is argued that\nreal valued quantities occur {\\it ab initio}. The implication of real valued\ntruth quantities is that the {\\bf Continuum Hypothesis} of pure mathematics is\nside-stepped. The existence of name strategy shows that thought/sememes and\ntalk/phonemes can be separate, and this vindicates the assumption of thought\noccurring before talk used in psycholinguistic speech production models.",
    "published": "1998-12-04T12:28:19Z",
    "link": "http://arxiv.org/pdf/cs/9812004v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "math.HO",
      "I.2.6;J.4;I.2.7"
    ],
    "authors": [
      "Mark D. Roberts"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9812010v1",
    "title": "Towards a computational theory of human daydreaming",
    "summary": "This paper examines the phenomenon of daydreaming: spontaneously recalling or\nimagining personal or vicarious experiences in the past or future. The\nfollowing important roles of daydreaming in human cognition are postulated:\nplan preparation and rehearsal, learning from failures and successes, support\nfor processes of creativity, emotion regulation, and motivation.\n  A computational theory of daydreaming and its implementation as the program\nDAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based\non relaxed planning, 2) a dynamic episodic memory of experiences used by the\nscenario generator, 3) a collection of personal goals and control goals which\nguide the scenario generator, 4) an emotion component in which daydreams\ninitiate, and are initiated by, emotional states arising from goal outcomes,\nand 5) domain knowledge of interpersonal relations and common everyday\noccurrences.\n  The role of emotions and control goals in daydreaming is discussed. Four\ncontrol goals commonly used in guiding daydreaming are presented:\nrationalization, failure/success reversal, revenge, and preparation. The role\nof episodic memory in daydreaming is considered, including how daydreamed\ninformation is incorporated into memory and later used. An initial version of\nDAYDREAMER which produces several daydreams (in English) is currently running.",
    "published": "1998-12-10T16:29:07Z",
    "link": "http://arxiv.org/pdf/cs/9812010v1.pdf",
    "category": [
      "cs.AI",
      "I.2.0"
    ],
    "authors": [
      "Erik T. Mueller",
      "Michael G. Dyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9812017v1",
    "title": "A reusable iterative optimization software library to solve\n  combinatorial problems with approximate reasoning",
    "summary": "Real world combinatorial optimization problems such as scheduling are\ntypically too complex to solve with exact methods. Additionally, the problems\noften have to observe vaguely specified constraints of different importance,\nthe available data may be uncertain, and compromises between antagonistic\ncriteria may be necessary. We present a combination of approximate reasoning\nbased constraints and iterative optimization based heuristics that help to\nmodel and solve such problems in a framework of C++ software libraries called\nStarFLIP++. While initially developed to schedule continuous caster units in\nsteel plants, we present in this paper results from reusing the library\ncomponents in a shift scheduling system for the workforce of an industrial\nproduction plant.",
    "published": "1998-12-15T21:45:15Z",
    "link": "http://arxiv.org/pdf/cs/9812017v1.pdf",
    "category": [
      "cs.AI",
      "I.2.8; I.2.1; J.6; I.2.4; F.2.2"
    ],
    "authors": [
      "Andreas Raggl",
      "Wolfgang Slany"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9812022v1",
    "title": "Hypertree Decompositions and Tractable Queries",
    "summary": "Several important decision problems on conjunctive queries (CQs) are\nNP-complete in general but become tractable, and actually highly\nparallelizable, if restricted to acyclic or nearly acyclic queries. Examples\nare the evaluation of Boolean CQs and query containment. These problems were\nshown tractable for conjunctive queries of bounded treewidth and of bounded\ndegree of cyclicity. The so far most general concept of nearly acyclic queries\nwas the notion of queries of bounded query-width introduced by Chekuri and\nRajaraman (1997). While CQs of bounded query width are tractable, it remained\nunclear whether such queries are efficiently recognizable. Chekuri and\nRajaraman stated as an open problem whether for each constant k it can be\ndetermined in polynomial time if a query has query width less than or equal to\nk. We give a negative answer by proving this problem NP-complete (specifically,\nfor k=4). In order to circumvent this difficulty, we introduce the new concept\nof hypertree decomposition of a query and the corresponding notion of hypertree\nwidth. We prove: (a) for each k, the class of queries with query width bounded\nby k is properly contained in the class of queries whose hypertree width is\nbounded by k; (b) unlike query width, constant hypertree-width is efficiently\nrecognizable; (c) Boolean queries of constant hypertree width can be\nefficiently evaluated.",
    "published": "1998-12-28T12:30:50Z",
    "link": "http://arxiv.org/pdf/cs/9812022v1.pdf",
    "category": [
      "cs.DB",
      "cs.AI",
      "F.2.2; H.2.4; I.2.8; G.2.2"
    ],
    "authors": [
      "G. Gottlob",
      "N. Leone",
      "F. Scarcello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9901001v1",
    "title": "TDLeaf(lambda): Combining Temporal Difference Learning with Game-Tree\n  Search",
    "summary": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda)\nalgorithm that enables it to be used in conjunction with minimax search. We\npresent some experiments in both chess and backgammon which demonstrate its\nutility and provide comparisons with TD(lambda) and another less radical\nvariant, TD-directed(lambda). In particular, our chess program, ``KnightCap,''\nused TDLeaf(lambda) to learn its evaluation function while playing on the Free\nInternet Chess Server (FICS, fics.onenet.net). It improved from a 1650 rating\nto a 2100 rating in just 308 games. We discuss some of the reasons for this\nsuccess and the relationship between our results and Tesauro's results in\nbackgammon.",
    "published": "1999-01-05T00:56:54Z",
    "link": "http://arxiv.org/pdf/cs/9901001v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "authors": [
      "Jonathan Baxter",
      "Andrew Tridgell",
      "Lex Weaver"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9901002v1",
    "title": "KnightCap: A chess program that learns by combining TD(lambda) with\n  game-tree search",
    "summary": "In this paper we present TDLeaf(lambda), a variation on the TD(lambda)\nalgorithm that enables it to be used in conjunction with game-tree search. We\npresent some experiments in which our chess program ``KnightCap'' used\nTDLeaf(lambda) to learn its evaluation function while playing on the Free\nInternet Chess Server (FICS, fics.onenet.net). The main success we report is\nthat KnightCap improved from a 1650 rating to a 2150 rating in just 308 games\nand 3 days of play. As a reference, a rating of 1650 corresponds to about level\nB human play (on a scale from E (1000) to A (1800)), while 2150 is human master\nlevel. We discuss some of the reasons for this success, principle among them\nbeing the use of on-line, rather than self-play.",
    "published": "1999-01-10T03:21:23Z",
    "link": "http://arxiv.org/pdf/cs/9901002v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "authors": [
      "Jonathan Baxter",
      "Andrew Tridgell",
      "Lex Weaver"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9901003v1",
    "title": "Fixpoint 3-valued semantics for autoepistemic logic",
    "summary": "The paper presents a constructive fixpoint semantics for autoepistemic logic\n(AEL). This fixpoint characterizes a unique but possibly three-valued belief\nset of an autoepistemic theory. It may be three-valued in the sense that for a\nsubclass of formulas F, the fixpoint may not specify whether F is believed or\nnot. The paper presents a constructive 3-valued semantics for autoepistemic\nlogic (AEL). We introduce a derivation operator and define the semantics as its\nleast fixpoint. The semantics is 3-valued in the sense that, for some formulas,\nthe least fixpoint does not specify whether they are believed or not. We show\nthat complete fixpoints of the derivation operator correspond to Moore's stable\nexpansions. In the case of modal representations of logic programs our least\nfixpoint semantics expresses well-founded semantics or 3-valued Fitting-Kunen\nsemantics (depending on the embedding used). We show that, computationally, our\nsemantics is simpler than the semantics proposed by Moore (assuming that the\npolynomial hierarchy does not collapse).",
    "published": "1999-01-12T18:44:40Z",
    "link": "http://arxiv.org/pdf/cs/9901003v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.4, F.4.1, I.2.3"
    ],
    "authors": [
      "M. Denecker",
      "V. Marek",
      "M. Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9901012v1",
    "title": "Extremal problems in logic programming and stable model computation",
    "summary": "We study the following problem: given a class of logic programs C, determine\nthe maximum number of stable models of a program from C. We establish the\nmaximum for the class of all logic programs with at most n clauses, and for the\nclass of all logic programs of size at most n. We also characterize the\nprograms for which the maxima are attained. We obtain similar results for the\nclass of all disjunctive logic programs with at most n clauses, each of length\nat most m, and for the class of all disjunctive logic programs of size at most\nn. Our results on logic programs have direct implication for the design of\nalgorithms to compute stable models. Several such algorithms, similar in spirit\nto the Davis-Putnam procedure, are described in the paper. Our results imply\nthat there is an algorithm that finds all stable models of a program with n\nclauses after considering the search space of size O(3^{n/3}) in the worst\ncase. Our results also provide some insights into the question of\nrepresentability of families of sets as families of stable models of logic\nprograms.",
    "published": "1999-01-25T14:44:20Z",
    "link": "http://arxiv.org/pdf/cs/9901012v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3;I.2.4;F.4.1"
    ],
    "authors": [
      "Pawel Cholewinski",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9901014v1",
    "title": "Minimum Description Length Induction, Bayesianism, and Kolmogorov\n  Complexity",
    "summary": "The relationship between the Bayesian approach and the minimum description\nlength approach is established. We sharpen and clarify the general modeling\nprinciples MDL and MML, abstracted as the ideal MDL principle and defined from\nBayes's rule by means of Kolmogorov complexity. The basic condition under which\nthe ideal principle should be applied is encapsulated as the Fundamental\nInequality, which in broad terms states that the principle is valid when the\ndata are random, relative to every contemplated hypothesis and also these\nhypotheses are random relative to the (universal) prior. Basically, the ideal\nprinciple states that the prior probability associated with the hypothesis\nshould be given by the algorithmic universal probability, and the sum of the\nlog universal probability of the model plus the log of the probability of the\ndata given the model should be minimized. If we restrict the model class to the\nfinite sets then application of the ideal principle turns into Kolmogorov's\nminimal sufficient statistic. In general we show that data compression is\nalmost always the best strategy, both in hypothesis identification and\nprediction.",
    "published": "1999-01-27T17:48:14Z",
    "link": "http://arxiv.org/pdf/cs/9901014v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "cs.IT",
      "cs.LO",
      "math.IT",
      "math.PR",
      "physics.data-an",
      "E.4,F.2,H.3,I.2,I.5,I.7"
    ],
    "authors": [
      "Paul Vitanyi",
      "Ming Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9901016v1",
    "title": "Representation Theory for Default Logic",
    "summary": "Default logic can be regarded as a mechanism to represent families of belief\nsets of a reasoning agent. As such, it is inherently second-order. In this\npaper, we study the problem of representability of a family of theories as the\nset of extensions of a default theory. We give a complete solution to the\nrepresentability by means of normal default theories. We obtain partial results\non representability by arbitrary default theories. We construct examples of\ndenumerable families of non-including theories that are not representable. We\nalso study the concept of equivalence between default theories.",
    "published": "1999-01-28T21:57:15Z",
    "link": "http://arxiv.org/pdf/cs/9901016v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.4, F.4.1, I.2.3"
    ],
    "authors": [
      "Victor Marek",
      "Jan Treur",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9902006v1",
    "title": "A Discipline of Evolutionary Programming",
    "summary": "Genetic fitness optimization using small populations or small population\nupdates across generations generally suffers from randomly diverging\nevolutions. We propose a notion of highly probable fitness optimization through\nfeasible evolutionary computing runs on small size populations. Based on\nrapidly mixing Markov chains, the approach pertains to most types of\nevolutionary genetic algorithms, genetic programming and the like. We establish\nthat for systems having associated rapidly mixing Markov chains and appropriate\nstationary distributions the new method finds optimal programs (individuals)\nwith probability almost 1. To make the method useful would require a structured\ndesign methodology where the development of the program and the guarantee of\nthe rapidly mixing property go hand in hand. We analyze a simple example to\nshow that the method is implementable. More significant examples require\ntheoretical advances, for example with respect to the Metropolis filter.",
    "published": "1999-02-02T16:17:16Z",
    "link": "http://arxiv.org/pdf/cs/9902006v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "cs.CC",
      "cs.DS",
      "cs.LG",
      "cs.MA",
      "I.2,E.1,F.1"
    ],
    "authors": [
      "Paul Vitanyi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9902015v1",
    "title": "Resource Discovery in Trilogy",
    "summary": "Trilogy is a collaborative project whose key aim is the development of an\nintegrated virtual laboratory to support research training within each\ninstitution and collaborative projects between the partners. In this paper, the\narchitecture and underpinning platform of the system is described with\nparticular emphasis being placed on the structure and the integration of the\ndistributed database. A key element is the ontology that provides the\nmulti-agent system with a conceptualisation specification of the domain; this\nontology is explained, accompanied by a discussion how such a system is\nintegrated and used within the virtual laboratory. Although in this paper,\nTelecommunications and in particular Broadband networks are used as exemplars,\nthe underlying system principles are applicable to any domain where a\ncombination of experimental and literature-based resources are required.",
    "published": "1999-02-08T21:23:39Z",
    "link": "http://arxiv.org/pdf/cs/9902015v1.pdf",
    "category": [
      "cs.DL",
      "cs.AI",
      "cs.MA",
      "H.3.4;I.2.0"
    ],
    "authors": [
      "Franck Chevalier",
      "David Harle",
      "Geoffrey Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9903002v1",
    "title": "An Algebraic Programming Style for Numerical Software and its\n  Optimization",
    "summary": "The abstract mathematical theory of partial differential equations (PDEs) is\nformulated in terms of manifolds, scalar fields, tensors, and the like, but\nthese algebraic structures are hardly recognizable in actual PDE solvers. The\ngeneral aim of the Sophus programming style is to bridge the gap between theory\nand practice in the domain of PDE solvers. Its main ingredients are a library\nof abstract datatypes corresponding to the algebraic structures used in the\nmathematical theory and an algebraic expression style similar to the expression\nstyle used in the mathematical theory. Because of its emphasis on abstract\ndatatypes, Sophus is most naturally combined with object-oriented languages or\nother languages supporting abstract datatypes. The resulting source code\npatterns are beyond the scope of current compiler optimizations, but are\nsufficiently specific for a dedicated source-to-source optimizer. The limited,\ndomain-specific, character of Sophus is the key to success here. This kind of\noptimization has been tested on computationally intensive Sophus style code\nwith promising results. The general approach may be useful for other styles and\nin other application domains as well.",
    "published": "1999-03-01T11:03:47Z",
    "link": "http://arxiv.org/pdf/cs/9903002v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CE",
      "cs.MS",
      "D.1.5; D.2.2; J.2"
    ],
    "authors": [
      "T. B. Dinesh",
      "M. Haveraaen",
      "J. Heering"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9903011v1",
    "title": "A complete anytime algorithm for balanced number partitioning",
    "summary": "Given a set of numbers, the balanced partioning problem is to divide them\ninto two subsets, so that the sum of the numbers in each subset are as nearly\nequal as possible, subject to the constraint that the cardinalities of the\nsubsets be within one of each other. We combine the balanced largest\ndifferencing method (BLDM) and Korf's complete Karmarkar-Karp algorithm to get\na new algorithm that optimally solves the balanced partitioning problem. For\nnumbers with twelve significant digits or less, the algorithm can optimally\nsolve balanced partioning problems of arbitrary size in practice. For numbers\nwith greater precision, it first returns the BLDM solution, then continues to\nfind better solutions as time allows.",
    "published": "1999-03-11T22:38:01Z",
    "link": "http://arxiv.org/pdf/cs/9903011v1.pdf",
    "category": [
      "cs.DS",
      "cond-mat.dis-nn",
      "cs.AI",
      "F.2.2"
    ],
    "authors": [
      "Stephan Mertens"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9903016v1",
    "title": "Modeling Belief in Dynamic Systems, Part II: Revision and Update",
    "summary": "The study of belief change has been an active area in philosophy and AI. In\nrecent years two special cases of belief change, belief revision and belief\nupdate, have been studied in detail. In a companion paper (Friedman & Halpern,\n1997), we introduce a new framework to model belief change. This framework\ncombines temporal and epistemic modalities with a notion of plausibility,\nallowing us to examine the change of beliefs over time. In this paper, we show\nhow belief revision and belief update can be captured in our framework. This\nallows us to compare the assumptions made by each method, and to better\nunderstand the principles underlying them. In particular, it shows that Katsuno\nand Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on\nseveral strong assumptions that may limit its applicability in artificial\nintelligence. Finally, our analysis allow us to identify a notion of minimal\nchange that underlies a broad range of belief change operations including\nrevision and update.",
    "published": "1999-03-24T00:22:01Z",
    "link": "http://arxiv.org/pdf/cs/9903016v1.pdf",
    "category": [
      "cs.AI",
      "I.2"
    ],
    "authors": [
      "N Friedman",
      "J. Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9904004v1",
    "title": "Mixing Metaphors",
    "summary": "Mixed metaphors have been neglected in recent metaphor research. This paper\nsuggests that such neglect is short-sighted. Though mixing is a more complex\nphenomenon than straight metaphors, the same kinds of reasoning and knowledge\nstructures are required. This paper provides an analysis of both parallel and\nserial mixed metaphors within the framework of an AI system which is already\ncapable of reasoning about straight metaphorical manifestations and argues that\nthe processes underlying mixing are central to metaphorical meaning. Therefore,\nany theory of metaphors must be able to account for mixing.",
    "published": "1999-04-12T11:37:49Z",
    "link": "http://arxiv.org/pdf/cs/9904004v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.0; I.2.7"
    ],
    "authors": [
      "Mark Lee",
      "John Barnden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9905008v1",
    "title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering",
    "summary": "We present a technique for automatic induction of slot annotations for\nsubcategorization frames, based on induction of hidden classes in the EM\nframework of statistical estimation. The models are empirically evalutated by a\ngeneral decision test. Induction of slot labeling for subcategorization frames\nis accomplished by a further application of EM, and applied experimentally on\nframe observations derived from parsing large corpora. We outline an\ninterpretation of the learned representations as theoretical-linguistic\ndecompositional lexical entries.",
    "published": "1999-05-19T14:52:33Z",
    "link": "http://arxiv.org/pdf/cs/9905008v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.7; I.5.3"
    ],
    "authors": [
      "Mats Rooth",
      "Stefan Riezler",
      "Detlef Prescher",
      "Glenn Carroll",
      "Franz Beil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9906002v1",
    "title": "The Symbol Grounding Problem",
    "summary": "How can the semantic interpretation of a formal symbol system be made\nintrinsic to the system, rather than just parasitic on the meanings in our\nheads? How can the meanings of the meaningless symbol tokens, manipulated\nsolely on the basis of their (arbitrary) shapes, be grounded in anything but\nother meaningless symbols? The problem is analogous to trying to learn Chinese\nfrom a Chinese/Chinese dictionary alone. A candidate solution is sketched:\nSymbolic representations must be grounded bottom-up in nonsymbolic\nrepresentations of two kinds: (1) \"iconic representations,\" which are analogs\nof the proximal sensory projections of distal objects and events, and (2)\n\"categorical representations,\" which are learned and innate feature-detectors\nthat pick out the invariant features of object and event categories from their\nsensory projections. Elementary symbols are the names of these object and event\ncategories, assigned on the basis of their (nonsymbolic) categorical\nrepresentations. Higher-order (3) \"symbolic representations,\" grounded in these\nelementary symbols, consist of symbol strings describing category membership\nrelations (e.g., \"An X is a Y that is Z\").",
    "published": "1999-06-01T19:57:24Z",
    "link": "http://arxiv.org/pdf/cs/9906002v1.pdf",
    "category": [
      "cs.AI",
      "I.2.0"
    ],
    "authors": [
      "Stevan Harnad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9906006v2",
    "title": "Learning Efficient Disambiguation",
    "summary": "This dissertation analyses the computational properties of current\nperformance-models of natural language parsing, in particular Data Oriented\nParsing (DOP), points out some of their major shortcomings and suggests\nsuitable solutions. It provides proofs that various problems of probabilistic\ndisambiguation are NP-Complete under instances of these performance-models, and\nit argues that none of these models accounts for attractive efficiency\nproperties of human language processing in limited domains, e.g. that frequent\ninputs are usually processed faster than infrequent ones. The central\nhypothesis of this dissertation is that these shortcomings can be eliminated by\nspecializing the performance-models to the limited domains. The dissertation\naddresses \"grammar and model specialization\" and presents a new framework, the\nAmbiguity-Reduction Specialization (ARS) framework, that formulates the\nnecessary and sufficient conditions for successful specialization. The\nframework is instantiated into specialization algorithms and applied to\nspecializing DOP. Novelties of these learning algorithms are 1) they limit the\nhypotheses-space to include only \"safe\" models, 2) are expressed as constrained\noptimization formulae that minimize the entropy of the training tree-bank given\nthe specialized grammar, under the constraint that the size of the specialized\nmodel does not exceed a predefined maximum, and 3) they enable integrating the\nspecialized model with the original one in a complementary manner. The\ndissertation provides experiments with initial implementations and compares the\nresulting Specialized DOP (SDOP) models to the original DOP models with\nencouraging results.",
    "published": "1999-06-02T15:50:26Z",
    "link": "http://arxiv.org/pdf/cs/9906006v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.6, I.2.7, J.5, F.2"
    ],
    "authors": [
      "Khalil Sima'an"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9906010v1",
    "title": "Predicate Logic with Definitions",
    "summary": "Predicate Logic with Definitions (PLD or D-logic) is a modification of\nfirst-order logic intended mostly for practical formalization of mathematics.\nThe main syntactic constructs of D-logic are terms, formulas and definitions. A\ndefinition is a definition of variables, a definition of constants, or a\ncomposite definition (D-logic has also abbreviation definitions called\nabbreviations). Definitions can be used inside terms and formulas. This\npossibility alleviates introducing new quantifier-like names. Composite\ndefinitions allow constructing new definitions from existing ones.",
    "published": "1999-06-07T20:16:55Z",
    "link": "http://arxiv.org/pdf/cs/9906010v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1; I.2.4"
    ],
    "authors": [
      "Victor Makarov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9906016v1",
    "title": "Automatically Selecting Useful Phrases for Dialogue Act Tagging",
    "summary": "We present an empirical investigation of various ways to automatically\nidentify phrases in a tagged corpus that are useful for dialogue act tagging.\nWe found that a new method (which measures a phrase's deviation from an\noptimally-predictive phrase), enhanced with a lexical filtering mechanism,\nproduces significantly better cues than manually-selected cue phrases, the\nexhaustive set of phrases in a training corpus, and phrases chosen by\ntraditional metrics, like mutual information and information gain.",
    "published": "1999-06-18T03:25:03Z",
    "link": "http://arxiv.org/pdf/cs/9906016v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "I.2.7; I.2.6"
    ],
    "authors": [
      "Ken Samuel",
      "Sandra Carberry",
      "K. Vijay-Shanker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9906019v2",
    "title": "Resolving Part-of-Speech Ambiguity in the Greek Language Using Learning\n  Techniques",
    "summary": "This article investigates the use of Transformation-Based Error-Driven\nlearning for resolving part-of-speech ambiguity in the Greek language. The aim\nis not only to study the performance, but also to examine its dependence on\ndifferent thematic domains. Results are presented here for two different test\ncases: a corpus on \"management succession events\" and a general-theme corpus.\nThe two experiments show that the performance of this method does not depend on\nthe thematic domain of the corpus, and its accuracy for the Greek language is\naround 95%.",
    "published": "1999-06-22T07:41:24Z",
    "link": "http://arxiv.org/pdf/cs/9906019v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.6 ; I.2.7"
    ],
    "authors": [
      "G. Petasis",
      "G. Paliouras",
      "V. Karkaletsis",
      "C. D. Spyropoulos",
      "I. Androutsopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9906029v2",
    "title": "Events in Property Patterns",
    "summary": "A pattern-based approach to the presentation, codification and reuse of\nproperty specifications for finite-state verification was proposed by Dwyer and\nhis collegues. The patterns enable non-experts to read and write formal\nspecifications for realistic systems and facilitate easy conversion of\nspecifications between formalisms, such as LTL, CTL, QRE. In this paper, we\nextend the pattern system with events - changes of values of variables in the\ncontext of LTL.",
    "published": "1999-06-28T17:06:51Z",
    "link": "http://arxiv.org/pdf/cs/9906029v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.SC",
      "D.2.4;F.3.1;F.4.1;I.2.4;D.2.1"
    ],
    "authors": [
      "M. Chechik",
      "D. Paun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9907026v1",
    "title": "Mixing representation levels: The hybrid approach to automatic text\n  generation",
    "summary": "Natural language generation systems (NLG) map non-linguistic representations\ninto strings of words through a number of steps using intermediate\nrepresentations of various levels of abstraction. Template based systems, by\ncontrast, tend to use only one representation level, i.e. fixed strings, which\nare combined, possibly in a sophisticated way, to generate the final text.\n  In some circumstances, it may be profitable to combine NLG and template based\ntechniques. The issue of combining generation techniques can be seen in more\nabstract terms as the issue of mixing levels of representation of different\ndegrees of linguistic abstraction. This paper aims at defining a reference\narchitecture for systems using mixed representations. We argue that mixed\nrepresentations can be used without abandoning a linguistically grounded\napproach to language generation.",
    "published": "1999-07-16T15:43:45Z",
    "link": "http://arxiv.org/pdf/cs/9907026v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Emanuele Pianta",
      "Lucia M. Tovena"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9907032v2",
    "title": "Clausal Temporal Resolution",
    "summary": "In this article, we examine how clausal resolution can be applied to a\nspecific, but widely used, non-classical logic, namely discrete linear temporal\nlogic. Thus, we first define a normal form for temporal formulae and show how\narbitrary temporal formulae can be translated into the normal form, while\npreserving satisfiability. We then introduce novel resolution rules that can be\napplied to formulae in this normal form, provide a range of examples and\nexamine the correctness and complexity of this approach is examined and. This\nclausal resolution approach. Finally, we describe related work and future\ndevelopments concerning this work.",
    "published": "1999-07-21T15:48:06Z",
    "link": "http://arxiv.org/pdf/cs/9907032v2.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3;F.4.1"
    ],
    "authors": [
      "Michael Fisher",
      "Clare Dixon",
      "Martin Peim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9908004v1",
    "title": "Extending the Stable Model Semantics with More Expressive Rules",
    "summary": "The rules associated with propositional logic programs and the stable model\nsemantics are not expressive enough to let one write concise programs. This\nproblem is alleviated by introducing some new types of propositional rules.\nTogether with a decision procedure that has been used as a base for an\nefficient implementation, the new rules supplant the standard ones in practical\napplications of the stable model semantics.",
    "published": "1999-08-06T06:01:43Z",
    "link": "http://arxiv.org/pdf/cs/9908004v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3; I.2.8; F.4.1"
    ],
    "authors": [
      "Patrik Simons"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9908013v1",
    "title": "Collective Intelligence for Control of Distributed Dynamical Systems",
    "summary": "We consider the El Farol bar problem, also known as the minority game (W. B.\nArthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet\nand Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of\nthe general problem of how to configure the nodal elements of a distributed\ndynamical system so that they do not ``work at cross purposes'', in that their\ncollective dynamics avoids frustration and thereby achieves a provided global\ngoal. We summarize a mathematical theory for such configuration applicable when\n(as in the bar problem) the global goal can be expressed as minimizing a global\nenergy function and the nodes can be expressed as minimizers of local free\nenergy functions. We show that a system designed with that theory performs\nnearly optimally for the bar problem.",
    "published": "1999-08-17T21:32:41Z",
    "link": "http://arxiv.org/pdf/cs/9908013v1.pdf",
    "category": [
      "cs.LG",
      "adap-org",
      "cond-mat",
      "cs.AI",
      "cs.DC",
      "cs.MA",
      "nlin.AO",
      "I.2.6 ; I.2.11"
    ],
    "authors": [
      "David H. Wolpert",
      "Kevin R. Wheeler",
      "Kagan Tumer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9908015v1",
    "title": "Representing Scholarly Claims in Internet Digital Libraries: A Knowledge\n  Modelling Approach",
    "summary": "This paper is concerned with tracking and interpreting scholarly documents in\ndistributed research communities. We argue that current approaches to document\ndescription, and current technological infrastructures particularly over the\nWorld Wide Web, provide poor support for these tasks. We describe the design of\na digital library server which will enable authors to submit a summary of the\ncontributions they claim their documents makes, and its relations to the\nliterature. We describe a knowledge-based Web environment to support the\nemergence of such a community-constructed semantic hypertext, and the services\nit could provide to assist the interpretation of an idea or document in the\ncontext of its literature. The discussion considers in detail how the approach\naddresses usability issues associated with knowledge structuring environments.",
    "published": "1999-08-19T09:51:29Z",
    "link": "http://arxiv.org/pdf/cs/9908015v1.pdf",
    "category": [
      "cs.DL",
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "H.3.7; H.1.2; H5.2; H.5.4; I.2.4; I.7.4"
    ],
    "authors": [
      "Simon Buckingham Shum",
      "Enrico Motta",
      "John Domingue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9909003v1",
    "title": "Iterative Deepening Branch and Bound",
    "summary": "In tree search problem the best-first search algorithm needs too much of\nspace . To remove such drawbacks of these algorithms the IDA* was developed\nwhich is both space and time cost efficient. But again IDA* can give an optimal\nsolution for real valued problems like Flow shop scheduling, Travelling\nSalesman and 0/1 Knapsack due to their real valued cost estimates. Thus further\nmodifications are done on it and the Iterative Deepening Branch and Bound\nSearch Algorithms is developed which meets the requirements. We have tried\nusing this algorithm for the Flow Shop Scheduling Problem and have found that\nit is quite effective.",
    "published": "1999-09-03T10:31:46Z",
    "link": "http://arxiv.org/pdf/cs/9909003v1.pdf",
    "category": [
      "cs.AI",
      "I.2.8"
    ],
    "authors": [
      "S. Mohanty",
      "R. N. Behera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9909009v1",
    "title": "The Rough Guide to Constraint Propagation",
    "summary": "We provide here a simple, yet very general framework that allows us to\nexplain several constraint propagation algorithms in a systematic way. In\nparticular, using the notions commutativity and semi-commutativity, we show how\nthe well-known AC-3, PC-2, DAC and DPC algorithms are instances of a single\ngeneric algorithm. The work reported here extends and simplifies that of Apt,\ncs.AI/9811024.",
    "published": "1999-09-08T13:50:01Z",
    "link": "http://arxiv.org/pdf/cs/9909009v1.pdf",
    "category": [
      "cs.AI",
      "cs.PL",
      "D.3.3; I.1.2; I.2.2"
    ],
    "authors": [
      "Krzysztof R. Apt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9909010v1",
    "title": "Automatic Generation of Constraint Propagation Algorithms for Small\n  Finite Domains",
    "summary": "We study here constraint satisfaction problems that are based on predefined,\nexplicitly given finite constraints. To solve them we propose a notion of rule\nconsistency that can be expressed in terms of rules derived from the explicit\nrepresentation of the initial constraints.\n  This notion of local consistency is weaker than arc consistency for\nconstraints of arbitrary arity but coincides with it when all domains are unary\nor binary. For Boolean constraints rule consistency coincides with the closure\nunder the well-known propagation rules for Boolean constraints.\n  By generalizing the format of the rules we obtain a characterization of arc\nconsistency in terms of so-called inclusion rules. The advantage of rule\nconsistency and this rule based characterization of the arc consistency is that\nthe algorithms that enforce both notions can be automatically generated, as CHR\nrules. So these algorithms could be integrated into constraint logic\nprogramming systems such as Eclipse.\n  We illustrate the usefulness of this approach to constraint propagation by\ndiscussing the implementations of both algorithms and their use on various\nexamples, including Boolean constraints, three valued logic of Kleene,\nconstraints dealing with Waltz's language for describing polyhedreal scenes,\nand Allen's qualitative approach to temporal logic.",
    "published": "1999-09-08T14:18:47Z",
    "link": "http://arxiv.org/pdf/cs/9909010v1.pdf",
    "category": [
      "cs.AI",
      "cs.PL",
      "D.#.2; I.2.2; I.2.3"
    ],
    "authors": [
      "Krzysztof R. Apt",
      "Eric Monfroy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9909014v1",
    "title": "Reasoning About Common Knowledge with Infinitely Many Agents",
    "summary": "Complete axiomatizations and exponential-time decision procedures are\nprovided for reasoning about knowledge and common knowledge when there are\ninfinitely many agents. The results show that reasoning about knowledge and\ncommon knowledge with infinitely many agents is no harder than when there are\nfinitely many agents, provided that we can check the cardinality of certain set\ndifferences G - G', where G and G' are sets of agents. Since our complexity\nresults are independent of the cardinality of the sets G involved, they\nrepresent improvements over the previous results even with the sets of agents\ninvolved are finite. Moreover, our results make clear the extent to which\nissues of complexity and completeness depend on how the sets of agents involved\nare represented.",
    "published": "1999-09-21T20:43:46Z",
    "link": "http://arxiv.org/pdf/cs/9909014v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1; I.2.4"
    ],
    "authors": [
      "Joseph Y. Halpern",
      "Richard A. Shore"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9909019v1",
    "title": "Knowledge in Multi-Agent Systems: Initial Configurations and Broadcast",
    "summary": "The semantic framework for the modal logic of knowledge due to Halpern and\nMoses provides a way to ascribe knowledge to agents in distributed and\nmulti-agent systems. In this paper we study two special cases of this\nframework: full systems and hypercubes. Both model static situations in which\nno agent has any information about another agent's state. Full systems and\nhypercubes are an appropriate model for the initial configurations of many\nsystems of interest. We establish a correspondence between full systems and\nhypercube systems and certain classes of Kripke frames. We show that these\nclasses of systems correspond to the same logic. Moreover, this logic is also\nthe same as that generated by the larger class of weakly directed frames. We\nprovide a sound and complete axiomatization, S5WDn, of this logic. Finally, we\nshow that under certain natural assumptions, in a model where knowledge evolves\nover time, S5WDn characterizes the properties of knowledge not just at the\ninitial configuration, but also at all later configurations. In particular,\nthis holds for homogeneous broadcast systems, which capture settings in which\nagents are initially ignorant of each others local states, operate\nsynchronously, have perfect recall and can communicate only by broadcasting.",
    "published": "1999-09-30T17:03:47Z",
    "link": "http://arxiv.org/pdf/cs/9909019v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1; I.2.4"
    ],
    "authors": [
      "A. R. Lomuscio",
      "R. van der Meyden",
      "M. D. Ryan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9910015v3",
    "title": "PIPE: Personalizing Recommendations via Partial Evaluation",
    "summary": "It is shown that personalization of web content can be advantageously viewed\nas a form of partial evaluation --- a technique well known in the programming\nlanguages community. The basic idea is to model a recommendation space as a\nprogram, then partially evaluate this program with respect to user preferences\n(and features) to obtain specialized content. This technique supports both\ncontent-based and collaborative approaches, and is applicable to a range of\napplications that require automatic information integration from multiple web\nsources. The effectiveness of this methodology is illustrated by two example\napplications --- (i) personalizing content for visitors to the Blacksburg\nElectronic Village (http://www.bev.net), and (ii) locating and selecting\nscientific software on the Internet. The scalability of this technique is\ndemonstrated by its ability to interface with online web ontologies that index\nthousands of web pages.",
    "published": "1999-10-18T15:47:29Z",
    "link": "http://arxiv.org/pdf/cs/9910015v3.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "H.4.2"
    ],
    "authors": [
      "Naren Ramakrishnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9910016v1",
    "title": "Probabilistic Agent Programs",
    "summary": "Agents are small programs that autonomously take actions based on changes in\ntheir environment or ``state.'' Over the last few years, there have been an\nincreasing number of efforts to build agents that can interact and/or\ncollaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd\nPick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top\nof legacy code. However, their framework assumes that agent states are\ncompletely determined, and there is no uncertainty in an agent's state. Thus,\ntheir framework allows an agent developer to specify how his agents will react\nwhen the agent is 100% sure about what is true/false in the world state. In\nthis paper, we propose the concept of a \\emph{probabilistic agent program} and\nshow how, given an arbitrary program written in any imperative language, we may\nbuild a declarative ``probabilistic'' agent program on top of it which supports\ndecision making in the presence of uncertainty. We provide two alternative\nsemantics for probabilistic agent programs. We show that the second semantics,\nthough more epistemically appealing, is more complex to compute. We provide\nsound and complete algorithms to compute the semantics of \\emph{positive} agent\nprograms.",
    "published": "1999-10-21T09:35:38Z",
    "link": "http://arxiv.org/pdf/cs/9910016v1.pdf",
    "category": [
      "cs.AI",
      "D.1.6"
    ],
    "authors": [
      "Juergen Dix",
      "Mirco Nanni",
      "VS Subrahmanian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9911012v2",
    "title": "Cox's Theorem Revisited",
    "summary": "The assumptions needed to prove Cox's Theorem are discussed and examined.\nVarious sets of assumptions under which a Cox-style theorem can be proved are\nprovided, although all are rather strong and, arguably, not natural.",
    "published": "1999-11-27T17:57:17Z",
    "link": "http://arxiv.org/pdf/cs/9911012v2.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.7"
    ],
    "authors": [
      "Joseph Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/9912008v2",
    "title": "New Error Bounds for Solomonoff Prediction",
    "summary": "Solomonoff sequence prediction is a scheme to predict digits of binary\nstrings without knowing the underlying probability distribution. We call a\nprediction scheme informed when it knows the true probability distribution of\nthe sequence. Several new relations between universal Solomonoff sequence\nprediction and informed prediction and general probabilistic prediction schemes\nwill be proved. Among others, they show that the number of errors in Solomonoff\nprediction is finite for computable distributions, if finite in the informed\ncase. Deterministic variants will also be studied. The most interesting result\nis that the deterministic variant of Solomonoff prediction is optimal compared\nto any other probabilistic or deterministic prediction scheme apart from\nadditive square root corrections only. This makes it well suited even for\ndifficult prediction problems, where it does not suffice when the number of\nerrors is minimal to within some factor greater than one. Solomonoff's original\nbound and the ones presented here complement each other in a useful way.",
    "published": "1999-12-13T08:33:43Z",
    "link": "http://arxiv.org/pdf/cs/9912008v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "I.2.6; F.1.3; E.4; F.2"
    ],
    "authors": [
      "Marcus Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0001002v1",
    "title": "Minimum Description Length and Compositionality",
    "summary": "We present a non-vacuous definition of compositionality. It is based on the\nidea of combining the minimum description length principle with the original\ndefinition of compositionality (that is, that the meaning of the whole is a\nfunction of the meaning of the parts).\n  The new definition is intuitive and allows us to distinguish between\ncompositional and non-compositional semantics, and between idiomatic and\nnon-idiomatic expressions. It is not ad hoc, since it does not make any\nreferences to non-intrinsic properties of meaning functions (like being a\npolynomial). Moreover, it allows us to compare different meaning functions with\nrespect to how compositional they are. It bridges linguistic and corpus-based,\nstatistical approaches to natural language understanding.",
    "published": "2000-01-04T21:46:29Z",
    "link": "http://arxiv.org/pdf/cs/0001002v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Wlodek Zadrozny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0001015v1",
    "title": "Multi-Agent Only Knowing",
    "summary": "Levesque introduced a notion of ``only knowing'', with the goal of capturing\ncertain types of nonmonotonic reasoning. Levesque's logic dealt with only the\ncase of a single agent. Recently, both Halpern and Lakemeyer independently\nattempted to extend Levesque's logic to the multi-agent case. Although there\nare a number of similarities in their approaches, there are some significant\ndifferences. In this paper, we reexamine the notion of only knowing, going back\nto first principles. In the process, we simplify Levesque's completeness proof,\nand point out some problems with the earlier definitions. This leads us to\nreconsider what the properties of only knowing ought to be. We provide an axiom\nsystem that captures our desiderata, and show that it has a semantics that\ncorresponds to it. The axiom system has an added feature of interest: it\nincludes a modal operator for satisfiability, and thus provides a complete\naxiomatization for satisfiability in the logic K45.",
    "published": "2000-01-19T22:13:38Z",
    "link": "http://arxiv.org/pdf/cs/0001015v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4, F.4.1"
    ],
    "authors": [
      "Joseph Y. Halpern",
      "Gerhard Lakemeyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002001v2",
    "title": "Computing large and small stable models",
    "summary": "In this paper, we focus on the problem of existence and computing of small\nand large stable models. We show that for every fixed integer k, there is a\nlinear-time algorithm to decide the problem LSM (large stable models problem):\ndoes a logic program P have a stable model of size at least |P|-k. In contrast,\nwe show that the problem SSM (small stable models problem) to decide whether a\nlogic program P has a stable model of size at most k is much harder. We present\ntwo algorithms for this problem but their running time is given by polynomials\nof order depending on k. We show that the problem SSM is fixed-parameter\nintractable by demonstrating that it is W[2]-hard. This result implies that it\nis unlikely, an algorithm exists to compute stable models of size at most k\nthat would run in time O(n^c), where c is a constant independent of k. We also\nprovide an upper bound on the fixed-parameter complexity of the problem SSM by\nshowing that it belongs to the class W[3].",
    "published": "2000-02-03T21:15:34Z",
    "link": "http://arxiv.org/pdf/cs/0002001v2.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3;I.2.4"
    ],
    "authors": [
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002002v1",
    "title": "Uniform semantic treatment of default and autoepistemic logics",
    "summary": "We revisit the issue of connections between two leading formalisms in\nnonmonotonic reasoning: autoepistemic logic and default logic. For each logic\nwe develop a comprehensive semantic framework based on the notion of a belief\npair. The set of all belief pairs together with the so called knowledge\nordering forms a complete lattice. For each logic, we introduce several\nsemantics by means of fixpoints of operators on the lattice of belief pairs.\nOur results elucidate an underlying isomorphism of the respective semantic\nconstructions. In particular, we show that the interpretation of defaults as\nmodal formulas proposed by Konolige allows us to represent all semantics for\ndefault logic in terms of the corresponding semantics for autoepistemic logic.\nThus, our results conclusively establish that default logic can indeed be\nviewed as a fragment of autoepistemic logic. However, as we also demonstrate,\nthe semantics of Moore and Reiter are given by different operators and occupy\ndifferent locations in their corresponding families of semantics. This result\nexplains the source of the longstanding difficulty to formally relate these two\nsemantics. In the paper, we also discuss approximating skeptical reasoning with\nautoepistemic and default logics and establish constructive principles behind\nsuch approximations.",
    "published": "2000-02-03T21:44:57Z",
    "link": "http://arxiv.org/pdf/cs/0002002v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Marc Denecker",
      "Victor W. Marek",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002003v1",
    "title": "On the accuracy and running time of GSAT",
    "summary": "Randomized algorithms for deciding satisfiability were shown to be effective\nin solving problems with thousands of variables. However, these algorithms are\nnot complete. That is, they provide no guarantee that a satisfying assignment,\nif one exists, will be found. Thus, when studying randomized algorithms, there\nare two important characteristics that need to be considered: the running time\nand, even more importantly, the accuracy --- a measure of likelihood that a\nsatisfying assignment will be found, provided one exists. In fact, we argue\nthat without a reference to the accuracy, the notion of the running time for\nrandomized algorithms is not well-defined. In this paper, we introduce a formal\nnotion of accuracy. We use it to define a concept of the running time. We use\nboth notions to study the random walk strategy GSAT algorithm. We investigate\nthe dependence of accuracy on properties of input formulas such as\nclause-to-variable ratio and the number of satisfying assignments. We\ndemonstrate that the running time of GSAT grows exponentially in the number of\nvariables of the input formula for randomly generated 3-CNF formulas and for\nthe formulas encoding 3- and 4-colorability of graphs.",
    "published": "2000-02-04T12:53:57Z",
    "link": "http://arxiv.org/pdf/cs/0002003v1.pdf",
    "category": [
      "cs.AI",
      "I.2.8"
    ],
    "authors": [
      "Deborah East",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002009v1",
    "title": "Syntactic Autonomy: Why There is no Autonomy without Symbols and How\n  Self-Organization Might Evolve Them",
    "summary": "Two different types of agency are discussed based on dynamically coherent and\nincoherent couplings with an environment respectively. I propose that until a\nprivate syntax (syntactic autonomy) is discovered by dynamically coherent\nagents, there are no significant or interesting types of closure or autonomy.\nWhen syntactic autonomy is established, then, because of a process of\ndescription-based selected self-organization, open-ended evolution is enabled.\nAt this stage, agents depend, in addition to dynamics, on localized, symbolic\nmemory, thus adding a level of dynamical incoherence to their interaction with\nthe environment. Furthermore, it is the appearance of syntactic autonomy which\nenables much more interesting types of closures amongst agents which share the\nsame syntax. To investigate how we can study the emergence of syntax from\ndynamical systems, experiments with cellular automata leading to emergent\ncomputation to solve non-trivial tasks are discussed. RNA editing is also\nmentioned as a process that may have been used to obtain a primordial\nbiological code necessary open-ended evolution.",
    "published": "2000-02-16T18:09:20Z",
    "link": "http://arxiv.org/pdf/cs/0002009v1.pdf",
    "category": [
      "cs.AI",
      "A.m"
    ],
    "authors": [
      "Luis M. Rocha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002015v1",
    "title": "Genetic Algorithms for Extension Search in Default Logic",
    "summary": "A default theory can be characterized by its sets of plausible conclusions,\ncalled its extensions. But, due to the theoretical complexity of Default Logic\n(Sigma_2p-complete), the problem of finding such an extension is very difficult\nif one wants to deal with non trivial knowledge bases. Based on the principle\nof natural selection, Genetic Algorithms have been quite successfully applied\nto combinatorial problems and seem useful for problems with huge search spaces\nand when no tractable algorithm is available. The purpose of this paper is to\nshow that techniques issued from Genetic Algorithms can be used in order to\nbuild an efficient default reasoning system. After providing a formal\ndescription of the components required for an extension search based on Genetic\nAlgorithms principles, we exhibit some experimental results.",
    "published": "2000-02-24T16:09:04Z",
    "link": "http://arxiv.org/pdf/cs/0002015v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "F.4.1"
    ],
    "authors": [
      "P. Nicolas",
      "F. Saubion",
      "I. Stephan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002014v1",
    "title": "Safe cooperative robot dynamics on graphs",
    "summary": "This paper initiates the use of vector fields to design, optimize, and\nimplement reactive schedules for safe cooperative robot patterns on planar\ngraphs. We consider Automated Guided Vehicles (AGV's) operating upon a\npredefined network of pathways. In contrast to the case of locally Euclidean\nconfiguration spaces, regularization of collisions is no longer a local\nprocedure, and issues concerning the global topology of configuration spaces\nmust be addressed. The focus of the present inquiry is the achievement of safe,\nefficient, cooperative patterns in the simplest nontrivial example (a pair of\nrobots on a Y-network) by means of a state-event heirarchical controller.",
    "published": "2000-02-24T18:13:33Z",
    "link": "http://arxiv.org/pdf/cs/0002014v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "I.2.9"
    ],
    "authors": [
      "Robert Ghrist",
      "Daniel Koditschek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0002016v3",
    "title": "SLT-Resolution for the Well-Founded Semantics",
    "summary": "Global SLS-resolution and SLG-resolution are two representative mechanisms\nfor top-down evaluation of the well-founded semantics of general logic\nprograms. Global SLS-resolution is linear for query evaluation but suffers from\ninfinite loops and redundant computations. In contrast, SLG-resolution resolves\ninfinite loops and redundant computations by means of tabling, but it is not\nlinear. The principal disadvantage of a non-linear approach is that it cannot\nbe implemented using a simple, efficient stack-based memory structure nor can\nit be easily extended to handle some strictly sequential operators such as cuts\nin Prolog.\n  In this paper, we present a linear tabling method, called SLT-resolution, for\ntop-down evaluation of the well-founded semantics. SLT-resolution is a\nsubstantial extension of SLDNF-resolution with tabling. Its main features\ninclude: (1) It resolves infinite loops and redundant computations while\npreserving the linearity. (2) It is terminating, and sound and complete w.r.t.\nthe well-founded semantics for programs with the bounded-term-size property\nwith non-floundering queries. Its time complexity is comparable with\nSLG-resolution and polynomial for function-free logic programs. (3) Because of\nits linearity for query evaluation, SLT-resolution bridges the gap between the\nwell-founded semantics and standard Prolog implementation techniques. It can be\nimplemented by an extension to any existing Prolog abstract machines such as\nWAM or ATOAM.",
    "published": "2000-02-27T19:20:05Z",
    "link": "http://arxiv.org/pdf/cs/0002016v3.pdf",
    "category": [
      "cs.AI",
      "cs.PL",
      "D.3.1; F.4.1; I.2.3"
    ],
    "authors": [
      "Yi-Dong Shen",
      "Li-Yan Yuan",
      "Jia-Huai You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003003v1",
    "title": "Prospects for in-depth story understanding by computer",
    "summary": "While much research on the hard problem of in-depth story understanding by\ncomputer was performed starting in the 1970s, interest shifted in the 1990s to\ninformation extraction and word sense disambiguation. Now that a degree of\nsuccess has been achieved on these easier problems, I propose it is time to\nreturn to in-depth story understanding. In this paper I examine the shift away\nfrom story understanding, discuss some of the major problems in building a\nstory understanding system, present some possible solutions involving a set of\ninteracting understanding agents, and provide pointers to useful tools and\nresources for building story understanding systems.",
    "published": "2000-03-01T18:01:06Z",
    "link": "http://arxiv.org/pdf/cs/0003003v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "authors": [
      "Erik T. Mueller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003004v1",
    "title": "A database and lexicon of scripts for ThoughtTreasure",
    "summary": "Since scripts were proposed in the 1970's as an inferencing mechanism for AI\nand natural language processing programs, there have been few attempts to build\na database of scripts. This paper describes a database and lexicon of scripts\nthat has been added to the ThoughtTreasure commonsense platform. The database\nprovides the following information about scripts: sequence of events, roles,\nprops, entry conditions, results, goals, emotions, places, duration, frequency,\nand cost. English and French words and phrases are linked to script concepts.",
    "published": "2000-03-01T18:07:02Z",
    "link": "http://arxiv.org/pdf/cs/0003004v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "I.2.7; I.2.4"
    ],
    "authors": [
      "Erik T. Mueller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003007v1",
    "title": "Computing Circumscriptive Databases by Integer Programming: Revisited\n  (Extended Abstract)",
    "summary": "In this paper, we consider a method of computing minimal models in\ncircumscription using integer programming in propositional logic and\nfirst-order logic with domain closure axioms and unique name axioms. This kind\nof treatment is very important since this enable to apply various technique\ndeveloped in operations research to nonmonotonic reasoning.\n  Nerode et al. (1995) are the first to propose a method of computing\ncircumscription using integer programming. They claimed their method was\ncorrect for circumscription with fixed predicate, but we show that their method\ndoes not correctly reflect their claim. We show a correct method of computing\nall the minimal models not only with fixed predicates but also with varied\npredicates and we extend our method to compute prioritized circumscription as\nwell.",
    "published": "2000-03-05T09:57:49Z",
    "link": "http://arxiv.org/pdf/cs/0003007v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3"
    ],
    "authors": [
      "Ken Satoh",
      "Hidenori Okamoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003008v1",
    "title": "Consistency Management of Normal Logic Program by Top-down Abductive\n  Proof Procedure",
    "summary": "This paper presents a method of computing a revision of a function-free\nnormal logic program. If an added rule is inconsistent with a program, that is,\nif it leads to a situation such that no stable model exists for a new program,\nthen deletion and addition of rules are performed to avoid inconsistency. We\nspecify a revision by translating a normal logic program into an abductive\nlogic program with abducibles to represent deletion and addition of rules. To\ncompute such deletion and addition, we propose an adaptation of our top-down\nabductive proof procedure to compute a relevant abducibles to an added rule. We\ncompute a minimally revised program, by choosing a minimal set of abducibles\namong all the sets of abducibles computed by a top-down proof procedure.",
    "published": "2000-03-05T10:29:03Z",
    "link": "http://arxiv.org/pdf/cs/0003008v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "Ken Satoh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003009v1",
    "title": "Conditional indifference and conditional preservation",
    "summary": "The idea of preserving conditional beliefs emerged recently as a new paradigm\napt to guide the revision of epistemic states. Conditionals are substantially\ndifferent from propositional beliefs and need specific treatment. In this\npaper, we present a new approach to conditionals, capturing particularly well\ntheir dynamic part as revision policies. We thoroughly axiomatize a principle\nof conditional preservation as an indifference property with respect to\nconditional structures of worlds. This principle is developed in a\nsemi-quantitative setting, so as to reveal its fundamental meaning for belief\nrevision in quantitative as well as in qualitative frameworks. In fact, it is\nshown to cover other proposed approaches to conditional preservation.",
    "published": "2000-03-06T12:08:06Z",
    "link": "http://arxiv.org/pdf/cs/0003009v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.0; I.2.3; I.2.6"
    ],
    "authors": [
      "Gabriele Kern-Isberner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003011v2",
    "title": "Automatic Belief Revision in SNePS",
    "summary": "SNePS is a logic- and network- based knowledge representation, reasoning, and\nacting system, based on a monotonic, paraconsistent, first-order term logic,\nwith compositional intensional semantics. It has an ATMS-style facility for\nbelief contraction, and an acting component, including a well-defined syntax\nand semantics for primitive and composite acts, as well as for ``rules'' that\nallow for acting in support of reasoning and reasoning in support of acting.\nSNePS has been designed to support natural language competent cognitive agents.\n  When the current version of SNePS detects an explicit contradiction, it\ninteracts with the user, providing information that helps the user decide what\nto remove from the knowledge base in order to remove the contradiction. The\nforthcoming SNePS 2.6 will also do automatic belief contraction if the\ninformation in the knowledge base warrents it.",
    "published": "2000-03-06T16:55:16Z",
    "link": "http://arxiv.org/pdf/cs/0003011v2.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Stuart C. Shapiro",
      "Frances L. Johnson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003012v1",
    "title": "Defeasible Reasoning in OSCAR",
    "summary": "This is a system description for the OSCAR defeasible reasoner.",
    "published": "2000-03-06T22:23:00Z",
    "link": "http://arxiv.org/pdf/cs/0003012v1.pdf",
    "category": [
      "cs.AI",
      "F.4.1"
    ],
    "authors": [
      "John L. Pollock"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003013v1",
    "title": "A flexible framework for defeasible logics",
    "summary": "Logics for knowledge representation suffer from over-specialization: while\neach logic may provide an ideal representation formalism for some problems, it\nis less than optimal for others. A solution to this problem is to choose from\nseveral logics and, when necessary, combine the representations. In general,\nsuch an approach results in a very difficult problem of combination. However,\nif we can choose the logics from a uniform framework then the problem of\ncombining them is greatly simplified. In this paper, we develop such a\nframework for defeasible logics. It supports all defeasible logics that satisfy\na strong negation principle. We use logic meta-programs as the basis for the\nframework.",
    "published": "2000-03-07T01:24:26Z",
    "link": "http://arxiv.org/pdf/cs/0003013v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3; D.1.6"
    ],
    "authors": [
      "G. Antoniou",
      "D. Billigton",
      "G. Governatori",
      "M. J. Maher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003014v2",
    "title": "Applying Maxi-adjustment to Adaptive Information Filtering Agents",
    "summary": "Learning and adaptation is a fundamental property of intelligent agents. In\nthe context of adaptive information filtering, a filtering agent's beliefs\nabout a user's information needs have to be revised regularly with reference to\nthe user's most current information preferences. This learning and adaptation\nprocess is essential for maintaining the agent's filtering performance. The AGM\nbelief revision paradigm provides a rigorous foundation for modelling rational\nand minimal changes to an agent's beliefs. In particular, the maxi-adjustment\nmethod, which follows the AGM rationale of belief change, offers a sound and\nrobust computational mechanism to develop adaptive agents so that learning\nautonomy of these agents can be enhanced. This paper describes how the\nmaxi-adjustment method is applied to develop the learning components of\nadaptive information filtering agents, and discusses possible difficulties of\napplying such a framework to these agents.",
    "published": "2000-03-07T02:12:55Z",
    "link": "http://arxiv.org/pdf/cs/0003014v2.pdf",
    "category": [
      "cs.AI",
      "cs.MA",
      "I.2.3;I.2.11;H.3.3"
    ],
    "authors": [
      "Raymond Lau",
      "Arthur H. M. ter Hofstede",
      "Peter D. Bruza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003016v1",
    "title": "Abductive and Consistency-Based Diagnosis Revisited: a Modeling\n  Perspective",
    "summary": "Diagnostic reasoning has been characterized logically as consistency-based\nreasoning or abductive reasoning. Previous analyses in the literature have\nshown, on the one hand, that choosing the (in general more restrictive)\nabductive definition may be appropriate or not, depending on the content of the\nknowledge base [Console&Torasso91], and, on the other hand, that, depending on\nthe choice of the definition the same knowledge should be expressed in\ndifferent form [Poole94].\n  Since in Model-Based Diagnosis a major problem is finding the right way of\nabstracting the behavior of the system to be modeled, this paper discusses the\nrelation between modeling, and in particular abstraction in the model, and the\nnotion of diagnosis.",
    "published": "2000-03-07T11:39:53Z",
    "link": "http://arxiv.org/pdf/cs/0003016v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Daniele Theseider Dupre'"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003017v2",
    "title": "The lexicographic closure as a revision process",
    "summary": "The connections between nonmonotonic reasoning and belief revision are\nwell-known. A central problem in the area of nonmonotonic reasoning is the\nproblem of default entailment, i.e., when should an item of default information\nrepresenting \"if A is true then, normally, B is true\" be said to follow from a\ngiven set of items of such information. Many answers to this question have been\nproposed but, surprisingly, virtually none have attempted any explicit\nconnection to belief revision. The aim of this paper is to give an example of\nhow such a connection can be made by showing how the lexicographic closure of a\nset of defaults may be conceptualised as a process of iterated revision by sets\nof sentences. Specifically we use the revision process of Nayak.",
    "published": "2000-03-07T11:44:35Z",
    "link": "http://arxiv.org/pdf/cs/0003017v2.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3"
    ],
    "authors": [
      "Richard Booth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003015v2",
    "title": "On the semantics of merging",
    "summary": "Intelligent agents are often faced with the problem of trying to merge\npossibly conflicting pieces of information obtained from different sources into\na consistent view of the world. We propose a framework for the modelling of\nsuch merging operations with roots in the work of Spohn (1988, 1991). Unlike\nmost approaches we focus on the merging of epistemic states, not knowledge\nbases. We construct a number of plausible merging operations and measure them\nagainst various properties that merging operations ought to satisfy. Finally,\nwe discuss the connection between merging and the use of infobases Meyer (1999)\nand Meyer et al. (2000).",
    "published": "2000-03-07T12:26:34Z",
    "link": "http://arxiv.org/pdf/cs/0003015v2.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3; I.2.4; I.2.11"
    ],
    "authors": [
      "Thomas Meyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003018v1",
    "title": "Description of GADEL",
    "summary": "This article describes the first implementation of the GADEL system : a\nGenetic Algorithm for Default Logic. The goal of GADEL is to compute extensions\nin Reiter's default logic. It accepts every kind of finite propositional\ndefault theories and is based on evolutionary principles of Genetic Algorithms.\nIts first experimental results on certain instances of the problem show that\nthis new approach of the problem can be successful.",
    "published": "2000-03-07T14:46:23Z",
    "link": "http://arxiv.org/pdf/cs/0003018v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "F.4.1"
    ],
    "authors": [
      "I. Stephan",
      "F. Saubion",
      "P. Nicolas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003019v1",
    "title": "Extending Classical Logic with Inductive Definitions",
    "summary": "The goal of this paper is to extend classical logic with a generalized notion\nof inductive definition supporting positive and negative induction, to\ninvestigate the properties of this logic, its relationships to other logics in\nthe area of non-monotonic reasoning, logic programming and deductive databases,\nand to show its application for knowledge representation by giving a typology\nof definitional knowledge.",
    "published": "2000-03-07T15:44:08Z",
    "link": "http://arxiv.org/pdf/cs/0003019v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3;I.2.4;F.4.1"
    ],
    "authors": [
      "Marc Denecker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003020v2",
    "title": "ACLP: Integrating Abduction and Constraint Solving",
    "summary": "ACLP is a system which combines abductive reasoning and constraint solving by\nintegrating the frameworks of Abductive Logic Programming (ALP) and Constraint\nLogic Programming (CLP). It forms a general high-level knowledge representation\nenvironment for abductive problems in Artificial Intelligence and other areas.\nIn ACLP, the task of abduction is supported and enhanced by its non-trivial\nintegration with constraint solving facilitating its application to complex\nproblems. The ACLP system is currently implemented on top of the CLP language\nof ECLiPSe as a meta-interpreter exploiting its underlying constraint solver\nfor finite domains. It has been applied to the problems of planning and\nscheduling in order to test its computational effectiveness compared with the\ndirect use of the (lower level) constraint solving framework of CLP on which it\nis built. These experiments provide evidence that the abductive framework of\nACLP does not compromise significantly the computational efficiency of the\nsolutions. Other experiments show the natural ability of ACLP to accommodate\neasily and in a robust way new or changing requirements of the original\nproblem.",
    "published": "2000-03-07T22:47:13Z",
    "link": "http://arxiv.org/pdf/cs/0003020v2.pdf",
    "category": [
      "cs.AI",
      "I.2.4;F.4.1"
    ],
    "authors": [
      "Antonis Kakas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003021v1",
    "title": "Relevance Sensitive Non-Monotonic Inference on Belief Sequences",
    "summary": "We present a method for relevance sensitive non-monotonic inference from\nbelief sequences which incorporates insights pertaining to prioritized\ninference and relevance sensitive, inconsistency tolerant belief revision.\n  Our model uses a finite, logically open sequence of propositional formulas as\na representation for beliefs and defines a notion of inference from\nmaxiconsistent subsets of formulas guided by two orderings: a temporal\nsequencing and an ordering based on relevance relations between the conclusion\nand formulas in the sequence. The relevance relations are ternary (using\ncontext as a parameter) as opposed to standard binary axiomatizations. The\ninference operation thus defined easily handles iterated revision by\nmaintaining a revision history, blocks the derivation of inconsistent answers\nfrom a possibly inconsistent sequence and maintains the distinction between\nexplicit and implicit beliefs. In doing so, it provides a finitely presented\nformalism and a plausible model of reasoning for automated agents.",
    "published": "2000-03-08T03:03:36Z",
    "link": "http://arxiv.org/pdf/cs/0003021v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "Samir Chopra",
      "Konstantinos Georgatos",
      "Rohit Parikh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003024v1",
    "title": "A Compiler for Ordered Logic Programs",
    "summary": "This paper describes a system, called PLP, for compiling ordered logic\nprograms into standard logic programs under the answer set semantics. In an\nordered logic program, rules are named by unique terms, and preferences among\nrules are given by a set of dedicated atoms. An ordered logic program is\ntransformed into a second, regular, extended logic program wherein the\npreferences are respected, in that the answer sets obtained in the transformed\ntheory correspond with the preferred answer sets of the original theory. Since\nthe result of the translation is an extended logic program, existing logic\nprogramming systems can be used as underlying reasoning engine. In particular,\nPLP is conceived as a front-end to the logic programming systems dlv and\nsmodels.",
    "published": "2000-03-08T10:15:51Z",
    "link": "http://arxiv.org/pdf/cs/0003024v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "James P. Delgrande",
      "Torsten Schaub",
      "Hans Tompits"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003023v1",
    "title": "Probabilistic Default Reasoning with Conditional Constraints",
    "summary": "We propose a combination of probabilistic reasoning from conditional\nconstraints with approaches to default reasoning from conditional knowledge\nbases. In detail, we generalize the notions of Pearl's entailment in system Z,\nLehmann's lexicographic entailment, and Geffner's conditional entailment to\nconditional constraints. We give some examples that show that the new notions\nof z-, lexicographic, and conditional entailment have similar properties like\ntheir classical counterparts. Moreover, we show that the new notions of z-,\nlexicographic, and conditional entailment are proper generalizations of both\ntheir classical counterparts and the classical notion of logical entailment for\nconditional constraints.",
    "published": "2000-03-08T11:05:45Z",
    "link": "http://arxiv.org/pdf/cs/0003023v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Thomas Lukasiewicz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003027v1",
    "title": "SLDNFA-system",
    "summary": "The SLDNFA-system results from the LP+ project at the K.U.Leuven, which\ninvestigates logics and proof procedures for these logics for declarative\nknowledge representation. Within this project inductive definition logic\n(ID-logic) is used as representation logic. Different solvers are being\ndeveloped for this logic and one of these is SLDNFA. A prototype of the system\nis available and used for investigating how to solve efficiently problems\nrepresented in ID-logic.",
    "published": "2000-03-08T13:22:44Z",
    "link": "http://arxiv.org/pdf/cs/0003027v1.pdf",
    "category": [
      "cs.AI",
      "F.4.1;I.2.3;I.2.4"
    ],
    "authors": [
      "Bert Van Nuffelen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003025v1",
    "title": "Logic Programming for Describing and Solving Planning Problems",
    "summary": "A logic programming paradigm which expresses solutions to problems as stable\nmodels has recently been promoted as a declarative approach to solving various\ncombinatorial and search problems, including planning problems. In this\nparadigm, all program rules are considered as constraints and solutions are\nstable models of the rule set. This is a rather radical departure from the\nstandard paradigm of logic programming. In this paper we revisit abductive\nlogic programming and argue that it allows a programming style which is as\ndeclarative as programming based on stable models. However, within abductive\nlogic programming, one has two kinds of rules. On the one hand predicate\ndefinitions (which may depend on the abducibles) which are nothing else than\nstandard logic programs (with their non-monotonic semantics when containing\nwith negation); on the other hand rules which constrain the models for the\nabducibles. In this sense abductive logic programming is a smooth extension of\nthe standard paradigm of logic programming, not a radical departure.",
    "published": "2000-03-08T14:00:35Z",
    "link": "http://arxiv.org/pdf/cs/0003025v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "D.1.6; D.3.1; F.4.1; I.2.3"
    ],
    "authors": [
      "Maurice Bruynooghe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003028v1",
    "title": "Logic Programs with Compiled Preferences",
    "summary": "We describe an approach for compiling preferences into logic programs under\nthe answer set semantics. An ordered logic program is an extended logic program\nin which rules are named by unique terms, and in which preferences among rules\nare given by a set of dedicated atoms. An ordered logic program is transformed\ninto a second, regular, extended logic program wherein the preferences are\nrespected, in that the answer sets obtained in the transformed theory\ncorrespond with the preferred answer sets of the original theory. Our approach\nallows both the specification of static orderings (as found in most previous\nwork), in which preferences are external to a logic program, as well as\norderings on sets of rules. In large part then, we are interested in describing\na general methodology for uniformly incorporating preference information in a\nlogic program. Since the result of our translation is an extended logic\nprogram, we can make use of existing implementations, such as dlv and smodels.\nTo this end, we have developed a compiler, available on the web, as a front-end\nfor these programming systems.",
    "published": "2000-03-08T14:09:56Z",
    "link": "http://arxiv.org/pdf/cs/0003028v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "James P. Delgrande",
      "Torsten Schaub",
      "Hans Tompits"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003029v1",
    "title": "Fuzzy Approaches to Abductive Inference",
    "summary": "This paper proposes two kinds of fuzzy abductive inference in the framework\nof fuzzy rule base. The abductive inference processes described here depend on\nthe semantic of the rule. We distinguish two classes of interpretation of a\nfuzzy rule, certainty generation rules and possible generation rules. In this\npaper we present the architecture of abductive inference in the first class of\ninterpretation. We give two kinds of problem that we can resolve by using the\nproposed models of inference.",
    "published": "2000-03-08T14:56:58Z",
    "link": "http://arxiv.org/pdf/cs/0003029v1.pdf",
    "category": [
      "cs.AI",
      "Artificial intelligence and nonmonotonic reasoning and belief\n  revision"
    ],
    "authors": [
      "Nedra Mellouli",
      "Bernadette Bouchon-Meunier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003030v1",
    "title": "Problem solving in ID-logic with aggregates: some experiments",
    "summary": "The goal of the LP+ project at the K.U.Leuven is to design an expressive\nlogic, suitable for declarative knowledge representation, and to develop\nintelligent systems based on Logic Programming technology for solving\ncomputational problems using the declarative specifications. The ID-logic is an\nintegration of typed classical logic and a definition logic. Different\nabductive solvers for this language are being developed. This paper is a report\nof the integration of high order aggregates into ID-logic and the consequences\non the solver SLDNFA.",
    "published": "2000-03-08T15:39:14Z",
    "link": "http://arxiv.org/pdf/cs/0003030v1.pdf",
    "category": [
      "cs.AI",
      "F.4.1;I.2.4;I.2.3"
    ],
    "authors": [
      "Bert Van Nuffelen",
      "Marc Denecker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003031v1",
    "title": "Optimal Belief Revision",
    "summary": "We propose a new approach to belief revision that provides a way to change\nknowledge bases with a minimum of effort. We call this way of revising belief\nstates optimal belief revision. Our revision method gives special attention to\nthe fact that most belief revision processes are directed to a specific\ninformational objective. This approach to belief change is founded on notions\nsuch as optimal context and accessibility. For the sentential model of belief\nstates we provide both a formal description of contexts as sub-theories\ndetermined by three parameters and a method to construct contexts. Next, we\nintroduce an accessibility ordering for belief sets, which we then use for\nselecting the best (optimal) contexts with respect to the processing effort\ninvolved in the revision. Then, for finitely axiomatizable knowledge bases, we\ncharacterize a finite accessibility ranking from which the accessibility\nordering for the entire base is generated and show how to determine the ranking\nof an arbitrary sentence in the language. Finally, we define the adjustment of\nthe accessibility ranking of a revised base of a belief set.",
    "published": "2000-03-08T15:54:50Z",
    "link": "http://arxiv.org/pdf/cs/0003031v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "Carmen Vodislav",
      "Robert E. Mercer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003022v1",
    "title": "Hypothetical revision and matter-of-fact supposition",
    "summary": "The paper studies the notion of supposition encoded in non-Archimedean\nconditional probability (and revealed in the acceptance of the so-called\nindicative conditionals). The notion of qualitative change of view that thus\narises is axiomatized and compared with standard notions like AGM and UPDATE.\nApplications in the following fields are discussed: (1) theory of games and\ndecisions, (2) causal models, (3) non-monotonic logic.",
    "published": "2000-03-08T16:06:58Z",
    "link": "http://arxiv.org/pdf/cs/0003022v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "I.2.4; I.2.6; I.2.7; I.2.11"
    ],
    "authors": [
      "Horacio Arlo-Costa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003032v1",
    "title": "cc-Golog: Towards More Realistic Logic-Based Robot Controllers",
    "summary": "High-level robot controllers in realistic domains typically deal with\nprocesses which operate concurrently, change the world continuously, and where\nthe execution of actions is event-driven as in ``charge the batteries as soon\nas the voltage level is low''. While non-logic-based robot control languages\nare well suited to express such scenarios, they fare poorly when it comes to\nprojecting, in a conspicuous way, how the world evolves when actions are\nexecuted. On the other hand, a logic-based control language like \\congolog,\nbased on the situation calculus, is well-suited for the latter. However, it has\nproblems expressing event-driven behavior. In this paper, we show how these\nproblems can be overcome by first extending the situation calculus to support\ncontinuous change and event-driven behavior and then presenting \\ccgolog, a\nvariant of \\congolog which is based on the extended situation calculus. One\nbenefit of \\ccgolog is that it narrows the gap in expressiveness compared to\nnon-logic-based control languages while preserving a semantically well-founded\nprojection mechanism.",
    "published": "2000-03-08T16:14:08Z",
    "link": "http://arxiv.org/pdf/cs/0003032v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3;I.2.8"
    ],
    "authors": [
      "Henrik Grosskreutz",
      "Gerhard Lakemeyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003034v2",
    "title": "E-RES: A System for Reasoning about Actions, Events and Observations",
    "summary": "E-RES is a system that implements the Language E, a logic for reasoning about\nnarratives of action occurrences and observations. E's semantics is\nmodel-theoretic, but this implementation is based on a sound and complete\nreformulation of E in terms of argumentation, and uses general computational\ntechniques of argumentation frameworks. The system derives sceptical\nnon-monotonic consequences of a given reformulated theory which exactly\ncorrespond to consequences entailed by E's model-theory. The computation relies\non a complimentary ability of the system to derive credulous non-monotonic\nconsequences together with a set of supporting assumptions which is sufficient\nfor the (credulous) conclusion to hold. E-RES allows theories to contain\ngeneral action laws, statements about action occurrences, observations and\nstatements of ramifications (or universal laws). It is able to derive\nconsequences both forward and backward in time. This paper gives a short\noverview of the theoretical basis of E-RES and illustrates its use on a variety\nof examples. Currently, E-RES is being extended so that the system can be used\nfor planning.",
    "published": "2000-03-08T16:18:52Z",
    "link": "http://arxiv.org/pdf/cs/0003034v2.pdf",
    "category": [
      "cs.AI",
      "I.2.4; F.4.1"
    ],
    "authors": [
      "Antonis Kakas",
      "Rob Miller",
      "Francesca Toni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003035v1",
    "title": "Declarative Representation of Revision Strategies",
    "summary": "In this paper we introduce a nonmonotonic framework for belief revision in\nwhich reasoning about the reliability of different pieces of information based\non meta-knowledge about the information is possible, and where revision\nstrategies can be described declaratively. The approach is based on a\nPoole-style system for default reasoning in which entrenchment information is\nrepresented in the logical language. A notion of inference based on the least\nfixed point of a monotone operator is used to make sure that all theories\npossess a consistent set of conclusions.",
    "published": "2000-03-08T16:22:03Z",
    "link": "http://arxiv.org/pdf/cs/0003035v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3"
    ],
    "authors": [
      "Gerhard Brewka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003037v1",
    "title": "QUIP - A Tool for Computing Nonmonotonic Reasoning Tasks",
    "summary": "In this paper, we outline the prototype of an automated inference tool,\ncalled QUIP, which provides a uniform implementation for several nonmonotonic\nreasoning formalisms. The theoretical basis of QUIP is derived from well-known\nresults about the computational complexity of nonmonotonic logics and exploits\na representation of the different reasoning tasks in terms of quantified\nboolean formulae.",
    "published": "2000-03-08T17:18:08Z",
    "link": "http://arxiv.org/pdf/cs/0003037v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "Uwe Egly",
      "Thomas Eiter",
      "Hans Tompits",
      "Stefan Woltran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003036v1",
    "title": "DLV - A System for Declarative Problem Solving",
    "summary": "DLV is an efficient logic programming and non-monotonic reasoning (LPNMR)\nsystem with advanced knowledge representation mechanisms and interfaces to\nclassic relational database systems.\n  Its core language is disjunctive datalog (function-free disjunctive logic\nprogramming) under the Answer Set Semantics with integrity constraints, both\ndefault and strong (or explicit) negation, and queries. Integer arithmetics and\nvarious built-in predicates are also supported.\n  In addition DLV has several frontends, namely brave and cautious reasoning,\nabductive diagnosis, consistency-based diagnosis, a subset of SQL3, planning\nwith action languages, and logic programming with inheritance.",
    "published": "2000-03-08T18:17:33Z",
    "link": "http://arxiv.org/pdf/cs/0003036v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "D.1.6; D.3.2; I.2.4; F.4.1"
    ],
    "authors": [
      "Thomas Eiter",
      "Wolfgang Faber",
      "Christoph Koch",
      "Nicola Leone",
      "Gerald Pfeifer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003038v1",
    "title": "A Splitting Set Theorem for Epistemic Specifications",
    "summary": "Over the past decade a considerable amount of research has been done to\nexpand logic programming languages to handle incomplete information. One such\nlanguage is the language of epistemic specifications. As is usual with logic\nprogramming languages, the problem of answering queries is intractable in the\ngeneral case. For extended disjunctive logic programs, an idea that has proven\nuseful in simplifying the investigation of answer sets is the use of splitting\nsets. In this paper we will present an extended definition of splitting sets\nthat will be applicable to epistemic specifications. Furthermore, an extension\nof the splitting set theorem will be presented. Also, a characterization of\nstratified epistemic specifications will be given in terms of splitting sets.\nThis characterization leads us to an algorithmic method of computing world\nviews of a subclass of epistemic logic programs.",
    "published": "2000-03-08T20:40:31Z",
    "link": "http://arxiv.org/pdf/cs/0003038v1.pdf",
    "category": [
      "cs.AI",
      "F.4.1; I.2.3"
    ],
    "authors": [
      "Richard Watson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003040v1",
    "title": "Implementing Integrity Constraints in an Existing Belief Revision System",
    "summary": "SNePS is a mature knowledge representation, reasoning, and acting system that\nhas long contained a belief revision subsystem, called SNeBR. SNeBR is\ntriggered when an explicit contradiction is introduced into the SNePS belief\nspace, either because of a user's new assertion, or because of a user's query.\nSNeBR then makes the user decide what belief to remove from the belief space in\norder to restore consistency, although it provides information to help the user\nin making that decision. We have recently added automatic belief revision to\nSNeBR, by which, under certain circumstances, SNeBR decides by itself which\nbelief to remove, and then informs the user of the decision and its\nconsequences. We have used the well-known belief revision integrity constraints\nas a guide in designing automatic belief revision, taking into account,\nhowever, that SNePS's belief space is not deductively closed, and that it would\nbe infeasible to form the deductive closure in order to decide what belief to\nremove. This paper briefly describes SNeBR both before and after this revision,\ndiscusses how we adapted the integrity constraints for this purpose, and gives\nan example of the new SNeBR in action.",
    "published": "2000-03-08T20:42:29Z",
    "link": "http://arxiv.org/pdf/cs/0003040v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.3;I.2.4"
    ],
    "authors": [
      "Frances L. Johnson",
      "Stuart C. Shapiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003039v1",
    "title": "DES: a Challenge Problem for Nonmonotonic Reasoning Systems",
    "summary": "The US Data Encryption Standard, DES for short, is put forward as an\ninteresting benchmark problem for nonmonotonic reasoning systems because (i) it\nprovides a set of test cases of industrial relevance which shares features of\nrandomly generated problems and real-world problems, (ii) the representation of\nDES using normal logic programs with the stable model semantics is simple and\neasy to understand, and (iii) this subclass of logic programs can be seen as an\ninteresting special case for many other formalizations of nonmonotonic\nreasoning. In this paper we present two encodings of DES as logic programs: a\ndirect one out of the standard specifications and an optimized one extending\nthe work of Massacci and Marraro. The computational properties of the encodings\nare studied by using them for DES key search with the Smodels system as the\nimplementation of the stable model semantics. Results indicate that the\nencodings and Smodels are quite competitive: they outperform state-of-the-art\nSAT-checkers working with an optimized encoding of DES into SAT and are\ncomparable with a SAT-checker that is customized and tuned for the optimized\nSAT encoding.",
    "published": "2000-03-08T21:49:57Z",
    "link": "http://arxiv.org/pdf/cs/0003039v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Maarit Hietalahti",
      "Fabio Massacci",
      "Ilkka Niemela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003033v1",
    "title": "Smodels: A System for Answer Set Programming",
    "summary": "The Smodels system implements the stable model semantics for normal logic\nprograms. It handles a subclass of programs which contain no function symbols\nand are domain-restricted but supports extensions including built-in functions\nas well as cardinality and weight constraints. On top of this core engine more\ninvolved systems can be built. As an example, we have implemented total and\npartial stable model computation for disjunctive logic programs. An interesting\napplication method is based on answer set programming, i.e., encoding an\napplication problem as a set of rules so that its solutions are captured by the\nstable models of the rules. Smodels has been applied to a number of areas\nincluding planning, model checking, reachability analysis, product\nconfiguration, dynamic constraint satisfaction, and feature interaction.",
    "published": "2000-03-08T23:25:51Z",
    "link": "http://arxiv.org/pdf/cs/0003033v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Ilkka Niemela",
      "Patrik Simons",
      "Tommi Syrjanen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003041v1",
    "title": "Coherence, Belief Expansion and Bayesian Networks",
    "summary": "We construct a probabilistic coherence measure for information sets which\ndetermines a partial coherence ordering. This measure is applied in\nconstructing a criterion for expanding our beliefs in the face of new\ninformation. A number of idealizations are being made which can be relaxed by\nan appeal to Bayesian Networks.",
    "published": "2000-03-08T23:34:52Z",
    "link": "http://arxiv.org/pdf/cs/0003041v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "F.4.0; G.3"
    ],
    "authors": [
      "Luc Bovens",
      "Stephan Hartmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003042v1",
    "title": "Fages' Theorem and Answer Set Programming",
    "summary": "We generalize a theorem by Francois Fages that describes the relationship\nbetween the completion semantics and the answer set semantics for logic\nprograms with negation as failure. The study of this relationship is important\nin connection with the emergence of answer set programming. Whenever the two\nsemantics are equivalent, answer sets can be computed by a satisfiability\nsolver, and the use of answer set solvers such as smodels and dlv is\nunnecessary. A logic programming representation of the blocks world due to\nIlkka Niemelae is discussed as an example.",
    "published": "2000-03-09T00:28:21Z",
    "link": "http://arxiv.org/pdf/cs/0003042v1.pdf",
    "category": [
      "cs.AI",
      "I.2.4"
    ],
    "authors": [
      "Yuliya Babovich",
      "Esra Erdem",
      "Vladimir Lifschitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003044v1",
    "title": "On the tractable counting of theory models and its application to belief\n  revision and truth maintenance",
    "summary": "We introduced decomposable negation normal form (DNNF) recently as a\ntractable form of propositional theories, and provided a number of powerful\nlogical operations that can be performed on it in polynomial time. We also\npresented an algorithm for compiling any conjunctive normal form (CNF) into\nDNNF and provided a structure-based guarantee on its space and time complexity.\nWe present in this paper a linear-time algorithm for converting an ordered\nbinary decision diagram (OBDD) representation of a propositional theory into an\nequivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a\nsubclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the\nprevious complexity guarantees on compiling DNNF continue to hold for this\nstricter subclass, which has stronger properties. In particular, we present a\nnew operation on d-DNNF which allows us to count its models under the\nassertion, retraction and flipping of every literal by traversing the d-DNNF\ntwice. That is, after such traversal, we can test in constant-time: the\nentailment of any literal by the d-DNNF, and the consistency of the d-DNNF\nunder the retraction or flipping of any literal. We demonstrate the\nsignificance of these new operations by showing how they allow us to implement\nlinear-time, complete truth maintenance systems and linear-time, complete\nbelief revision systems for two important classes of propositional theories.",
    "published": "2000-03-09T08:58:15Z",
    "link": "http://arxiv.org/pdf/cs/0003044v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "Adnan Darwiche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003046v1",
    "title": "Linear Tabulated Resolution Based on Prolog Control Strategy",
    "summary": "Infinite loops and redundant computations are long recognized open problems\nin Prolog. Two ways have been explored to resolve these problems: loop checking\nand tabling. Loop checking can cut infinite loops, but it cannot be both sound\nand complete even for function-free logic programs. Tabling seems to be an\neffective way to resolve infinite loops and redundant computations. However,\nexisting tabulated resolutions, such as OLDT-resolution, SLG- resolution, and\nTabulated SLS-resolution, are non-linear because they rely on the\nsolution-lookup mode in formulating tabling. The principal disadvantage of\nnon-linear resolutions is that they cannot be implemented using a simple\nstack-based memory structure like that in Prolog. Moreover, some strictly\nsequential operators such as cuts may not be handled as easily as in Prolog.\n  In this paper, we propose a hybrid method to resolve infinite loops and\nredundant computations. We combine the ideas of loop checking and tabling to\nestablish a linear tabulated resolution called TP-resolution. TP-resolution has\ntwo distinctive features: (1) It makes linear tabulated derivations in the same\nway as Prolog except that infinite loops are broken and redundant computations\nare reduced. It handles cuts as effectively as Prolog. (2) It is sound and\ncomplete for positive logic programs with the bounded-term-size property. The\nunderlying algorithm can be implemented by an extension to any existing Prolog\nabstract machines such as WAM or ATOAM.",
    "published": "2000-03-09T17:11:39Z",
    "link": "http://arxiv.org/pdf/cs/0003046v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "F.4.1; I.2.3"
    ],
    "authors": [
      "Yi-Dong Shen",
      "Li-Yan Yuan",
      "Jia-Huai You",
      "Neng-Fa Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003047v1",
    "title": "BDD-based reasoning in the fluent calculus - first results",
    "summary": "The paper reports on first preliminary results and insights gained in a\nproject aiming at implementing the fluent calculus using methods and techniques\nbased on binary decision diagrams. After reporting on an initial experiment\nshowing promising results we discuss our findings concerning various techniques\nand heuristics used to speed up the reasoning process.",
    "published": "2000-03-09T17:18:12Z",
    "link": "http://arxiv.org/pdf/cs/0003047v1.pdf",
    "category": [
      "cs.AI",
      "I.2.8; I.2.3; F.4.1"
    ],
    "authors": [
      "Steffen Hoelldobler",
      "Hans-Peter Stoerr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003048v1",
    "title": "PAL: Pertinence Action Language",
    "summary": "The current document contains a brief description of a system for Reasoning\nabout Actions and Change called PAL (Pertinence Action Language) which makes\nuse of several reasoning properties extracted from a Temporal Expert Systems\ntool called Medtool.",
    "published": "2000-03-09T19:50:50Z",
    "link": "http://arxiv.org/pdf/cs/0003048v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4; F.4.1"
    ],
    "authors": [
      "Pedro Cabalar",
      "Manuel Cabarcos",
      "Ramon P. Otero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003049v1",
    "title": "Planning with Incomplete Information",
    "summary": "Planning is a natural domain of application for frameworks of reasoning about\nactions and change. In this paper we study how one such framework, the Language\nE, can form the basis for planning under (possibly) incomplete information. We\ndefine two types of plans: weak and safe plans, and propose a planner, called\nthe E-Planner, which is often able to extend an initial weak plan into a safe\nplan even though the (explicit) information available is incomplete, e.g. for\ncases where the initial state is not completely known. The E-Planner is based\nupon a reformulation of the Language E in argumentation terms and a natural\nproof theory resulting from the reformulation. It uses an extension of this\nproof theory by means of abduction for the generation of plans and adopts\nargumentation-based techniques for extending weak plans into safe plans. We\nprovide representative examples illustrating the behaviour of the E-Planner, in\nparticular for cases where the status of fluents is incompletely known.",
    "published": "2000-03-09T22:30:27Z",
    "link": "http://arxiv.org/pdf/cs/0003049v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3;I.2.4;I.2.8"
    ],
    "authors": [
      "Antonis Kakas",
      "Rob Miller",
      "Francesca Toni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003050v1",
    "title": "A tableau methodology for deontic conditional logics",
    "summary": "In this paper we present a theorem proving methodology for a restricted but\nsignificant fragment of the conditional language made up of (boolean\ncombinations of) conditional statements with unnested antecedents. The method\nis based on the possible world semantics for conditional logics. The KEM label\nformalism, designed to account for the semantics of normal modal logics, is\neasily adapted to the semantics of conditional logics by simply indexing labels\nwith formulas. The inference rules are provided by the propositional system KE+\n- a tableau-like analytic proof system devised to be used both as a refutation\nand a direct method of proof - enlarged with suitable elimination rules for the\nconditional connective. The theorem proving methodology we are going to present\ncan be viewed as a first step towards developing an appropriate algorithmic\nframework for several conditional logics for (defeasible) conditional\nobligation.",
    "published": "2000-03-10T07:30:32Z",
    "link": "http://arxiv.org/pdf/cs/0003050v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1; I.2.3; I.2.4"
    ],
    "authors": [
      "Alberto Artosi",
      "Guido Governatori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003051v1",
    "title": "Local Diagnosis",
    "summary": "In an earlier work, we have presented operations of belief change which only\naffect the relevant part of a belief base. In this paper, we propose the\napplication of the same strategy to the problem of model-based diangosis. We\nfirst isolate the subset of the system description which is relevant for a\ngiven observation and then solve the diagnosis problem for this subset.",
    "published": "2000-03-10T22:54:55Z",
    "link": "http://arxiv.org/pdf/cs/0003051v1.pdf",
    "category": [
      "cs.AI",
      "I.2.4"
    ],
    "authors": [
      "Renata Wassermann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003052v3",
    "title": "A Consistency-Based Model for Belief Change: Preliminary Report",
    "summary": "We present a general, consistency-based framework for belief change.\nInformally, in revising K by A, we begin with A and incorporate as much of K as\nconsistently possible. Formally, a knowledge base K and sentence A are\nexpressed, via renaming propositions in K, in separate languages. Using a\nmaximization process, we assume the languages are the same insofar as\nconsistently possible. Lastly, we express the resultant knowledge base in a\nsingle language. There may be more than one way in which A can be so extended\nby K: in choice revision, one such ``extension'' represents the revised state;\nalternately revision consists of the intersection of all such extensions.\n  The most general formulation of our approach is flexible enough to express\nother approaches to revision and update, the merging of knowledge bases, and\nthe incorporation of static and dynamic integrity constraints. Our framework\ndiffers from work based on ordinal conditional functions, notably with respect\nto iterated revision. We argue that the approach is well-suited for\nimplementation: the choice revision operator gives better complexity results\nthan general revision; the approach can be expressed in terms of a finite\nknowledge base; and the scope of a revision can be restricted to just those\npropositions mentioned in the sentence for revision A.",
    "published": "2000-03-11T06:29:02Z",
    "link": "http://arxiv.org/pdf/cs/0003052v3.pdf",
    "category": [
      "cs.AI",
      "I.2.4"
    ],
    "authors": [
      "James Delgrande",
      "Torsten Schaub"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003056v1",
    "title": "A note on the Declarative reading(s) of Logic Programming",
    "summary": "This paper analyses the declarative readings of logic programming. Logic\nprogramming - and negation as failure - has no unique declarative reading. One\ncommon view is that logic programming is a logic for default reasoning, a\nsub-formalism of default logic or autoepistemic logic. In this view, negation\nas failure is a modal operator. In an alternative view, a logic program is\ninterpreted as a definition. In this view, negation as failure is classical\nobjective negation. From a commonsense point of view, there is definitely a\ndifference between these views. Surprisingly though, both types of declarative\nreadings lead to grosso modo the same model semantics. This note investigates\nthe causes for this.",
    "published": "2000-03-13T16:21:41Z",
    "link": "http://arxiv.org/pdf/cs/0003056v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3;I.2.4;F.4.1"
    ],
    "authors": [
      "Marc Denecker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003057v1",
    "title": "XNMR: A tool for knowledge bases exploration",
    "summary": "XNMR is a system designed to explore the results of combining the\nwell-founded semantics system XSB with the stable-models evaluator SMODELS. Its\nmain goal is to work as a tool for fast and interactive exploration of\nknowledge bases.",
    "published": "2000-03-13T23:18:12Z",
    "link": "http://arxiv.org/pdf/cs/0003057v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "D.1.6"
    ],
    "authors": [
      "L. Castro",
      "D. Warren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003059v1",
    "title": "SATEN: An Object-Oriented Web-Based Revision and Extraction Engine",
    "summary": "SATEN is an object-oriented web-based extraction and belief revision engine.\nIt runs on any computer via a Java 1.1 enabled browser such as Netscape 4.\nSATEN performs belief revision based on the AGM approach. The extraction and\nbelief revision reasoning engines operate on a user specified ranking of\ninformation. One of the features of SATEN is that it can be used to integrate\nmutually inconsistent commensuate rankings into a consistent ranking.",
    "published": "2000-03-14T04:58:18Z",
    "link": "http://arxiv.org/pdf/cs/0003059v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3"
    ],
    "authors": [
      "Mary-Anne Williams",
      "Aidan Sims"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003061v1",
    "title": "dcs: An Implementation of DATALOG with Constraints",
    "summary": "Answer-set programming (ASP) has emerged recently as a viable programming\nparadigm. We describe here an ASP system, DATALOG with constraints or DC, based\non non-monotonic logic. Informally, DC theories consist of propositional\nclauses (constraints) and of Horn rules. The semantics is a simple and natural\nextension of the semantics of the propositional logic. However, thanks to the\npresence of Horn rules in the system, modeling of transitive closure becomes\nstraightforward. We describe the syntax, use and implementation of DC and\nprovide experimental results.",
    "published": "2000-03-14T18:06:38Z",
    "link": "http://arxiv.org/pdf/cs/0003061v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3;I.2.4;I.2.8;F.4.1;F.2.2"
    ],
    "authors": [
      "Deborah East",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003067v1",
    "title": "Detecting Unsolvable Queries for Definite Logic Programs",
    "summary": "In solving a query, the SLD proof procedure for definite programs sometimes\nsearches an infinite space for a non existing solution. For example, querying a\nplanner for an unreachable goal state. Such programs motivate the development\nof methods to prove the absence of a solution. Considering the definite program\nand the query ``<- Q'' as clauses of a first order theory, one can apply model\ngenerators which search for a finite interpretation in which the program\nclauses as well as the clause ``false <- Q'' are true. This paper develops a\nnew approach which exploits the fact that all clauses are definite. It is based\non a goal directed abductive search in the space of finite pre-interpretations\nfor a pre-interpretation such that ``Q'' is false in the least model of the\nprogram based on it. Several methods for efficiently searching the space of\npre-interpretations are presented. Experimental results confirm that our\napproach find solutions with less search than with the use of a first order\nmodel generator.",
    "published": "2000-03-17T10:59:03Z",
    "link": "http://arxiv.org/pdf/cs/0003067v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "D.1.6; F.3.1; F.4.1"
    ],
    "authors": [
      "Maurice Bruynooghe",
      "Henk Vandecasteele",
      "D. Andre de Waal",
      "Marc Denecker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003073v1",
    "title": "Proceedings of the 8th International Workshop on Non-Monotonic\n  Reasoning, NMR'2000",
    "summary": "The papers gathered in this collection were presented at the 8th\nInternational Workshop on Nonmonotonic Reasoning, NMR2000. The series was\nstarted by John McCarthy in 1978. The first international NMR workshop was held\nat Mohonk Mountain House, New Paltz, New York in June, 1984, and was organized\nby Ray Reiter and Bonnie Webber.\n  In the last 10 years the area of nonmonotonic reasoning has seen a number of\nimportant developments. Significant theoretical advances were made in the\nunderstanding of general abstract principles underlying nonmonotonicity. Key\nresults on the expressibility and computational complexity of nonmonotonic\nlogics were established. The role of nonmonotonic reasoning in belief revision,\nabduction, reasoning about action, planing and uncertainty was further\nclarified. Several successful NMR systems were built and used in applications\nsuch as planning, scheduling, logic programming and constraint satisfaction.\n  The papers in the proceedings reflect these recent advances in the field.\nThey are grouped into sections corresponding to special sessions as they were\nheld at the workshop:\n  1. General NMR track\n  2. Abductive reasonig\n  3. Belief revision: theory and practice\n  4. Representing action and planning\n  5. Systems descriptions and demonstrations\n  6. Uncertainty frameworks in NMR",
    "published": "2000-03-22T15:33:20Z",
    "link": "http://arxiv.org/pdf/cs/0003073v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I2.2;I2.3;I2.4;I2.8;F4.1"
    ],
    "authors": [
      "Chitta Baral",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003076v2",
    "title": "Constraint Programming viewed as Rule-based Programming",
    "summary": "We study here a natural situation when constraint programming can be entirely\nreduced to rule-based programming. To this end we explain first how one can\ncompute on constraint satisfaction problems using rules represented by simple\nfirst-order formulas. Then we consider constraint satisfaction problems that\nare based on predefined, explicitly given constraints. To solve them we first\nderive rules from these explicitly given constraints and limit the computation\nprocess to a repeated application of these rules, combined with labeling.We\nconsider here two types of rules. The first type, that we call equality rules,\nleads to a new notion of local consistency, called {\\em rule consistency} that\nturns out to be weaker than arc consistency for constraints of arbitrary arity\n(called hyper-arc consistency in \\cite{MS98b}). For Boolean constraints rule\nconsistency coincides with the closure under the well-known propagation rules\nfor Boolean constraints. The second type of rules, that we call membership\nrules, yields a rule-based characterization of arc consistency. To show\nfeasibility of this rule-based approach to constraint programming we show how\nboth types of rules can be automatically generated, as {\\tt CHR} rules of\n\\cite{fruhwirth-constraint-95}. This yields an implementation of this approach\nto programming by means of constraint logic programming. We illustrate the\nusefulness of this approach to constraint programming by discussing various\nexamples, including Boolean constraints, two typical examples of many valued\nlogics, constraints dealing with Waltz's language for describing polyhedral\nscenes, and Allen's qualitative approach to temporal logic.",
    "published": "2000-03-24T16:12:22Z",
    "link": "http://arxiv.org/pdf/cs/0003076v2.pdf",
    "category": [
      "cs.AI",
      "cs.PL",
      "D.3.3;I.2.2;I.2.3"
    ],
    "authors": [
      "Krzysztof R. Apt",
      "Eric Monfroy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003077v1",
    "title": "DATALOG with constraints - an answer-set programming system",
    "summary": "Answer-set programming (ASP) has emerged recently as a viable programming\nparadigm well attuned to search problems in AI, constraint satisfaction and\ncombinatorics. Propositional logic is, arguably, the simplest ASP system with\nan intuitive semantics supporting direct modeling of problem constraints.\nHowever, for some applications, especially those requiring that transitive\nclosure be computed, it requires additional variables and results in large\ntheories. Consequently, it may not be a practical computational tool for such\nproblems. On the other hand, ASP systems based on nonmonotonic logics, such as\nstable logic programming, can handle transitive closure computation efficiently\nand, in general, yield very concise theories as problem representations. Their\nsemantics is, however, more complex. Searching for the middle ground, in this\npaper we introduce a new nonmonotonic logic, DATALOG with constraints or DC.\nInformally, DC theories consist of propositional clauses (constraints) and of\nHorn rules. The semantics is a simple and natural extension of the semantics of\nthe propositional logic. However, thanks to the presence of Horn rules in the\nsystem, modeling of transitive closure becomes straightforward. We describe the\nsyntax and semantics of DC, and study its properties. We discuss an\nimplementation of DC and present results of experimental study of the\neffectiveness of DC, comparing it with CSAT, a satisfiability checker and\nSMODELS implementation of stable logic programming. Our results show that DC is\ncompetitive with the other two approaches, in case of many search problems,\noften yielding much more efficient solutions.",
    "published": "2000-03-24T19:09:59Z",
    "link": "http://arxiv.org/pdf/cs/0003077v1.pdf",
    "category": [
      "cs.AI",
      "D.1.6;F.2.2;F.4.2;I.2.8;I.6.5"
    ],
    "authors": [
      "Deborah East",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003080v1",
    "title": "Some Remarks on Boolean Constraint Propagation",
    "summary": "We study here the well-known propagation rules for Boolean constraints. First\nwe propose a simple notion of completeness for sets of such rules and establish\na completeness result. Then we show an equivalence in an appropriate sense\nbetween Boolean constraint propagation and unit propagation, a form of\nresolution for propositional logic.\n  Subsequently we characterize one set of such rules by means of the notion of\nhyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify\nthe status of a similar, though different, set of rules introduced in (Simonis\n1989a) and more fully in (Codognet and Diaz 1996).",
    "published": "2000-03-28T11:49:37Z",
    "link": "http://arxiv.org/pdf/cs/0003080v1.pdf",
    "category": [
      "cs.AI",
      "D.3.2;D.3.3"
    ],
    "authors": [
      "Krzysztof R. Apt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0003082v1",
    "title": "Representation results for defeasible logic",
    "summary": "The importance of transformations and normal forms in logic programming, and\ngenerally in computer science, is well documented. This paper investigates\ntransformations and normal forms in the context of Defeasible Logic, a simple\nbut efficient formalism for nonmonotonic reasoning based on rules and\npriorities. The transformations described in this paper have two main benefits:\non one hand they can be used as a theoretical tool that leads to a deeper\nunderstanding of the formalism, and on the other hand they have been used in\nthe development of an efficient implementation of defeasible logic.",
    "published": "2000-03-30T02:23:21Z",
    "link": "http://arxiv.org/pdf/cs/0003082v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1; I.2.3; I.2.4"
    ],
    "authors": [
      "G. Antoniou",
      "D. Billington",
      "G. Governatori",
      "M. J. Maher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0004001v1",
    "title": "A Theory of Universal Artificial Intelligence based on Algorithmic\n  Complexity",
    "summary": "Decision theory formally solves the problem of rational agents in uncertain\nworlds if the true environmental prior probability distribution is known.\nSolomonoff's theory of universal induction formally solves the problem of\nsequence prediction for unknown prior distribution. We combine both ideas and\nget a parameterless theory of universal Artificial Intelligence. We give strong\narguments that the resulting AIXI model is the most intelligent unbiased agent\npossible. We outline for a number of problem classes, including sequence\nprediction, strategic games, function minimization, reinforcement and\nsupervised learning, how the AIXI model can formally solve them. The major\ndrawback of the AIXI model is that it is uncomputable. To overcome this\nproblem, we construct a modified algorithm AIXI-tl, which is still effectively\nmore intelligent than any other time t and space l bounded agent. The\ncomputation time of AIXI-tl is of the order tx2^l. Other discussed topics are\nformal definitions of intelligence order relations, the horizon problem and\nrelations of the AIXI theory to other AI approaches.",
    "published": "2000-04-03T06:16:16Z",
    "link": "http://arxiv.org/pdf/cs/0004001v1.pdf",
    "category": [
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "I.2; F.1.3; E.4"
    ],
    "authors": [
      "Marcus Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0004002v1",
    "title": "Programming in Alma-0, or Imperative and Declarative Programming\n  Reconciled",
    "summary": "In (Apt et al, TOPLAS 1998) we introduced the imperative programming language\nAlma-0 that supports declarative programming. In this paper we illustrate the\nhybrid programming style of Alma-0 by means of various examples that complement\nthose presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs\nillustrate the versatility of the language and show that ``don't know''\nnondeterminism can be naturally combined with assignment.",
    "published": "2000-04-05T16:04:26Z",
    "link": "http://arxiv.org/pdf/cs/0004002v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.PL",
      "D.3.2;F.3.2;F.3.3;I.2.8;I.5.5"
    ],
    "authors": [
      "Krzysztof R. Apt",
      "Andrea Schaerf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0004003v2",
    "title": "Searching for Spaceships",
    "summary": "We describe software that searches for spaceships in Conway's Game of Life\nand related two-dimensional cellular automata. Our program searches through a\nstate space related to the de Bruijn graph of the automaton, using a method\nthat combines features of breadth first and iterative deepening search, and\nincludes fast bit-parallel graph reachability and path enumeration algorithms\nfor finding the successors of each state. Successful results include a new 2c/7\nspaceship in Life, found by searching a space with 2^126 states.",
    "published": "2000-04-10T20:55:55Z",
    "link": "http://arxiv.org/pdf/cs/0004003v2.pdf",
    "category": [
      "cs.AI",
      "nlin.CG",
      "I.2.8; F.1.1"
    ],
    "authors": [
      "David Eppstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0004005v1",
    "title": "Exact Phase Transitions in Random Constraint Satisfaction Problems",
    "summary": "In this paper we propose a new type of random CSP model, called Model RB,\nwhich is a revision to the standard Model B. It is proved that phase\ntransitions from a region where almost all problems are satisfiable to a region\nwhere almost all problems are unsatisfiable do exist for Model RB as the number\nof variables approaches infinity. Moreover, the critical values at which the\nphase transitions occur are also known exactly. By relating the hardness of\nModel RB to Model B, it is shown that there exist a lot of hard instances in\nModel RB.",
    "published": "2000-04-16T07:13:09Z",
    "link": "http://arxiv.org/pdf/cs/0004005v1.pdf",
    "category": [
      "cs.AI",
      "cs.CC",
      "cs.DM",
      "I.2.8; G.3"
    ],
    "authors": [
      "Ke Xu",
      "Wei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005008v1",
    "title": "A Denotational Semantics for First-Order Logic",
    "summary": "In Apt and Bezem [AB99] (see cs.LO/9811017) we provided a computational\ninterpretation of first-order formulas over arbitrary interpretations. Here we\ncomplement this work by introducing a denotational semantics for first-order\nlogic. Additionally, by allowing an assignment of a non-ground term to a\nvariable we introduce in this framework logical variables.\n  The semantics combines a number of well-known ideas from the areas of\nsemantics of imperative programming languages and logic programming. In the\nresulting computational view conjunction corresponds to sequential composition,\ndisjunction to ``don't know'' nondeterminism, existential quantification to\ndeclaration of a local variable, and negation to the ``negation as finite\nfailure'' rule. The soundness result shows correctness of the semantics with\nrespect to the notion of truth. The proof resembles in some aspects the proof\nof the soundness of the SLDNF-resolution.",
    "published": "2000-05-08T12:23:07Z",
    "link": "http://arxiv.org/pdf/cs/0005008v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "F.3.2; D.3.2"
    ],
    "authors": [
      "Krzysztof R. Apt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005009v1",
    "title": "PSPACE Reasoning for Graded Modal Logics",
    "summary": "We present a PSPACE algorithm that decides satisfiability of the graded modal\nlogic Gr(K_R)---a natural extension of propositional modal logic K_R by\ncounting expressions---which plays an important role in the area of knowledge\nrepresentation. The algorithm employs a tableaux approach and is the first\nknown algorithm which meets the lower bound for the complexity of the problem.\nThus, we exactly fix the complexity of the problem and refute an\nExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \\cap\nI)), which augments Gr(K_R) with inverse relations and intersection of\naccessibility relations. This establishes a kind of ``theoretical benchmark''\nthat all algorithmic approaches can be measured against.",
    "published": "2000-05-08T14:51:58Z",
    "link": "http://arxiv.org/pdf/cs/0005009v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.CC",
      "cs.DS",
      "F.4.1"
    ],
    "authors": [
      "Stephan Tobies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005010v1",
    "title": "Extending and Implementing the Stable Model Semantics",
    "summary": "An algorithm for computing the stable model semantics of logic programs is\ndeveloped. It is shown that one can extend the semantics and the algorithm to\nhandle new and more expressive types of rules. Emphasis is placed on the use of\nefficient implementation techniques. In particular, an implementation of\nlookahead that safely avoids testing every literal for failure and that makes\nthe use of lookahead feasible is presented. In addition, a good heuristic is\nderived from the principle that the search space should be minimized.\n  Due to the lack of competitive algorithms and implementations for the\ncomputation of stable models, the system is compared with three satisfiability\nsolvers. This shows that the heuristic can be improved by breaking ties, but\nleaves open the question of how to break them. It also demonstrates that the\nmore expressive rules of the stable model semantics make the semantics clearly\npreferable over propositional logic when a problem has a more compact logic\nprogram representation. Conjunctive normal form representations are never more\ncompact than logic program ones.",
    "published": "2000-05-08T18:26:54Z",
    "link": "http://arxiv.org/pdf/cs/0005010v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "I.2.3; I.2.8; F.4.1"
    ],
    "authors": [
      "Patrik Simons"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005011v1",
    "title": "An Average Analysis of Backtracking on Random Constraint Satisfaction\n  Problems",
    "summary": "In this paper we propose a random CSP model, called Model GB, which is a\nnatural generalization of standard Model B. It is proved that Model GB in which\neach constraint is easy to satisfy exhibits non-trivial behaviour (not\ntrivially satisfiable or unsatisfiable) as the number of variables approaches\ninfinity. A detailed analysis to obtain an asymptotic estimate (good to 1+o(1))\nof the average number of nodes in a search tree used by the backtracking\nalgorithm on Model GB is also presented. It is shown that the average number of\nnodes required for finding all solutions or proving that no solution exists\ngrows exponentially with the number of variables. So this model might be an\ninteresting distribution for studying the nature of hard instances and\nevaluating the performance of CSP algorithms. In addition, we further\ninvestigate the behaviour of the average number of nodes as r (the ratio of\nconstraints to variables) varies. The results indicate that as r increases,\nrandom CSP instances get easier and easier to solve, and the base for the\naverage number of nodes that is exponential in r tends to 1 as r approaches\ninfinity. Therefore, although the average number of nodes used by the\nbacktracking algorithm on random CSP is exponential, many CSP instances will be\nvery easy to solve when r is sufficiently large.",
    "published": "2000-05-09T02:12:58Z",
    "link": "http://arxiv.org/pdf/cs/0005011v1.pdf",
    "category": [
      "cs.CC",
      "cs.AI",
      "F.2.2; I.2.8"
    ],
    "authors": [
      "Ke Xu",
      "Wei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005012v1",
    "title": "Reasoning with Axioms: Theory and Pratice",
    "summary": "When reasoning in description, modal or temporal logics it is often useful to\nconsider axioms representing universal truths in the domain of discourse.\nReasoning with respect to an arbitrary set of axioms is hard, even for\nrelatively inexpressive logics, and it is essential to deal with such axioms in\nan efficient manner if implemented systems are to be effective in real\napplications. This is particularly relevant to Description Logics, where\nsubsumption reasoning with respect to a terminology is a fundamental problem.\nTwo optimisation techniques that have proved to be particularly effective in\ndealing with terminologies are lazy unfolding and absorption. In this paper we\nseek to improve our theoretical understanding of these important techniques. We\ndefine a formal framework that allows the techniques to be precisely described,\nestablish conditions under which they can be safely applied, and prove that,\nprovided these conditions are respected, subsumption testing algorithms will\nstill function correctly. These results are used to show that the procedures\nused in the FaCT system are correct and, moreover, to show how efficiency can\nbe significantly improved, while still retaining the guarantee of correctness,\nby relaxing the safety conditions for absorption.",
    "published": "2000-05-09T07:17:29Z",
    "link": "http://arxiv.org/pdf/cs/0005012v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1, I.2.3, I.2.4"
    ],
    "authors": [
      "Ian Horrocks",
      "Stephan Tobies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005013v1",
    "title": "Practical Reasoning for Very Expressive Description Logics",
    "summary": "Description Logics (DLs) are a family of knowledge representation formalisms\nmainly characterised by constructors to build complex concepts and roles from\natomic ones. Expressive role constructors are important in many applications,\nbut can be computationally problematical. We present an algorithm that decides\nsatisfiability of the DL ALC extended with transitive and inverse roles and\nfunctional restrictions with respect to general concept inclusion axioms and\nrole hierarchies; early experiments indicate that this algorithm is well-suited\nfor implementation. Additionally, we show that ALC extended with just\ntransitive and inverse roles is still in PSPACE. We investigate the limits of\ndecidability for this family of DLs, showing that relaxing the constraints\nplaced on the kinds of roles used in number restrictions leads to the\nundecidability of all inference problems. Finally, we describe a number of\noptimisation techniques that are crucial in obtaining implementations of the\ndecision procedures, which, despite the worst-case complexity of the problem,\nexhibit good performance with real-life problems.",
    "published": "2000-05-09T13:02:40Z",
    "link": "http://arxiv.org/pdf/cs/0005013v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1, I.2.3, I.2.4"
    ],
    "authors": [
      "Ian Horrocks",
      "Ulrike Sattler",
      "Stephan Tobies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005014v1",
    "title": "Practical Reasoning for Expressive Description Logics",
    "summary": "Description Logics (DLs) are a family of knowledge representation formalisms\nmainly characterised by constructors to build complex concepts and roles from\natomic ones. Expressive role constructors are important in many applications,\nbut can be computationally problematical. We present an algorithm that decides\nsatisfiability of the DL ALC extended with transitive and inverse roles, role\nhierarchies, and qualifying number restrictions. Early experiments indicate\nthat this algorithm is well-suited for implementation. Additionally, we show\nthat ALC extended with just transitive and inverse roles is still in PSPACE.\nFinally, we investigate the limits of decidability for this family of DLs.",
    "published": "2000-05-10T08:19:41Z",
    "link": "http://arxiv.org/pdf/cs/0005014v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1, I.2.3, I.2.4"
    ],
    "authors": [
      "Ian Horrocks",
      "Ulrike Sattler",
      "Stephan Tobies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005017v1",
    "title": "Reasoning with Individuals for the Description Logic SHIQ",
    "summary": "While there has been a great deal of work on the development of reasoning\nalgorithms for expressive description logics, in most cases only Tbox reasoning\nis considered. In this paper we present an algorithm for combined Tbox and Abox\nreasoning in the SHIQ description logic. This algorithm is of particular\ninterest as it can be used to decide the problem of (database) conjunctive\nquery containment w.r.t. a schema. Moreover, the realisation of an efficient\nimplementation should be relatively straightforward as it can be based on an\nexisting highly optimised implementation of the Tbox algorithm in the FaCT\nsystem.",
    "published": "2000-05-11T08:16:21Z",
    "link": "http://arxiv.org/pdf/cs/0005017v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1, I.2.3, I.2.4"
    ],
    "authors": [
      "Ian Horrock",
      "Ulrike Sattler",
      "Stephan Tobies"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005020v1",
    "title": "Centroid-based summarization of multiple documents: sentence extraction,\n  utility-based evaluation, and user studies",
    "summary": "We present a multi-document summarizer, called MEAD, which generates\nsummaries using cluster centroids produced by a topic detection and tracking\nsystem. We also describe two new techniques, based on sentence utility and\nsubsumption, which we have applied to the evaluation of both single and\nmultiple document summaries. Finally, we describe two user studies that test\nour models of multi-document summarization.",
    "published": "2000-05-12T17:24:06Z",
    "link": "http://arxiv.org/pdf/cs/0005020v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.HC",
      "cs.IR",
      "H.3.1; H.3.4; H.3.7; H.5.2; I.2.7"
    ],
    "authors": [
      "Dragomir R. Radev",
      "Hongyan Jing",
      "Malgorzata Budzikowska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005021v1",
    "title": "Modeling the Uncertainty in Complex Engineering Systems",
    "summary": "Existing procedures for model validation have been deemed inadequate for many\nengineering systems. The reason of this inadequacy is due to the high degree of\ncomplexity of the mechanisms that govern these systems. It is proposed in this\npaper to shift the attention from modeling the engineering system itself to\nmodeling the uncertainty that underlies its behavior. A mathematical framework\nfor modeling the uncertainty in complex engineering systems is developed. This\nframework uses the results of computational learning theory. It is based on the\npremise that a system model is a learning machine.",
    "published": "2000-05-14T14:35:20Z",
    "link": "http://arxiv.org/pdf/cs/0005021v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "I.2.6;I.6.4;J.2;G.3"
    ],
    "authors": [
      "A. Guergachi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005024v2",
    "title": "The SAT Phase Transition",
    "summary": "Phase transition is an important feature of SAT problem. For random k-SAT\nmodel, it is proved that as r (ratio of clauses to variables) increases, the\nstructure of solutions will undergo a sudden change like satisfiability phase\ntransition when r reaches a threshold point. This phenomenon shows that the\nsatisfying truth assignments suddenly shift from being relatively different\nfrom each other to being very similar to each other.",
    "published": "2000-05-22T04:45:53Z",
    "link": "http://arxiv.org/pdf/cs/0005024v2.pdf",
    "category": [
      "cs.AI",
      "cs.CC",
      "F.2.m; I.2.8"
    ],
    "authors": [
      "Ke Xu",
      "Wei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/physics/0005062v1",
    "title": "Applying MDL to Learning Best Model Granularity",
    "summary": "The Minimum Description Length (MDL) principle is solidly based on a provably\nideal method of inference using Kolmogorov complexity. We test how the theory\nbehaves in practice on a general problem in model selection: that of learning\nthe best model granularity. The performance of a model depends critically on\nthe granularity, for example the choice of precision of the parameters. Too\nhigh precision generally involves modeling of accidental noise and too low\nprecision may lead to confusion of models that should be distinguished. This\nprecision is often determined ad hoc. In MDL the best model is the one that\nmost compresses a two-part code of the data set: this embodies ``Occam's\nRazor.'' In two quite different experimental settings the theoretical value\ndetermined using MDL coincides with the best value found experimentally. In the\nfirst experiment the task is to recognize isolated handwritten characters in\none subject's handwriting, irrespective of size and orientation. Based on a new\nmodification of elastic matching, using multiple prototypes per character, the\noptimal prediction rate is predicted for the learned parameter (length of\nsampling interval) considered most likely by MDL, which is shown to coincide\nwith the best value found experimentally. In the second experiment the task is\nto model a robot arm with two degrees of freedom using a three layer\nfeed-forward neural network where we need to determine the number of nodes in\nthe hidden layer giving best modeling performance. The optimal model (the one\nthat extrapolizes best on unseen examples) is predicted for the number of nodes\nin the hidden layer considered most likely by MDL, which again is found to\ncoincide with the best value found experimentally.",
    "published": "2000-05-23T14:50:07Z",
    "link": "http://arxiv.org/pdf/physics/0005062v1.pdf",
    "category": [
      "physics.data-an",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Qiong Gao",
      "Ming Li",
      "Paul Vitanyi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005030v1",
    "title": "Axiomatizing Causal Reasoning",
    "summary": "Causal models defined in terms of a collection of equations, as defined by\nPearl, are axiomatized here. Axiomatizations are provided for three\nsuccessively more general classes of causal models: (1) the class of recursive\ntheories (those without feedback), (2) the class of theories where the\nsolutions to the equations are unique, (3) arbitrary theories (where the\nequations may not have solutions and, if they do, they are not necessarily\nunique). It is shown that to reason about causality in the most general third\nclass, we must extend the language used by Galles and Pearl. In addition, the\ncomplexity of the decision procedures is characterized for all the languages\nand classes of models considered.",
    "published": "2000-05-30T18:56:46Z",
    "link": "http://arxiv.org/pdf/cs/0005030v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4; F.4.1"
    ],
    "authors": [
      "Joseph Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0005031v3",
    "title": "Conditional Plausibility Measures and Bayesian Networks",
    "summary": "A general notion of algebraic conditional plausibility measures is defined.\nProbability measures, ranking functions, possibility measures, and (under the\nappropriate definitions) sets of probability measures can all be viewed as\ndefining algebraic conditional plausibility measures. It is shown that\nalgebraic conditional plausibility measures can be represented using Bayesian\nnetworks.",
    "published": "2000-05-30T19:05:21Z",
    "link": "http://arxiv.org/pdf/cs/0005031v3.pdf",
    "category": [
      "cs.AI",
      "I.2.4"
    ],
    "authors": [
      "Joseph Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0006009v1",
    "title": "Knowledge and common knowledge in a distributed environment",
    "summary": "Reasoning about knowledge seems to play a fundamental role in distributed\nsystems. Indeed, such reasoning is a central part of the informal intuitive\narguments used in the design of distributed protocols. Communication in a\ndistributed system can be viewed as the act of transforming the system's state\nof knowledge. This paper presents a general framework for formalizing and\nreasoning about knowledge in distributed systems. We argue that states of\nknowledge of groups of processors are useful concepts for the design and\nanalysis of distributed protocols. In particular, distributed knowledge\ncorresponds to knowledge that is ``distributed'' among the members of the\ngroup, while common knowledge corresponds to a fact being ``publicly known''.\nThe relationship between common knowledge and a variety of desirable actions in\na distributed system is illustrated. Furthermore, it is shown that, formally\nspeaking, in practical systems common knowledge cannot be attained. A number of\nweaker variants of common knowledge that are attainable in many cases of\ninterest are introduced and investigated.",
    "published": "2000-06-02T18:43:33Z",
    "link": "http://arxiv.org/pdf/cs/0006009v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI",
      "C.2.2, C.2.4, D.2.4, I.2.4, F.3.1, F.3.1"
    ],
    "authors": [
      "Joseph Y. Halpern",
      "Yoram Moses"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0006013v1",
    "title": "An evaluation of Naive Bayesian anti-spam filtering",
    "summary": "It has recently been argued that a Naive Bayesian classifier can be used to\nfilter unsolicited bulk e-mail (\"spam\"). We conduct a thorough evaluation of\nthis proposal on a corpus that we make publicly available, contributing towards\nstandard benchmarks. At the same time we investigate the effect of\nattribute-set size, training-corpus size, lemmatization, and stop-lists on the\nfilter's performance, issues that had not been previously explored. After\nintroducing appropriate cost-sensitive evaluation measures, we reach the\nconclusion that additional safety nets are needed for the Naive Bayesian\nanti-spam filter to be viable in practice.",
    "published": "2000-06-07T11:10:50Z",
    "link": "http://arxiv.org/pdf/cs/0006013v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "H.4.3; I.2.6; I.2.7; I.5.4; K.4.1"
    ],
    "authors": [
      "Ion Androutsopoulos",
      "John Koutsias",
      "Konstantinos V. Chandrinos",
      "George Paliouras",
      "Constantine D. Spyropoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0006031v1",
    "title": "Verifying Termination of General Logic Programs with Concrete Queries",
    "summary": "We introduce a method of verifying termination of logic programs with respect\nto concrete queries (instead of abstract query patterns). A necessary and\nsufficient condition is established and an algorithm for automatic verification\nis developed. In contrast to existing query pattern-based approaches, our\nmethod has the following features: (1) It applies to all general logic programs\nwith non-floundering queries. (2) It is very easy to automate because it does\nnot need to search for a level mapping or a model, nor does it need to compute\nan interargument relation based on additional mode or type information. (3) It\nbridges termination analysis with loop checking, the two problems that have\nbeen studied separately in the past despite their close technical relation with\neach other.",
    "published": "2000-06-21T17:30:30Z",
    "link": "http://arxiv.org/pdf/cs/0006031v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "D.3.1; F.4.1; I.2.3"
    ],
    "authors": [
      "Yi-Dong Shen",
      "Li-Yan Yuan",
      "Jia-Huai You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0006041v1",
    "title": "Using a Diathesis Model for Semantic Parsing",
    "summary": "This paper presents a semantic parsing approach for unrestricted texts.\nSemantic parsing is one of the major bottlenecks of Natural Language\nUnderstanding (NLU) systems and usually requires building expensive resources\nnot easily portable to other domains. Our approach obtains a case-role\nanalysis, in which the semantic roles of the verb are identified. In order to\ncover all the possible syntactic realisations of a verb, our system combines\ntheir argument structure with a set of general semantic labelled diatheses\nmodels. Combining them, the system builds a set of syntactic-semantic patterns\nwith their own role-case representation. Once the patterns are build, we use an\napproximate tree pattern-matching algorithm to identify the most reliable\npattern for a sentence. The pattern matching is performed between the\nsyntactic-semantic patterns and the feature-structure tree representing the\nmorphological, syntactical and semantic information of the analysed sentence.\nFor sentences assigned to the correct model, the semantic parsing system we are\npresenting identifies correctly more than 73% of possible semantic case-roles.",
    "published": "2000-06-29T07:44:16Z",
    "link": "http://arxiv.org/pdf/cs/0006041v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7;I.5"
    ],
    "authors": [
      "Jordi Atserias",
      "Irene Castellon",
      "Montse Civit",
      "German Rigau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0006042v1",
    "title": "Semantic Parsing based on Verbal Subcategorization",
    "summary": "The aim of this work is to explore new methodologies on Semantic Parsing for\nunrestricted texts. Our approach follows the current trends in Information\nExtraction (IE) and is based on the application of a verbal subcategorization\nlexicon (LEXPIR) by means of complex pattern recognition techniques. LEXPIR is\nframed on the theoretical model of the verbal subcategorization developed in\nthe Pirapides project.",
    "published": "2000-06-29T09:17:45Z",
    "link": "http://arxiv.org/pdf/cs/0006042v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7;I.5"
    ],
    "authors": [
      "Jordi Atserias",
      "Irene Castellon",
      "Montse Civit",
      "German Rigau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0006043v1",
    "title": "Constraint compiling into rules formalism constraint compiling into\n  rules formalism for dynamic CSPs computing",
    "summary": "In this paper we present a rule based formalism for filtering variables\ndomains of constraints. This formalism is well adapted for solving dynamic CSP.\nWe take diagnosis as an instance problem to illustrate the use of these rules.\nA diagnosis problem is seen like finding all the minimal sets of constraints to\nbe relaxed in the constraint network that models the device to be diagnosed",
    "published": "2000-06-30T10:25:06Z",
    "link": "http://arxiv.org/pdf/cs/0006043v1.pdf",
    "category": [
      "cs.AI",
      "F.4.1"
    ],
    "authors": [
      "S. Piechowiak",
      "J. Rodriguez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007001v1",
    "title": "Constraint Exploration and Envelope of Simulation Trajectories",
    "summary": "The implicit theory that a simulation represents is precisely not in the\nindividual choices but rather in the 'envelope' of possible trajectories - what\nis important is the shape of the whole envelope. Typically a huge amount of\ncomputation is required when experimenting with factors bearing on the dynamics\nof a simulation to tease out what affects the shape of this envelope. In this\npaper we present a methodology aimed at systematically exploring this envelope.\nWe propose a method for searching for tendencies and proving their necessity\nrelative to a range of parameterisations of the model and agents' choices, and\nto the logic of the simulation language. The exploration consists of a forward\nchaining generation of the trajectories associated to and constrained by such a\nrange of parameterisations and choices. Additionally, we propose a\ncomputational procedure that helps implement this exploration by translating a\nMulti Agent System simulation into a constraint-based search over possible\ntrajectories by 'compiling' the simulation rules into a more specific form,\nnamely by partitioning the simulation rules using appropriate modularity in the\nsimulation. An example of this procedure is exhibited.\n  Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence,\nTendencies",
    "published": "2000-07-03T10:10:09Z",
    "link": "http://arxiv.org/pdf/cs/0007001v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "cs.LO",
      "D.3.3; F.3.1; F.4.1"
    ],
    "authors": [
      "Oswaldo Teran",
      "Bruce Edmonds",
      "Steve Wallis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007002v2",
    "title": "Interval Constraint Solving for Camera Control and Motion Planning",
    "summary": "Many problems in robust control and motion planning can be reduced to either\nfind a sound approximation of the solution space determined by a set of\nnonlinear inequalities, or to the ``guaranteed tuning problem'' as defined by\nJaulin and Walter, which amounts to finding a value for some tuning parameter\nsuch that a set of inequalities be verified for all the possible values of some\nperturbation vector. A classical approach to solve these problems, which\nsatisfies the strong soundness requirement, involves some quantifier\nelimination procedure such as Collins' Cylindrical Algebraic Decomposition\nsymbolic method. Sound numerical methods using interval arithmetic and local\nconsistency enforcement to prune the search space are presented in this paper\nas much faster alternatives for both soundly solving systems of nonlinear\ninequalities, and addressing the guaranteed tuning problem whenever the\nperturbation vector has dimension one. The use of these methods in camera\ncontrol is investigated, and experiments with the prototype of a declarative\nmodeller to express camera motion using a cinematic language are reported and\ncommented.",
    "published": "2000-07-03T17:03:39Z",
    "link": "http://arxiv.org/pdf/cs/0007002v2.pdf",
    "category": [
      "cs.AI",
      "cs.NA",
      "math.NA",
      "D.3.3;D.2.2;G.1.0;H.5.1"
    ],
    "authors": [
      "Frederic Benhamou",
      "Frederic Goualard",
      "Eric Languenou",
      "Marc Christie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007004v1",
    "title": "Brainstorm/J: a Java Framework for Intelligent Agents",
    "summary": "Despite the effort of many researchers in the area of multi-agent systems\n(MAS) for designing and programming agents, a few years ago the research\ncommunity began to take into account that common features among different MAS\nexists. Based on these common features, several tools have tackled the problem\nof agent development on specific application domains or specific types of\nagents. As a consequence, their scope is restricted to a subset of the huge\napplication domain of MAS. In this paper we propose a generic infrastructure\nfor programming agents whose name is Brainstorm/J. The infrastructure has been\nimplemented as an object oriented framework. As a consequence, our approach\nsupports a broader scope of MAS applications than previous efforts, being\nflexible and reusable.",
    "published": "2000-07-04T16:31:40Z",
    "link": "http://arxiv.org/pdf/cs/0007004v1.pdf",
    "category": [
      "cs.AI",
      "I.2.11"
    ],
    "authors": [
      "Alejandro Zunino",
      "Analia Amandi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007010v1",
    "title": "Boosting Applied to Word Sense Disambiguation",
    "summary": "In this paper Schapire and Singer's AdaBoost.MH boosting algorithm is applied\nto the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of\n15 selected polysemous words show that the boosting approach surpasses Naive\nBayes and Exemplar-based approaches, which represent state-of-the-art accuracy\non supervised WSD. In order to make boosting practical for a real learning\ndomain of thousands of words, several ways of accelerating the algorithm by\nreducing the feature space are studied. The best variant, which we call\nLazyBoosting, is tested on the largest sense-tagged corpus available containing\n192,800 examples of the 191 most frequent and ambiguous English words. Again,\nboosting compares favourably to the other benchmark algorithms.",
    "published": "2000-07-07T14:10:05Z",
    "link": "http://arxiv.org/pdf/cs/0007010v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7;I.2.6"
    ],
    "authors": [
      "Gerard Escudero",
      "Lluis Marquez",
      "German Rigau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007011v1",
    "title": "Naive Bayes and Exemplar-Based approaches to Word Sense Disambiguation\n  Revisited",
    "summary": "This paper describes an experimental comparison between two standard\nsupervised learning methods, namely Naive Bayes and Exemplar-based\nclassification, on the Word Sense Disambiguation (WSD) problem. The aim of the\nwork is twofold. Firstly, it attempts to contribute to clarify some confusing\ninformation about the comparison between both methods appearing in the related\nliterature. In doing so, several directions have been explored, including:\ntesting several modifications of the basic learning algorithms and varying the\nfeature space. Secondly, an improvement of both algorithms is proposed, in\norder to deal with large attribute sets. This modification, which basically\nconsists in using only the positive information appearing in the examples,\nallows to improve greatly the efficiency of the methods, with no loss in\naccuracy. The experiments have been performed on the largest sense-tagged\ncorpus available containing the most frequent and ambiguous English words.\nResults show that the Exemplar-based approach to WSD is generally superior to\nthe Bayesian approach, especially when a specific metric for dealing with\nsymbolic attributes is used.",
    "published": "2000-07-07T15:00:44Z",
    "link": "http://arxiv.org/pdf/cs/0007011v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7;I.2.6"
    ],
    "authors": [
      "Gerard Escudero",
      "Lluis Marquez",
      "German Rigau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007012v1",
    "title": "Using Learning-based Filters to Detect Rule-based Filtering Obsolescence",
    "summary": "For years, Caisse des Depots et Consignations has produced information\nfiltering applications. To be operational, these applications require high\nfiltering performances which are achieved by using rule-based filters. With\nthis technique, an administrator has to tune a set of rules for each topic.\nHowever, filters become obsolescent over time. The decrease of their\nperformances is due to diachronic polysemy of terms that involves a loss of\nprecision and to diachronic polymorphism of concepts that involves a loss of\nrecall.\n  To help the administrator to maintain his filters, we have developed a method\nwhich automatically detects filtering obsolescence. It consists in making a\nlearning-based control filter using a set of documents which have already been\ncategorised as relevant or not relevant by the rule-based filter. The idea is\nto supervise this filter by processing a differential comparison of its\noutcomes with those of the control one.\n  This method has many advantages. It is simple to implement since the training\nset used by the learning is supplied by the rule-based filter. Thus, both the\nmaking and the use of the control filter are fully automatic. With automatic\ndetection of obsolescence, learning-based filtering finds a rich application\nwhich offers interesting prospects.",
    "published": "2000-07-07T15:13:09Z",
    "link": "http://arxiv.org/pdf/cs/0007012v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "H.3.3; I.2.6"
    ],
    "authors": [
      "Francis Wolinski",
      "Frantz Vichot",
      "Mathieu Stricker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007016v1",
    "title": "Two Steps Feature Selection and Neural Network Classification for the\n  TREC-8 Routing",
    "summary": "For the TREC-8 routing, one specific filter is built for each topic. Each\nfilter is a classifier trained to recognize the documents that are relevant to\nthe topic. When presented with a document, each classifier estimates the\nprobability for the document to be relevant to the topic for which it has been\ntrained. Since the procedure for building a filter is topic-independent, the\nsystem is fully automatic.\n  By making use of a sample of documents that have previously been evaluated as\nrelevant or not relevant to a particular topic, a term selection is performed,\nand a neural network is trained. Each document is represented by a vector of\nfrequencies of a list of selected terms. This list depends on the topic to be\nfiltered; it is constructed in two steps. The first step defines the\ncharacteristic words used in the relevant documents of the corpus; the second\none chooses, among the previous list, the most discriminant ones. The length of\nthe vector is optimized automatically for each topic. At the end of the term\nselection, a vector of typically 25 words is defined for the topic, so that\neach document which has to be processed is represented by a vector of term\nfrequencies.\n  This vector is subsequently input to a classifier that is trained from the\nsame sample. After training, the classifier estimates for each document of a\ntest set its probability of being relevant; for submission to TREC, the top\n1000 documents are ranked in order of decreasing relevance.",
    "published": "2000-07-11T13:21:03Z",
    "link": "http://arxiv.org/pdf/cs/0007016v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "H.3.3; K.3.2"
    ],
    "authors": [
      "Mathieu Stricker",
      "Frantz Vichot",
      "Gerard Dreyfus",
      "Francis Wolinski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007020v2",
    "title": "Polynomial-time Computation via Local Inference Relations",
    "summary": "We consider the concept of a local set of inference rules. A local rule set\ncan be automatically transformed into a rule set for which bottom-up evaluation\nterminates in polynomial time. The local-rule-set transformation gives\npolynomial-time evaluation strategies for a large variety of rule sets that\ncannot be given terminating evaluation strategies by any other known automatic\ntechnique. This paper discusses three new results. First, it is shown that\nevery polynomial-time predicate can be defined by an (unstratified) local rule\nset. Second, a new machine-recognizable subclass of the local rule sets is\nidentified. Finally we show that locality, as a property of rule sets, is\nundecidable in general.",
    "published": "2000-07-13T17:19:43Z",
    "link": "http://arxiv.org/pdf/cs/0007020v2.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.PL",
      "I.2.2; I.2.3; I.2.4; F.4.m"
    ],
    "authors": [
      "Robert Givan",
      "David McAllester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007026v1",
    "title": "Integrating E-Commerce and Data Mining: Architecture and Challenges",
    "summary": "We show that the e-commerce domain can provide all the right ingredients for\nsuccessful data mining and claim that it is a killer domain for data mining. We\ndescribe an integrated architecture, based on our expe-rience at Blue Martini\nSoftware, for supporting this integration. The architecture can dramatically\nreduce the pre-processing, cleaning, and data understanding effort often\ndocumented to take 80% of the time in knowledge discovery projects. We\nemphasize the need for data collection at the application server layer (not the\nweb server) in order to support logging of data and metadata that is essential\nto the discovery process. We describe the data transformation bridges required\nfrom the transaction processing systems and customer event streams (e.g.,\nclickstreams) to the data warehouse. We detail the mining workbench, which\nneeds to provide multiple views of the data through reporting, data mining\nalgorithms, visualization, and OLAP. We con-clude with a set of challenges.",
    "published": "2000-07-14T00:33:12Z",
    "link": "http://arxiv.org/pdf/cs/0007026v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DB",
      "I.2.6;H.2.8"
    ],
    "authors": [
      "Suhail Ansari",
      "Ron Kohavi",
      "Llew Mason",
      "Zijian Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007032v1",
    "title": "Knowledge on Treelike Spaces",
    "summary": "This paper presents a bimodal logic for reasoning about knowledge during\nknowledge acquisition. One of the modalities represents (effort during)\nnon-deterministic time and the other represents knowledge. The semantics of\nthis logic are tree-like spaces which are a generalization of semantics used\nfor modeling branching time and historical necessity. A finite system of axiom\nschemes is shown to be canonically complete for the formentioned spaces. A\ncharacterization of the satisfaction relation implies the small model property\nand decidability for this system.",
    "published": "2000-07-21T09:52:25Z",
    "link": "http://arxiv.org/pdf/cs/0007032v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1;I.2.0"
    ],
    "authors": [
      "Konstantinos Georgatos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007033v1",
    "title": "To Preference via Entrenchment",
    "summary": "We introduce a simple generalization of Gardenfors and Makinson's epistemic\nentrenchment called partial entrenchment. We show that preferential inference\ncan be generated as the sceptical counterpart of an inference mechanism defined\ndirectly on partial entrenchment.",
    "published": "2000-07-21T10:16:47Z",
    "link": "http://arxiv.org/pdf/cs/0007033v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1;I.2.3"
    ],
    "authors": [
      "Konstantinos Georgatos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007038v1",
    "title": "Modal Logics for Topological Spaces",
    "summary": "In this thesis we shall present two logical systems, MP and MP, for the\npurpose of reasoning about knowledge and effort. These logical systems will be\ninterpreted in a spatial context and therefore, the abstract concepts of\nknowledge and effort will be defined by concrete mathematical concepts.",
    "published": "2000-07-26T18:41:17Z",
    "link": "http://arxiv.org/pdf/cs/0007038v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1"
    ],
    "authors": [
      "Konstantinos Georgatos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007039v1",
    "title": "Ordering-based Representations of Rational Inference",
    "summary": "Rational inference relations were introduced by Lehmann and Magidor as the\nideal systems for drawing conclusions from a conditional base. However, there\nhas been no simple characterization of these relations, other than its original\nrepresentation by preferential models. In this paper, we shall characterize\nthem with a class of total preorders of formulas by improving and extending\nGardenfors and Makinson's results for expectation inference relations. A second\nrepresentation is application-oriented and is obtained by considering a class\nof consequence operators that grade sets of defaults according to our reliance\non them. The finitary fragment of this class of consequence operators has been\nemployed by recent default logic formalisms based on maxiconsistency.",
    "published": "2000-07-26T18:58:09Z",
    "link": "http://arxiv.org/pdf/cs/0007039v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1;I.2.3"
    ],
    "authors": [
      "Konstantinos Georgatos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0007040v1",
    "title": "Entrenchment Relations: A Uniform Approach to Nonmonotonicity",
    "summary": "We show that Gabbay's nonmonotonic consequence relations can be reduced to a\nnew family of relations, called entrenchment relations. Entrenchment relations\nprovide a direct generalization of epistemic entrenchment and expectation\nordering introduced by Gardenfors and Makinson for the study of belief revision\nand expectation inference, respectively.",
    "published": "2000-07-26T19:20:35Z",
    "link": "http://arxiv.org/pdf/cs/0007040v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1;I.2.3"
    ],
    "authors": [
      "Konstantinos Georgatos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0008008v2",
    "title": "On the Average Similarity Degree between Solutions of Random k-SAT and\n  Random CSPs",
    "summary": "To study the structure of solutions for random k-SAT and random CSPs, this\npaper introduces the concept of average similarity degree to characterize how\nsolutions are similar to each other. It is proved that under certain\nconditions, as r (i.e. the ratio of constraints to variables) increases, the\nlimit of average similarity degree when the number of variables approaches\ninfinity exhibits phase transitions at a threshold point, shifting from a\nsmaller value to a larger value abruptly. For random k-SAT this phenomenon will\noccur when k>4 . It is further shown that this threshold point is also a\nsingular point with respect to r in the asymptotic estimate of the second\nmoment of the number of solutions. Finally, we discuss how this work is helpful\nto understand the hardness of solving random instances and a possible\napplication of it to the design of search algorithms.",
    "published": "2000-08-11T08:10:25Z",
    "link": "http://arxiv.org/pdf/cs/0008008v2.pdf",
    "category": [
      "cs.AI",
      "cs.CC",
      "cs.DM",
      "F.2.2; I.2.8"
    ],
    "authors": [
      "Ke Xu",
      "Wei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0008016v1",
    "title": "Processing Self Corrections in a speech to speech system",
    "summary": "Speech repairs occur often in spontaneous spoken dialogues. The ability to\ndetect and correct those repairs is necessary for any spoken language system.\nWe present a framework to detect and correct speech repairs where all relevant\nlevels of information, i.e., acoustics, lexis, syntax and semantics can be\nintegrated. The basic idea is to reduce the search space for repairs as soon as\npossible by cascading filters that involve more and more features. At first an\nacoustic module generates hypotheses about the existence of a repair. Second a\nstochastic model suggests a correction for every hypothesis. Well scored\ncorrections are inserted as new paths in the word lattice. Finally a lattice\nparser decides on accepting the rep air.",
    "published": "2000-08-21T10:54:11Z",
    "link": "http://arxiv.org/pdf/cs/0008016v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I 2.7"
    ],
    "authors": [
      "Joerg Spilker",
      "Martin Klarner",
      "Guenther Goerz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0008020v1",
    "title": "Explaining away ambiguity: Learning verb selectional preference with\n  Bayesian networks",
    "summary": "This paper presents a Bayesian model for unsupervised learning of verb\nselectional preferences. For each verb the model creates a Bayesian network\nwhose architecture is determined by the lexical hierarchy of Wordnet and whose\nparameters are estimated from a list of verb-object pairs found from a corpus.\n``Explaining away'', a well-known property of Bayesian networks, helps the\nmodel deal in a natural fashion with word sense ambiguity in the training data.\nOn a word sense disambiguation test our model performed better than other state\nof the art systems for unsupervised learning of selectional preferences.\nComputational complexity problems, ways of improving this approach and methods\nfor implementing ``explaining away'' in other graphical frameworks are\ndiscussed.",
    "published": "2000-08-22T15:01:21Z",
    "link": "http://arxiv.org/pdf/cs/0008020v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Massimiliano Ciaramita",
      "Mark Johnson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009012v1",
    "title": "Modeling Ambiguity in a Multi-Agent System",
    "summary": "This paper investigates the formal pragmatics of ambiguous expressions by\nmodeling ambiguity in a multi-agent system. Such a framework allows us to give\na more refined notion of the kind of information that is conveyed by ambiguous\nexpressions. We analyze how ambiguity affects the knowledge of the dialog\nparticipants and, especially, what they know about each other after an\nambiguous sentence has been uttered. The agents communicate with each other by\nmeans of a TELL-function, whose application is constrained by an implementation\nof some of Grice's maxims. The information states of the multi-agent system\nitself are represented as a Kripke structures and TELL is an update function on\nthose structures. This framework enables us to distinguish between the\ninformation conveyed by ambiguous sentences vs. the information conveyed by\ndisjunctions, and between semantic ambiguity vs. perceived ambiguity.",
    "published": "2000-09-19T15:43:18Z",
    "link": "http://arxiv.org/pdf/cs/0009012v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.MA",
      "F.4.1; I.2.7"
    ],
    "authors": [
      "Christof Monz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009016v1",
    "title": "Contextual Inference in Computational Semantics",
    "summary": "In this paper, an application of automated theorem proving techniques to\ncomputational semantics is considered. In order to compute the presuppositions\nof a natural language discourse, several inference tasks arise. Instead of\ntreating these inferences independently of each other, we show how integrating\ntechniques from formal approaches to context into deduction can help to compute\npresuppositions more efficiently. Contexts are represented as Discourse\nRepresentation Structures and the way they are nested is made explicit. In\naddition, a tableau calculus is present which keeps track of contextual\ninformation, and thereby allows to avoid carrying out redundant inference steps\nas it happens in approaches that neglect explicit nesting of contexts.",
    "published": "2000-09-20T13:41:06Z",
    "link": "http://arxiv.org/pdf/cs/0009016v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "F.4.1; I.2.7"
    ],
    "authors": [
      "Christof Monz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009017v1",
    "title": "A Tableau Calculus for Pronoun Resolution",
    "summary": "We present a tableau calculus for reasoning in fragments of natural language.\nWe focus on the problem of pronoun resolution and the way in which it\ncomplicates automated theorem proving for natural language processing. A method\nfor explicitly manipulating contextual information during deduction is\nproposed, where pronouns are resolved against this context during deduction. As\na result, pronoun resolution and deduction can be interleaved in such a way\nthat pronouns are only resolved if this is licensed by a deduction rule; this\nhelps us to avoid the combinatorial complexity of total pronoun disambiguation.",
    "published": "2000-09-21T14:49:19Z",
    "link": "http://arxiv.org/pdf/cs/0009017v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "F.4.1; I.2.7"
    ],
    "authors": [
      "Christof Monz",
      "Maarten de Rijke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009018v1",
    "title": "A Resolution Calculus for Dynamic Semantics",
    "summary": "This paper applies resolution theorem proving to natural language semantics.\nThe aim is to circumvent the computational complexity triggered by natural\nlanguage ambiguities like pronoun binding, by interleaving pronoun binding with\nresolution deduction. Therefore disambiguation is only applied to expression\nthat actually occur during derivations.",
    "published": "2000-09-21T15:21:01Z",
    "link": "http://arxiv.org/pdf/cs/0009018v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "F.4.1; I.2.7"
    ],
    "authors": [
      "Christof Monz",
      "Maarten de Rijke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009019v1",
    "title": "Computing Presuppositions by Contextual Reasoning",
    "summary": "This paper describes how automated deduction methods for natural language\nprocessing can be applied more efficiently by encoding context in a more\nelaborate way. Our work is based on formal approaches to context, and we\nprovide a tableau calculus for contextual reasoning. This is explained by\nconsidering an example from the problem area of presupposition projection.",
    "published": "2000-09-21T15:32:17Z",
    "link": "http://arxiv.org/pdf/cs/0009019v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "F.4.1; I.2.7"
    ],
    "authors": [
      "Christof Monz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009022v1",
    "title": "A Comparison between Supervised Learning Algorithms for Word Sense\n  Disambiguation",
    "summary": "This paper describes a set of comparative experiments, including cross-corpus\nevaluation, between five alternative algorithms for supervised Word Sense\nDisambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW,\nDecision Lists, and Boosting. Two main conclusions can be drawn: 1) The\nLazyBoosting algorithm outperforms the other four state-of-the-art algorithms\nin terms of accuracy and ability to tune to new domains; 2) The domain\ndependence of WSD systems seems very strong and suggests that some kind of\nadaptation or tuning is required for cross-corpus application.",
    "published": "2000-09-22T15:02:26Z",
    "link": "http://arxiv.org/pdf/cs/0009022v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7;I.2.6"
    ],
    "authors": [
      "Gerard Escudero",
      "Lluis Marquez",
      "German Rigau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0009027v1",
    "title": "A Classification Approach to Word Prediction",
    "summary": "The eventual goal of a language model is to accurately predict the value of a\nmissing word given its context. We present an approach to word prediction that\nis based on learning a representation for each word as a function of words and\nlinguistics predicates in its context. This approach raises a few new questions\nthat we address. First, in order to learn good word representations it is\nnecessary to use an expressive representation of the context. We present a way\nthat uses external knowledge to generate expressive context representations,\nalong with a learning method capable of handling the large number of features\ngenerated this way that can, potentially, contribute to each prediction.\nSecond, since the number of words ``competing'' for each prediction is large,\nthere is a need to ``focus the attention'' on a smaller subset of these. We\nexhibit the contribution of a ``focus of attention'' mechanism to the\nperformance of the word predictor. Finally, we describe a large scale\nexperimental study in which the approach presented is shown to yield\nsignificant improvements in word prediction tasks.",
    "published": "2000-09-28T14:25:51Z",
    "link": "http://arxiv.org/pdf/cs/0009027v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.6;I.2.7"
    ],
    "authors": [
      "Yair Even-Zohar",
      "Dan Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0010022v1",
    "title": "Noise-Tolerant Learning, the Parity Problem, and the Statistical Query\n  Model",
    "summary": "We describe a slightly sub-exponential time algorithm for learning parity\nfunctions in the presence of random classification noise. This results in a\npolynomial-time algorithm for the case of parity functions that depend on only\nthe first O(log n log log n) bits of input. This is the first known instance of\nan efficient noise-tolerant algorithm for a concept class that is provably not\nlearnable in the Statistical Query model of Kearns. Thus, we demonstrate that\nthe set of problems learnable in the statistical query model is a strict subset\nof those problems learnable in the presence of noise in the PAC model.\n  In coding-theory terms, what we give is a poly(n)-time algorithm for decoding\nlinear k by n codes in the presence of random noise for the case of k = c log n\nloglog n for some c > 0. (The case of k = O(log n) is trivial since one can\njust individually check each of the 2^k possible messages and choose the one\nthat yields the closest codeword.)\n  A natural extension of the statistical query model is to allow queries about\nstatistical properties that involve t-tuples of examples (as opposed to single\nexamples). The second result of this paper is to show that any class of\nfunctions learnable (strongly or weakly) with t-wise queries for t = O(log n)\nis also weakly learnable with standard unary queries. Hence this natural\nextension to the statistical query model does not increase the set of weakly\nlearnable functions.",
    "published": "2000-10-15T20:14:08Z",
    "link": "http://arxiv.org/pdf/cs/0010022v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.DS",
      "I.2.6"
    ],
    "authors": [
      "Avrim Blum",
      "Adam Kalai",
      "Hal Wasserman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0010023v1",
    "title": "Oracle Complexity and Nontransitivity in Pattern Recognition",
    "summary": "Different mathematical models of recognition processes are known. In the\npresent paper we consider a pattern recognition algorithm as an oracle\ncomputation on a Turing machine. Such point of view seems to be useful in\npattern recognition as well as in recursion theory. Use of recursion theory in\npattern recognition shows connection between a recognition algorithm comparison\nproblem and complexity problems of oracle computation. That is because in many\ncases we can take into account only the number of sign computations or in other\nwords volume of oracle information needed. Therefore, the problem of\nrecognition algorithm preference can be formulated as a complexity optimization\nproblem of oracle computation. Furthermore, introducing a certain \"natural\"\npreference relation on a set of recognizing algorithms, we discover it to be\nnontransitive. This relates to the well known nontransitivity paradox in\nprobability theory.\n  Keywords: Pattern Recognition, Recursion Theory, Nontransitivity, Preference\nRelation",
    "published": "2000-10-16T07:42:23Z",
    "link": "http://arxiv.org/pdf/cs/0010023v1.pdf",
    "category": [
      "cs.CC",
      "cs.AI",
      "cs.CV",
      "cs.DS",
      "F.2.2; F.4.1; I.2.10; I.5.2"
    ],
    "authors": [
      "Vadim Bulitko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0010032v2",
    "title": "Super Logic Programs",
    "summary": "The Autoepistemic Logic of Knowledge and Belief (AELB) is a powerful\nnonmonotic formalism introduced by Teodor Przymusinski in 1994. In this paper,\nwe specialize it to a class of theories called `super logic programs'. We argue\nthat these programs form a natural generalization of standard logic programs.\nIn particular, they allow disjunctions and default negation of arbibrary\npositive objective formulas.\n  Our main results are two new and powerful characterizations of the static\nsemant ics of these programs, one syntactic, and one model-theoretic. The\nsyntactic fixed point characterization is much simpler than the fixed point\nconstruction of the static semantics for arbitrary AELB theories. The\nmodel-theoretic characterization via Kripke models allows one to construct\nfinite representations of the inherently infinite static expansions.\n  Both characterizations can be used as the basis of algorithms for query\nanswering under the static semantics. We describe a query-answering interpreter\nfor super programs which we developed based on the model-theoretic\ncharacterization and which is available on the web.",
    "published": "2000-10-25T13:32:51Z",
    "link": "http://arxiv.org/pdf/cs/0010032v2.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "F.4.1; I.2.3; I.2.4"
    ],
    "authors": [
      "Stefan Brass",
      "Juergen Dix",
      "Teodor C. Przymusinski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0010037v1",
    "title": "On the relationship between fuzzy logic and four-valued relevance logic",
    "summary": "In fuzzy propositional logic, to a proposition a partial truth in [0,1] is\nassigned. It is well known that under certain circumstances, fuzzy logic\ncollapses to classical logic. In this paper, we will show that under dual\nconditions, fuzzy logic collapses to four-valued (relevance) logic, where\npropositions have truth-value true, false, unknown, or contradiction. As a\nconsequence, fuzzy entailment may be considered as ``in between'' four-valued\n(relevance) entailment and classical entailment.",
    "published": "2000-10-31T14:14:26Z",
    "link": "http://arxiv.org/pdf/cs/0010037v1.pdf",
    "category": [
      "cs.AI",
      "F.4.1;I.2.3;I.2.4"
    ],
    "authors": [
      "Umberto Straccia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0011007v1",
    "title": "Tree-gram Parsing: Lexical Dependencies and Structural Relations",
    "summary": "This paper explores the kinds of probabilistic relations that are important\nin syntactic disambiguation. It proposes that two widely used kinds of\nrelations, lexical dependencies and structural relations, have complementary\ndisambiguation capabilities. It presents a new model based on structural\nrelations, the Tree-gram model, and reports experiments showing that structural\nrelations should benefit from enrichment by lexical dependencies.",
    "published": "2000-11-06T13:56:42Z",
    "link": "http://arxiv.org/pdf/cs/0011007v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "I.2; K.3.2; J.5"
    ],
    "authors": [
      "Khalil Sima'an"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0011008v1",
    "title": "A Lambda-Calculus with letrec, case, constructors and non-determinism",
    "summary": "A non-deterministic call-by-need lambda-calculus \\calc with case,\nconstructors, letrec and a (non-deterministic) erratic choice, based on\nrewriting rules is investigated. A standard reduction is defined as a variant\nof left-most outermost reduction. The semantics is defined by contextual\nequivalence of expressions instead of using $\\alpha\\beta(\\eta)$-equivalence. It\nis shown that several program transformations are correct, for example all\n(deterministic) rules of the calculus, and in addition the rules for garbage\ncollection, removing indirections and unique copy.\n  This shows that the combination of a context lemma and a meta-rewriting on\nreductions using complete sets of commuting (forking, resp.) diagrams is a\nuseful and successful method for providing a semantics of a functional\nprogramming language and proving correctness of program transformations.",
    "published": "2000-11-06T14:52:33Z",
    "link": "http://arxiv.org/pdf/cs/0011008v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "cs.SC",
      "F.4.1;D.3.2;I.2.2"
    ],
    "authors": [
      "Manfred Schmidt-Schauß",
      "Michael Huber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0011012v3",
    "title": "Causes and Explanations: A Structural-Model Approach, Part I: Causes",
    "summary": "We propose a new definition of actual cause, using structural equations to\nmodel counterfactuals. We show that the definition yields a plausible and\nelegant account of causation that handles well examples which have caused\nproblems for other definitions and resolves major difficulties in the\ntraditional account.",
    "published": "2000-11-07T23:21:38Z",
    "link": "http://arxiv.org/pdf/cs/0011012v3.pdf",
    "category": [
      "cs.AI",
      "I.2.4"
    ],
    "authors": [
      "Joseph Y. Halpern",
      "Judea Pearl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0011030v1",
    "title": "Logic Programming Approaches for Representing and Solving Constraint\n  Satisfaction Problems: A Comparison",
    "summary": "Many logic programming based approaches can be used to describe and solve\ncombinatorial search problems. On the one hand there is constraint logic\nprogramming which computes a solution as an answer substitution to a query\ncontaining the variables of the constraint satisfaction problem. On the other\nhand there are systems based on stable model semantics, abductive systems, and\nfirst order logic model generators which compute solutions as models of some\ntheory. This paper compares these different approaches from the point of view\nof knowledge representation (how declarative are the programs) and from the\npoint of view of performance (how good are they at solving typical problems).",
    "published": "2000-11-21T13:56:21Z",
    "link": "http://arxiv.org/pdf/cs/0011030v1.pdf",
    "category": [
      "cs.AI",
      "I.2.3; I.2.4"
    ],
    "authors": [
      "Nikolay Pelov",
      "Emmanuel De Mot",
      "Marc Denecker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0011042v1",
    "title": "Order-consistent programs are cautiously monotonic",
    "summary": "Some normal logic programs under the answer set (stable model) semantics lack\nthe appealing property of \"cautious monotonicity.\" That is, augmenting a\nprogram with one of its consequences may cause it to lose another of its\nconsequences. The syntactic condition of \"order-consistency\" was shown by Fages\nto guarantee existence of an answer set. This note establishes that\norder-consistent programs are not only consistent, but cautiously monotonic.\n  From this it follows that they are also \"cumulative.\" That is, augmenting an\norder-consistent with some of its consequences does not alter its consequences.\nIn fact, as we show, its answer sets remain unchanged.",
    "published": "2000-11-27T19:21:10Z",
    "link": "http://arxiv.org/pdf/cs/0011042v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "F.4.1;I.2.3"
    ],
    "authors": [
      "Hudson Turner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/quant-ph/0011122v2",
    "title": "Algorithmic Theories of Everything",
    "summary": "The probability distribution P from which the history of our universe is\nsampled represents a theory of everything or TOE. We assume P is formally\ndescribable. Since most (uncountably many) distributions are not, this imposes\na strong inductive bias. We show that P(x) is small for any universe x lacking\na short description, and study the spectrum of TOEs spanned by two Ps, one\nreflecting the most compact constructive descriptions, the other the fastest\nway of computing everything. The former derives from generalizations of\ntraditional computability, Solomonoff's algorithmic probability, Kolmogorov\ncomplexity, and objects more random than Chaitin's Omega, the latter from\nLevin's universal search and a natural resource-oriented postulate: the\ncumulative prior probability of all x incomputable within time t by this\noptimal algorithm should be 1/t. Between both Ps we find a universal\ncumulatively enumerable measure that dominates traditional enumerable measures;\nany such CEM must assign low probability to any universe lacking a short\nenumerating program. We derive P-specific consequences for evolving observers,\ninductive reasoning, quantum physics, philosophy, and the expected duration of\nour universe.",
    "published": "2000-11-30T14:23:55Z",
    "link": "http://arxiv.org/pdf/quant-ph/0011122v2.pdf",
    "category": [
      "quant-ph",
      "cs.AI",
      "cs.CC",
      "cs.LG",
      "hep-th",
      "math-ph",
      "math.MP",
      "physics.comp-ph"
    ],
    "authors": [
      "Juergen Schmidhuber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0012004v1",
    "title": "Improving Performance of heavily loaded agents",
    "summary": "With the increase in agent-based applications, there are now agent systems\nthat support \\emph{concurrent} client accesses. The ability to process large\nvolumes of simultaneous requests is critical in many such applications. In such\na setting, the traditional approach of serving these requests one at a time via\nqueues (e.g. \\textsf{FIFO} queues, priority queues) is insufficient.\nAlternative models are essential to improve the performance of such\n\\emph{heavily loaded} agents. In this paper, we propose a set of\n\\emph{cost-based algorithms} to \\emph{optimize} and \\emph{merge} multiple\nrequests submitted to an agent. In order to merge a set of requests, one first\nneeds to identify commonalities among such requests. First, we provide an\n\\emph{application independent framework} within which an agent developer may\nspecify relationships (called \\emph{invariants}) between requests. Second, we\nprovide two algorithms (and various accompanying heuristics) which allow an\nagent to automatically rewrite requests so as to avoid redundant work---these\nalgorithms take invariants associated with the agent into account. Our\nalgorithms are independent of any specific agent framework. For an\nimplementation, we implemented both these algorithms on top of the \\impact\nagent development platform, and on top of a (non-\\impact) geographic database\nagent. Based on these implementations, we conducted experiments and show that\nour algorithms are considerably more efficient than methods that use the $A^*$\nalgorithm.",
    "published": "2000-12-11T10:17:36Z",
    "link": "http://arxiv.org/pdf/cs/0012004v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "I.2.12;I.2.3;D.2.12;H.2.4"
    ],
    "authors": [
      "Fatma Ozcan",
      "VS Subrahmanian",
      "Juergen Dix"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0012010v1",
    "title": "The Role of Commutativity in Constraint Propagation Algorithms",
    "summary": "Constraint propagation algorithms form an important part of most of the\nconstraint programming systems. We provide here a simple, yet very general\nframework that allows us to explain several constraint propagation algorithms\nin a systematic way. In this framework we proceed in two steps. First, we\nintroduce a generic iteration algorithm on partial orderings and prove its\ncorrectness in an abstract setting. Then we instantiate this algorithm with\nspecific partial orderings and functions to obtain specific constraint\npropagation algorithms.\n  In particular, using the notions commutativity and semi-commutativity, we\nshow that the {\\tt AC-3}, {\\tt PC-2}, {\\tt DAC} and {\\tt DPC} algorithms for\nachieving (directional) arc consistency and (directional) path consistency are\ninstances of a single generic algorithm. The work reported here extends and\nsimplifies that of Apt \\citeyear{Apt99b}.",
    "published": "2000-12-15T14:04:28Z",
    "link": "http://arxiv.org/pdf/cs/0012010v1.pdf",
    "category": [
      "cs.PF",
      "cs.AI",
      "D.3.3;I.1.2;I.1.3"
    ],
    "authors": [
      "Krzysztof R. Apt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0012011v1",
    "title": "Towards a Universal Theory of Artificial Intelligence based on\n  Algorithmic Probability and Sequential Decision Theory",
    "summary": "Decision theory formally solves the problem of rational agents in uncertain\nworlds if the true environmental probability distribution is known.\nSolomonoff's theory of universal induction formally solves the problem of\nsequence prediction for unknown distribution. We unify both theories and give\nstrong arguments that the resulting universal AIXI model behaves optimal in any\ncomputable environment. The major drawback of the AIXI model is that it is\nuncomputable. To overcome this problem, we construct a modified algorithm\nAIXI^tl, which is still superior to any other time t and space l bounded agent.\nThe computation time of AIXI^tl is of the order t x 2^l.",
    "published": "2000-12-16T09:38:13Z",
    "link": "http://arxiv.org/pdf/cs/0012011v1.pdf",
    "category": [
      "cs.AI",
      "cs.CC",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "I.2; I.2.3; I.2.6; I.2.8; F.1.3; F.2"
    ],
    "authors": [
      "Marcus Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0012020v1",
    "title": "Creativity and Delusions: A Neurocomputational Approach",
    "summary": "Thinking is one of the most interesting mental processes. Its complexity is\nsometimes simplified and its different manifestations are classified into\nnormal and abnormal, like the delusional and disorganized thought or the\ncreative one. The boundaries between these facets of thinking are fuzzy causing\ndifficulties in medical, academic, and philosophical discussions. Considering\nthe dopaminergic signal-to-noise neuronal modulation in the central nervous\nsystem, and the existence of semantic maps in human brain, a self-organizing\nneural network model was developed to unify the different thought processes\ninto a single neurocomputational substrate. Simulations were performed varying\nthe dopaminergic modulation and observing the different patterns that emerged\nat the semantic map. Assuming that the thought process is the total pattern\nelicited at the output layer of the neural network, the model shows how the\nnormal and abnormal thinking are generated and that there are no borders\nbetween their different manifestations. Actually, a continuum of different\nqualitative reasoning, ranging from delusion to disorganization of thought, and\npassing through the normal and the creative thinking, seems to be more\nplausible. The model is far from explaining the complexities of human thinking\nbut, at least, it seems to be a good metaphorical and unifying view of the many\nfacets of this phenomenon usually studied in separated settings.",
    "published": "2000-12-22T12:00:07Z",
    "link": "http://arxiv.org/pdf/cs/0012020v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "I.5.1"
    ],
    "authors": [
      "Daniele Quintella Mendes",
      "Luis Alfredo Vidal de Carvalho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0101014v1",
    "title": "On the problem of computing the well-founded semantics",
    "summary": "The well-founded semantics is one of the most widely studied and used\nsemantics of logic programs with negation. In the case of finite propositional\nprograms, it can be computed in polynomial time, more specifically, in\nO(|At(P)|size(P)) steps, where size(P) denotes the total number of occurrences\nof atoms in a logic program P. This bound is achieved by an algorithm\nintroduced by Van Gelder and known as the alternating-fixpoint algorithm.\nImproving on the alternating-fixpoint algorithm turned out to be difficult. In\nthis paper we study extensions and modifications of the alternating-fixpoint\napproach. We then restrict our attention to the class of programs whose rules\nhave no more than one positive occurrence of an atom in their bodies. For\nprograms in that class we propose a new implementation of the\nalternating-fixpoint method in which false atoms are computed in a top-down\nfashion. We show that our algorithm is faster than other known algorithms and\nthat for a wide class of programs it is linear and so, asymptotically optimal.",
    "published": "2001-01-17T13:33:12Z",
    "link": "http://arxiv.org/pdf/cs/0101014v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.DS",
      "I.2.3; F.2.2"
    ],
    "authors": [
      "Zbigniew Lonc",
      "Miroslaw Truszczynski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0101019v2",
    "title": "General Loss Bounds for Universal Sequence Prediction",
    "summary": "The Bayesian framework is ideally suited for induction problems. The\nprobability of observing $x_t$ at time $t$, given past observations\n$x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\\mu$\nof the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many\ncases one does not even have a reasonable estimate of the true distribution. In\norder to overcome this problem a universal distribution $\\xi$ is defined as a\nweighted sum of distributions $\\mu_i\\inM$, where $M$ is any countable set of\ndistributions including $\\mu$. This is a generalization of Solomonoff\ninduction, in which $M$ is the set of all enumerable semi-measures. Systems\nwhich predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$\nif $x_t$ is the true next symbol of the sequence are considered. It is proven\nthat using the universal $\\xi$ as a prior is nearly as good as using the\nunknown true distribution $\\mu$. Furthermore, games of chance, defined as a\nsequence of bets, observations, and rewards are studied. The time needed to\nreach the winning zone is bounded in terms of the relative entropy of $\\mu$ and\n$\\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and\nmore active systems are discussed.",
    "published": "2001-01-21T17:19:37Z",
    "link": "http://arxiv.org/pdf/cs/0101019v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH",
      "I.2; I.2.6; I.2.8; F.1.3"
    ],
    "authors": [
      "Marcus Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0101036v1",
    "title": "The Generalized Universal Law of Generalization",
    "summary": "It has been argued by Shepard that there is a robust psychological law that\nrelates the distance between a pair of items in psychological space and the\nprobability that they will be confused with each other. Specifically, the\nprobability of confusion is a negative exponential function of the distance\nbetween the pair of items. In experimental contexts, distance is typically\ndefined in terms of a multidimensional Euclidean space-but this assumption\nseems unlikely to hold for complex stimuli. We show that, nonetheless, the\nUniversal Law of Generalization can be derived in the more complex setting of\narbitrary stimuli, using a much more universal measure of distance. This\nuniversal distance is defined as the length of the shortest program that\ntransforms the representations of the two items of interest into one another:\nthe algorithmic information distance. It is universal in the sense that it\nminorizes every computable distance: it is the smallest computable distance. We\nshow that the universal law of generalization holds with probability going to\none-provided the confusion probabilities are computable. We also give a\nmathematically more appealing form",
    "published": "2001-01-29T17:54:50Z",
    "link": "http://arxiv.org/pdf/cs/0101036v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "math.PR",
      "physics.soc-ph",
      "J.4"
    ],
    "authors": [
      "Nick Chater",
      "Paul Vitanyi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0102014v1",
    "title": "On the predictability of Rainfall in Kerala- An application of ABF\n  Neural Network",
    "summary": "Rainfall in Kerala State, the southern part of Indian Peninsula in particular\nis caused by the two monsoons and the two cyclones every year. In general,\nclimate and rainfall are highly nonlinear phenomena in nature giving rise to\nwhat is known as the `butterfly effect'. We however attempt to train an ABF\nneural network on the time series rainfall data and show for the first time\nthat in spite of the fluctuations resulting from the nonlinearity in the\nsystem, the trends in the rainfall pattern in this corner of the globe have\nremained unaffected over the past 87 years from 1893 to 1980. We also\nsuccessfully filter out the chaotic part of the system and illustrate that its\neffects are marginal over long term predictions.",
    "published": "2001-02-18T19:17:18Z",
    "link": "http://arxiv.org/pdf/cs/0102014v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "A0"
    ],
    "authors": [
      "Ninan Sajeeth Philip",
      "K. Babu Joseph"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0102018v1",
    "title": "An effective Procedure for Speeding up Algorithms",
    "summary": "The provably asymptotically fastest algorithm within a factor of 5 for\nformally described problems will be constructed. The main idea is to enumerate\nall programs provably equivalent to the original problem by enumerating all\nproofs. The algorithm could be interpreted as a generalization and improvement\nof Levin search, which is, within a multiplicative constant, the fastest\nalgorithm for inverting functions. Blum's speed-up theorem is avoided by taking\ninto account only programs for which a correctness proof exists. Furthermore,\nit is shown that the fastest program that computes a certain function is also\none of the shortest programs provably computing this function. To quantify this\nstatement, the definition of Kolmogorov complexity is extended, and two new\nnatural measures for the complexity of a function are defined.",
    "published": "2001-02-21T20:52:28Z",
    "link": "http://arxiv.org/pdf/cs/0102018v1.pdf",
    "category": [
      "cs.CC",
      "cs.AI",
      "cs.LG",
      "F.2.3"
    ],
    "authors": [
      "Marcus Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0102027v3",
    "title": "Gene Expression Programming: a New Adaptive Algorithm for Solving\n  Problems",
    "summary": "Gene expression programming, a genotype/phenotype genetic algorithm (linear\nand ramified), is presented here for the first time as a new technique for the\ncreation of computer programs. Gene expression programming uses character\nlinear chromosomes composed of genes structurally organized in a head and a\ntail. The chromosomes function as a genome and are subjected to modification by\nmeans of mutation, transposition, root transposition, gene transposition, gene\nrecombination, and one- and two-point recombination. The chromosomes encode\nexpression trees which are the object of selection. The creation of these\nseparate entities (genome and expression tree) with distinct functions allows\nthe algorithm to perform with high efficiency that greatly surpasses existing\nadaptive techniques. The suite of problems chosen to illustrate the power and\nversatility of gene expression programming includes symbolic regression,\nsequence induction with and without constant creation, block stacking, cellular\nautomata rules for the density-classification problem, and two problems of\nboolean concept learning: the 11-multiplexer and the GP rule problem.",
    "published": "2001-02-25T19:29:55Z",
    "link": "http://arxiv.org/pdf/cs/0102027v3.pdf",
    "category": [
      "cs.AI",
      "cs.NE",
      "I.2.2"
    ],
    "authors": [
      "Candida Ferreira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0103002v1",
    "title": "Quantitative Neural Network Model of the Tip-of-the-Tongue Phenomenon\n  Based on Synthesized Memory-Psycholinguistic-Metacognitive Approach",
    "summary": "A new three-stage computer artificial neural network model of the\ntip-of-the-tongue phenomenon is proposed. Each word's node is build from some\ninterconnected learned auto-associative two-layer neural networks each of which\nrepresents separate word's semantic, lexical, or phonological components. The\nmodel synthesizes memory, psycholinguistic, and metamemory approaches, bridges\nspeech errors and naming chronometry research traditions, and can explain\nquantitatively many tip-of-the-tongue effects.",
    "published": "2001-03-02T00:20:01Z",
    "link": "http://arxiv.org/pdf/cs/0103002v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "q-bio.NC",
      "q-bio.QM",
      "I.2.7"
    ],
    "authors": [
      "Petro M. Gopych"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0103015v1",
    "title": "Fitness Uniform Selection to Preserve Genetic Diversity",
    "summary": "In evolutionary algorithms, the fitness of a population increases with time\nby mutating and recombining individuals and by a biased selection of more fit\nindividuals. The right selection pressure is critical in ensuring sufficient\noptimization progress on the one hand and in preserving genetic diversity to be\nable to escape from local optima on the other. We propose a new selection\nscheme, which is uniform in the fitness values. It generates selection pressure\ntowards sparsely populated fitness regions, not necessarily towards higher\nfitness, as is the case for all other selection schemes. We show that the new\nselection scheme can be much more effective than standard selection schemes.",
    "published": "2001-03-14T18:40:32Z",
    "link": "http://arxiv.org/pdf/cs/0103015v1.pdf",
    "category": [
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "q-bio",
      "I.2; I.2.6; I.2.8; F.2"
    ],
    "authors": [
      "Marcus Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0103020v1",
    "title": "Belief Revision: A Critique",
    "summary": "We examine carefully the rationale underlying the approaches to belief change\ntaken in the literature, and highlight what we view as methodological problems.\nWe argue that to study belief change carefully, we must be quite explicit about\nthe ``ontology'' or scenario underlying the belief change process. This is\nsomething that has been missing in previous work, with its focus on postulates.\nOur analysis shows that we must pay particular attention to two issues that\nhave often been taken for granted: The first is how we model the agent's\nepistemic state. (Do we use a set of beliefs, or a richer structure, such as an\nordering on worlds? And if we use a set of beliefs, in what language are these\nbeliefs are expressed?) We show that even postulates that have been called\n``beyond controversy'' are unreasonable when the agent's beliefs include\nbeliefs about her own epistemic state as well as the external world. The second\nis the status of observations. (Are observations known to be true, or just\nbelieved? In the latter case, how firm is the belief?) Issues regarding the\nstatus of observations arise particularly when we consider iterated belief\nrevision, and we must confront the possibility of revising by p and then by\nnot-p.",
    "published": "2001-03-27T20:33:51Z",
    "link": "http://arxiv.org/pdf/cs/0103020v1.pdf",
    "category": [
      "cs.AI",
      "cs.LO",
      "I.2.4, F.4.1"
    ],
    "authors": [
      "Nir Friedman",
      "Joseph Y. Halpern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0104017v1",
    "title": "Local Search Techniques for Constrained Portfolio Selection Problems",
    "summary": "We consider the problem of selecting a portfolio of assets that provides the\ninvestor a suitable balance of expected return and risk. With respect to the\nseminal mean-variance model of Markowitz, we consider additional constraints on\nthe cardinality of the portfolio and on the quantity of individual shares. Such\nconstraints better capture the real-world trading system, but make the problem\nmore difficult to be solved with exact methods. We explore the use of local\nsearch techniques, mainly tabu search, for the portfolio selection problem. We\ncompare and combine previous work on portfolio selection that makes use of the\nlocal search approach and we propose new algorithms that combine different\nneighborhood relations. In addition, we show how the use of randomization and\nof a simple form of adaptiveness simplifies the setting of a large number of\ncritical parameters. Finally, we show how our techniques perform on public\nbenchmarks.",
    "published": "2001-04-18T13:42:49Z",
    "link": "http://arxiv.org/pdf/cs/0104017v1.pdf",
    "category": [
      "cs.CE",
      "cs.AI",
      "J.1; I.2.8"
    ],
    "authors": [
      "Andrea Schaerf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0104020v1",
    "title": "Coaxing Confidences from an Old Friend: Probabilistic Classifications\n  from Transformation Rule Lists",
    "summary": "Transformation-based learning has been successfully employed to solve many\nnatural language processing problems. It has many positive features, but one\ndrawback is that it does not provide estimates of class membership\nprobabilities.\n  In this paper, we present a novel method for obtaining class membership\nprobabilities from a transformation-based rule list classifier. Three\nexperiments are presented which measure the modeling accuracy and cross-entropy\nof the probabilistic classifier on unseen data and the degree to which the\noutput probabilities from the classifier can be used to estimate confidences in\nits classification decisions.\n  The results of these experiments show that, for the task of text chunking,\nthe estimates produced by this technique are more informative than those\ngenerated by a state-of-the-art decision tree.",
    "published": "2001-04-27T23:16:21Z",
    "link": "http://arxiv.org/pdf/cs/0104020v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Radu Florian",
      "John C. Henderson",
      "Grace Ngai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0105003v1",
    "title": "Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun\n  Phrase Chunking",
    "summary": "This paper presents a comprehensive empirical comparison between two\napproaches for developing a base noun phrase chunker: human rule writing and\nactive learning using interactive real-time human annotation. Several novel\nvariations on active learning are investigated, and underlying cost models for\ncross-modal machine learning comparison are presented and explored. Results\nshow that it is more efficient and more successful by several measures to train\na system using active learning annotation rather than hand-crafted rule writing\nat a comparable level of human labor investment.",
    "published": "2001-05-02T08:39:32Z",
    "link": "http://arxiv.org/pdf/cs/0105003v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "authors": [
      "Grace Ngai",
      "David Yarowsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0105015v1",
    "title": "The alldifferent Constraint: A Survey",
    "summary": "The constraint of difference is known to the constraint programming community\nsince Lauriere introduced Alice in 1978. Since then, several solving strategies\nhave been designed for this constraint. In this paper we give both a practical\noverview and an abstract comparison of these different strategies.",
    "published": "2001-05-08T13:13:04Z",
    "link": "http://arxiv.org/pdf/cs/0105015v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "D.3.3"
    ],
    "authors": [
      "W. J. van Hoeve"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0105017v1",
    "title": "Optimization Over Zonotopes and Training Support Vector Machines",
    "summary": "We make a connection between classical polytopes called zonotopes and Support\nVector Machine (SVM) classifiers. We combine this connection with the ellipsoid\nmethod to give some new theoretical results on training SVMs. We also describe\nsome special properties of soft margin C-SVMs as parameter C goes to infinity.",
    "published": "2001-05-08T21:07:43Z",
    "link": "http://arxiv.org/pdf/cs/0105017v1.pdf",
    "category": [
      "cs.CG",
      "cs.AI",
      "F.2.2; G.1.6; I.2.6; I.5.1"
    ],
    "authors": [
      "Marshall Bern",
      "David Eppstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0105021v1",
    "title": "Solving Composed First-Order Constraints from Discrete-Time Robust\n  Control",
    "summary": "This paper deals with a problem from discrete-time robust control which\nrequires the solution of constraints over the reals that contain both universal\nand existential quantifiers. For solving this problem we formulate it as a\nprogram in a (fictitious) constraint logic programming language with explicit\nquantifier notation. This allows us to clarify the special structure of the\nproblem, and to extend an algorithm for computing approximate solution sets of\nfirst-order constraints over the reals to exploit this structure. As a result\nwe can deal with inputs that are clearly out of reach for current symbolic\nsolvers.",
    "published": "2001-05-11T08:28:33Z",
    "link": "http://arxiv.org/pdf/cs/0105021v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.CE",
      "F.4.1;I.2.8"
    ],
    "authors": [
      "Stefan Ratschan",
      "Luc Jaulin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0105022v1",
    "title": "Multi-Channel Parallel Adaptation Theory for Rule Discovery",
    "summary": "In this paper, we introduce a new machine learning theory based on\nmulti-channel parallel adaptation for rule discovery. This theory is\ndistinguished from the familiar parallel-distributed adaptation theory of\nneural networks in terms of channel-based convergence to the target rules. We\nshow how to realize this theory in a learning system named CFRule. CFRule is a\nparallel weight-based model, but it departs from traditional neural computing\nin that its internal knowledge is comprehensible. Furthermore, when the model\nconverges upon training, each channel converges to a target rule. The model\nadaptation rule is derived by multi-level parallel weight optimization based on\ngradient descent. Since, however, gradient descent only guarantees local\noptimization, a multi-channel regression-based optimization strategy is\ndeveloped to effectively deal with this problem. Formally, we prove that the\nCFRule model can explicitly and precisely encode any given rule set. Also, we\nprove a property related to asynchronous parallel convergence, which is a\ncritical element of the multi-channel parallel adaptation theory for rule\nlearning. Thanks to the quantizability nature of the CFRule model, rules can be\nextracted completely and soundly via a threshold-based mechanism. Finally, the\npractical application of the theory is demonstrated in DNA promoter recognition\nand hepatitis prognosis prediction.",
    "published": "2001-05-11T14:17:42Z",
    "link": "http://arxiv.org/pdf/cs/0105022v1.pdf",
    "category": [
      "cs.AI",
      "I2.6"
    ],
    "authors": [
      "Li Min Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/cs/0105025v1",
    "title": "Market-Based Reinforcement Learning in Partially Observable Worlds",
    "summary": "Unlike traditional reinforcement learning (RL), market-based RL is in\nprinciple applicable to worlds described by partially observable Markov\nDecision Processes (POMDPs), where an agent needs to learn short-term memories\nof relevant previous events in order to execute optimal actions. Most previous\nwork, however, has focused on reactive settings (MDPs) instead of POMDPs. Here\nwe reimplement a recent approach to market-based RL and for the first time\nevaluate it in a toy POMDP setting.",
    "published": "2001-05-15T19:07:28Z",
    "link": "http://arxiv.org/pdf/cs/0105025v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.NE",
      "I.2"
    ],
    "authors": [
      "Ivo Kwee",
      "Marcus Hutter",
      "Juergen Schmidhuber"
    ]
  }
]