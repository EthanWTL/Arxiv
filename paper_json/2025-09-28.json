[
  {
    "id": "http://arxiv.org/abs/2509.24008v1",
    "title": "FrameMind: Frame-Interleaved Chain-of-Thought for Video Reasoning via\n  Reinforcement Learning",
    "summary": "Current video understanding models rely on fixed frame sampling strategies,\nprocessing predetermined visual inputs regardless of the specific reasoning\nrequirements of each question. This static approach limits their ability to\nadaptively gather visual evidence, leading to suboptimal performance on tasks\nthat require either broad temporal coverage or fine-grained spatial detail. In\nthis paper, we introduce FrameMind, an end-to-end framework trained with\nreinforcement learning that enables models to dynamically request visual\ninformation during reasoning through Frame-Interleaved Chain-of-Thought\n(FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns\nwhere the model alternates between textual reasoning and active visual\nperception, using tools to extract targeted frames or video clips based on\nidentified knowledge gaps. To train effective dynamic sampling policies, we\npropose Dynamic Resolution Frame Sampling (DRFS), which exposes models to\ndiverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a\ngroup-relative policy optimization algorithm that learns from outcome-based\nrewards without requiring frame-level annotations. Extensive experiments on\nchallenging benchmarks like MLVU and VideoMME demonstrate that our method\nsignificantly outperforms existing models, advancing the state of the art in\nflexible and efficient video understanding.",
    "published": "2025-09-28T17:59:43Z",
    "link": "http://arxiv.org/pdf/2509.24008v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haonan Ge",
      "Yiwei Wang",
      "Kai-Wei Chang",
      "Hang Wu",
      "Yujun Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24006v1",
    "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention",
    "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
    "published": "2025-09-28T17:58:59Z",
    "link": "http://arxiv.org/pdf/2509.24006v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Jintao Zhang",
      "Haoxu Wang",
      "Kai Jiang",
      "Shuo Yang",
      "Kaiwen Zheng",
      "Haocheng Xi",
      "Ziteng Wang",
      "Hongzhou Zhu",
      "Min Zhao",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Jun Zhu",
      "Jianfei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24002v1",
    "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use",
    "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of $127$\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n$30$\\% pass@1 and $15$\\% pass^4. On average, LLMs require $16.2$ execution\nturns and $17.4$ tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
    "published": "2025-09-28T17:53:27Z",
    "link": "http://arxiv.org/pdf/2509.24002v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zijian Wu",
      "Xiangyan Liu",
      "Xinyuan Zhang",
      "Lingjun Chen",
      "Fanqing Meng",
      "Lingxiao Du",
      "Yiran Zhao",
      "Fanshi Zhang",
      "Yaoqi Ye",
      "Jiawei Wang",
      "Zirui Wang",
      "Jinjie Ni",
      "Yufan Yang",
      "Arvin Xu",
      "Michael Qizhe Shieh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23996v1",
    "title": "Future-Proofing Programmers: Optimal Knowledge Tracing for AI-Assisted\n  Personalized Education",
    "summary": "Learning to learn is becoming a science, driven by the convergence of\nknowledge tracing, signal processing, and generative AI to model student\nlearning states and optimize education. We propose CoTutor, an AI-driven model\nthat enhances Bayesian Knowledge Tracing with signal processing techniques to\nimprove student progress modeling and deliver adaptive feedback and strategies.\nDeployed as an AI copilot, CoTutor combines generative AI with adaptive\nlearning technology. In university trials, it has demonstrated measurable\nimprovements in learning outcomes while outperforming conventional educational\ntools. Our results highlight its potential for AI-driven personalization,\nscalability, and future opportunities for advancing privacy and ethical\nconsiderations in educational technology. Inspired by Richard Hamming's vision\nof computer-aided 'learning to learn,' CoTutor applies convex optimization and\nsignal processing to automate and scale up learning analytics, while reserving\npedagogical judgment for humans, ensuring AI facilitates the process of\nknowledge tracing while enabling learners to uncover new insights.",
    "published": "2025-09-28T17:40:39Z",
    "link": "http://arxiv.org/pdf/2509.23996v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuchen Wang",
      "Pei-Duo Yu",
      "Chee Wei Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23994v1",
    "title": "The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt\n  Synthesis",
    "summary": "As autonomous AI agents are increasingly deployed in industry, it is\nessential to safeguard them. We introduce a novel framework that automates the\ntranslation of unstructured design documents into verifiable, real-time\nguardrails. We introduce \"Policy as Prompt,\" a new approach that uses Large\nLanguage Models (LLMs) to interpret and enforce natural language policies by\napplying contextual understanding and the principle of least privilege. Our\nsystem first ingests technical artifacts to construct a verifiable policy tree,\nwhich is then compiled into lightweight, prompt-based classifiers that audit\nagent behavior at runtime. We validate our approach across diverse\napplications, demonstrating a scalable and auditable pipeline that bridges the\ncritical policy-to-practice gap, paving the way for verifiably safer and more\nregulatable AI.",
    "published": "2025-09-28T17:36:52Z",
    "link": "http://arxiv.org/pdf/2509.23994v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Gauri Kholkar",
      "Ratinder Ahuja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23992v1",
    "title": "Guide: Generalized-Prior and Data Encoders for DAG Estimation",
    "summary": "Modern causal discovery methods face critical limitations in scalability,\ncomputational efficiency, and adaptability to mixed data types, as evidenced by\nbenchmarks on node scalability (30, $\\le 50$, $\\ge 70$ nodes), computational\nenergy demands, and continuous/non-continuous data handling. While traditional\nalgorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,\nexhibiting prohibitive energy costs for higher-order nodes and poor scalability\nbeyond 70 nodes, we propose \\textbf{GUIDE}, a framework that integrates Large\nLanguage Model (LLM)-generated adjacency matrices with observational data\nthrough a dual-encoder architecture. GUIDE uniquely optimizes computational\nefficiency, reducing runtime on average by $\\approx 42%$ compared to RL-BIC and\nKCRL methods, while achieving an average $\\approx 117%$ improvement in accuracy\nover both NOTEARS and GraN-DAG individually. During training, GUIDE's\nreinforcement learning agent dynamically balances reward maximization\n(accuracy) and penalty avoidance (DAG constraints), enabling robust performance\nacross mixed data types and scalability to $\\ge 70$ nodes -- a setting where\nbaseline methods fail.",
    "published": "2025-09-28T17:35:21Z",
    "link": "http://arxiv.org/pdf/2509.23992v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Amartya Roy",
      "Devharish N",
      "Shreya Ganguly",
      "Kripabandhu Ghosh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23990v1",
    "title": "The Hidden Costs of Translation Accuracy: Distillation, Quantization,\n  and Environmental Impact",
    "summary": "The rapid expansion of large language models (LLMs) has heightened concerns\nabout their computational and environmental costs. This study investigates the\ntrade-offs between translation quality and efficiency by comparing full-scale,\ndistilled, and quantized models using machine translation as a case study. We\nevaluated performance on the Flores+ benchmark and through human judgments of\nconversational translations in French, Hindi, and Kannada. Our analysis of\ncarbon emissions per evaluation run revealed that the full 3.3B fp32 model,\nwhile achieving the highest BLEU scores, incurred the largest environmental\nfootprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an\ninference of up to 4.5x faster than the full 3.3B model, with only minimal\nreductions in BLEU scores. Human evaluations also showed that even aggressive\nquantization (INT4) preserved high levels of accuracy and fluency, with\ndifferences between models generally minor. These findings demonstrate that\nmodel compression strategies can substantially reduce computational demands and\nenvironmental impact while maintaining competitive translation quality, though\ntrade-offs are more pronounced in low-resource settings. We argue for\nevaluation frameworks that integrate efficiency and sustainability alongside\nobjective metrics as central dimensions of progress in NLP.",
    "published": "2025-09-28T17:32:52Z",
    "link": "http://arxiv.org/pdf/2509.23990v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Dhaathri Vijay",
      "Anandaswarup Vadapalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23988v1",
    "title": "LLM/Agent-as-Data-Analyst: A Survey",
    "summary": "Large language model (LLM) and agent techniques for data analysis (a.k.a\nLLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both\nacademica and industry. In comparison with traditional rule or small-model\nbased approaches, (agentic) LLMs enable complex data understanding, natural\nlanguage interfaces, semantic analysis functions, and autonomous pipeline\norchestration. The technical evolution further distills five key design goals\nfor intelligent data analysis agents, namely semantic-aware design,\nmodality-hybrid integration, autonomous pipelines, tool-augmented workflows,\nand support for open-world tasks. From a modality perspective, we review\nLLM-based techniques for (i) structured data (e.g., table question answering\nfor relational data and NL2GQL for graph data), (ii) semi-structured data\n(e.g., markup languages understanding and semi-structured table modeling),\n(iii) unstructured data (e.g., chart understanding, document understanding,\nprogramming languages vulnerable detection), and (iv) heterogeneous data (e.g.,\ndata retrieval and modality alignment for data lakes). Finally, we outline the\nremaining challenges and propose several insights and practical directions for\nadvancing LLM/Agent-powered data analysis.",
    "published": "2025-09-28T17:31:38Z",
    "link": "http://arxiv.org/pdf/2509.23988v1.pdf",
    "category": [
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Zirui Tang",
      "Weizheng Wang",
      "Zihang Zhou",
      "Yang Jiao",
      "Bangrui Xu",
      "Boyu Niu",
      "Xuanhe Zhou",
      "Guoliang Li",
      "Yeye He",
      "Wei Zhou",
      "Yitong Song",
      "Cheng Tan",
      "Bin Wang",
      "Conghui He",
      "Xiaoyang Wang",
      "Fan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23986v1",
    "title": "TusoAI: Agentic Optimization for Scientific Methods",
    "summary": "Scientific discovery is often slowed by the manual development of\ncomputational tools needed to analyze complex experimental data. Building such\ntools is costly and time-consuming because scientists must iteratively review\nliterature, test modeling and scientific assumptions against empirical data,\nand implement these insights into efficient software. Large language models\n(LLMs) have demonstrated strong capabilities in synthesizing literature,\nreasoning with empirical data, and generating domain-specific code, offering\nnew opportunities to accelerate computational method development. Existing\nLLM-based systems either focus on performing scientific analyses using existing\ncomputational methods or on developing computational methods or models for\ngeneral machine learning without effectively integrating the often unstructured\nknowledge specific to scientific domains. Here, we introduce TusoAI , an\nagentic AI system that takes a scientific task description with an evaluation\nfunction and autonomously develops and optimizes computational methods for the\napplication. TusoAI integrates domain knowledge into a knowledge tree\nrepresentation and performs iterative, domain-specific optimization and model\ndiagnosis, improving performance over a pool of candidate solutions. We\nconducted comprehensive benchmark evaluations demonstrating that TusoAI\noutperforms state-of-the-art expert methods, MLE agents, and scientific AI\nagents across diverse tasks, such as single-cell RNA-seq data denoising and\nsatellite-based earth monitoring. Applying TusoAI to two key open problems in\ngenetics improved existing computational methods and uncovered novel biology,\nincluding 9 new associations between autoimmune diseases and T cell subtypes\nand 7 previously unreported links between disease variants linked to their\ntarget genes. Our code is publicly available at\nhttps://github.com/Alistair-Turcan/TusoAI.",
    "published": "2025-09-28T17:30:44Z",
    "link": "http://arxiv.org/pdf/2509.23986v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Alistair Turcan",
      "Kexin Huang",
      "Lei Li",
      "Martin Jinye Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23982v1",
    "title": "Toward Preference-aligned Large Language Models via Residual-based Model\n  Steering",
    "summary": "Preference alignment is a critical step in making Large Language Models\n(LLMs) useful and aligned with (human) preferences. Existing approaches such as\nReinforcement Learning from Human Feedback or Direct Preference Optimization\ntypically require curated data and expensive optimization over billions of\nparameters, and eventually lead to persistent task-specific models. In this\nwork, we introduce Preference alignment of Large Language Models via Residual\nSteering (PaLRS), a training-free method that exploits preference signals\nencoded in the residual streams of LLMs. From as few as one hundred preference\npairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be\napplied at inference time to push models toward preferred behaviors. We\nevaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that\nPaLRS-aligned models achieve consistent gains on mathematical reasoning and\ncode generation benchmarks while preserving baseline general-purpose\nperformance. Moreover, when compared to DPO-aligned models, they perform better\nwith huge time savings. Our findings highlight that PaLRS offers an effective,\nmuch more efficient and flexible alternative to standard preference\noptimization pipelines, offering a training-free, plug-and-play mechanism for\nalignment with minimal data.",
    "published": "2025-09-28T17:16:16Z",
    "link": "http://arxiv.org/pdf/2509.23982v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Lucio La Cava",
      "Andrea Tagarelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23981v1",
    "title": "Automatic selection of primary studies in systematic reviews with\n  evolutionary rule-based classification",
    "summary": "Searching, filtering and analysing scientific literature are time-consuming\ntasks when performing a systematic literature review. With the rise of\nartificial intelligence, some steps in the review process are progressively\nbeing automated. In particular, machine learning for automatic paper selection\ncan greatly reduce the effort required to identify relevant literature in\nscientific databases. We propose an evolutionary machine learning approach,\ncalled \\ourmodel, to automatically determine whether a paper retrieved from a\nliterature search process is relevant. \\ourmodel builds an interpretable\nrule-based classifier using grammar-guided genetic programming. The use of a\ngrammar to define the syntax and the structure of the rules allows \\ourmodel to\neasily combine the usual textual information with other bibliometric data not\nconsidered by state-of-the-art methods. Our experiments demonstrate that it is\npossible to generate accurate classifiers without impairing interpretability\nand using configurable information sources not supported so far.",
    "published": "2025-09-28T17:13:20Z",
    "link": "http://arxiv.org/pdf/2509.23981v1.pdf",
    "category": [
      "cs.AI",
      "68",
      "I.2"
    ],
    "authors": [
      "José de la Torre-López",
      "Aurora Ramírez",
      "José Raúl Romero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23962v1",
    "title": "Conditional Advantage Estimation for Reinforcement Learning in Large\n  Reasoning Models",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language\nmodels (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning\ncapabilities on tasks with clear correctness criteria, such as mathematical\nreasoning tasks. Several training metrics, such as entropy or response length,\nhave been observed to correlate with different reasoning behaviors in\nreinforcement learning. Prior approaches incorporate such priors through reward\nor advantage shaping, which often relies on hand-crafted penalties and\npreferences (e.g., higher-is-better or lower-is-better). However, without\ncareful hyperparameter tuning, these directional priors can be overly biased\nand may lead to failure. To this end, we introduce Conditional advANtage\nestimatiON (CANON), amplifying the impact of the target metric without\npresuming its direction. Specifically, CANON regroups the sampled responses\ninto two groups based on the higher or lower value of a target metric, measures\nwhich metric trend contributes to better performance through inter-group\ncomparison, and identifies the better response within the same group. In\nsummary, CANON based on entropy consistently outperforms prior methods across\nthree LLMs on both math reasoning and high-complexity logic tasks. When applied\nto response length, CANON further improves token efficiency, yielding a more\nfavorable Pareto frontier in the performance-cost trade-off.",
    "published": "2025-09-28T16:33:07Z",
    "link": "http://arxiv.org/pdf/2509.23962v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Guanxu Chen",
      "Yafu Li",
      "Yuxian Jiang",
      "Chen Qian",
      "Qihan Ren",
      "Jingyi Yang",
      "Yu Cheng",
      "Dongrui Liu",
      "Jing Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23960v1",
    "title": "MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework\n  for Safe and Optimal Multi-Agent Control",
    "summary": "Co-optimizing safety and performance in large-scale multi-agent systems\nremains a fundamental challenge. Existing approaches based on multi-agent\nreinforcement learning (MARL), safety filtering, or Model Predictive Control\n(MPC) either lack strict safety guarantees, suffer from conservatism, or fail\nto scale effectively. We propose MAD-PINN, a decentralized physics-informed\nmachine learning framework for solving the multi-agent state-constrained\noptimal control problem (MASC-OCP). Our method leverages an epigraph-based\nreformulation of SC-OCP to simultaneously capture performance and safety, and\napproximates its solution via a physics-informed neural network. Scalability is\nachieved by training the SC-OCP value function on reduced-agent systems and\ndeploying them in a decentralized fashion, where each agent relies only on\nlocal observations of its neighbours for decision-making. To further enhance\nsafety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based\nneighbour selection strategy to prioritize safety-critical interactions, and a\nreceding-horizon policy execution scheme that adapts to dynamic interactions\nwhile reducing computational burden. Experiments on multi-agent navigation\ntasks demonstrate that MAD-PINN achieves superior safety-performance\ntrade-offs, maintains scalability as the number of agents grows, and\nconsistently outperforms state-of-the-art baselines.",
    "published": "2025-09-28T16:31:22Z",
    "link": "http://arxiv.org/pdf/2509.23960v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Somil Bansal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23957v1",
    "title": "Vision-Grounded Machine Interpreting: Improving the Translation Process\n  through Visual Cues",
    "summary": "Machine Interpreting systems are currently implemented as unimodal, real-time\nspeech-to-speech architectures, processing translation exclusively on the basis\nof the linguistic signal. Such reliance on a single modality, however,\nconstrains performance in contexts where disambiguation and adequacy depend on\nadditional cues, such as visual, situational, or pragmatic information. This\npaper introduces Vision-Grounded Interpreting (VGI), a novel approach designed\nto address the limitations of unimodal machine interpreting. We present a\nprototype system that integrates a vision-language model to process both speech\nand visual input from a webcam, with the aim of priming the translation process\nthrough contextual visual information. To evaluate the effectiveness of this\napproach, we constructed a hand-crafted diagnostic corpus targeting three types\nof ambiguity. In our evaluation, visual grounding substantially improves\nlexical disambiguation, yields modest and less stable gains for gender\nresolution, and shows no benefit for syntactic ambiguities. We argue that\nembracing multimodality represents a necessary step forward for advancing\ntranslation quality in machine interpreting.",
    "published": "2025-09-28T16:25:33Z",
    "link": "http://arxiv.org/pdf/2509.23957v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Claudio Fantinuoli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23946v1",
    "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning\n  Paradigm",
    "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning\nabilities of Large Language Models (LLMs), yet their monolithic and\nauto-regressive architecture inherently conflates high-level strategic planning\nwith low-level step-by-step execution, leading to computational inefficiency,\nlimited exploration of reasoning paths, and reduced interpretability. To\novercome these issues, we propose the Explore-Execute Chain ($E^2C$), a\nstructured reasoning framework that decouples reasoning into two distinct\nphases: an exploratory phase that stochastically generates succinct high-level\nplans, followed by an execution phase that deterministically carries out the\nchosen plan. Our approach incorporates a two-stage training methodology, which\ncombines Supervised Fine-Tuning (SFT) - augmented by a novel data generation\nalgorithm enforcing strict plan adherence - with a subsequent Reinforcement\nLearning (RL) stage that capitalizes on the informativeness of exploration and\nreinforces the determinism of execution.This decomposition enables an efficient\ntest-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches\n58.1% accuracy using <10% of the decoding tokens required by comparable methods\n(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For\ncross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with\nonly 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher\naccuracy than standard SFT on medical benchmarks, delivering state-of-the-art\nperformance, strong generalization, and greater interpretability by separating\nplanning from execution. The code and pre-trained models for the project are\navailable at: https://github.com/yks23/Explore-Execute-Chain.git",
    "published": "2025-09-28T15:48:40Z",
    "link": "http://arxiv.org/pdf/2509.23946v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "authors": [
      "Kaisen Yang",
      "Lixuan He",
      "Rushi Shah",
      "Kaicheng Yang",
      "Qinwei Ma",
      "Dianbo Liu",
      "Alex Lamb"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23938v1",
    "title": "Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust\n  Turn-Taking in Full-Duplex Spoken Dialogue Systems",
    "summary": "Full-duplex interaction is crucial for natural human-machine communication,\nyet remains challenging as it requires robust turn-taking detection to decide\nwhen the system should speak, listen, or remain silent. Existing solutions\neither rely on dedicated turn-taking models, most of which are not\nopen-sourced. The few available ones are limited by their large parameter size\nor by supporting only a single modality, such as acoustic or linguistic.\nAlternatively, some approaches finetune LLM backbones to enable full-duplex\ncapability, but this requires large amounts of full-duplex data, which remain\nscarce in open-source form. To address these issues, we propose Easy Turn, an\nopen-source, modular turn-taking detection model that integrates acoustic and\nlinguistic bimodal information to predict four dialogue turn states: complete,\nincomplete, backchannel, and wait, accompanied by the release of Easy Turn\ntrainset, a 1,145-hour speech dataset designed for training turn-taking\ndetection models. Compared to existing open-source models like TEN Turn\nDetection and Smart Turn V2, our model achieves state-of-the-art turn-taking\ndetection accuracy on our open-source Easy Turn testset. The data and model\nwill be made publicly available on GitHub.",
    "published": "2025-09-28T15:29:44Z",
    "link": "http://arxiv.org/pdf/2509.23938v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Guojian Li",
      "Chengyou Wang",
      "Hongfei Xue",
      "Shuiyuan Wang",
      "Dehui Gao",
      "Zihan Zhang",
      "Yuke Lin",
      "Wenjie Li",
      "Longshuai Xiao",
      "Zhonghua Fu",
      "Lei Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23937v1",
    "title": "Diffusion Models are Kelly Gamblers",
    "summary": "We draw a connection between diffusion models and the Kelly criterion for\nmaximizing returns in betting games. We find that conditional diffusion models\nstore additional information to bind the signal $X$ with the conditioning\ninformation $Y$, equal to the mutual information between them. Classifier-free\nguidance effectively boosts the mutual information between $X$ and $Y$ at\nsampling time. This is especially helpful in image models, since the mutual\ninformation between images and their labels is low, a fact which is intimately\nconnected to the manifold hypothesis. Finally, we point out some nuances in the\npopular perspective that diffusion models are infinitely deep autoencoders. In\ndoing so, we relate the denoising loss to the Fermi Golden Rule from quantum\nmechanics.",
    "published": "2025-09-28T15:27:25Z",
    "link": "http://arxiv.org/pdf/2509.23937v1.pdf",
    "category": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "authors": [
      "Akhil Premkumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23928v1",
    "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
    "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
    "published": "2025-09-28T15:05:21Z",
    "link": "http://arxiv.org/pdf/2509.23928v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhinan Xie",
      "Peisong Wang",
      "Jian Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23924v1",
    "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
    "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.",
    "published": "2025-09-28T15:01:15Z",
    "link": "http://arxiv.org/pdf/2509.23924v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jingyi Yang",
      "Guanxu Chen",
      "Xuhao Hu",
      "Jing Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23923v1",
    "title": "Graph Mixing Additive Networks",
    "summary": "We introduce GMAN, a flexible, interpretable, and expressive framework that\nextends Graph Neural Additive Networks (GNANs) to learn from sets of sparse\ntime-series data. GMAN represents each time-dependent trajectory as a directed\ngraph and applies an enriched, more expressive GNAN to each graph. It allows\nusers to control the interpretability-expressivity trade-off by grouping\nfeatures and graphs to encode priors, and it provides feature, node, and\ngraph-level interpretability. On real-world datasets, including mortality\nprediction from blood tests and fake-news detection, GMAN outperforms strong\nnon-interpretable black-box baselines while delivering actionable,\ndomain-aligned explanations.",
    "published": "2025-09-28T14:58:58Z",
    "link": "http://arxiv.org/pdf/2509.23923v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Maya Bechler-Speicher",
      "Andrea Zerio",
      "Maor Huri",
      "Marie Vibeke Vestergaard",
      "Ran Gilad-Bachrach",
      "Tine Jess",
      "Samir Bhatt",
      "Aleksejs Sazonovs"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23913v1",
    "title": "Continual Learning to Generalize Forwarding Strategies for Diverse\n  Mobile Wireless Networks",
    "summary": "Deep reinforcement learning (DRL) has been successfully used to design\nforwarding strategies for multi-hop mobile wireless networks. While such\nstrategies can be used directly for networks with varied connectivity and\ndynamic conditions, developing generalizable approaches that are effective on\nscenarios significantly different from the training environment remains largely\nunexplored. In this paper, we propose a framework to address the challenge of\ngeneralizability by (i) developing a generalizable base model considering\ndiverse mobile network scenarios, and (ii) using the generalizable base model\nfor new scenarios, and when needed, fine-tuning the base model using a small\namount of data from the new scenarios. To support this framework, we first\ndesign new features to characterize network variation and feature quality,\nthereby improving the information used in DRL-based forwarding decisions. We\nthen develop a continual learning (CL) approach able to train DRL models across\ndiverse network scenarios without ``catastrophic forgetting.'' Using extensive\nevaluation, including real-world scenarios in two cities, we show that our\napproach is generalizable to unseen mobility scenarios. Compared to a\nstate-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction\nin delay, 24% improvement in delivery rate, and comparable or slightly higher\nnumber of forwards.",
    "published": "2025-09-28T14:37:15Z",
    "link": "http://arxiv.org/pdf/2509.23913v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Cheonjin Park",
      "Victoria Manfredi",
      "Xiaolan Zhang",
      "Chengyi Liu",
      "Alicia P Wolfe",
      "Dongjin Song",
      "Sarah Tasneem",
      "Bing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23912v1",
    "title": "From Neural Networks to Logical Theories: The Correspondence between\n  Fibring Modal Logics and Fibring Neural Networks",
    "summary": "Fibring of modal logics is a well-established formalism for combining\ncountable families of modal logics into a single fibred language with common\nsemantics, characterized by fibred models. Inspired by this formalism, fibring\nof neural networks was introduced as a neurosymbolic framework for combining\nlearning and reasoning in neural networks. Fibring of neural networks uses the\n(pre-)activations of a trained network to evaluate a fibring function computing\nthe weights of another network whose outputs are injected back into the\noriginal network. However, the exact correspondence between fibring of neural\nnetworks and fibring of modal logics was never formally established. In this\npaper, we close this gap by formalizing the idea of fibred models\n\\emph{compatible} with fibred neural networks. Using this correspondence, we\nthen derive non-uniform logical expressiveness results for Graph Neural\nNetworks (GNNs), Graph Attention Networks (GATs) and Transformer encoders.\nLonger-term, the goal of this paper is to open the way for the use of fibring\nas a formalism for interpreting the logical theories learnt by neural networks\nwith the tools of computational logic.",
    "published": "2025-09-28T14:32:42Z",
    "link": "http://arxiv.org/pdf/2509.23912v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ouns El Harzli",
      "Bernardo Cuenca Grau",
      "Artur d'Avila Garcez",
      "Ian Horrocks",
      "Tarek R. Besold"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23906v1",
    "title": "EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in\n  Medical Imaging",
    "summary": "Medical imaging foundation models must adapt over time, yet full retraining\nis often blocked by privacy constraints and cost. We present a continual\nlearning framework that avoids storing patient exemplars by pairing class\nconditional diffusion replay with Elastic Weight Consolidation. Using a compact\nVision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and\nCheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by\nmore than 30\\% relative to DER\\texttt{++}, and approaches joint training at\n0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect\nforgetting to two measurable factors: fidelity of replay and Fisher weighted\nparameter drift, highlighting the complementary roles of replay diffusion and\nsynaptic stability. The results indicate a practical route for scalable,\nprivacy aware continual adaptation of clinical imaging models.",
    "published": "2025-09-28T14:23:46Z",
    "link": "http://arxiv.org/pdf/2509.23906v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Anoushka Harit",
      "William Prew",
      "Zhongtian Sun",
      "Florian Markowetz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23901v1",
    "title": "Interpreting deep learning-based stellar mass estimation via causal\n  analysis and mutual information decomposition",
    "summary": "End-to-end deep learning models fed with multi-band galaxy images are\npowerful data-driven tools used to estimate galaxy physical properties in the\nabsence of spectroscopy. However, due to a lack of interpretability and the\nassociational nature of such models, it is difficult to understand how the\ninformation additional to integrated photometry (e.g., morphology) contributes\nto the estimation task. Improving our understanding in this field would enable\nfurther advances into unraveling the physical connections among galaxy\nproperties and optimizing data exploitation. Therefore, our work is aimed at\ninterpreting the deep learning-based estimation of stellar mass via two\ninterpretability techniques: causal analysis and mutual information\ndecomposition. The former reveals the causal paths between multiple variables\nbeyond nondirectional statistical associations, while the latter quantifies the\nmulticomponent contributions (i.e., redundant, unique, and synergistic) of\ndifferent input data to the stellar mass estimation. Using data from the Sloan\nDigital Sky Survey (SDSS) and the Wide-field Infrared Survey Explorer (WISE),\nwe obtained meaningful results that provide physical interpretations for\nimage-based models. Our work demonstrates the gains from combining deep\nlearning with interpretability techniques, and holds promise in promoting more\ndata-driven astrophysical research (e.g., astrophysical parameter estimations\nand investigations on complex multivariate physical processes).",
    "published": "2025-09-28T14:17:25Z",
    "link": "http://arxiv.org/pdf/2509.23901v1.pdf",
    "category": [
      "astro-ph.IM",
      "astro-ph.GA",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Wei Zhang",
      "Qiufan Lin",
      "Yuan-Sen Ting",
      "Shupei Chen",
      "Hengxin Ruan",
      "Song Li",
      "Yifan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23895v1",
    "title": "Preserving Cross-Modal Stability for Visual Unlearning in Multimodal\n  Scenarios",
    "summary": "Visual modality is the most vulnerable to privacy leakage in real-world\nmultimodal applications like autonomous driving with visual and radar data;\nMachine unlearning removes specific training data from pre-trained models to\naddress privacy leakage, however, existing methods fail to preserve cross-modal\nknowledge and maintain intra-class structural stability of retain data, leading\nto reduced overall and other modalities' performance during visual unlearning;\nto address these challenges, we propose a Cross-modal Contrastive Unlearning\n(CCU) framework, which integrates three key components: (a) selective visual\nunlearning: employing inverse contrastive learning to dissociate visual\nrepresentations from their original semantics, (b) cross-modal knowledge\nretention: preserving other modalities' discriminability through semantic\nconsistency, and (c) dual-set contrastive separation: preserving the model\nperformance via isolation of structural perturbations between the unlearn set\nand retain set; extensive experiments on three datasets demonstrate the\nsuperiority of CCU, and our method achieves a 7.12% accuracy improvement with\nonly 7% of the unlearning time compared to the top-accuracy baseline.",
    "published": "2025-09-28T14:03:37Z",
    "link": "http://arxiv.org/pdf/2509.23895v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23893v1",
    "title": "Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic\n  Forgettings",
    "summary": "Catastrophic forgetting remains a critical challenge in continual learning\nfor large language models (LLMs), where models struggle to retain performance\non historical tasks when fine-tuning on new sequential data without access to\npast datasets. In this paper, we first reveal that the drift of functional\ndirections during the fine-tuning process is a key reason why existing\nregularization-based methods fail in long-term LLM continual learning. To\naddress this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a\nnovel approach that tracks the drift of these functional directions and\ndynamically updates them during the fine-tuning process. Furthermore, by\nadjusting the gradients of new task parameters to be orthogonal to the tracked\nhistorical function directions, our method mitigates interference between new\nand old tasks. Extensive experiments on various LLM continual learning\nbenchmarks demonstrate that this approach outperforms prior methods,\neffectively reducing catastrophic forgetting and providing a robust tool for\ncontinuous LLM fine-tuning. Our code is available at\nhttps://github.com/meloxxxxxx/DOC.",
    "published": "2025-09-28T13:55:05Z",
    "link": "http://arxiv.org/pdf/2509.23893v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "math.OC"
    ],
    "authors": [
      "Zhixin Zhang",
      "Zeming Wei",
      "Meng Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23887v1",
    "title": "Gradient Flow Convergence Guarantee for General Neural Network\n  Architectures",
    "summary": "A key challenge in modern deep learning theory is to explain the remarkable\nsuccess of gradient-based optimization methods when training large-scale,\ncomplex deep neural networks. Though linear convergence of such methods has\nbeen proved for a handful of specific architectures, a united theory still\nevades researchers. This article presents a unified proof for linear\nconvergence of continuous gradient descent, also called gradient flow, while\ntraining any neural network with piecewise non-zero polynomial activations or\nReLU, sigmoid activations. Our primary contribution is a single, general\ntheorem that not only covers architectures for which this result was previously\nunknown but also consolidates existing results under weaker assumptions. While\nour focus is theoretical and our results are only exact in the infinitesimal\nstep size limit, we nevertheless find excellent empirical agreement between the\npredictions of our result and those of the practical step-size gradient descent\nmethod.",
    "published": "2025-09-28T13:52:13Z",
    "link": "http://arxiv.org/pdf/2509.23887v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yash Jakhmola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23886v1",
    "title": "Towards Understanding Subliminal Learning: When and How Hidden Biases\n  Transfer",
    "summary": "Language models can transfer hidden biases during distillation. For example,\na teacher that \"likes owls\" can make its student \"like owls\" too, even when the\ntraining data consists only of lists of numbers. This surprising phenomenon is\ncalled subliminal learning. Subliminal learning can be expected under soft\ndistillation, where the student is trained on the teacher's full next-token\ndistribution. But the fact that this also occurs under hard distillation-where\nthe student only sees sampled tokens-raises a deeper question: when and how\ndoes subliminal learning actually occur? We answer this question through\ncontrolled experiments and mechanistic analysis. Our results show that\nsubliminal learning does not need (global) token entanglement or logit leakage.\nInstead, it comes down to a small set of divergence tokens-rare cases where\nteachers with different biases would predict different tokens. Masking out\nthese tokens mostly removes the hidden bias transfer. Mechanistically,\ndivergence tokens reveal that early layers are critical. Surprisingly,\nfinetuning even a single such early layer is sufficient for subliminal\nlearning. Finally, we find that subliminal learning is fragile. Even small\nchanges, like paraphrasing prompts, are usually sufficient to suppress it.",
    "published": "2025-09-28T13:51:22Z",
    "link": "http://arxiv.org/pdf/2509.23886v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Simon Schrodi",
      "Elias Kempf",
      "Fazl Barez",
      "Thomas Brox"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23885v1",
    "title": "Tunable-Generalization Diffusion Powered by Self-Supervised Contextual\n  Sub-Data for Low-Dose CT Reconstruction",
    "summary": "Current models based on deep learning for low-dose CT denoising rely heavily\non paired data and generalize poorly. Even the more concerned diffusion models\nneed to learn the distribution of clean data for reconstruction, which is\ndifficult to satisfy in medical clinical applications. At the same time,\nself-supervised-based methods face the challenge of significant degradation of\ngeneralizability of models pre-trained for the current dose to expand to other\ndoses. To address these issues, this paper proposes a novel method of\ntunable-generalization diffusion powered by self-supervised contextual sub-data\nfor low-dose CT reconstruction, named SuperDiff. Firstly, a contextual subdata\nsimilarity adaptive sensing strategy is designed for denoising centered on the\nLDCT projection domain, which provides an initial prior for the subsequent\nprogress. Subsequently, the initial prior is used to combine knowledge\ndistillation with a deep combination of latent diffusion models for optimizing\nimage details. The pre-trained model is used for inference reconstruction, and\nthe pixel-level self-correcting fusion technique is proposed for fine-grained\nreconstruction of the image domain to enhance the image fidelity, using the\ninitial prior and the LDCT image as a guide. In addition, the technique is\nflexibly applied to the generalization of upper and lower doses or even unseen\ndoses. Dual-domain strategy cascade for self-supervised LDCT denoising,\nSuperDiff requires only LDCT projection domain data for training and testing.\nFull qualitative and quantitative evaluations on both datasets and real data\nshow that SuperDiff consistently outperforms existing state-of-the-art methods\nin terms of reconstruction and generalization performance.",
    "published": "2025-09-28T13:50:29Z",
    "link": "http://arxiv.org/pdf/2509.23885v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Guoquan Wei",
      "Zekun Zhou",
      "Liu Shi",
      "Wenzhe Shan",
      "Qiegen Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23882v1",
    "title": "Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More:\n  Probing GPT-OSS-20B",
    "summary": "OpenAI's GPT-OSS family provides open-weight language models with explicit\nchain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an\nextensive security evaluation of GPT-OSS-20B that probes the model's behavior\nunder different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a\nsystematic LLM evaluation tool, the study uncovers several failure modes\nincluding quant fever, reasoning blackholes, Schrodinger's compliance,\nreasoning procedure mirage, and chain-oriented prompting. Experiments\ndemonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading\nto severe consequences.",
    "published": "2025-09-28T13:44:37Z",
    "link": "http://arxiv.org/pdf/2509.23882v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Shuyi Lin",
      "Tian Lu",
      "Zikai Wang",
      "Bo Wen",
      "Yibo Zhao",
      "Cheng Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23879v1",
    "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise\n  Applications",
    "summary": "The reliability of Multimodal Large Language Models (MLLMs) in real-world\nsettings is often undermined by sensitivity to irrelevant or distracting visual\ncontext, an aspect not captured by existing evaluation metrics. We introduce\nthe \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and\ninterpretable score for quantifying MLLM robustness to variations in visual\ncontext granularity, measuring performance changes between localized image\npatches and full-image input.\n  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language\nbenchmarks, we find that most leading models remain brittle to background\nnoise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating\nconsistent robustness across tasks. PCRI analysis also highlights how different\nmodel architectures handle and integrate visual context, offering actionable\ndiagnostic insight for both researchers and practitioners.\n  PCRI enables rigorous comparison of context robustness, supporting principled\nmodel selection and guiding the development of future architectures and\ntraining strategies for robust, real-world deployment.",
    "published": "2025-09-28T13:39:57Z",
    "link": "http://arxiv.org/pdf/2509.23879v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "68T50, 68T45",
      "I.2.7; I.2.10; I.4.8; I.4.10; I.4.0"
    ],
    "authors": [
      "Hitesh Laxmichand Patel",
      "Amit Agarwal",
      "Srikant Panda",
      "Hansa Meghwani",
      "Karan Dua",
      "Paul Li",
      "Tao Sheng",
      "Sujith Ravi",
      "Dan Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23878v1",
    "title": "Disentangling Score Content and Performance Style for Joint Piano\n  Rendering and Transcription",
    "summary": "Expressive performance rendering (EPR) and automatic piano transcription\n(APT) are fundamental yet inverse tasks in music information retrieval: EPR\ngenerates expressive performances from symbolic scores, while APT recovers\nscores from performances. Despite their dual nature, prior work has addressed\nthem independently. In this paper we propose a unified framework that jointly\nmodels EPR and APT by disentangling note-level score content and global\nperformance style representations from both paired and unpaired data. Our\nframework is built on a transformer-based sequence-to-sequence architecture and\nis trained using only sequence-aligned data, without requiring fine-grained\nnote-level alignment. To automate the rendering process while ensuring\nstylistic compatibility with the score, we introduce an independent\ndiffusion-based performance style recommendation module that generates style\nembeddings directly from score content. This modular component supports both\nstyle transfer and flexible rendering across a range of expressive styles.\nExperimental results from both objective and subjective evaluations demonstrate\nthat our framework achieves competitive performance on EPR and APT tasks, while\nenabling effective content-style disentanglement, reliable style transfer, and\nstylistically appropriate rendering. Demos are available at\nhttps://jointpianist.github.io/epr-apt/",
    "published": "2025-09-28T13:36:33Z",
    "link": "http://arxiv.org/pdf/2509.23878v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Wei Zeng",
      "Junchuan Zhao",
      "Ye Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23876v1",
    "title": "Not All Tokens are Guided Equal: Improving Guidance in Visual\n  Autoregressive Models",
    "summary": "Autoregressive (AR) models based on next-scale prediction are rapidly\nemerging as a powerful tool for image generation, but they face a critical\nweakness: information inconsistencies between patches across timesteps\nintroduced by progressive resolution scaling. These inconsistencies scatter\nguidance signals, causing them to drift away from conditioning information and\nleaving behind ambiguous, unfaithful features. We tackle this challenge with\nInformation-Grounding Guidance (IGG), a novel mechanism that anchors guidance\nto semantically important regions through attention. By adaptively reinforcing\ninformative patches during sampling, IGG ensures that guidance and content\nremain tightly aligned. Across both class-conditioned and text-to-image\ngeneration tasks, IGG delivers sharper, more coherent, and semantically\ngrounded images, setting a new benchmark for AR-based methods.",
    "published": "2025-09-28T13:33:49Z",
    "link": "http://arxiv.org/pdf/2509.23876v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ky Dan Nguyen",
      "Hoang Lam Tran",
      "Anh-Dung Dinh",
      "Daochang Liu",
      "Weidong Cai",
      "Xiuying Wang",
      "Chang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23874v1",
    "title": "Multi-Value-Product Retrieval-Augmented Generation for Industrial\n  Product Attribute Value Identification",
    "summary": "Identifying attribute values from product profiles is a key task for\nimproving product search, recommendation, and business analytics on e-commerce\nplatforms, which we called Product Attribute Value Identification (PAVI) .\nHowever, existing PAVI methods face critical challenges, such as cascading\nerrors, inability to handle out-of-distribution (OOD) attribute values, and\nlack of generalization capability. To address these limitations, we introduce\nMulti-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the\nstrengths of retrieval, generation, and classification paradigms. MVP-RAG\ndefines PAVI as a retrieval-generation task, where the product title\ndescription serves as the query, and products and attribute values act as the\ncorpus. It first retrieves similar products of the same category and candidate\nattribute values, and then generates the standardized attribute values. The key\nadvantages of this work are: (1) the proposal of a multi-level retrieval\nscheme, with products and attribute values as distinct hierarchical levels in\nPAVI domain (2) attribute value generation of large language model to\nsignificantly alleviate the OOD problem and (3) its successful deployment in a\nreal-world industrial environment. Extensive experimental results demonstrate\nthat MVP-RAG performs better than the state-of-the-art baselines.",
    "published": "2025-09-28T13:29:20Z",
    "link": "http://arxiv.org/pdf/2509.23874v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Huike Zou",
      "Haiyang Yang",
      "Yindu Su",
      "Liyu Chen",
      "Chengbao Lian",
      "Qingheng Zhang",
      "Shuguang Han",
      "Jufeng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23871v1",
    "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor\n  Attack",
    "summary": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR.",
    "published": "2025-09-28T13:24:46Z",
    "link": "http://arxiv.org/pdf/2509.23871v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yukun Chen",
      "Boheng Li",
      "Yu Yuan",
      "Leyi Qi",
      "Yiming Li",
      "Tianwei Zhang",
      "Zhan Qin",
      "Kui Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23870v1",
    "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL",
    "summary": "Building autonomous agents capable of solving long-horizon, real-world tasks\nhas garnered significant research interest. But outcome based rewards may cause\nreward miscalibration which means it might mistakenly allocate positive reward\nto flawed middle steps which is regarded as the key reason making the bad\nactions being reinforced during training. However we reveal that outcome based\nreward ensures expected negative advantage for those flawed middle steps, which\nmeans the flawed actions should be punished during training. Even accounting\nfor the ``squeezing effect\", the probability mass of good actions should\nincrease and the actor should gradually get rid of harmful actions. This shows\nthat flawed actions should be punished during training. We further identify\ngradient coupling between similar samples as a key issue in agentic RL, the\ninput prompt is extremely similar and the output action space is limited,\ntherefore during training, gradients from well-performing samples can\ninadvertently strengthen suboptimal or incorrect actions due to similar input\nobservation and output actions. We show that with gradient coupling, some\nflawed actions might be enhanced. To address this, we propose training the\nactor to classify good or bad actions to separate the embedding of good/bad\nactions and alleviate the gradient interference, extensive experiments shows\nits effectiveness.",
    "published": "2025-09-28T13:24:38Z",
    "link": "http://arxiv.org/pdf/2509.23870v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jingyu Liu",
      "Xiaopeng Wu",
      "Jingquan Peng",
      "Kehan Chen",
      "Chuan Yu",
      "Lizhong Ding",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23866v1",
    "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation",
    "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.",
    "published": "2025-09-28T13:19:20Z",
    "link": "http://arxiv.org/pdf/2509.23866v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Pengxiang Li",
      "Zechen Hu",
      "Zirui Shang",
      "Jingrong Wu",
      "Yang Liu",
      "Hui Liu",
      "Zhi Gao",
      "Chenrui Shi",
      "Bofei Zhang",
      "Zihao Zhang",
      "Xiaochuan Shi",
      "Zedong YU",
      "Yuwei Wu",
      "Xinxiao Wu",
      "Yunde Jia",
      "Liuyu Xiang",
      "Zhaofeng He",
      "Qing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23864v1",
    "title": "AgentGuard: Runtime Verification of AI Agents",
    "summary": "The rapid evolution to autonomous, agentic AI systems introduces significant\nrisks due to their inherent unpredictability and emergent behaviors; this also\nrenders traditional verification methods inadequate and necessitates a shift\ntowards probabilistic guarantees where the question is no longer if a system\nwill fail, but the probability of its failure within given constraints. This\npaper presents AgentGuard, a framework for runtime verification of Agentic AI\nsystems that provides continuous, quantitative assurance through a new paradigm\ncalled Dynamic Probabilistic Assurance. AgentGuard operates as an inspection\nlayer that observes an agent's raw I/O and abstracts it into formal events\ncorresponding to transitions in a state model. It then uses online learning to\ndynamically build and update a Markov Decision Process (MDP) that formally\nmodels the agent's emergent behavior. Using probabilistic model checking, the\nframework then verifies quantitative properties in real-time.",
    "published": "2025-09-28T13:08:50Z",
    "link": "http://arxiv.org/pdf/2509.23864v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Roham Koohestani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23860v1",
    "title": "GSID: Generative Semantic Indexing for E-Commerce Product Understanding",
    "summary": "Structured representation of product information is a major bottleneck for\nthe efficiency of e-commerce platforms, especially in second-hand ecommerce\nplatforms. Currently, most product information are organized based on manually\ncurated product categories and attributes, which often fail to adequately cover\nlong-tail products and do not align well with buyer preference. To address\nthese problems, we propose \\textbf{G}enerative \\textbf{S}emantic\n\\textbf{I}n\\textbf{D}exings (GSID), a data-driven approach to generate product\nstructured representations. GSID consists of two key components: (1)\nPre-training on unstructured product metadata to learn in-domain semantic\nembeddings, and (2) Generating more effective semantic codes tailored for\ndownstream product-centric applications. Extensive experiments are conducted to\nvalidate the effectiveness of GSID, and it has been successfully deployed on\nthe real-world e-commerce platform, achieving promising results on product\nunderstanding and other downstream tasks.",
    "published": "2025-09-28T12:58:05Z",
    "link": "http://arxiv.org/pdf/2509.23860v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Haiyang Yang",
      "Qinye Xie",
      "Qingheng Zhang",
      "Liyu Chen",
      "Huike Zou",
      "Chengbao Lian",
      "Shuguang Han",
      "Fei Huang",
      "Jufeng Chen",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23846v1",
    "title": "Adversarial Diffusion for Robust Reinforcement Learning",
    "summary": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods.",
    "published": "2025-09-28T12:34:35Z",
    "link": "http://arxiv.org/pdf/2509.23846v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Daniele Foffano",
      "Alessio Russo",
      "Alexandre Proutiere"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23836v1",
    "title": "Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain\n  Rules",
    "summary": "E-commerce agents contribute greatly to helping users complete their\ne-commerce needs. To promote further research and application of e-commerce\nagents, benchmarking frameworks are introduced for evaluating LLM agents in the\ne-commerce domain. Despite the progress, current benchmarks lack evaluating\nagents' capability to handle mixed-type e-commerce dialogue and complex domain\nrules. To address the issue, this work first introduces a novel corpus, termed\nMix-ECom, which is constructed based on real-world customer-service dialogues\nwith post-processing to remove user privacy and add CoT process. Specifically,\nMix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce\ndialogue, covering four dialogue types (QA, recommendation, task-oriented\ndialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,\nafter-sales), and 82 e-commerce rules. Furthermore, this work build baselines\non Mix-Ecom and propose a dynamic framework to further improve the performance.\nResults show that current e-commerce agents lack sufficient capabilities to\nhandle e-commerce dialogues, due to the hallucination cased by complex domain\nrules. The dataset will be publicly available.",
    "published": "2025-09-28T12:19:27Z",
    "link": "http://arxiv.org/pdf/2509.23836v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chenyu Zhou",
      "Xiaoming Shi",
      "Hui Qiu",
      "Xiawu Zheng",
      "Haitao Leng",
      "Yankai Jiang",
      "Shaoguo Liu",
      "Tingting Gao",
      "Rongrong Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23835v1",
    "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via\n  Phrase-based Fuzzing",
    "summary": "Large Language Models (LLMs) are widely used for code generation, but they\nface critical security risks when applied to practical production due to\npackage hallucinations, in which LLMs recommend non-existent packages. These\nhallucinations can be exploited in software supply chain attacks, where\nmalicious attackers exploit them to register harmful packages. It is critical\nto test LLMs for package hallucinations to mitigate package hallucinations and\ndefend against potential attacks. Although researchers have proposed testing\nframeworks for fact-conflicting hallucinations in natural language generation,\nthere is a lack of research on package hallucinations. To fill this gap, we\npropose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for\npackage hallucinations. HFUZZER adopts fuzzing technology and guides the model\nto infer a wider range of reasonable information based on phrases, thereby\ngenerating enough and diverse coding tasks. Furthermore, HFUZZER extracts\nphrases from package information or coding tasks to ensure the relevance of\nphrases and code, thereby improving the relevance of generated tasks and code.\nWe evaluate HFUZZER on multiple LLMs and find that it triggers package\nhallucinations across all selected models. Compared to the mutational fuzzing\nframework, HFUZZER identifies 2.60x more unique hallucinated packages and\ngenerates more diverse tasks. Additionally, when testing the model GPT-4o,\nHFUZZER finds 46 unique hallucinated packages. Further analysis reveals that\nfor GPT-4o, LLMs exhibit package hallucinations not only during code generation\nbut also when assisting with environment configuration.",
    "published": "2025-09-28T12:16:43Z",
    "link": "http://arxiv.org/pdf/2509.23835v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Yukai Zhao",
      "Menghan Wu",
      "Xing Hu",
      "Xin Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23822v1",
    "title": "Space Group Conditional Flow Matching",
    "summary": "Inorganic crystals are periodic, highly-symmetric arrangements of atoms in\nthree-dimensional space. Their structures are constrained by the symmetry\noperations of a crystallographic \\emph{space group} and restricted to lie in\nspecific affine subspaces known as \\emph{Wyckoff positions}. The frequency an\natom appears in the crystal and its rough positioning are determined by its\nWyckoff position. Most generative models that predict atomic coordinates\noverlook these symmetry constraints, leading to unrealistically high\npopulations of proposed crystals exhibiting limited symmetry. We introduce\nSpace Group Conditional Flow Matching, a novel generative framework that\nsamples significantly closer to the target population of highly-symmetric,\nstable crystals. We achieve this by conditioning the entire generation process\non a given space group and set of Wyckoff positions; specifically, we define a\nconditionally symmetric noise base distribution and a group-conditioned,\nequivariant, parametric vector field that restricts the motion of atoms to\ntheir initial Wyckoff position. Our form of group-conditioned equivariance is\nachieved using an efficient reformulation of \\emph{group averaging} tailored\nfor symmetric crystals. Importantly, it reduces the computational overhead of\nsymmetrization to a negligible level. We achieve state of the art results on\ncrystal structure prediction and de novo generation benchmarks. We also perform\nrelevant ablations.",
    "published": "2025-09-28T11:51:29Z",
    "link": "http://arxiv.org/pdf/2509.23822v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Omri Puny",
      "Yaron Lipman",
      "Benjamin Kurt Miller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23815v1",
    "title": "A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality\n  Control",
    "summary": "Quality control is a critical aspect of manufacturing, particularly in\nensuring the proper assembly of small components in production lines. Existing\nsolutions often rely on single-view imaging or manual inspection, which are\nprone to errors due to occlusions, restricted perspectives, or lighting\ninconsistencies. These limitations require the installation of additional\ninspection stations, which could disrupt the assembly line and lead to\nincreased downtime and costs. This paper introduces a novel multi-view quality\ncontrol module designed to address these challenges, integrating a multi-camera\nimaging system with advanced object detection algorithms. By capturing images\nfrom three camera views, the system provides comprehensive visual coverage of\ncomponents of an assembly process. A tailored image fusion methodology combines\nresults from multiple views, effectively resolving ambiguities and enhancing\ndetection reliability. To support this system, we developed a unique dataset\ncomprising annotated images across diverse scenarios, including varied lighting\nconditions, occlusions, and angles, to enhance applicability in real-world\nmanufacturing environments. Experimental results show that our approach\nsignificantly outperforms single-view methods, achieving high precision and\nrecall rates in the identification of improperly fastened small assembly parts\nsuch as screws. This work contributes to industrial automation by overcoming\nsingle-view limitations, and providing a scalable, cost-effective, and accurate\nquality control mechanism that ensures the reliability and safety of the\nassembly line. The dataset used in this study is publicly available to\nfacilitate further research in this domain.",
    "published": "2025-09-28T11:37:48Z",
    "link": "http://arxiv.org/pdf/2509.23815v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T45",
      "I.4.8; I.4.1; I.2.10"
    ],
    "authors": [
      "Ali Nazeri",
      "Shashank Mishra",
      "Achim Wagner",
      "Martin Ruskowski",
      "Didier Stricker",
      "Jason Rambach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23813v1",
    "title": "IndexNet: Timestamp and Variable-Aware Modeling for Time Series\n  Forecasting",
    "summary": "Multivariate time series forecasting (MTSF) plays a vital role in a wide\nrange of real-world applications, such as weather prediction and traffic flow\nforecasting. Although recent advances have significantly improved the modeling\nof temporal dynamics and inter-variable dependencies, most existing methods\noverlook index-related descriptive information, such as timestamps and variable\nindices, which carry rich contextual semantics. To unlock the potential of such\ninformation and take advantage of the lightweight and powerful periodic capture\nability of MLP-based architectures, we propose IndexNet, an MLP-based framework\naugmented with an Index Embedding (IE) module. The IE module consists of two\nkey components: Timestamp Embedding (TE) and Channel Embedding (CE).\nSpecifically, TE transforms timestamps into embedding vectors and injects them\ninto the input sequence, thereby improving the model's ability to capture\nlong-term complex periodic patterns. In parallel, CE assigns each variable a\nunique and trainable identity embedding based on its index, allowing the model\nto explicitly distinguish between heterogeneous variables and avoid homogenized\npredictions when input sequences seem close. Extensive experiments on 12\ndiverse real-world datasets demonstrate that IndexNet achieves comparable\nperformance across mainstream baselines, validating the effectiveness of our\ntemporally and variably aware design. Moreover, plug-and-play experiments and\nvisualization analyses further reveal that IndexNet exhibits strong generality\nand interpretability, two aspects that remain underexplored in current MTSF\nresearch.",
    "published": "2025-09-28T11:30:17Z",
    "link": "http://arxiv.org/pdf/2509.23813v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Beiliang Wu",
      "Peiyuan Liu",
      "Yifan Hu",
      "Luyan Zhang",
      "Ao Hu",
      "Zenglin Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23812v1",
    "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large\n  Language Models",
    "summary": "Unit testing is essential for software quality assurance, yet writing and\nmaintaining tests remains time-consuming and error-prone. To address this\nchallenge, researchers have proposed various techniques for automating unit\ntest generation, including traditional heuristic-based methods and more recent\napproaches that leverage large language models (LLMs). However, these existing\napproaches are inherently path-insensitive because they rely on fixed\nheuristics or limited contextual information and fail to reason about deep\ncontrol-flow structures. As a result, they often struggle to achieve adequate\ncoverage, particularly for deep or complex execution paths. In this work, we\npresent a path-sensitive framework, JUnitGenie, to fill this gap by combining\ncode knowledge with the semantic capabilities of LLMs in guiding context-aware\nunit test generation. After extracting code knowledge from Java projects,\nJUnitGenie distills this knowledge into structured prompts to guide the\ngeneration of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex\nfocal methods from ten real-world Java projects. The results show that\nJUnitGenie generates valid tests and improves branch and line coverage by\n29.60% and 31.00% on average over both heuristic and LLM-based baselines. We\nfurther demonstrate that the generated test cases can uncover real-world bugs,\nwhich were later confirmed and fixed by developers.",
    "published": "2025-09-28T11:29:57Z",
    "link": "http://arxiv.org/pdf/2509.23812v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Dianshu Liao",
      "Xin Yin",
      "Shidong Pan",
      "Chao Ni",
      "Zhenchang Xing",
      "Xiaoyu Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23811v1",
    "title": "AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through\n  Automated Question Generation and Interactive Assessment",
    "summary": "We propose AnveshanaAI, an application-based learning platform for artificial\nintelligence. With AnveshanaAI, learners are presented with a personalized\ndashboard featuring streaks, levels, badges, and structured navigation across\ndomains such as data science, machine learning, deep learning, transformers,\ngenerative AI, large language models, and multimodal AI, with scope to include\nmore in the future. The platform incorporates gamified tracking with points and\nachievements to enhance engagement and learning, while switching between\nPlayground, Challenges, Simulator, Dashboard, and Community supports\nexploration and collaboration. Unlike static question repositories used in\nexisting platforms, AnveshanaAI ensures balanced learning progression through a\ndataset grounded in Bloom's taxonomy, with semantic similarity checks and\nexplainable AI techniques improving transparency and reliability. Adaptive,\nautomated, and domain-aware assessment methods are also employed. Experiments\ndemonstrate broad dataset coverage, stable fine-tuning with reduced perplexity,\nand measurable gains in learner engagement. Together, these features illustrate\nhow AnveshanaAI integrates adaptivity, gamification, interactivity, and\nexplainability to support next-generation AI education.",
    "published": "2025-09-28T11:24:22Z",
    "link": "http://arxiv.org/pdf/2509.23811v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Rakesh Thakur",
      "Diksha Khandelwal",
      "Shreya Tiwari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23809v1",
    "title": "Tequila: Trapping-free Ternary Quantization for Large Language Models",
    "summary": "Quantization techniques are essential for the deployment of Large Language\nModels (LLMs) on edge devices. However, prevailing methods often rely on\nmixed-precision multiplication that lacks efficient hardware support, making it\nnot feasible. Ternary weight quantization addresses this by constraining\nweights to {-1, 0, 1}, replacing expensive multiplications with\nhardware-efficient additions. However, such aggressive compression leads to\nsignificant accuracy degradation, even after costly quantization-aware training\nwith massive data. We identify the core issue as deadzone trapping: a large\nnumber of weights are trapped at the deadzone boundary. This occurs because\nthese weights receive only noisy, uninformative gradients, preventing stable\nescape from the deadzone and severely impeding model capacity and optimization.\nTo address this issue, we propose Tequila, a trapping-free quantization\noptimization method that reactivates deadzone-trapped weights by repurposing\nthem as dynamic biases. This allows the repurposed weights to provide a\ncontinuous signal in the forward pass and, critically, receive direct,\nmeaningful gradient signals during backpropagation, thereby enhancing model\ncapacity and optimization with nearly zero inference overhead. Extensive\nevaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)\nternary quantization methods across five benchmarks. Specifically, on the ARC\nbenchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly\nmatching full-precision performance (within <1% gap) with a 3.0x inference\nspeedup. Consequently, Tequila offers a highly practical and efficient\nimplementation for the deployment of advanced LLMs in resource-constrained\nenvironments. The code is available at https://github.com/Tencent/AngelSlim.",
    "published": "2025-09-28T11:17:40Z",
    "link": "http://arxiv.org/pdf/2509.23809v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hong Huang",
      "Decheng Wu",
      "Rui Cen",
      "Guanghua Yu",
      "Zonghang Li",
      "Kai Liu",
      "Jianchen Zhu",
      "Peng Chen",
      "Xue Liu",
      "Dapeng Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23803v1",
    "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image\n  Analysis with Server-Client LLM Agents",
    "summary": "Federated learning (FL) allows collaborative model training across healthcare\nsites without sharing sensitive patient data. However, real-world FL deployment\nis often hindered by complex operational challenges that demand substantial\nhuman efforts. This includes: (a) selecting appropriate clients (hospitals),\n(b) coordinating between the central server and clients, (c) client-level data\npre-processing, (d) harmonizing non-standardized data and labels across\nclients, and (e) selecting FL algorithms based on user instructions and\ncross-client data characteristics. However, the existing FL works overlook\nthese practical orchestration challenges. These operational bottlenecks\nmotivate the need for autonomous, agent-driven FL systems, where intelligent\nagents at each hospital client and the central server agent collaboratively\nmanage FL setup and model training with minimal human intervention. To this\nend, we first introduce an agent-driven FL framework that captures key phases\nof real-world FL workflows from client selection to training completion and a\nbenchmark dubbed FedAgentBench that evaluates the ability of LLM agents to\nautonomously coordinate healthcare FL. Our framework incorporates 40 FL\nalgorithms, each tailored to address diverse task-specific requirements and\ncross-client characteristics. Furthermore, we introduce a diverse set of\ncomplex tasks across 201 carefully curated datasets, simulating 6\nmodality-specific real-world healthcare environments, viz., Dermatoscopy,\nUltrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic\nperformance of 14 open-source and 10 proprietary LLMs spanning small, medium,\nand large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3\ncan automate various stages of the FL pipeline, our results reveal that more\ncomplex, interdependent tasks based on implicit goals remain challenging for\neven the strongest models.",
    "published": "2025-09-28T11:06:07Z",
    "link": "http://arxiv.org/pdf/2509.23803v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.MA"
    ],
    "authors": [
      "Pramit Saha",
      "Joshua Strong",
      "Divyanshu Mishra",
      "Cheng Ouyang",
      "J. Alison Noble"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23799v1",
    "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector\n  Refinement",
    "summary": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs.",
    "published": "2025-09-28T10:49:22Z",
    "link": "http://arxiv.org/pdf/2509.23799v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Anyi Wang",
      "Xuansheng Wu",
      "Dong Shu",
      "Yunpu Ma",
      "Ninghao Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23796v1",
    "title": "From Frustration to Fun: An Adaptive Problem-Solving Puzzle Game Powered\n  by Genetic Algorithm",
    "summary": "This paper explores adaptive problem solving with a game designed to support\nthe development of problem-solving skills. Using an adaptive, AI-powered puzzle\ngame, our adaptive problem-solving system dynamically generates\npathfinding-based puzzles using a genetic algorithm, tailoring the difficulty\nof each puzzle to individual players in an online real-time approach. A\nplayer-modeling system records user interactions and informs the generation of\npuzzles to approximate a target difficulty level based on various metrics of\nthe player. By combining procedural content generation with online adaptive\ndifficulty adjustment, the system aims to maintain engagement, mitigate\nfrustration, and maintain an optimal level of challenge. A pilot user study\ninvestigates the effectiveness of this approach, comparing different types of\nadaptive difficulty systems and interpreting players' responses. This work lays\nthe foundation for further research into emotionally informed player models,\nadvanced AI techniques for adaptivity, and broader applications beyond gaming\nin educational settings.",
    "published": "2025-09-28T10:40:14Z",
    "link": "http://arxiv.org/pdf/2509.23796v1.pdf",
    "category": [
      "cs.AI",
      "cs.MM",
      "cs.NE"
    ],
    "authors": [
      "Matthew McConnell",
      "Richard Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23787v1",
    "title": "From Unstable to Playable: Stabilizing Angry Birds Levels via Object\n  Segmentation",
    "summary": "Procedural Content Generation (PCG) techniques enable automatic creation of\ndiverse and complex environments. While PCG facilitates more efficient content\ncreation, ensuring consistently high-quality, industry-standard content remains\na significant challenge. In this research, we propose a method to identify and\nrepair unstable levels generated by existing PCG models. We use Angry Birds as\na case study, demonstrating our method on game levels produced by established\nPCG approaches. Our method leverages object segmentation and visual analysis of\nlevel images to detect structural gaps and perform targeted repairs. We\nevaluate multiple object segmentation models and select the most effective one\nas the basis for our repair pipeline. Experimental results show that our method\nimproves the stability and playability of AI-generated levels. Although our\nevaluation is specific to Angry Birds, our image-based approach is designed to\nbe applicable to a wide range of 2D games with similar level structures.",
    "published": "2025-09-28T10:15:19Z",
    "link": "http://arxiv.org/pdf/2509.23787v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Mahdi Farrokhimaleki",
      "Parsa Rahmati",
      "Richard Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23783v1",
    "title": "Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety\n  Perception",
    "summary": "Existing methods for evaluating the harmfulness of content generated by large\nlanguage models (LLMs) have been well studied. However, approaches tailored to\nmultimodal large language models (MLLMs) remain underdeveloped and lack depth.\nThis work highlights the crucial role of visual information in moderating\ncontent in visual question answering (VQA), a dimension often overlooked in\ncurrent research. To bridge this gap, we introduce Falcon, a large-scale\nvision-language safety dataset containing 57,515 VQA pairs across 13 harm\ncategories. The dataset provides explicit annotations for harmful attributes\nacross images, instructions, and responses, thereby facilitating a\ncomprehensive evaluation of the content generated by MLLMs. In addition, it\nincludes the relevant harm categories along with explanations supporting the\ncorresponding judgments. We further propose FalconEye, a specialized evaluator\nfine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results\ndemonstrate that FalconEye reliably identifies harmful content in complex and\nsafety-critical multimodal dialogue scenarios. It outperforms all other\nbaselines in overall accuracy across our proposed Falcon-test dataset and two\nwidely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as\na practical safety auditing tool for MLLMs.",
    "published": "2025-09-28T10:00:37Z",
    "link": "http://arxiv.org/pdf/2509.23783v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Qi Xue",
      "Minrui Jiang",
      "Runjia Zhang",
      "Xiurui Xie",
      "Pei Ke",
      "Guisong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23781v1",
    "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
    "summary": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)\nexcels in various vision tasks thanks to the rich knowledge and generalization\nability of VLMs. However, recent studies revealed that such fine-tuned VLMs are\nvulnerable to spurious correlations stemming from the subgroup imbalance in the\nfine-tuning datasets. To resolve this issue, we propose Group Context\nOptimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm\nthat enhances the group robustness of fine-tuned VLMs. Its key idea is to\nemploy group-specific text prompts as group representatives serving as multiple\nclassifiers for their target class. The rich semantic knowledge of the text\nencoder of VLM enables the discovery of effective group prompts even for groups\nwith a small number of training samples. Leveraging the group prompts for each\nclass addresses the issues caused by the group-imbalanced training set, such as\nthe neglect of minority groups and the scattered distribution of each class in\nthe embedding space. GroupCoOp achieved the best results on five benchmarks\nacross five CLIP architectures and occasionally outperformed prior methods that\nfine-tune the entire network, despite training only 0.016\\% of the network's\nparameters.",
    "published": "2025-09-28T09:54:30Z",
    "link": "http://arxiv.org/pdf/2509.23781v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Nayeong Kim",
      "Seong Joon Oh",
      "Suha Kwak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23778v1",
    "title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse",
    "summary": "Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of\nMulti-Agent Path Finding (MAPF), where agents are required to sequentially\ncomplete tasks with fixed-location pickup and delivery demands. Although\nlearning-based methods have made progress in MAPD, they often perform poorly in\nwarehouse-like environments with narrow pathways and long corridors when\nrelying only on local observations for distributed decision-making.\nCommunication learning can alleviate the lack of global information but\nintroduce high computational complexity due to point-to-point communication. To\naddress this challenge, we formulate MAPF as a sequence modeling problem and\nprove that path-finding policies under sequence modeling possess\norder-invariant optimality, ensuring its effectiveness in MAPD. Building on\nthis, we propose the Sequential Pathfinder (SePar), which leverages the\nTransformer paradigm to achieve implicit information exchange, reducing\ndecision-making complexity from exponential to linear while maintaining\nefficiency and global awareness. Experiments demonstrate that SePar\nconsistently outperforms existing learning-based methods across various MAPF\ntasks and their variants, and generalizes well to unseen environments.\nFurthermore, we highlight the necessity of integrating imitation learning in\ncomplex maps like warehouses.",
    "published": "2025-09-28T09:48:13Z",
    "link": "http://arxiv.org/pdf/2509.23778v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Zeyuan Zhang",
      "Chaoran Li",
      "Shao Zhang",
      "Ying Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23773v1",
    "title": "Knowledge Homophily in Large Language Models",
    "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.",
    "published": "2025-09-28T09:40:27Z",
    "link": "http://arxiv.org/pdf/2509.23773v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "authors": [
      "Utkarsh Sahu",
      "Zhisheng Qi",
      "Mahantesh Halappanavar",
      "Nedim Lipka",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Yu Zhang",
      "Yao Ma",
      "Yu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23768v1",
    "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical\n  Reaction Condition Reasoning",
    "summary": "The chemical reaction recommendation is to select proper reaction condition\nparameters for chemical reactions, which is pivotal to accelerating chemical\nscience. With the rapid development of large language models (LLMs), there is\ngrowing interest in leveraging their reasoning and planning capabilities for\nreaction condition recommendation. Despite their success, existing methods\nrarely explain the rationale behind the recommended reaction conditions,\nlimiting their utility in high-stakes scientific workflows. In this work, we\npropose ChemMAS, a multi-agent system that reframes condition prediction as an\nevidence-based reasoning task. ChemMAS decomposes the task into mechanistic\ngrounding, multi-channel recall, constraint-aware agentic debate, and rationale\naggregation. Each decision is backed by interpretable justifications grounded\nin chemical knowledge and retrieved precedents. Experiments show that ChemMAS\nachieves 20-35% gains over domain-specific baselines and outperforms\ngeneral-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,\nhuman-trustable rationales, which establishes a new paradigm for explainable AI\nin scientific discovery.",
    "published": "2025-09-28T09:34:35Z",
    "link": "http://arxiv.org/pdf/2509.23768v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Cheng Yang",
      "Jiaxuan Lu",
      "Haiyuan Wan",
      "Junchi Yu",
      "Feiwei Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23767v1",
    "title": "From Personal to Collective: On the Role of Local and Global Memory in\n  LLM Personalization",
    "summary": "Large language model (LLM) personalization aims to tailor model behavior to\nindividual users based on their historical interactions. However, its\neffectiveness is often hindered by two key challenges: the \\textit{cold-start\nproblem}, where users with limited history provide insufficient context for\naccurate personalization, and the \\textit{biasing problem}, where users with\nabundant but skewed history cause the model to overfit to narrow preferences.\nWe identify both issues as symptoms of a common underlying limitation, i.e.,\nthe inability to model collective knowledge across users. To address this, we\npropose a local-global memory framework (LoGo) that combines the personalized\nlocal memory with a collective global memory that captures shared interests\nacross the population. To reconcile discrepancies between these two memory\nsources, we introduce a mediator module designed to resolve conflicts between\nlocal and global signals. Extensive experiments on multiple benchmarks\ndemonstrate that LoGo consistently improves personalization quality by both\nwarming up cold-start users and mitigating biased predictions. These results\nhighlight the importance of incorporating collective knowledge to enhance LLM\npersonalization.",
    "published": "2025-09-28T09:32:18Z",
    "link": "http://arxiv.org/pdf/2509.23767v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zehong Wang",
      "Junlin Wu",
      "ZHaoxuan Tan",
      "Bolian Li",
      "Xianrui Zhong",
      "Zheli Liu",
      "Qingkai Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23765v1",
    "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment\n  for Long-Form Factuality",
    "summary": "Hallucination and factuality deficits remain key obstacles to the reliability\nof large language models (LLMs) in long-form generation. Existing reinforcement\nlearning from human feedback (RLHF) frameworks primarily rely on preference\nrewards, yet they often overlook the model's internal knowledge boundaries,\nexacerbating the so-called \"hallucination tax\". To address this challenge, we\npropose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a\nnovel framework that focuses on the knowledge consistency between the policy\nmodel's expressed knowledge and the base model's parametric knowledge, and\nintroduces a Dual-Fact Alignment mechanism to jointly optimize factual recall\nand precision. Specifically, KLCF leverages pretrained knowledge boundaries to\nconstruct fact checklist, guiding online reinforcement learning to improve\nfactual coverage and recall; simultaneously, it trains a self-assessment module\nbased on the base model's internal knowledge to enhance factual precision\nduring generation. Unlike prior methods that rely on external retrieval or\nheavy verification, our reward design is fully external-knowledge-free and\nlightweight, making KLCF efficient and easily scalable to large-scale training.\nExperimental results demonstrate that KLCF substantially improves factuality\nmetrics across multiple long-form benchmarks and effectively alleviates model\nhallucinations.",
    "published": "2025-09-28T09:23:06Z",
    "link": "http://arxiv.org/pdf/2509.23765v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Junliang Li",
      "Yucheng Wang",
      "Yan Chen",
      "Yu Ran",
      "Ruiqing Zhang",
      "Jing Liu",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23762v1",
    "title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient\n  Sparsity Trail",
    "summary": "Spiking Neural Networks (SNNs) have attracted growing interest in both\ncomputational neuroscience and artificial intelligence, primarily due to their\ninherent energy efficiency and compact memory footprint. However, achieving\nadversarial robustness in SNNs, particularly for vision-related tasks, remains\na nascent and underexplored challenge. Recent studies have proposed leveraging\nsparse gradients as a form of regularization to enhance robustness against\nadversarial perturbations. In this work, we present a surprising finding: under\nspecific architectural configurations, SNNs exhibit natural gradient sparsity\nand can achieve state-of-the-art adversarial defense performance without the\nneed for any explicit regularization. Further analysis reveals a trade-off\nbetween robustness and generalization: while sparse gradients contribute to\nimproved adversarial resilience, they can impair the model's ability to\ngeneralize; conversely, denser gradients support better generalization but\nincrease vulnerability to attacks.",
    "published": "2025-09-28T09:15:33Z",
    "link": "http://arxiv.org/pdf/2509.23762v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Nhan T. Luu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23757v1",
    "title": "Transparent Visual Reasoning via Object-Centric Agent Collaboration",
    "summary": "A central challenge in explainable AI, particularly in the visual domain, is\nproducing explanations grounded in human-understandable concepts. To tackle\nthis, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a\nnovel, inherently interpretable framework built on object-centric\nrepresentations and a transparent multi-agent reasoning process. The\ngame-theoretic reasoning process drives agents to agree on coherent and\ndiscriminative evidence, resulting in a faithful and interpretable\ndecision-making process. We train OCEAN end-to-end and benchmark it against\nstandard visual classifiers and popular posthoc explanation tools like GradCAM\nand LIME across two diagnostic multi-object datasets. Our results demonstrate\ncompetitive performance with respect to state-of-the-art black-box models with\na faithful reasoning process, which was reflected by our user study, where\nparticipants consistently rated OCEAN's explanations as more intuitive and\ntrustworthy.",
    "published": "2025-09-28T09:06:52Z",
    "link": "http://arxiv.org/pdf/2509.23757v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Benjamin Teoh",
      "Ben Glocker",
      "Francesca Toni",
      "Avinash Kori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23756v1",
    "title": "SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk\n  Scoring via Shapley Values",
    "summary": "Interpretable risk scores play a vital role in clinical decision support, yet\ntraditional methods for deriving such scores often rely on manual\npreprocessing, task-specific modeling, and simplified assumptions that limit\ntheir flexibility and predictive power. We present SHAPoint, a novel,\ntask-agnostic framework that integrates the predictive accuracy of gradient\nboosted trees with the interpretability of point-based risk scores. SHAPoint\nsupports classification, regression, and survival tasks, while also inheriting\nvaluable properties from tree-based models, such as native handling of missing\ndata and support for monotonic constraints. Compared to existing frameworks,\nSHAPoint offers superior flexibility, reduced reliance on manual preprocessing,\nand faster runtime performance. Empirical results show that SHAPoint produces\ncompact and interpretable scores with predictive performance comparable to\nstate-of-the-art methods, but at a fraction of the runtime, making it a\npowerful tool for transparent and scalable risk stratification.",
    "published": "2025-09-28T09:05:19Z",
    "link": "http://arxiv.org/pdf/2509.23756v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.6; J.3; H.4.2"
    ],
    "authors": [
      "Tomer D. Meirman",
      "Bracha Shapira",
      "Noa Dagan",
      "Lior S. Rokach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23755v1",
    "title": "Understanding Textual Capability Degradation in Speech LLMs via\n  Parameter Importance Analysis",
    "summary": "The integration of speech into Large Language Models (LLMs) has substantially\nexpanded their capabilities, but often at the cost of weakening their core\ntextual competence. This degradation limits the ability of speech-enabled LLMs\nto fully exploit their pre-trained text-based knowledge. In this work, we\nanalyze the underlying mechanisms of this issue through a focused study of the\nwidely used encoder-adaptor paradigm. We propose an analytical framework based\non parameter importance estimation, which reveals that fine-tuning for speech\nintroduces a textual importance distribution shift: the layer-wise allocation\nof parameters critical to textual reasoning is disrupted. Building on this\ninsight, we investigate two mitigation strategies: layer-wise learning rate\nscheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original\nparameter distribution. Experimental results show that both approaches better\nmaintain textual competence than full fine-tuning, while also improving\ndownstream spoken question answering performance. Furthermore, our analysis\noffers a principled explanation for the effectiveness of the proposed\nmitigation strategies, linking their benefits to the structural properties of\ntextual knowledge in LLMs.",
    "published": "2025-09-28T09:04:40Z",
    "link": "http://arxiv.org/pdf/2509.23755v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chao Wang",
      "Rui-Chen Zheng",
      "Yang Ai",
      "Zhen-Hua Ling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23751v1",
    "title": "PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a\n  novel Adapter block",
    "summary": "Colorectal cancer ranks among the most common and deadly cancers, emphasizing\nthe need for effective early detection and treatment. To address the\nlimitations of traditional colonoscopy, including high miss rates due to polyp\nvariability, we introduce the Pyramid Vision Transformer Adapter Residual\nNetwork (PVTAdpNet). This model integrates a U-Net-style encoder-decoder\nstructure with a Pyramid Vision Transformer backbone, novel residual blocks,\nand adapter-based skip connections. The design enhances feature extraction,\ndense prediction, and gradient flow, supported by squeeze-and-excitation\nattention for improved channel-wise feature refinement. PVTAdpNet achieves\nreal-time, accurate polyp segmentation, demonstrating superior performance on\nbenchmark datasets with high mDice and mIoU scores, making it highly suitable\nfor clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851\nand a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution\npolyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's\ncapability for real-time, accurate performance within familiar distributions.\nThe source code of our network is available at\nhttps://github.com/ayousefinejad/PVTAdpNet.git",
    "published": "2025-09-28T08:55:50Z",
    "link": "http://arxiv.org/pdf/2509.23751v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Arshia Yousefi Nezhad",
      "Helia Aghaei",
      "Hedieh Sajedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23746v1",
    "title": "Poivre: Self-Refining Visual Pointing with Reinforcement Learning",
    "summary": "Visual pointing, which aims to localize a target by predicting its\ncoordinates on an image, has emerged as an important problem in the realm of\nvision-language models (VLMs). Despite its broad applicability, recent\nbenchmarks show that current VLMs still fall far behind human performance on\nthis task. A key limitation is that VLMs are typically required to complete the\npointing task in a single step, akin to asking humans to point at an object\nwithout seeing their own fingers. To address this issue, we propose a simple\nyet effective self-refining procedure: Point, Visualize, then Refine (Poivre).\nThis procedure enables a VLM to first mark its estimated point, then\niteratively refine the coordinates if necessary. Inspired by advances of\nreasoning models in the natural language domain, we employ reinforcement\nlearning (RL) to incentivize this self-refining ability. For the RL training,\nwe design a neat process reward that is not only empirically effective but also\ngrounded in appealing properties. Our trained model, Poivre-7B, sets a new\nstate of the art on Point-Bench, outperforming both proprietary models such as\nGemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To\nsupport future research, we release our training and inference code, dataset,\nand the Poivre-7B checkpoint.",
    "published": "2025-09-28T08:51:47Z",
    "link": "http://arxiv.org/pdf/2509.23746v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wenjie Yang",
      "Zengfeng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23745v1",
    "title": "LocoFormer: Generalist Locomotion via Long-context Adaptation",
    "summary": "Modern locomotion controllers are manually tuned for specific embodiments. We\npresent LocoFormer, a generalist omni-bodied locomotion model that can control\npreviously unseen legged and wheeled robots, even without precise knowledge of\ntheir kinematics. LocoFormer is able to adapt to changes in morphology and\ndynamics at test time. We find that two key choices enable adaptation. First,\nwe train massive scale RL on procedurally generated robots with aggressive\ndomain randomization. Second, in contrast to previous policies that are myopic\nwith short context lengths, we extend context by orders of magnitude to span\nepisode boundaries. We deploy the same LocoFormer to varied robots and show\nrobust control even with large disturbances such as weight change and motor\nfailures. In extreme scenarios, we see emergent adaptation across episodes,\nLocoFormer learns from falls in early episodes to improve control strategies in\nlater ones. We believe that this simple, yet general recipe can be used to\ntrain foundation models for other robotic skills in the future. Videos at\ngeneralist-locomotion.github.io.",
    "published": "2025-09-28T08:50:28Z",
    "link": "http://arxiv.org/pdf/2509.23745v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Min Liu",
      "Deepak Pathak",
      "Ananye Agarwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23744v1",
    "title": "Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal\n  Reasoning",
    "summary": "Multimodal large language models (MLLMs) promise enhanced reasoning by\nintegrating diverse inputs such as text, vision, and audio. Yet cross-modal\nreasoning remains underexplored, with conflicting reports on whether added\nmodalities help or harm performance. These inconsistencies stem from a lack of\ncontrolled evaluation frameworks and analysis of models' internals to isolate\nwhen and why modality interactions support or undermine reasoning. We address\nthis gap through a logic-grounded evaluation framework that categorizes\nmultimodal reasoning into six interaction patterns, varying how facts are\ndistributed across modalities and logically combined. Empirically, additional\nmodalities enhance reasoning only when they provide independent and sufficient\nreasoning paths, while redundant or chained entailment support often hurts\nperformance. Moreover, reasoning degrades in three systematic ways: weaker\nmodalities drag down overall performance, conflicts bias preference toward\ncertain modalities, and joint signals from different modalities fail to be\nintegrated effectively. Therefore, we identify two core failures:\ntask-composition bottleneck, where recognition and reasoning cannot be jointly\nexecuted in one pass, and fusion bottleneck, where early integration introduces\nbias. For further investigation, we find that attention patterns fail to encode\nfact usefulness, but a simple two-step prompting (recognize then reason)\nrestores performance, confirming the task-composition bottleneck. Moreover,\nmodality identity remains recoverable in early layers, and softening attention\nin early fusion improves reasoning, highlighting biased fusion as another\nfailure mode. Overall, our findings show that integration, not perception, is\nthe main barrier to multimodal reasoning, suggesting composition-aware training\nand early fusion control as promising directions.",
    "published": "2025-09-28T08:46:11Z",
    "link": "http://arxiv.org/pdf/2509.23744v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yucheng Wang",
      "Yifan Hou",
      "Aydin Javadov",
      "Mubashara Akhtar",
      "Mrinmaya Sachan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23738v1",
    "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence\n  GUI Tasks",
    "summary": "Autonomous agents for long-sequence Graphical User Interface tasks are\nhindered by sparse rewards and the intractable credit assignment problem. To\naddress these challenges, we introduce GUI-Shepherd, a Process Reward Model\nthat provides dense, step-by-step feedback to guide agents. GUI-Shepherd is\ntrained on a diverse large-scale data set of $52$k interactions that features\nhuman-annotated scores and GPT-4o generated rationales, enabling it to serve\nboth as a reward provider for RL training and as a verifier for inference. As\nfar as we know, we are the first to conduct a systematic study of process\nsupervision in GUI agents, across diverse settings from online long-horizon\ntasks to offline single-step prediction. On the online AndroidWorld benchmark,\nGUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,\nsignificantly outperforming Outcome Reward Model based competitors. When used\nas an inference verifier, it brings $5.1$ points improvements. The benefits\ngeneralize to the offline AndroidControl benchmark, with gains of $2.2$ points\nas a reward provider and $4.3$ points as a verifier. Collectively, our results\nestablish that high-fidelity process supervision is critical for building more\ncapable GUI agents and present a generalizable solution.",
    "published": "2025-09-28T08:35:16Z",
    "link": "http://arxiv.org/pdf/2509.23738v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Cong Chen",
      "Kaixiang Ji",
      "Hao Zhong",
      "Muzhi Zhu",
      "Anzhou Li",
      "Guo Gan",
      "Ziyuan Huang",
      "Cheng Zou",
      "Jiajia Liu",
      "Jingdong Chen",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23736v1",
    "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and\n  Generation",
    "summary": "In this work, we present HieraTok, a novel multi-scale Vision Transformer\n(ViT)-based tokenizer that overcomes the inherent limitation of modeling\nsingle-scale representations. This is realized through two key designs: (1)\nmulti-scale downsampling applied to the token map generated by the tokenizer\nencoder, producing a sequence of multi-scale tokens, and (2) a scale-causal\nattention mechanism that enables the progressive flow of information from\nlow-resolution global semantic features to high-resolution structural details.\nCoupling these designs, HieraTok achieves significant improvements in both\nimage reconstruction and generation tasks. Under identical settings, the\nmulti-scale visual tokenizer outperforms its single-scale counterpart by a\n27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into\ndownstream generation frameworks, it achieves a $1.38\\times$ faster convergence\nrate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be\nattributed to the smoother and more uniformly distributed latent space.\nFurthermore, by scaling up the tokenizer's training, we demonstrate its\npotential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To\nthe best of our knowledge, we are the first to introduce multi-scale ViT-based\ntokenizer in image reconstruction and image generation. We hope our findings\nand designs advance the ViT-based tokenizers in visual generation tasks.",
    "published": "2025-09-28T08:30:26Z",
    "link": "http://arxiv.org/pdf/2509.23736v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Cong Chen",
      "Ziyuan Huang",
      "Cheng Zou",
      "Muzhi Zhu",
      "Kaixiang Ji",
      "Jiajia Liu",
      "Jingdong Chen",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23735v1",
    "title": "Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems:\n  Dataset, Taxonomy, and Benchmark",
    "summary": "Agentic systems consisting of multiple LLM-driven agents coordinating through\ntools and structured interactions, are increasingly deployed for complex\nreasoning and problem-solving tasks. At the same time, emerging low-code and\ntemplate-based agent development platforms (e.g., Dify) enable users to rapidly\nbuild and orchestrate agentic systems, which we refer to as\nplatform-orchestrated agentic systems. However, these systems are also fragile\nand it remains unclear how to systematically identify their potential failure\nroot cause. This paper presents a study of root cause identification of these\nplatform-orchestrated agentic systems. To support this initiative, we construct\na dataset AgentFail containing 307 failure logs from ten agentic systems, each\nwith fine-grained annotations linking failures to their root causes. We\nadditionally utilize counterfactual reasoning-based repair strategy to ensure\nthe reliability of the annotation. Building on the dataset, we develop a\ntaxonomy that characterizes failure root causes and analyze their distribution\nacross different platforms and task domains. Furthermore, we introduce a\nbenchmark that leverages LLMs for automatically identifying root causes, in\nwhich we also utilize the proposed taxonomy as guidance for LLMs. Results show\nthat the taxonomy can largely improve the performance, thereby confirming its\nutility. Nevertheless, the accuracy of root cause identification reaches at\nmost 33.6%, which indicates that this task still remains challenging. In light\nof these results, we also provide actionable guidelines for building such\nagentic systems. In summary, this paper provides a reliable dataset of failure\nroot cause for platform-orchestrated agentic systems, corresponding taxonomy\nand benchmark, which serves as a foundation for advancing the development of\nmore reliable agentic systems.",
    "published": "2025-09-28T08:30:03Z",
    "link": "http://arxiv.org/pdf/2509.23735v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Xuyan Ma",
      "Xiaofei Xie",
      "Yawen Wang",
      "Junjie Wang",
      "Boyu Wu",
      "Mingyang Li",
      "Qing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23730v1",
    "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance",
    "summary": "Large language models (LLMs) have recently advanced in reasoning when\noptimized with reinforcement learning (RL) under verifiable rewards. Existing\nmethods primarily rely on outcome-based supervision to strengthen internal LLM\nreasoning, often leading to inefficient exploration and sparse rewards. To\nmitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a\nnovel RL framework that enhances exploration by incorporating multi-turn\ninteractions with external experts during training. Unlike prior methods, where\npolicies reason in isolation, EAPO incentivizes the policy to adaptively\ndetermine when and how to consult experts, yielding richer reward signals and\nmore reliable reasoning trajectories. External assistance ultimately\ninternalizes expert knowledge into the policy model, amplifying the model's\ninherent reasoning capabilities. During evaluation, the policy model has been\nwell-optimized to solve questions independently, producing improved reasoning\npaths and more accurate solutions. Experiments on mathematical reasoning\nbenchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO\nconsistently outperforms expert-assisted workflow, expert-distilled models, and\nRL baselines, with an average gain of 5 points over self-exploratory models.",
    "published": "2025-09-28T08:20:22Z",
    "link": "http://arxiv.org/pdf/2509.23730v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Siyao Song",
      "Cong Ma",
      "Zhihao Cheng",
      "Shiye Lei",
      "Minghao Li",
      "Ying Zeng",
      "Huaixiao Tou",
      "Kai Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23729v1",
    "title": "LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language\n  Models",
    "summary": "Large Language Models (LLMs) with multimodal capabilities have revolutionized\nvision-language tasks, but their deployment often requires huge memory and\ncomputational resources. While post-training quantization (PTQ) has\nsuccessfully compressed language models to as low as 1-bit precision without\nsignificant performance loss, its effectiveness for multimodal LLMs (MLLMs)\nremains relatively unexplored. In this paper, we present the first study on\nultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals\nthat multimodal tokens and intermediate layer activations produced by them\nexhibit significantly higher statistical variance and entropy compared to text\ntokens, making them less tolerant to ultra-low bit quantization. However, the\nactivation distributions of multimodal tokens varies significantly over\ndifferent layers, with some layers having lower entropy activation\ndistributions. We empirically show that such layers in these models can better\ntolerate ultra-low bit quantization. Building on these insights, we propose a\nnovel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit\nQuantization, which selectively applies ultra-low bit quantization to layers\nthat are more resilient to it. Additionally, we also show that using a mix of\nmultimodal tokens (image and text) for PTQ boosts VQA performance in the\nultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL\nacross 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less\nmemory than their 4-bit counterparts, respectively, while exhibiting a\nperformance degradation of less than 10% on the MME benchmark.",
    "published": "2025-09-28T08:20:00Z",
    "link": "http://arxiv.org/pdf/2509.23729v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Shubhang Bhatnagar",
      "Andy Xu",
      "Kar-Han Tan",
      "Narendra Ahuja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23728v1",
    "title": "M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured\n  Descriptions for 3D Generation",
    "summary": "In text-driven 3D scene generation, object layout serves as a crucial\nintermediate representation that bridges high-level language instructions with\ndetailed geometric output. It not only provides a structural blueprint for\nensuring physical plausibility but also supports semantic controllability and\ninteractive editing. However, the learning capabilities of current 3D indoor\nlayout generation models are constrained by the limited scale, diversity, and\nannotation quality of existing datasets. To address this, we introduce\nM3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation.\nM3DLayout comprises 15,080 layouts and over 258k object instances, integrating\nthree distinct sources: real-world scans, professional CAD designs, and\nprocedurally generated scenes. Each layout is paired with detailed structured\ntext describing global scene summaries, relational placements of large\nfurniture, and fine-grained arrangements of smaller items. This diverse and\nrichly annotated resource enables models to learn complex spatial and semantic\npatterns across a wide variety of indoor environments. To assess the potential\nof M3DLayout, we establish a benchmark using a text-conditioned diffusion\nmodel. Experimental results demonstrate that our dataset provides a solid\nfoundation for training layout generation models. Its multi-source composition\nenhances diversity, notably through the Inf3DLayout subset which provides rich\nsmall-object information, enabling the generation of more complex and detailed\nscenes. We hope that M3DLayout can serve as a valuable resource for advancing\nresearch in text-driven 3D scene synthesis.",
    "published": "2025-09-28T08:16:08Z",
    "link": "http://arxiv.org/pdf/2509.23728v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yiheng Zhang",
      "Zhuojiang Cai",
      "Mingdao Wang",
      "Meitong Guo",
      "Tianxiao Li",
      "Li Lin",
      "Yuwang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23727v1",
    "title": "AudioMoG: Guiding Audio Generation with Mixture-of-Guidance",
    "summary": "Guidance methods have demonstrated significant improvements in cross-modal\naudio generation, including text-to-audio (T2A) and video-to-audio (V2A)\ngeneration. The popularly adopted method, classifier-free guidance (CFG),\nsteers generation by emphasizing condition alignment, enhancing fidelity but\noften at the cost of diversity. Recently, autoguidance (AG) has been explored\nfor audio generation, encouraging the sampling to faithfully reconstruct the\ntarget distribution and showing increased diversity. Despite these advances,\nthey usually rely on a single guiding principle, e.g., condition alignment in\nCFG or score accuracy in AG, leaving the full potential of guidance for audio\ngeneration untapped. In this work, we explore enriching the composition of the\nguidance method and present a mixture-of-guidance framework, AudioMoG. Within\nthe design space, AudioMoG can exploit the complementary advantages of\ndistinctive guiding principles by fulfilling their cumulative benefits. With a\nreduced form, AudioMoG can consider parallel complements or recover a single\nguiding principle, without sacrificing generality. We experimentally show that,\ngiven the same inference speed, AudioMoG approach consistently outperforms\nsingle guidance in T2A generation across sampling steps, concurrently showing\nadvantages in V2A, text-to-music, and image generation. These results highlight\na \"free lunch\" in current cross-modal audio generation systems: higher quality\ncan be achieved through mixed guiding principles at the sampling stage without\nsacrificing inference efficiency. Demo samples are available at:\nhttps://audio-mog.github.io.",
    "published": "2025-09-28T08:12:43Z",
    "link": "http://arxiv.org/pdf/2509.23727v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Junyou Wang",
      "Zehua Chen",
      "Binjie Yuan",
      "Kaiwen Zheng",
      "Chang Li",
      "Yuxuan Jiang",
      "Jun Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23725v1",
    "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical\n  Reasoning with Large Language Models",
    "summary": "Answering complex medical questions requires not only domain expertise and\npatient-specific information, but also structured and multi-perspective\nreasoning. Existing multi-agent approaches often rely on fixed roles or shallow\ninteraction prompts, limiting their ability to detect and resolve fine-grained\nlogical inconsistencies. To address this, we propose \\textsc{MedLA}, a\nlogic-driven multi-agent framework built on large language models. Each agent\norganizes its reasoning process into an explicit logical tree based on\nsyllogistic triads (major premise, minor premise, and conclusion), enabling\ntransparent inference and premise-level alignment. Agents engage in a\nmulti-round, graph-guided discussion to compare and iteratively refine their\nlogic trees, achieving consensus through error correction and contradiction\nresolution. We demonstrate that \\textsc{MedLA} consistently outperforms both\nstatic role-based systems and single-agent baselines on challenging benchmarks\nsuch as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA}\nscales effectively across both open-source and commercial LLM backbones,\nachieving state-of-the-art performance and offering a generalizable paradigm\nfor trustworthy medical reasoning.",
    "published": "2025-09-28T08:06:39Z",
    "link": "http://arxiv.org/pdf/2509.23725v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Siqi Ma",
      "Jiajie Huang",
      "Bolin Yang",
      "Fan Zhang",
      "Jinlin Wu",
      "Yue Shen",
      "Guohui Fan",
      "Zhu Zhang",
      "Zelin Zang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23724v1",
    "title": "Video Panels for Long Video Understanding",
    "summary": "Recent Video-Language Models (VLMs) achieve promising results on long-video\nunderstanding, but their performance still lags behind that achieved on tasks\ninvolving images or short videos. This has led to great interest in improving\nthe long context modeling of VLMs by introducing novel modules and additional\ncomplexity. % additional training time. In this paper, we take a different\napproach: rather than fine-tuning VLMs with the limited data available, we\nattempt to maximize the performance of existing models. To this end, we propose\na novel visual prompting strategy specifically designed for long-video\nunderstanding. By combining multiple frames as panels into one image, we\neffectively trade off spatial details for temporal resolution. Our approach is\ntraining-free, parameter-free, and model-agnostic, and can be seamlessly\nintegrated into existing VLMs. Extensive experiments on five established\nbenchmarks across a wide range of model architectures, sizes, and context\nwindows confirm the consistency of our approach. For the TimeScope (Long)\ndataset, which has the longest videos, the accuracy for video question\nanswering is improved by up to 19.4\\%. Overall, our method raises the bar for\nlong video understanding models. We will make our code available upon\nacceptance.",
    "published": "2025-09-28T08:05:55Z",
    "link": "http://arxiv.org/pdf/2509.23724v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Lars Doorenbos",
      "Federico Spurio",
      "Juergen Gall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23722v1",
    "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on\n  Heterogeneous Models",
    "summary": "Pipeline parallelism is widely used to train large language models (LLMs).\nHowever, increasing heterogeneity in model architectures exacerbates pipeline\nbubbles, thereby reducing training efficiency. Existing approaches overlook the\nco-optimization of model partition, model placement, and workload scheduling,\nresulting in limited efficiency improvement or even performance degradation. To\nrespond, we propose AdaPtis, an LLM training system that supports adaptive\npipeline parallelism. First, we develop a pipeline performance model to\naccurately estimate training throughput. Second, AdaPtis jointly optimizes\nmodel partition, model placement, and workload scheduling policies guided by\nthis performance model. Third, we design a unified pipeline executor that\nefficiently supports the execution of diverse pipeline strategies. Extensive\nexperiments show that AdaPtis achieves an average speedup of 1.42x (up to\n2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.",
    "published": "2025-09-28T08:05:13Z",
    "link": "http://arxiv.org/pdf/2509.23722v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Jihu Guo",
      "Tenghui Ma",
      "Wei Gao",
      "Peng Sun",
      "Jiaxing Li",
      "Xun Chen",
      "Yuyang Jin",
      "Dahua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23717v1",
    "title": "Measuring Sparse Autoencoder Feature Sensitivity",
    "summary": "Sparse Autoencoder (SAE) features have become essential tools for mechanistic\ninterpretability research. SAE features are typically characterized by\nexamining their activating examples, which are often \"monosemantic\" and align\nwith human interpretable concepts. However, these examples don't reveal feature\nsensitivity: how reliably a feature activates on texts similar to its\nactivating examples. In this work, we develop a scalable method to evaluate\nfeature sensitivity. Our approach avoids the need to generate natural language\ndescriptions for features; instead we use language models to generate text with\nthe same semantic properties as a feature's activating examples. We then test\nwhether the feature activates on these generated texts. We demonstrate that\nsensitivity measures a new facet of feature quality and find that many\ninterpretable features have poor sensitivity. Human evaluation confirms that\nwhen features fail to activate on our generated text, that text genuinely\nresembles the original activating examples. Lastly, we study feature\nsensitivity at the SAE level and observe that average feature sensitivity\ndeclines with increasing SAE width across 7 SAE variants. Our work establishes\nfeature sensitivity as a new dimension for evaluating both individual features\nand SAE architectures.",
    "published": "2025-09-28T07:58:53Z",
    "link": "http://arxiv.org/pdf/2509.23717v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Claire Tian",
      "Katherine Tian",
      "Nathan Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23711v1",
    "title": "Bridging Discrete and Continuous RL: Stable Deterministic Policy\n  Gradient with Martingale Characterization",
    "summary": "The theory of discrete-time reinforcement learning (RL) has advanced rapidly\nover the past decades. Although primarily designed for discrete environments,\nmany real-world RL applications are inherently continuous and complex. A major\nchallenge in extending discrete-time algorithms to continuous-time settings is\ntheir sensitivity to time discretization, often leading to poor stability and\nslow convergence. In this paper, we investigate deterministic policy gradient\nmethods for continuous-time RL. We derive a continuous-time policy gradient\nformula based on an analogue of the advantage function and establish its\nmartingale characterization. This theoretical foundation leads to our proposed\nalgorithm, CT-DDPG, which enables stable learning with deterministic policies\nin continuous-time environments. Numerical experiments show that the proposed\nCT-DDPG algorithm offers improved stability and faster convergence compared to\nexisting discrete-time and continuous-time methods, across a wide range of\ncontrol tasks with varying time discretizations and noise levels.",
    "published": "2025-09-28T07:53:33Z",
    "link": "http://arxiv.org/pdf/2509.23711v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Ziheng Cheng",
      "Xin Guo",
      "Yufei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23708v1",
    "title": "CrimEdit: Controllable Editing for Counterfactual Object Removal,\n  Insertion, and Movement",
    "summary": "Recent works on object removal and insertion have enhanced their performance\nby handling object effects such as shadows and reflections, using diffusion\nmodels trained on counterfactual datasets. However, the performance impact of\napplying classifier-free guidance to handle object effects across removal and\ninsertion tasks within a unified model remains largely unexplored. To address\nthis gap and improve efficiency in composite editing, we propose CrimEdit,\nwhich jointly trains the task embeddings for removal and insertion within a\nsingle model and leverages them in a classifier-free guidance scheme --\nenhancing the removal of both objects and their effects, and enabling\ncontrollable synthesis of object effects during insertion. CrimEdit also\nextends these two task prompts to be applied to spatially distinct regions,\nenabling object movement (repositioning) within a single denoising step. By\nemploying both guidance techniques, extensive experiments show that CrimEdit\nachieves superior object removal, controllable effect insertion, and efficient\nobject movement without requiring additional training or separate removal and\ninsertion stages.",
    "published": "2025-09-28T07:41:25Z",
    "link": "http://arxiv.org/pdf/2509.23708v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Boseong Jeon",
      "Junghyuk Lee",
      "Jimin Park",
      "Kwanyoung Kim",
      "Jingi Jung",
      "Sangwon Lee",
      "Hyunbo Shim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23695v1",
    "title": "Estimating Time Series Foundation Model Transferability via In-Context\n  Learning",
    "summary": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via\nlarge-scale pre-training, yet fine-tuning remains critical for boosting\nperformance in domains with limited public data. With the growing number of\nTSFMs, efficiently identifying the best model for downstream fine-tuning\nbecomes increasingly challenging. In this work, we introduce TimeTic, a\ntransferability estimation framework that recasts model selection as an\nin-context-learning problem: given observations on known (source) datasets, it\npredicts how a TSFM will perform after fine-tuning on a downstream (target)\ndataset. TimeTic flexibly organizes the observed model-data relationships as\ncontextual information, allowing it to adapt seamlessly to various test-time\nscenarios. Leveraging the natural tabular structure formed by dataset\nmeta-features, model characteristics, and fine-tuned performance, we employ\ntabular foundation models to serve as in-context learners. We further introduce\na novel model characterization based on entropy evolution across model layers,\ncapturing embedding-space distinctions and enabling TimeTic to generalize\nacross arbitrary model sets. We establish a comprehensive benchmark for\ntransferability estimation including 10 datasets, 10 foundation models, and 3\nforecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong\nalignment with actual fine-tuned performance for previously unseen datasets,\nachieving a mean rank correlation of approximately 0.6 and a 30% improvement\ncompared to using zero-shot performance as the transferability score.",
    "published": "2025-09-28T07:07:13Z",
    "link": "http://arxiv.org/pdf/2509.23695v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qingren Yao",
      "Ming Jin",
      "Chengqi Zhang",
      "Chao-Han Huck Yang",
      "Jun Qi",
      "Shirui Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23694v1",
    "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search\n  Agents",
    "summary": "Search agents connect LLMs to the Internet, enabling access to broader and\nmore up-to-date information. However, unreliable search results may also pose\nsafety threats to end users, establishing a new threat surface. In this work,\nwe conduct two in-the-wild experiments to demonstrate both the prevalence of\nlow-quality search results and their potential to misguide agent behaviors. To\ncounter this threat, we introduce an automated red-teaming framework that is\nsystematic, scalable, and cost-efficient, enabling lightweight and harmless\nsafety assessments of search agents. Building on this framework, we construct\nthe SafeSearch benchmark, which includes 300 test cases covering five\ncategories of risks (e.g., misinformation and indirect prompt injection). Using\nthis benchmark, we evaluate three representative search agent scaffolds,\ncovering search workflow, tool-calling, and deep research, across 7 proprietary\nand 8 open-source backend LLMs. Our results reveal substantial vulnerabilities\nof LLM-based search agents: when exposed to unreliable websites, the highest\nASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,\nour analysis highlights the limited effectiveness of common defense practices,\nsuch as reminder prompting. This emphasizes the value of our framework in\npromoting transparency for safer agent development. Our codebase and test cases\nare publicly available: https://github.com/jianshuod/SafeSearch.",
    "published": "2025-09-28T07:05:17Z",
    "link": "http://arxiv.org/pdf/2509.23694v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Jianshuo Dong",
      "Sheng Guo",
      "Hao Wang",
      "Zhuotao Liu",
      "Tianwei Zhang",
      "Ke Xu",
      "Minlie Huang",
      "Han Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23687v1",
    "title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure\n  Multi-UAV ISAC Networks",
    "summary": "Integrated sensing and communication (ISAC) emerges as a key enabler for\nnext-generation applications such as smart cities and autonomous systems. Its\nintegration with unmanned aerial vehicles (UAVs) unlocks new potentials for\nreliable communication and precise sensing in dynamic aerial environments.\nHowever, existing research predominantly treats UAVs as aerial base stations,\noverlooking their role as ISAC users, and fails to leverage large-scale antenna\narrays at terrestrial base stations to enhance security and spectral\nefficiency. This paper propose a secure and spectral efficient ISAC framework\nfor multi-UAV networks, and a two-stage optimization approach is developed to\njointly design hybrid beamforming (HBF), artificial noise (AN) injection, and\nUAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage\nemploys Proximal Policy Optimization (PPO) to optimize digital beamformers and\ntrajectories, and the second stage decomposes the digital solution into analog\nand digital components via low-complexity matrix factorization. Simulation\nresults demonstrate the effectiveness of the proposed framework compared to\nbenchmark schemes.",
    "published": "2025-09-28T06:58:04Z",
    "link": "http://arxiv.org/pdf/2509.23687v1.pdf",
    "category": [
      "eess.SP",
      "cs.AI"
    ],
    "authors": [
      "Runze Dong",
      "Buhong Wang",
      "Cunqian Feng",
      "Jiang Weng",
      "Chen Han",
      "Jiwei Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23678v1",
    "title": "Towards a Comprehensive Scaling Law of Mixture-of-Experts",
    "summary": "Mixture-of-Experts (MoE) models have become the consensus approach for\nenabling parameter-efficient scaling and cost-effective deployment in large\nlanguage models. However, existing scaling laws for dense models are\ninapplicable to MoE models, which stems from three critical challenges: the\nmultiplicity of influencing factors, their intricate coupling relationships and\nthe non-monotonic nature of their performance impacts. They collectively\nnecessitate a fine-grained investigation into MoE-specific scaling laws. In\nthis work, we perform a systematic decomposition of MoE settings, identifying\nfive key factors that influence model performance from both size and structural\nperspectives (data size ($D$), total model size ($N$), activated model size\n($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).\nSpecifically, we design $446$ controlled experiments to characterize their\nmarginal effects, ultimately constructing a comprehensive and precise joint MoE\nscaling law that considers all essential factors. Furthermore, we derive the\ntheoretically optimal and practically efficiency-aware optimal configurations\nfor $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that\nthe optimal settings for $G$ and $S$ are independent of both the model\narchitecture and data size. With the scaling of $N$, the optimal activation\nparameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could\nfunction as an accurate and insightful guidance to facilitate future MoE model\ndesign and training.",
    "published": "2025-09-28T06:35:34Z",
    "link": "http://arxiv.org/pdf/2509.23678v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Guoliang Zhao",
      "Yuhan Fu",
      "Shuaipeng Li",
      "Xingwu Sun",
      "Ruobing Xie",
      "An Wang",
      "Weidong Han",
      "Zhen Yang",
      "Weixuan Sun",
      "Yudong Zhang",
      "Cheng-zhong Xu",
      "Di Wang",
      "Jie Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23676v1",
    "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic\n  Insights into Distilled DeepSeek R1 Models",
    "summary": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside\nfinal answers, yet the extent to which these traces influence answer generation\nremains unclear. In this work, we conduct a three-stage investigation into the\ninterplay between reasoning and answer generation in three distilled DeepSeek\nR1 models. First, through empirical evaluation, we demonstrate that including\nexplicit reasoning consistently improves answer quality across diverse domains.\nSecond, attention analysis reveals that answer tokens attend substantially to\nreasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely\ntracking the reasoning trajectory, including self-reflective cues. Third, we\napply mechanistic interventions using activation patching to assess the\ndependence of answer tokens on reasoning activations. Our results show that\nperturbations to key reasoning tokens can reliably alter the final answers,\nconfirming a directional and functional flow of information from reasoning to\nanswer. These findings deepen our understanding of how LRMs leverage reasoning\ntokens for answer generation, highlighting the functional role of intermediate\nreasoning in shaping model outputs. Our data and code are publicly available at\n\\href{https://aka.ms/R2A-code}{this URL}.",
    "published": "2025-09-28T06:32:21Z",
    "link": "http://arxiv.org/pdf/2509.23676v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jue Zhang",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23673v1",
    "title": "RCI: A Score for Evaluating Global and Local Reasoning in Multimodal\n  Benchmarks",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvision-language benchmarks, yet it remains unclear whether these benchmarks\nassess genuine global reasoning or allow success via localized visual cues.\nExisting evaluation methods do not explicitly measure this distinction,\nhindering effective dataset curation and real-world focused model development.\n  We introduce Region Comprehension Index (RCI), the first model-based score to\ndirectly quantify a dataset's reliance on global versus local visual\ninformation. RCI systematically compares reference-model performance on image\npatches versus full images, revealing if tasks require holistic image\nunderstanding or can be solved with partial or localized visual cues.\n  When applying RCI to 13 widely used multimodal benchmarks, we observed that\nmost of them favor localized reasoning and exhibit significant spatial biases,\nindicating potential risks in real-world applications. RCI equips researchers &\npractitioners with an actionable tool for diagnosing & mitigating these biases,\nenabling the construction of datasets and benchmarks to foster the development\nof robust, enterprise-ready multimodal systems.",
    "published": "2025-09-28T06:26:11Z",
    "link": "http://arxiv.org/pdf/2509.23673v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM",
      "68T45, 68T50",
      "I.2.7; I.2.10; I.4.7; I.4.8"
    ],
    "authors": [
      "Amit Agarwal",
      "Hitesh Laxmichand Patel",
      "Srikant Panda",
      "Hansa Meghwani",
      "Jyotika Singh",
      "Karan Dua",
      "Paul Li",
      "Tao Sheng",
      "Sujith Ravi",
      "Dan Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23671v1",
    "title": "Graph Neural Networks with Diversity-aware Neighbor Selection and\n  Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting",
    "summary": "Recently, numerous deep models have been proposed to enhance the performance\nof multivariate time series (MTS) forecasting. Among them, Graph Neural\nNetworks (GNNs)-based methods have shown great potential due to their\ncapability to explicitly model inter-variable dependencies. However, these\nmethods often overlook the diversity of information among neighbors, which may\nlead to redundant information aggregation. In addition, their final prediction\ntypically relies solely on the representation from a single temporal scale. To\ntackle these issues, we propose a Graph Neural Networks (GNNs) with\nDiversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN).\nDIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to\nensure that each variable shares high informational similarity with its\nneighbors while maintaining diversity among neighbors themselves. Furthermore,\na Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust\nthe contributions of prediction results from different temporal scales to the\nfinal forecasting result. Extensive experiments on real-world datasets\ndemonstrate that DIMIGNN consistently outperforms prior methods.",
    "published": "2025-09-28T06:23:43Z",
    "link": "http://arxiv.org/pdf/2509.23671v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jingqi Xu",
      "Guibin Chen",
      "Jingxi Lu",
      "Yuzhang Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23666v1",
    "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and\n  Reliability",
    "summary": "Early-Exit Deep Neural Networks enable adaptive inference by allowing\nprediction at intermediary layers, significantly reducing computational costs\nand latency. Most of the early exit strategies greedily exit a sample at an\nintermediary layer if the confidence in class prediction exceeds a predefined\nthreshold that is set using a static validation set. This is problematic as the\nmodel might be overconfident in a wrong class. Also, they are not robust to\ndistribution shifts encountered in deployment, which can undermine model\ntrustworthiness and accuracy. To address these challenges, we propose UAT that\nadapts the threshold for exit decisions using a Multi-Armed Bandit framework,\nenabling online, unsupervised adjustment of exit decisions. UAT makes decisions\nbased on a new reward function that assesses predictive certainty and its\nreliability to balance computational efficiency and prediction quality while\npenalizing unnecessary late exits. We provide guarantees on risk achieved by\nUAT and validate its performance on diverse tasks spanning vision-language\nunderstanding, text generation, and classification. Our framework demonstrates\nconsistent improvements in speedup (1.70-2.10x) with a minimal performance drop\n(<2%) as compared to full model performance. Our source code is available at\nhttps://github.com/Div290/UAT.",
    "published": "2025-09-28T06:05:24Z",
    "link": "http://arxiv.org/pdf/2509.23666v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Divya Jyoti Bajpai",
      "Manjesh Kumar Hanawal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23665v1",
    "title": "Calibration Meets Reality: Making Machine Learning Predictions\n  Trustworthy",
    "summary": "Post-hoc calibration methods are widely used to improve the reliability of\nprobabilistic predictions from machine learning models. Despite their\nprevalence, a comprehensive theoretical understanding of these methods remains\nelusive, particularly regarding their performance across different datasets and\nmodel architectures. Input features play a crucial role in shaping model\npredictions and, consequently, their calibration. However, the interplay\nbetween feature quality and calibration performance has not been thoroughly\ninvestigated. In this work, we present a rigorous theoretical analysis of\npost-hoc calibration methods, focusing on Platt scaling and isotonic\nregression. We derive convergence guarantees, computational complexity bounds,\nand finite-sample performance metrics for these methods. Furthermore, we\nexplore the impact of feature informativeness on calibration performance\nthrough controlled synthetic experiments. Our empirical evaluation spans a\ndiverse set of real-world datasets and model architectures, demonstrating\nconsistent improvements in calibration metrics across various scenarios. By\nexamining calibration performance under varying feature conditions utilizing\nonly informative features versus complete feature spaces including noise\ndimensions, we provide fundamental insights into the robustness and reliability\nof different calibration approaches. Our findings offer practical guidelines\nfor selecting appropriate calibration methods based on dataset characteristics\nand computational constraints, bridging the gap between theoretical\nunderstanding and practical implementation in uncertainty quantification. Code\nand experimental data are available at:\nhttps://github.com/Ajwebdevs/calibration-analysis-experiments.",
    "published": "2025-09-28T06:04:56Z",
    "link": "http://arxiv.org/pdf/2509.23665v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.PR"
    ],
    "authors": [
      "Kristina P. Sinaga",
      "Arjun S. Nair"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23662v1",
    "title": "Pure Node Selection for Imbalanced Graph Node Classification",
    "summary": "The problem of class imbalance refers to an uneven distribution of quantity\namong classes in a dataset, where some classes are significantly\nunderrepresented compared to others. Class imbalance is also prevalent in\ngraph-structured data. Graph neural networks (GNNs) are typically based on the\nassumption of class balance, often overlooking the issue of class imbalance. In\nour investigation, we identified a problem, which we term the Randomness\nAnomalous Connectivity Problem (RACP), where certain off-the-shelf models are\naffected by random seeds, leading to a significant performance degradation. To\neliminate the influence of random factors in algorithms, we proposed PNS (Pure\nNode Sampling) to address the RACP in the node synthesis stage. Unlike existing\napproaches that design specialized algorithms to handle either quantity\nimbalance or topological imbalance, PNS is a novel plug-and-play module that\noperates directly during node synthesis to mitigate RACP. Moreover, PNS also\nalleviates performance degradation caused by abnormal distribution of node\nneighbors. We conduct a series of experiments to identify what factors are\ninfluenced by random seeds. Experimental results demonstrate the effectiveness\nand stability of our method, which not only eliminates the effect of\nunfavorable random seeds but also outperforms the baseline across various\nbenchmark datasets with different GNN backbones. Data and code are available at\nhttps://github.com/flzeng1/PNS.",
    "published": "2025-09-28T05:53:33Z",
    "link": "http://arxiv.org/pdf/2509.23662v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Fanlong Zeng",
      "Wensheng Gan",
      "Jiayang Wu",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23659v1",
    "title": "Aligning LLMs for Multilingual Consistency in Enterprise Applications",
    "summary": "Large language models (LLMs) remain unreliable for global enterprise\napplications due to substantial performance gaps between high-resource and\nmid/low-resource languages, driven by English-centric pretraining and internal\nreasoning biases. This inconsistency undermines customer experience and\noperational reliability in multilingual settings such as customer support,\ncontent moderation, and information retrieval. Even with advanced\nRetrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy\ndrop in non-English languages compared to English.\n  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,\nleveraging semantically equivalent multilingual data in each training batch to\ndirectly align model outputs across languages. This approach improves\nnon-English accuracy by up to 23.9\\% without compromising English performance,\nmodel reasoning, or retrieval quality. Our method is simple to implement,\nscalable, and integrates seamlessly with existing LLM training \\& deployment\npipelines, enabling more robust and equitable multilingual AI solutions in\nindustry.",
    "published": "2025-09-28T05:51:22Z",
    "link": "http://arxiv.org/pdf/2509.23659v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Amit Agarwal",
      "Hansa Meghwani",
      "Hitesh Laxmichand Patel",
      "Tao Sheng",
      "Sujith Ravi",
      "Dan Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23655v1",
    "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision\n  Language Action models",
    "summary": "Vision-Language-Action (VLA) models offer a pivotal approach to learning\nrobotic manipulation at scale by repurposing large pre-trained\nVision-Language-Models (VLM) to output robotic actions. However, adapting VLMs\nfor robotic domains comes with an unnecessarily high computational cost, which\nwe attribute to the tokenization scheme of visual inputs. In this work, we aim\nto enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric\nTokenization for VLAs. Building on the insights of object-centric\nrepresentation learning, our method introduces an inductive bias towards scene\nobjects and the agent's own visual information. As a result, we find that\nOat-VLA can drastically reduce the number of visual tokens to just a few tokens\nwithout sacrificing performance. We reveal that Oat-VLA converges at least\ntwice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in\ndiverse real-world pick and place tasks.",
    "published": "2025-09-28T05:42:53Z",
    "link": "http://arxiv.org/pdf/2509.23655v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Rokas Bendikas",
      "Daniel Dijkman",
      "Markus Peschl",
      "Sanjay Haresh",
      "Pietro Mazzaglia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23652v1",
    "title": "ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language\n  Models through Agentic Data Synthesis",
    "summary": "While Reinforcement Learning with Verifiable Reward (RLVR) significantly\nadvances image reasoning in Large Vision-Language Models (LVLMs), its\napplication to complex video reasoning remains underdeveloped. This gap stems\nprimarily from a critical data bottleneck: existing datasets lack the\nchallenging, multi-hop questions and high-quality, video-grounded\nChain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address\nthis, we introduce ReWatch, a large-scale dataset built to foster advanced\nvideo reasoning. We propose a novel multi-stage synthesis pipeline to\nsynthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT.\nA core innovation is our Multi-Agent ReAct framework for CoT synthesis, which\nsimulates a human-like \"re-watching\" process to generate video-grounded\nreasoning traces by explicitly modeling information retrieval and verification.\nBuilding on this dataset, we develop ReWatch-R1 by post-training a strong\nbaseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This\nframework incorporates a novel Observation \\& Reasoning (O\\&R) reward mechanism\nthat evaluates both the final answer's correctness and the reasoning's\nalignment with video content, directly penalizing hallucination. Our\nexperiments show that ReWatch-R1 achieves state-of-the-art average performance\non five challenging video reasoning benchmarks.",
    "published": "2025-09-28T05:38:16Z",
    "link": "http://arxiv.org/pdf/2509.23652v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Congzhi Zhang",
      "Zhibin Wang",
      "Yinchao Ma",
      "Jiawei Peng",
      "Yihan Wang",
      "Qiang Zhou",
      "Jun Song",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23639v1",
    "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via\n  Debiasing Pre-trained Text Encoders",
    "summary": "This paper explores a novel lightweight approach LightFair to achieve fair\ntext-to-image diffusion models (T2I DMs) by addressing the adverse effects of\nthe text encoder. Most existing methods either couple different parts of the\ndiffusion model for full-parameter training or rely on auxiliary networks for\ncorrection. They incur heavy training or sampling burden and unsatisfactory\nperformance. Since T2I DMs consist of multiple components, with the text\nencoder being the most fine-tunable and front-end module, this paper focuses on\nmitigating bias by fine-tuning text embeddings. To validate feasibility, we\nobserve that the text encoder's neutral embedding output shows substantial\nskewness across image embeddings of various attributes in the CLIP space. More\nimportantly, the noise prediction network further amplifies this imbalance. To\nfinetune the text embedding, we propose a collaborative distance-constrained\ndebiasing strategy that balances embedding distances to improve fairness\nwithout auxiliary references. However, mitigating bias can compromise the\noriginal generation quality. To address this, we introduce a two-stage\ntext-guided sampling strategy to limit when the debiased text encoder\nintervenes. Extensive experiments demonstrate that LightFair is effective and\nefficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA\ndebiasing at just $1/4$ of the training burden, with virtually no increase in\nsampling burden. The code is available at https://github.com/boyuh/LightFair.",
    "published": "2025-09-28T04:46:39Z",
    "link": "http://arxiv.org/pdf/2509.23639v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Boyu Han",
      "Qianqian Xu",
      "Shilong Bao",
      "Zhiyong Yang",
      "Kangli Zi",
      "Qingming Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23630v1",
    "title": "Game-Oriented ASR Error Correction via RAG-Enhanced LLM",
    "summary": "With the rise of multiplayer online games, real-time voice communication is\nessential for team coordination. However, general ASR systems struggle with\ngaming-specific challenges like short phrases, rapid speech, jargon, and noise,\nleading to frequent errors. To address this, we propose the GO-AEC framework,\nwhich integrates large language models, Retrieval-Augmented Generation (RAG),\nand a data augmentation strategy using LLMs and TTS. GO-AEC includes data\naugmentation, N-best hypothesis-based correction, and a dynamic game knowledge\nbase. Experiments show GO-AEC reduces character error rate by 6.22% and\nsentence error rate by 29.71%, significantly improving ASR accuracy in gaming\nscenarios.",
    "published": "2025-09-28T04:12:07Z",
    "link": "http://arxiv.org/pdf/2509.23630v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yan Jiang",
      "Yongle Luo",
      "Qixian Zhou",
      "Elvis S. Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23629v1",
    "title": "How LLMs Learn to Reason: A Complex Network Perspective",
    "summary": "Training large language models with Reinforcement Learning from Verifiable\nRewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain\npoorly understood, including a two-stage learning curve, V-shaped\nresponse-length trajectories, and a pronounced vulnerability to catastrophic\nforgetting. In this work, we propose that these seemingly disparate phenomena\ncan be explained using a single unifying theory: the model's reasoning process\nmaps to the self-organization of a semantic complex network whose topology\nremains persistently sparse, with the average degree pinned close to two. This\ntopology imposes a fundamental mechanism for forgetting and learning: it first\ndrives the system into a maximally frustrated state where ``skill islands''\nform, slow-learning happens, and forgetting is induced; then it enters a sharp\ngrowth phase where the new skills are ``bolted on'', driven by\nphase-transition-like learning at the web's frontier. Equipped with the theory,\nwe propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an\nSFT-based ``heating'' step at the point of maximal frustration to resolve the\ncompetitive bottleneck and enhance the reasoning capability of the model.\nExperiments on a 1.5B-parameter model demonstrate that the approach outperforms\nstandard RLVR on both in-distribution and out-of-distribution benchmarks. By\nrecasting RLVR from black-box optimization into a predictable process of\nstructural self-organization, our work provides a new physical intuition for\nengineering the emergent reasoning capabilities of future AI systems.",
    "published": "2025-09-28T04:10:37Z",
    "link": "http://arxiv.org/pdf/2509.23629v1.pdf",
    "category": [
      "cs.AI",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.LG",
      "physics.soc-ph"
    ],
    "authors": [
      "Sihan Hu",
      "Xiansheng Cai",
      "Yuan Huang",
      "Zhiyuan Yao",
      "Linfeng Zhang",
      "Pan Zhang",
      "Youjin Deng",
      "Kun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23625v1",
    "title": "RIV: Recursive Introspection Mask Diffusion Vision Language Model",
    "summary": "Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable\nprogress in multimodal understanding tasks. However, these models are unable to\ncorrect errors in generated tokens, meaning they lack self-correction\ncapability. In this paper, we propose Recursive Introspection Mask Diffusion\nVision Language Model (RIV), which equips the model with self-correction\nability through two novel mechanisms. The first is Introspection Training,\nwhere an Introspection Model is introduced to identify errors within generated\nsequences. Introspection Training enables the model to detect not only\ngrammatical and spelling mistakes, but more importantly, logical errors. The\nsecond is Recursive Inference. Beginning with the standard unmasking step, the\nlearned Introspection Model helps to identify errors in the output sequence and\nremask them. This alternating\n($\\text{unmask}\\rightarrow\\text{introspection}\\rightarrow\\text{remask}$)\nprocess is repeated recursively until reliable results are obtained.\nExperimental results on multiple benchmarks demonstrate that the proposed RIV\nachieves state-of-the-art performance, outperforming most existing MDVLMs.",
    "published": "2025-09-28T04:01:46Z",
    "link": "http://arxiv.org/pdf/2509.23625v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "YuQian Li",
      "Limeng Qiao",
      "Lin Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23619v1",
    "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
    "summary": "The prevailing approach to distilling reasoning from Large Language Models\n(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It\nteaches Small Language Models (SLMs) to mimic surface-level patterns rather\nthan the underlying algorithmic structure of thought, resulting in a critical\nlack of logical robustness. We argue that instead of cloning text, distillation\nshould transfer this algorithmic structure directly. We introduce Reasoning\nScaffolding}, a framework that reframes reasoning as a structured generation\nprocess. Our method first abstracts the teacher's thought process into a\nsequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)\nthat act as a scaffold. The student model is then trained via a multi-task\nobjective to both (1)predict the next semantic signal, anticipating the\nreasoning flow, and (2)generate the corresponding step, conditioned on that\nsignal. This multi-task scheme acts as a powerful regularizer, compelling the\nstudent to internalize the computational patterns of coherent reasoning. On a\nsuite of challenging reasoning benchmarks, our method significantly outperforms\nstate-of-the-art distillation in both accuracy and logical consistency,\nproviding a path towards creating smaller models that are genuine reasoners,\nnot just fluent mimics.",
    "published": "2025-09-28T03:49:32Z",
    "link": "http://arxiv.org/pdf/2509.23619v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xiangyu Wen",
      "Junhua Huang",
      "Zeju Li",
      "Min Li",
      "Jianyuan Zhong",
      "Zhijian Xu",
      "Mingxuan Yuan",
      "Yongxiang Huang",
      "Qiang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23618v1",
    "title": "Generalizable Speech Deepfake Detection via Information Bottleneck\n  Enhanced Adversarial Alignment",
    "summary": "Neural speech synthesis techniques have enabled highly realistic speech\ndeepfakes, posing major security risks. Speech deepfake detection is\nchallenging due to distribution shifts across spoofing methods and variability\nin speakers, channels, and recording conditions. We explore learning shared\ndiscriminative features as a path to robust detection and propose Information\nBottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN).\nConfidence-guided adversarial alignment adaptively suppresses attack-specific\nartifacts without erasing discriminative cues, while the information bottleneck\nremoves nuisance variability to preserve transferable features. Experiments on\nASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN\nconsistently outperforms baseline and achieves state-of-the-art performance on\nmany benchmarks.",
    "published": "2025-09-28T03:48:49Z",
    "link": "http://arxiv.org/pdf/2509.23618v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Pu Huang",
      "Shouguang Wang",
      "Siya Yao",
      "Mengchu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23617v1",
    "title": "BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation\n  from OCTA Images",
    "summary": "Structural changes in retinal blood vessels are critical biomarkers for the\nonset and progression of glaucoma and other ocular diseases. However, current\nvessel segmentation approaches largely rely on supervised learning and\nextensive manual annotations, which are costly, error-prone, and difficult to\nobtain in optical coherence tomography angiography. Here we present\nBioVessel-Net, an unsupervised generative framework that integrates vessel\nbiostatistics with adversarial refinement and a radius-guided segmentation\nstrategy. Unlike pixel-based methods, BioVessel-Net directly models vascular\nstructures with biostatistical coherence, achieving accurate and explainable\nvessel extraction without labeled data or high-performance computing. To\nsupport training and evaluation, we introduce RetinaMix, a new benchmark\ndataset of 2D and 3D OCTA images with high-resolution vessel details from\ndiverse populations. Experimental results demonstrate that BioVessel-Net\nachieves near-perfect segmentation accuracy across RetinaMix and existing\ndatasets, substantially outperforming state-of-the-art supervised and\nsemi-supervised methods. Together, BioVessel-Net and RetinaMix provide a\nlabel-free, computationally efficient, and clinically interpretable solution\nfor retinal vessel analysis, with broad potential for glaucoma monitoring,\nblood flow modeling, and progression prediction. Code and dataset are\navailable: https://github.com/VikiXie/SatMar8.",
    "published": "2025-09-28T03:46:20Z",
    "link": "http://arxiv.org/pdf/2509.23617v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Cheng Huang",
      "Weizheng Xie",
      "Fan Gao",
      "Yutong Liu",
      "Ruoling Wu",
      "Zeyu Han",
      "Jingxi Qiu",
      "Xiangxiang Wang",
      "Zhenglin Yang",
      "Hao Wang",
      "Yongbin Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23616v1",
    "title": "GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant\n  Learning",
    "summary": "The class imbalance problem refers to the disproportionate distribution of\nsamples across different classes within a dataset, where the minority classes\nare significantly underrepresented. This issue is also prevalent in\ngraph-structured data. Most graph neural networks (GNNs) implicitly assume a\nbalanced class distribution and therefore often fail to account for the\nchallenges introduced by class imbalance, which can lead to biased learning and\ndegraded performance on minority classes. We identify a quality inconsistency\nproblem in synthesized nodes, which leads to suboptimal performance under graph\nimbalance conditions. To mitigate this issue, we propose GraphIFE (Graph\nInvariant Feature Extraction), a novel framework designed to mitigate quality\ninconsistency in synthesized nodes. Our approach incorporates two key concepts\nfrom graph invariant learning and introduces strategies to strengthen the\nembedding space representation, thereby enhancing the model's ability to\nidentify invariant features. Extensive experiments demonstrate the framework's\nefficiency and robust generalization, as GraphIFE consistently outperforms\nvarious baselines across multiple datasets. The code is publicly available at\nhttps://github.com/flzeng1/GraphIFE.",
    "published": "2025-09-28T03:41:16Z",
    "link": "http://arxiv.org/pdf/2509.23616v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Fanlong Zeng",
      "Wensheng Gan",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23614v1",
    "title": "PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents",
    "summary": "Effective guardrails are essential for safely deploying LLM-based agents in\ncritical applications. Despite recent advances, existing guardrails suffer from\ntwo fundamental limitations: (i) they apply uniform guardrail policies to all\nusers, ignoring that the same agent behavior can harm some users while being\nsafe for others; (ii) they check each response in isolation, missing how risks\nevolve and accumulate across multiple interactions. To solve these issues, we\npropose PSG-Agent, a personalized and dynamic system for LLM-based agents.\nFirst, PSG-Agent creates personalized guardrails by mining the interaction\nhistory for stable traits and capturing real-time states from current queries,\ngenerating user-specific risk thresholds and protection strategies. Second,\nPSG-Agent implements continuous monitoring across the agent pipeline with\nspecialized guards, including Plan Monitor, Tool Firewall, Response Guard,\nMemory Guardian, that track cross-turn risk accumulation and issue verifiable\nverdicts. Finally, we validate PSG-Agent in multiple scenarios including\nhealthcare, finance, and daily life automation scenarios with diverse user\nprofiles. It significantly outperform existing agent guardrails including\nLlamaGuard3 and AGrail, providing an executable and auditable path toward\npersonalized safety for LLM-based agents.",
    "published": "2025-09-28T03:31:59Z",
    "link": "http://arxiv.org/pdf/2509.23614v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yaozu Wu",
      "Jizhou Guo",
      "Dongyuan Li",
      "Henry Peng Zou",
      "Wei-Chieh Huang",
      "Yankai Chen",
      "Zhen Wang",
      "Weizhi Zhang",
      "Yangning Li",
      "Meng Zhang",
      "Renhe Jiang",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23612v1",
    "title": "InteractMove: Text-Controlled Human-Object Interaction Generation in 3D\n  Scenes with Movable Objects",
    "summary": "We propose a novel task of text-controlled human object interaction\ngeneration in 3D scenes with movable objects. Existing human-scene interaction\ndatasets suffer from insufficient interaction categories and typically only\nconsider interactions with static objects (do not change object positions), and\nthe collection of such datasets with movable objects is difficult and costly.\nTo address this problem, we construct the InteractMove dataset for Movable\nHuman-Object Interaction in 3D Scenes by aligning existing human object\ninteraction data with scene contexts, featuring three key characteristics: 1)\nscenes containing multiple movable objects with text-controlled interaction\nspecifications (including same-category distractors requiring spatial and 3D\nscene context understanding), 2) diverse object types and sizes with varied\ninteraction patterns (one-hand, two-hand, etc.), and 3) physically plausible\nobject manipulation trajectories. With the introduction of various movable\nobjects, this task becomes more challenging, as the model needs to identify\nobjects to be interacted with accurately, learn to interact with objects of\ndifferent sizes and categories, and avoid collisions between movable objects\nand the scene. To tackle such challenges, we propose a novel pipeline solution.\nWe first use 3D visual grounding models to identify the interaction object.\nThen, we propose a hand-object joint affordance learning to predict contact\nregions for different hand joints and object parts, enabling accurate grasping\nand manipulation of diverse objects. Finally, we optimize interactions with\nlocal-scene modeling and collision avoidance constraints, ensuring physically\nplausible motions and avoiding collisions between objects and the scene.\nComprehensive experiments demonstrate our method's superiority in generating\nphysically plausible, text-compliant interactions compared to existing\napproaches.",
    "published": "2025-09-28T03:29:15Z",
    "link": "http://arxiv.org/pdf/2509.23612v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xinhao Cai",
      "Minghang Zheng",
      "Xin Jin",
      "Yang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23597v1",
    "title": "Characteristic Root Analysis and Regularization for Linear Time Series\n  Forecasting",
    "summary": "Time series forecasting remains a critical challenge across numerous domains,\nyet the effectiveness of complex models often varies unpredictably across\ndatasets. Recent studies highlight the surprising competitiveness of simple\nlinear models, suggesting that their robustness and interpretability warrant\ndeeper theoretical investigation. This paper presents a systematic study of\nlinear models for time series forecasting, with a focus on the role of\ncharacteristic roots in temporal dynamics. We begin by analyzing the noise-free\nsetting, where we show that characteristic roots govern long-term behavior and\nexplain how design choices such as instance normalization and channel\nindependence affect model capabilities. We then extend our analysis to the\nnoisy regime, revealing that models tend to produce spurious roots. This leads\nto the identification of a key data-scaling property: mitigating the influence\nof noise requires disproportionately large training data, highlighting the need\nfor structural regularization. To address these challenges, we propose two\ncomplementary strategies for robust root restructuring. The first uses rank\nreduction techniques, including Reduced-Rank Regression and Direct Weight Rank\nReduction, to recover the low-dimensional latent dynamics. The second, a novel\nadaptive method called Root Purge, encourages the model to learn a\nnoise-suppressing null space during training. Extensive experiments on standard\nbenchmarks demonstrate the effectiveness of both approaches, validating our\ntheoretical insights and achieving state-of-the-art results in several\nsettings. Our findings underscore the potential of integrating classical\ntheories for linear systems with modern learning techniques to build robust,\ninterpretable, and data-efficient forecasting models.",
    "published": "2025-09-28T03:06:30Z",
    "link": "http://arxiv.org/pdf/2509.23597v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zheng Wang",
      "Kaixuan Zhang",
      "Wanfang Chen",
      "Xiaonan Lu",
      "Longyuan Li",
      "Tobias Schlagenhauf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23596v1",
    "title": "Multi-Level Heterogeneous Knowledge Transfer Network on Forward\n  Scattering Center Model for Limited Samples SAR ATR",
    "summary": "Simulated data-assisted SAR target recognition methods are the research\nhotspot currently, devoted to solving the problem of limited samples. Existing\nworks revolve around simulated images, but the large amount of irrelevant\ninformation embedded in the images, such as background, noise, etc., seriously\naffects the quality of the migrated information. Our work explores a new\nsimulated data to migrate purer and key target knowledge, i.e., forward\nscattering center model (FSCM) which models the actual local structure of the\ntarget with strong physical meaning and interpretability. To achieve this\npurpose, multi-level heterogeneous knowledge transfer (MHKT) network is\nproposed, which fully migrates FSCM knowledge from the feature, distribution\nand category levels, respectively. Specifically, we permit the more suitable\nfeature representations for the heterogeneous data and separate non-informative\nknowledge by task-associated information selector (TAIS), to complete purer\ntarget feature migration. In the distribution alignment, the new metric\nfunction maximum discrimination divergence (MDD) in target generic knowledge\ntransfer (TGKT) module perceives transferable knowledge efficiently while\npreserving discriminative structure about classes. Moreover, category relation\nknowledge transfer (CRKT) module leverages the category relation consistency\nconstraint to break the dilemma of optimization bias towards simulation data\ndue to imbalance between simulated and measured data. Such stepwise knowledge\nselection and migration will ensure the integrity of the migrated FSCM\nknowledge. Notably, extensive experiments on two new datasets formed by FSCM\ndata and measured SAR images demonstrate the superior performance of our\nmethod.",
    "published": "2025-09-28T03:04:04Z",
    "link": "http://arxiv.org/pdf/2509.23596v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chenxi Zhao",
      "Daochang Wang",
      "Siqian Zhang",
      "Gangyao Kuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23595v1",
    "title": "Timber: Training-free Instruct Model Refining with Base via Effective\n  Rank",
    "summary": "Post-training, which elicits a pretrained Base model into the corresponding\nInstruct model, is widely considered to be superficial. In this work, we first\nreinforce this hypothesis by providing novel quantitative evidence from the\nweight level that the effective rank (eRank) remains negligibly changed.\nHowever, this superficiality also suffers a critical trade-off, improving the\nexploitation capabilities at the cost of limiting its exploration. To tackle\nthis issue, we propose Timber, a simple yet effective training-free method that\nenhances the exploration capability of the Instruct model while preserving its\nexploitation. The key insight is to partially revert Instruct towards the\npaired Base model by subtle yet targeted refinement of the weight deltas.\nExtensive experiments on Llama and Qwen series demonstrate that Timber\nconsistently improves vanilla Instruct models, particularly on Pass@k\nperformance. Our findings offer new insights into the post-training stage at\nthe weight level and practical strategies to refine the Instruct model without\ntraining.",
    "published": "2025-09-28T02:59:43Z",
    "link": "http://arxiv.org/pdf/2509.23595v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Taiqiang Wu",
      "Runming Yang",
      "Tao Liu",
      "Jiahao Wang",
      "Zenan Xu",
      "Ngai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23592v1",
    "title": "Toward a Holistic Approach to Continual Model Merging",
    "summary": "We present a holistic framework for continual model merging that intervenes\nat three critical stages: pre-merging, during merging, and post-merging-to\naddress two fundamental challenges in continual learning. In particular,\nconventional approaches either maintain a growing list of per-domain task\nvectors, leading to scalability issues or rely solely on weight-space merging\nwhen old data is inaccessible, thereby losing crucial functional information.\nOur method overcomes these limitations by first fine-tuning the main model\nwithin its tangent space on domain-specific data; this linearization amplifies\nper-task weight disentanglement, effectively mitigating across-task\ninterference. During merging, we leverage functional information from available\noptimizer states beyond mere parameter averages to avoid the need to revisit\nold data. Finally, a post-merging correction aligns the representation\ndiscrepancy between pre- and post-merged models, reducing bias and enhancing\noverall performance-all while operating under constant memory constraints\nwithout accessing historical data. Extensive experiments on standard\nclass-incremental and domain-incremental benchmarks demonstrate that our\napproach not only achieves competitive performance but also provides a scalable\nand efficient solution to the catastrophic forgetting problem.",
    "published": "2025-09-28T02:51:04Z",
    "link": "http://arxiv.org/pdf/2509.23592v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hoang Phan",
      "Sungmin Cha",
      "Tung Lam Tran",
      "Qi Lei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23589v1",
    "title": "BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning\n  in Autonomous Driving",
    "summary": "Diffusion-based planners have shown great promise for autonomous driving due\nto their ability to capture multi-modal driving behaviors. However, guiding\nthese models effectively in reactive, closed-loop environments remains a\nsignificant challenge. Simple conditioning often fails to provide sufficient\nguidance in complex and dynamic driving scenarios. Recent work attempts to use\ntypical expert driving behaviors (i.e., anchors) to guide diffusion models but\nrelies on a truncated schedule, which introduces theoretical inconsistencies\nand can compromise performance. To address this, we introduce BridgeDrive, a\nnovel anchor-guided diffusion bridge policy for closed-loop trajectory\nplanning. Our approach provides a principled diffusion framework that\neffectively translates anchors into fine-grained trajectory plans,\nappropriately responding to varying traffic conditions. Our planner is\ncompatible with efficient ODE solvers, a critical factor for real-time\nautonomous driving deployment. We achieve state-of-the-art performance on the\nBench2Drive benchmark, improving the success rate by 5% over prior arts.",
    "published": "2025-09-28T02:47:12Z",
    "link": "http://arxiv.org/pdf/2509.23589v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shu Liu",
      "Wenlin Chen",
      "Weihao Li",
      "Zheng Wang",
      "Lijin Yang",
      "Jianing Huang",
      "Yipin Zhang",
      "Zhongzhan Huang",
      "Ze Cheng",
      "Hao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23586v1",
    "title": "Improving the Efficiency of LLM Agent Systems through Trajectory\n  Reduction",
    "summary": "Multi-turn agent systems based on Large Language Models (LLMs) have been\nincreasingly popular for software engineering tasks. While LLM agents show\ndecent effectiveness, the high computational cost of input tokens due to the\never-growing trajectory remains an efficiency concern for their applications.\nEfficiency is largely neglected in existing studies and agent products, and\nthis paper fills the gap by introducing an inference-time trajectory reduction\napproach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless,\nredundant, and expired information is widespread in all trajectories, which can\nbe identified and reduced without harming the agent's performance. We then\ndesign a simple yet effective trajectory reduction approach, AgentDiet, which\nautomatically removes such waste information. We implement AgentDiet on a\ntop-performing coding agent, and the evaluation on two LLMs and two benchmarks\nshows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final\ncomputational cost by 21.1% ~ 35.9%, while maintaining the same agent\nperformance. This indicates that trajectory reduction is a promising direction\nfor agent systems.",
    "published": "2025-09-28T02:43:41Z",
    "link": "http://arxiv.org/pdf/2509.23586v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Yuan-An Xiao",
      "Pengfei Gao",
      "Chao Peng",
      "Yingfei Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23577v1",
    "title": "ML-Asset Management: Curation, Discovery, and Utilization",
    "summary": "Machine learning (ML) assets, such as models, datasets, and metadata, are\ncentral to modern ML workflows. Despite their explosive growth in practice,\nthese assets are often underutilized due to fragmented documentation, siloed\nstorage, inconsistent licensing, and lack of unified discovery mechanisms,\nmaking ML-asset management an urgent challenge. This tutorial offers a\ncomprehensive overview of ML-asset management activities across its lifecycle,\nincluding curation, discovery, and utilization. We provide a categorization of\nML assets, and major management issues, survey state-of-the-art techniques, and\nidentify emerging opportunities at each stage. We further highlight\nsystem-level challenges related to scalability, lineage, and unified indexing.\nThrough live demonstrations of systems, this tutorial equips both researchers\nand practitioners with actionable insights and practical tools for advancing\nML-asset management in real-world and domain-specific settings.",
    "published": "2025-09-28T02:14:33Z",
    "link": "http://arxiv.org/pdf/2509.23577v1.pdf",
    "category": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Mengying Wang",
      "Moming Duan",
      "Yicong Huang",
      "Chen Li",
      "Bingsheng He",
      "Yinghui Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23574v1",
    "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for\n  Better Performance with Fewer Rationales",
    "summary": "Chain-of-thought (CoT) distillation aims to enhance small language models'\n(SLMs) reasoning by transferring multi-step reasoning capability from the\nlarger teacher models. However, existing work underestimates rationale quality,\nfocusing primarily on data quantity, which may transfer noisy or incorrect\ninformation to the student model. To address the above issues, we proposed\n\\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election\n\\textbf{D}istillation (MoRSD), which can discern and select high quality\nrationales for distillation to improve performance further. We further propose\na Rationale Difficulty (RD) metric to measure the ability of the student model\nto generate the correct answer under a given rationale. Compared to the\nbaseline, we achieved 4.6$\\%$ average improvement on seven datasets over three\ntasks, using fewer rationales by controlling their accuracy, diversity, and\ndifficulty. Our results reveal that a small portion of the high quality\nrationales can enhance the reasoning ability of student models than the entire\ndataset. Our method promises to be a possible solution for efficient CoT\ndistillation. Our code will be released in https://github.com/Leon221220/MoRSD.",
    "published": "2025-09-28T02:09:07Z",
    "link": "http://arxiv.org/pdf/2509.23574v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jianzhi Yan",
      "Le Liu",
      "Youcheng Pan",
      "Shiwei Chen",
      "Yang Xiang",
      "Buzhou Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23573v1",
    "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence",
    "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.",
    "published": "2025-09-28T02:08:27Z",
    "link": "http://arxiv.org/pdf/2509.23573v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Yuqiao Meng",
      "Luoxi Tang",
      "Feiyang Yu",
      "Jinyuan Jia",
      "Guanhua Yan",
      "Ping Yang",
      "Zhaohan Xi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23571v1",
    "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting",
    "summary": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting.",
    "published": "2025-09-28T02:08:17Z",
    "link": "http://arxiv.org/pdf/2509.23571v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Yuqiao Meng",
      "Luoxi Tang",
      "Feiyang Yu",
      "Xi Li",
      "Guanhua Yan",
      "Ping Yang",
      "Zhaohan Xi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23568v1",
    "title": "Node Classification via Simplicial Interaction with Augmented Maximal\n  Clique Selection",
    "summary": "Considering higher-order interactions allows for a more comprehensive\nunderstanding of network structures beyond simple pairwise connections. While\nleveraging all cliques in a network to handle higher-order interactions is\nintuitive, it often leads to computational inefficiencies due to overlapping\ninformation between higher-order and lower-order cliques. To address this\nissue, we propose an augmented maximal clique strategy. Although using only\nmaximal cliques can reduce unnecessary overlap and provide a concise\nrepresentation of the network, certain nodes may still appear in multiple\nmaximal cliques, resulting in imbalanced training data. Therefore, our\naugmented maximal clique approach selectively includes some non-maximal cliques\nto mitigate the overrepresentation of specific nodes and promote more balanced\nlearning across the network. Comparative analyses on synthetic networks and\nreal-world citation datasets demonstrate that our method outperforms approaches\nbased on pairwise interactions, all cliques, or only maximal cliques. Finally,\nby integrating this strategy into GNN-based semi-supervised learning, we\nestablish a link between maximal clique-based methods and GNNs, showing that\nincorporating higher-order structures improves predictive accuracy. As a\nresult, the augmented maximal clique strategy offers a computationally\nefficient and effective solution for higher-order network learning.",
    "published": "2025-09-28T01:57:01Z",
    "link": "http://arxiv.org/pdf/2509.23568v1.pdf",
    "category": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Eunho Koo",
      "Tongseok Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23564v1",
    "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment",
    "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench.",
    "published": "2025-09-28T01:44:05Z",
    "link": "http://arxiv.org/pdf/2509.23564v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Min-Hsuan Yeh",
      "Yixuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23563v1",
    "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and\n  Behavior Adaptation",
    "summary": "Aerial outdoor semantic navigation requires robots to explore large,\nunstructured environments to locate target objects. Recent advances in semantic\nnavigation have demonstrated open-set object-goal navigation in indoor\nsettings, but these methods remain limited by constrained spatial ranges and\nstructured layouts, making them unsuitable for long-range outdoor search. While\noutdoor semantic navigation approaches exist, they either rely on reactive\npolicies based on current observations, which tend to produce short-sighted\nbehaviors, or precompute scene graphs offline for navigation, limiting\nadaptability to online deployment. We present RAVEN, a 3D memory-based,\nbehavior tree framework for aerial semantic navigation in unstructured outdoor\nenvironments. It (1) uses a spatially consistent semantic voxel-ray map as\npersistent memory, enabling long-horizon planning and avoiding purely reactive\nbehaviors, (2) combines short-range voxel search and long-range ray search to\nscale to large environments, (3) leverages a large vision-language model to\nsuggest auxiliary cues, mitigating sparsity of outdoor targets. These\ncomponents are coordinated by a behavior tree, which adaptively switches\nbehaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor\nsimulation environments over 100 semantic tasks, encompassing single-object\nsearch, multi-class, multi-instance navigation and sequential task changes.\nResults show RAVEN outperforms baselines by 85.25% in simulation and\ndemonstrate its real-world applicability through deployment on an aerial robot\nin outdoor field tests.",
    "published": "2025-09-28T01:43:25Z",
    "link": "http://arxiv.org/pdf/2509.23563v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Seungchan Kim",
      "Omar Alama",
      "Dmytro Kurdydyk",
      "John Keller",
      "Nikhil Keetha",
      "Wenshan Wang",
      "Yonatan Bisk",
      "Sebastian Scherer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23562v1",
    "title": "Pancreas Part Segmentation under Federated Learning Paradigm",
    "summary": "We present the first federated learning (FL) approach for pancreas part(head,\nbody and tail) segmentation in MRI, addressing a critical clinical challenge as\na significant innovation. Pancreatic diseases exhibit marked regional\nheterogeneity cancers predominantly occur in the head region while chronic\npancreatitis causes tissue loss in the tail, making accurate segmentation of\nthe organ into head, body, and tail regions essential for precise diagnosis and\ntreatment planning. This segmentation task remains exceptionally challenging in\nMRI due to variable morphology, poor soft-tissue contrast, and anatomical\nvariations across patients. Our novel contribution tackles two fundamental\nchallenges: first, the technical complexity of pancreas part delineation in\nMRI, and second the data scarcity problem that has hindered prior approaches.\nWe introduce a privacy-preserving FL framework that enables collaborative model\ntraining across seven medical institutions without direct data sharing,\nleveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key\ninnovations include: (1) a systematic evaluation of three state-of-the-art\nsegmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two\nFL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as\noptimal for pancreatic heterogeneity, which was never been done before; (2) a\nnovel anatomically-informed loss function prioritizing region-specific texture\ncontrasts in MRI. Comprehensive evaluation demonstrates that our approach\nachieves clinically viable performance despite training on distributed,\nheterogeneous datasets.",
    "published": "2025-09-28T01:42:43Z",
    "link": "http://arxiv.org/pdf/2509.23562v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ziliang Hong",
      "Halil Ertugrul Aktas",
      "Andrea Mia Bejar",
      "Katherine Wu",
      "Hongyi Pan",
      "Gorkem Durak",
      "Zheyuan Zhang",
      "Sait Kayali",
      "Temel Tirkes",
      "Federica Proietto Salanitri",
      "Concetto Spampinato",
      "Michael Goggins",
      "Tamas Gonda",
      "Candice Bolan",
      "Raj Keswani",
      "Frank Miller",
      "Michael Wallace",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23560v1",
    "title": "A Hierarchical Structure-Enhanced Personalized Recommendation Model for\n  Traditional Chinese Medicine Formulas Based on KG Diffusion Guidance",
    "summary": "Artificial intelligence technology plays a crucial role in recommending\nprescriptions for traditional Chinese medicine (TCM). Previous studies have\nmade significant progress by focusing on the symptom-herb relationship in\nprescriptions. However, several limitations hinder model performance: (i)\nInsufficient attention to patient-personalized information such as age, BMI,\nand medical history, which hampers accurate identification of syndrome and\nreduces efficacy. (ii) The typical long-tailed distribution of herb data\nintroduces training biases and affects generalization ability. (iii) The\noversight of the 'monarch, minister, assistant and envoy' compatibility among\nherbs increases the risk of toxicity or side effects, opposing the 'treatment\nbased on syndrome differentiation' principle in clinical TCM. Therefore, we\npropose a novel hierarchical structure-enhanced personalized recommendation\nmodel for TCM formulas based on knowledge graph diffusion guidance, namely\nTCM-HEDPR. Specifically, we pre-train symptom representations using\npatient-personalized prompt sequences and apply prompt-oriented contrastive\nlearning for data augmentation. Furthermore, we employ a KG-guided homogeneous\ngraph diffusion method integrated with a self-attention mechanism to globally\ncapture the non-linear symptom-herb relationship. Lastly, we design a\nheterogeneous graph hierarchical network to integrate herbal dispensing\nrelationships with implicit syndromes, guiding the prescription generation\nprocess at a fine-grained level and mitigating the long-tailed herb data\ndistribution problem. Extensive experiments on two public datasets and one\nclinical dataset demonstrate the effectiveness of TCM-HEDPR. In addition, we\nincorporate insights from modern medicine and network pharmacology to evaluate\nthe recommended prescriptions comprehensively. It can provide a new paradigm\nfor the recommendation of modern TCM.",
    "published": "2025-09-28T01:40:01Z",
    "link": "http://arxiv.org/pdf/2509.23560v1.pdf",
    "category": [
      "cs.AI",
      "68T35"
    ],
    "authors": [
      "ChaoBo Zhang",
      "Long Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23558v1",
    "title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, yet\nthey also introduce novel security challenges. For instance, prompt\njailbreaking attacks involve adversaries crafting sophisticated prompts to\nelicit responses from LLMs that deviate from human values. To uncover\nvulnerabilities in LLM alignment methods, we propose the PASS framework\n(\\underline{P}rompt J\\underline{a}ilbreaking via \\underline{S}emantic and\n\\underline{S}tructural Formalization). Specifically, PASS employs reinforcement\nlearning to transform initial jailbreak prompts into formalized descriptions,\nwhich enhances stealthiness and enables bypassing existing alignment defenses.\nThe jailbreak outputs are then structured into a GraphRAG system that, by\nleveraging extracted relevant terms and formalized symbols as contextual input\nalongside the original query, strengthens subsequent attacks and facilitates\nmore effective jailbreaks. We conducted extensive experiments on common\nopen-source models, demonstrating the effectiveness of our attack.",
    "published": "2025-09-28T01:38:00Z",
    "link": "http://arxiv.org/pdf/2509.23558v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Zhaoqi Wang",
      "Daqing He",
      "Zijian Zhang",
      "Xin Li",
      "Liehuang Zhu",
      "Meng Li",
      "Jiamou Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23552v1",
    "title": "Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial\n  Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost\n  Ensemble",
    "summary": "Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis.\nWhile genomic sequencing enables rapid prediction of resistance phenotypes,\ncurrent computational methods have limitations. Standard machine learning\nmodels treat the genome as an unordered collection of features, ignoring the\nsequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art\nsequence models like Transformers are often too data-hungry and computationally\nexpensive for the moderately-sized datasets that are typical in this domain. To\naddress these challenges, we propose AMR-EnsembleNet, an ensemble framework\nthat synergistically combines sequence-based and feature-based learning. We\ndeveloped a lightweight, custom 1D Convolutional Neural Network (CNN) to\nefficiently learn predictive sequence motifs from high-dimensional SNP data.\nThis sequence-aware model was ensembled with an XGBoost model, a powerful\ngradient boosting system adept at capturing complex, non-local feature\ninteractions. We trained and evaluated our framework on a benchmark dataset of\n809 E. coli strains, predicting resistance across four antibiotics with varying\nclass imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier\nperformance across all the antibiotics, reaching a Matthews Correlation\nCoefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro\nF1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also\nshow that our model consistently focuses on SNPs within well-known AMR genes\nlike fusA and parC, confirming it learns the correct genetic signals for\nresistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a\nfeature-based XGBoost model creates a powerful ensemble, overcoming the\nlimitations of using either an order-agnostic or a standalone sequence model.",
    "published": "2025-09-28T01:19:11Z",
    "link": "http://arxiv.org/pdf/2509.23552v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN",
      "q-bio.QM"
    ],
    "authors": [
      "Md. Saiful Bari Siddiqui",
      "Nowshin Tarannum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23550v1",
    "title": "Automatic Speech Recognition for Greek Medical Dictation",
    "summary": "Medical dictation systems are essential tools in modern healthcare, enabling\naccurate and efficient conversion of speech into written medical documentation.\nThe main objective of this paper is to create a domain-specific system for\nGreek medical speech transcriptions. The ultimate goal is to assist healthcare\nprofessionals by reducing the overload of manual documentation and improving\nworkflow efficiency. Towards this goal, we develop a system that combines\nautomatic speech recognition techniques with text correction model, allowing\nbetter handling of domain-specific terminology and linguistic variations in\nGreek. Our approach leverages both acoustic and textual modeling to create more\nrealistic and reliable transcriptions. We focused on adapting existing language\nand speech technologies to the Greek medical context, addressing challenges\nsuch as complex medical terminology and linguistic inconsistencies. Through\ndomain-specific fine-tuning, our system achieves more accurate and coherent\ntranscriptions, contributing to the development of practical language\ntechnologies for the Greek healthcare sector.",
    "published": "2025-09-28T01:15:47Z",
    "link": "http://arxiv.org/pdf/2509.23550v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Vardis Georgilas",
      "Themos Stafylakis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23548v1",
    "title": "Disentanglement of Variations with Multimodal Generative Modeling",
    "summary": "Multimodal data are prevalent across various domains, and learning robust\nrepresentations of such data is paramount to enhancing generation quality and\ndownstream task performance. To handle heterogeneity and interconnections among\ndifferent modalities, recent multimodal generative models extract shared and\nprivate (modality-specific) information with two separate variables. Despite\nattempts to enforce disentanglement between these two variables, these methods\nstruggle with challenging datasets where the likelihood model is insufficient.\nIn this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to\nexplicitly address this issue, with rigorous mutual information-based\nregularizations, including cross-view mutual information maximization for\nextracting shared variables, and a cycle-consistency style loss for redundancy\nremoval using generative augmentations. We further introduce diffusion models\nto improve the capacity of latent priors. These newly proposed components are\ncomplementary to each other. Compared to existing approaches, IDMVAE shows a\nclean separation between shared and private information, demonstrating superior\ngeneration quality and semantic coherence on challenging datasets.",
    "published": "2025-09-28T00:54:39Z",
    "link": "http://arxiv.org/pdf/2509.23548v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yijie Zhang",
      "Yiyang Shen",
      "Weiran Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23544v1",
    "title": "End-to-End Deep Learning for Predicting Metric Space-Valued Outputs",
    "summary": "Many modern applications involve predicting structured, non-Euclidean outputs\nsuch as probability distributions, networks, and symmetric positive-definite\nmatrices. These outputs are naturally modeled as elements of general metric\nspaces, where classical regression techniques that rely on vector space\nstructure no longer apply. We introduce E2M (End-to-End Metric regression), a\ndeep learning framework for predicting metric space-valued outputs. E2M\nperforms prediction via a weighted Fr\\'echet means over training outputs, where\nthe weights are learned by a neural network conditioned on the input. This\nconstruction provides a principled mechanism for geometry-aware prediction that\navoids surrogate embeddings and restrictive parametric assumptions, while fully\npreserving the intrinsic geometry of the output space. We establish theoretical\nguarantees, including a universal approximation theorem that characterizes the\nexpressive capacity of the model and a convergence analysis of the\nentropy-regularized training objective. Through extensive simulations involving\nprobability distributions, networks, and symmetric positive-definite matrices,\nwe show that E2M consistently achieves state-of-the-art performance, with its\nadvantages becoming more pronounced at larger sample sizes. Applications to\nhuman mortality distributions and New York City taxi networks further\ndemonstrate the flexibility and practical utility of the framework.",
    "published": "2025-09-28T00:46:12Z",
    "link": "http://arxiv.org/pdf/2509.23544v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Yidong Zhou",
      "Su I Iao",
      "Hans-Georg Müller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23542v1",
    "title": "On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward\n  Compatibility, and Question Generalization",
    "summary": "The LLM-as-a-judge paradigm is widely used in both evaluating free-text model\nresponses and reward modeling for model alignment and finetuning. Recently,\nfinetuning judges with judge-specific data has emerged as an often preferred\nchoice over directly prompting frontier models as judges, as the former\nachieves better performance with smaller model sizes while being more robust to\ncommon biases. However, the standard evaluation ignores several practical\nconcerns of finetuned judges regarding their real world deployment. In this\npaper, we identify and formalize three aspects that affect the shelf life of\nthese judges: future proofing and backward compatibility -- how well judges\nfinetuned on responses by today's generator models perform on responses by\nfuture models or past models, as well as question generalization -- how well\njudges generalize to unseen questions at test time. We study these three\naspects in the math domain under a unified framework with varying train and\ntest distributions, three SFT- and DPO-based finetuning algorithms and three\ndifferent base models. Experiments suggest that future-proofing is challenging\nfor most models, while backward compatibility is relatively easy, with\nDPO-trained models consistently improving performance. We further find that\ncontinual learning provides a more balanced adaptation to shifts between older\nand newer response distributions than training solely on stronger or weaker\nresponses. Moreover, all models observe certain degrees of performance\ndegradation when moving from questions seen during training to unseen ones,\nshowing that current judges do not fully generalize to unseen questions. These\nfindings provide insights into practical considerations for developing and\ndeploying judge models in the face of ever-changing generators.",
    "published": "2025-09-28T00:43:52Z",
    "link": "http://arxiv.org/pdf/2509.23542v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Janvijay Singh",
      "Austin Xu",
      "Yilun Zhou",
      "Yefan Zhou",
      "Dilek Hakkani-Tur",
      "Shafiq Joty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23537v1",
    "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs.\n  Single LLMs on Benchmarks",
    "summary": "We study multi-turn multi-agent orchestration, where multiple large language\nmodel (LLM) agents interact over multiple turns by iteratively proposing\nanswers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5\nPro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we\nconduct two experiments: (i) benchmarking orchestration against single-LLM\nbaselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who\nauthored answers and whether they can observe ongoing votes. Orchestration\nmatches or exceeds the strongest single model and consistently outperforms the\nothers. Analysis of best-achievable orchestration performance shows potential\nfor further gains. The ablations show that revealing authorship increases\nself-voting and ties, and that showing ongoing votes amplifies herding, which\nspeeds convergence but can sometimes yield premature consensus.",
    "published": "2025-09-28T00:15:21Z",
    "link": "http://arxiv.org/pdf/2509.23537v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Aaron Xuxiang Tian",
      "Ruofan Zhang",
      "Jiayao Tang",
      "Young Min Cho",
      "Xueqian Li",
      "Qiang Yi",
      "Ji Wang",
      "Zhunping Zhang",
      "Danrui Qi",
      "Sharath Chandra Guntuku",
      "Lyle Ungar",
      "Tianyu Shi",
      "Chi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23530v1",
    "title": "Imaging-Based Mortality Prediction in Patients with Systemic Sclerosis",
    "summary": "Interstitial lung disease (ILD) is a leading cause of morbidity and mortality\nin systemic sclerosis (SSc). Chest computed tomography (CT) is the primary\nimaging modality for diagnosing and monitoring lung complications in SSc\npatients. However, its role in disease progression and mortality prediction has\nnot yet been fully clarified. This study introduces a novel, large-scale\nlongitudinal chest CT analysis framework that utilizes radiomics and deep\nlearning to predict mortality associated with lung complications of SSc. We\ncollected and analyzed 2,125 CT scans from SSc patients enrolled in the\nNorthwestern Scleroderma Registry, conducting mortality analyses at one, three,\nand five years using advanced imaging analysis techniques. Death labels were\nassigned based on recorded deaths over the one-, three-, and five-year\nintervals, confirmed by expert physicians. In our dataset, 181, 326, and 428 of\nthe 2,125 CT scans were from patients who died within one, three, and five\nyears, respectively. Using ResNet-18, DenseNet-121, and Swin Transformer we use\npre-trained models, and fine-tuned on 2,125 images of SSc patients. Models\nachieved an AUC of 0.769, 0.801, 0.709 for predicting mortality within one-,\nthree-, and five-years, respectively. Our findings highlight the potential of\nboth radiomics and deep learning computational methods to improve early\ndetection and risk assessment of SSc-related interstitial lung disease, marking\na significant advancement in the literature.",
    "published": "2025-09-27T23:46:57Z",
    "link": "http://arxiv.org/pdf/2509.23530v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Alec K. Peltekian",
      "Karolina Senkow",
      "Gorkem Durak",
      "Kevin M. Grudzinski",
      "Bradford C. Bemiss",
      "Jane E. Dematte",
      "Carrie Richardson",
      "Nikolay S. Markov",
      "Mary Carns",
      "Kathleen Aren",
      "Alexandra Soriano",
      "Matthew Dapas",
      "Harris Perlman",
      "Aaron Gundersheimer",
      "Kavitha C. Selvan",
      "John Varga",
      "Monique Hinchcliff",
      "Krishnan Warrior",
      "Catherine A. Gao",
      "Richard G. Wunderink",
      "GR Scott Budinger",
      "Alok N. Choudhary",
      "Anthony J. Esposito",
      "Alexander V. Misharin",
      "Ankit Agrawal",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23529v1",
    "title": "DOoM: Difficult Olympiads of Math",
    "summary": "This paper introduces DOoM, a new open-source benchmark designed to assess\nthe capabilities of language models in solving mathematics and physics problems\nin Russian. The benchmark includes problems of varying difficulty, ranging from\nschool-level tasks to university Olympiad and entrance exam questions. In this\npaper we discuss the motivation behind its creation, describe dataset's\nstructure and evaluation methodology, and present initial results from testing\nvarious models. Analysis of the results shows a correlation between model\nperformance and the number of tokens used, and highlights differences in\nperformance between mathematics and physics tasks.",
    "published": "2025-09-27T23:37:19Z",
    "link": "http://arxiv.org/pdf/2509.23529v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ilya Kuleshov",
      "Ilin Pavel",
      "Nikolay Kompanets",
      "Ksenia Sycheva",
      "Aleksandr Nikolich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23525v1",
    "title": "Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI\n  Product Concepts",
    "summary": "AI creates and exacerbates privacy risks, yet practitioners lack effective\nresources to identify and mitigate these risks. We present Privy, a tool that\nguides practitioners through structured privacy impact assessments to: (i)\nidentify relevant risks in novel AI product concepts, and (ii) propose\nappropriate mitigations. Privy was shaped by a formative study with 11\npractitioners, which informed two versions -- one LLM-powered, the other\ntemplate-based. We evaluated these two versions of Privy through a\nbetween-subjects, controlled study with 24 separate practitioners, whose\nassessments were reviewed by 13 independent privacy experts. Results show that\nPrivy helps practitioners produce privacy assessments that experts deemed high\nquality: practitioners identified relevant risks and proposed appropriate\nmitigation strategies. These effects were augmented in the LLM-powered version.\nPractitioners themselves rated Privy as being useful and usable, and their\nfeedback illustrates how it helps overcome long-standing awareness, motivation,\nand ability barriers in privacy work.",
    "published": "2025-09-27T23:08:24Z",
    "link": "http://arxiv.org/pdf/2509.23525v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Hao-Ping Lee",
      "Yu-Ju Yang",
      "Matthew Bilik",
      "Isadora Krsek",
      "Thomas Serban von Davier",
      "Kyzyl Monteiro",
      "Jason Lin",
      "Shivani Agarwal",
      "Jodi Forlizzi",
      "Sauvik Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23519v1",
    "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based\n  Web-Search",
    "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by\ngrounding their outputs in external documents. These systems, however, remain\nvulnerable to attacks on the retrieval corpus, such as prompt injection.\nRAG-based search systems (e.g., Google's Search AI Overview) present an\ninteresting setting for studying and protecting against such threats, as\ndefense algorithms can benefit from built-in reliability signals -- like\ndocument ranking -- and represent a non-LLM challenge for the adversary due to\ndecades of work to thwart SEO.\n  Motivated by, but not limited to, this scenario, this work introduces\nReliabilityRAG, a framework for adversarial robustness that explicitly\nleverages reliability information of retrieved documents.\n  Our first contribution adopts a graph-theoretic perspective to identify a\n\"consistent majority\" among retrieved documents to filter out malicious ones.\nWe introduce a novel algorithm based on finding a Maximum Independent Set (MIS)\non a document graph where edges encode contradiction. Our MIS variant\nexplicitly prioritizes higher-reliability documents and provides provable\nrobustness guarantees against bounded adversarial corruption under natural\nassumptions. Recognizing the computational cost of exact MIS for large\nretrieval sets, our second contribution is a scalable weighted sample and\naggregate framework. It explicitly utilizes reliability information, preserving\nsome robustness guarantees while efficiently handling many documents.\n  We present empirical results showing ReliabilityRAG provides superior\nrobustness against adversarial attacks compared to prior methods, maintains\nhigh benign accuracy, and excels in long-form generation tasks where prior\nrobustness-focused methods struggled. Our work is a significant step towards\nmore effective, provably robust defenses against retrieved corpus corruption in\nRAG.",
    "published": "2025-09-27T22:36:42Z",
    "link": "http://arxiv.org/pdf/2509.23519v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Zeyu Shen",
      "Basileal Imana",
      "Tong Wu",
      "Chong Xiang",
      "Prateek Mittal",
      "Aleksandra Korolova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23517v1",
    "title": "Evaluating point-light biological motion in multimodal large language\n  models",
    "summary": "Humans can extract rich semantic information from minimal visual cues, as\ndemonstrated by point-light displays (PLDs), which consist of sparse sets of\ndots localized to key joints of the human body. This ability emerges early in\ndevelopment and is largely attributed to human embodied experience. Since PLDs\nisolate body motion as the sole source of meaning, they represent key stimuli\nfor testing the constraints of action understanding in these systems. Here we\nintroduce ActPLD, the first benchmark to evaluate action processing in MLLMs\nfrom human PLDs. Tested models include state-of-the-art proprietary and\nopen-source systems on single-actor and socially interacting PLDs. Our results\nreveal consistently low performance across models, introducing fundamental gaps\nin action and spatiotemporal understanding.",
    "published": "2025-09-27T22:33:05Z",
    "link": "http://arxiv.org/pdf/2509.23517v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Akila Kadambi",
      "Marco Iacoboni",
      "Lisa Aziz-Zadeh",
      "Srini Narayanan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23515v1",
    "title": "From Human Annotation to Automation: LLM-in-the-Loop Active Learning for\n  Arabic Sentiment Analysis",
    "summary": "Natural language processing (NLP), particularly sentiment analysis, plays a\nvital role in areas like marketing, customer service, and social media\nmonitoring by providing insights into user opinions and emotions. However,\nprogress in Arabic sentiment analysis remains limited due to the lack of large,\nhigh-quality labeled datasets. While active learning has proven effective in\nreducing annotation efforts in other languages, few studies have explored it in\nArabic sentiment tasks. Likewise, the use of large language models (LLMs) for\nassisting annotation and comparing their performance to human labeling is still\nlargely unexplored in the Arabic context. In this paper, we propose an active\nlearning framework for Arabic sentiment analysis designed to reduce annotation\ncosts while maintaining high performance. We evaluate multiple deep learning\narchitectures: Specifically, long short-term memory (LSTM), gated recurrent\nunits (GRU), and recurrent neural networks (RNN), across three benchmark\ndatasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard\nArabic and dialectal variations. Additionally, two annotation strategies are\ncompared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as\nannotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3\n70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for\nHunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our\nresults show that LLM-assisted active learning achieves competitive or superior\nperformance compared to human labeling. For example, on the Hunger Station\ndataset, the LSTM model achieved 93% accuracy with only 450 labeled samples\nusing GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat\nreached 82% accuracy with 650 labeled samples, matching the accuracy obtained\nthrough human labeling.",
    "published": "2025-09-27T22:23:46Z",
    "link": "http://arxiv.org/pdf/2509.23515v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Dania Refai",
      "Alaa Dalaq",
      "Doaa Dalaq",
      "Irfan Ahmad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23510v1",
    "title": "Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores",
    "summary": "New large language models (LLMs) are being released every day. Some perform\nsignificantly better or worse than expected given their parameter count.\nTherefore, there is a need for a method to independently evaluate models. The\ncurrent best way to evaluate a model is to measure its Elo score by comparing\nit to other models in a series of contests - an expensive operation since\nhumans are ideally required to compare LLM outputs. We observe that when an LLM\nis asked to judge such contests, the consistency with which it selects a model\nas the best in a matchup produces a metric that is 91% correlated with its own\nhuman-produced Elo score. This provides a simple proxy for Elo scores that can\nbe computed cheaply, without any human data or prior knowledge.",
    "published": "2025-09-27T22:00:30Z",
    "link": "http://arxiv.org/pdf/2509.23510v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ashwin Ramaswamy",
      "Nestor Demeure",
      "Ermal Rrapaj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23502v1",
    "title": "Enhancing Polyp Segmentation via Encoder Attention and Dynamic Kernel\n  Update",
    "summary": "Polyp segmentation is a critical step in colorectal cancer detection, yet it\nremains challenging due to the diverse shapes, sizes, and low contrast\nboundaries of polyps in medical imaging. In this work, we propose a novel\nframework that improves segmentation accuracy and efficiency by integrating a\nDynamic Kernel (DK) mechanism with a global Encoder Attention module. The DK\nmechanism, initialized by a global context vector from the EA module,\niteratively refines segmentation predictions across decoding stages, enabling\nthe model to focus on and accurately delineate complex polyp boundaries. The EA\nmodule enhances the network's ability to capture critical lesion features by\naggregating multi scale information from all encoder layers. In addition, we\nemploy Unified Channel Adaptation (UCA) in the decoder to standardize feature\ndimensions across stages, ensuring consistent and computationally efficient\ninformation fusion. Our approach extends the lesion-aware kernel framework by\nintroducing a more flexible, attention driven kernel initialization and a\nunified decoder design. Extensive experiments on the KvasirSEG and CVC ClinicDB\nbenchmark datasets demonstrate that our model outperforms several state of the\nart segmentation methods, achieving superior Dice and Intersection over Union\nscores. Moreover, UCA simplifies the decoder structure, reducing computational\ncost without compromising accuracy. Overall, the proposed method provides a\nrobust and adaptable solution for polyp segmentation, with promising\napplications in clinical and automated diagnostic systems.",
    "published": "2025-09-27T21:16:09Z",
    "link": "http://arxiv.org/pdf/2509.23502v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Fatemeh Salahi Chashmi",
      "Roya Sotoudeh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23501v1",
    "title": "The Impact of Role Design in In-Context Learning for Large Language\n  Models",
    "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to generate\npredictions based on prompts without additional fine-tuning. While prompt\nengineering has been widely studied, the impact of role design within prompts\nremains underexplored. This study examines the influence of role configurations\nin zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from\nOpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'\nperformance across datasets, focusing on tasks like sentiment analysis, text\nclassification, question answering, and math reasoning. Our findings suggest\nthe potential of role-based prompt structuring to enhance LLM performance.",
    "published": "2025-09-27T21:15:30Z",
    "link": "http://arxiv.org/pdf/2509.23501v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Hamidreza Rouzegar",
      "Masoud Makrehchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23497v1",
    "title": "Dynamic Trust Calibration Using Contextual Bandits",
    "summary": "Trust calibration between humans and Artificial Intelligence (AI) is crucial\nfor optimal decision-making in collaborative settings. Excessive trust can lead\nusers to accept AI-generated outputs without question, overlooking critical\nflaws, while insufficient trust may result in disregarding valuable insights\nfrom AI systems, hindering performance. Despite its importance, there is\ncurrently no definitive and objective method for measuring trust calibration\nbetween humans and AI. Current approaches lack standardization and consistent\nmetrics that can be broadly applied across various contexts, and they don't\ndistinguish between the formation of opinions and subsequent human decisions.\nIn this work, we propose a novel and objective method for dynamic trust\ncalibration, introducing a standardized trust calibration measure and an\nindicator. By utilizing Contextual Bandits-an adaptive algorithm that\nincorporates context into decision-making-our indicator dynamically assesses\nwhen to trust AI contributions based on learned contextual information. We\nevaluate this indicator across three diverse datasets, demonstrating that\neffective trust calibration results in significant improvements in\ndecision-making performance, as evidenced by 10 to 38% increase in reward\nmetrics. These findings not only enhance theoretical understanding but also\nprovide practical guidance for developing more trustworthy AI systems\nsupporting decisions in critical domains, for example, disease diagnoses and\ncriminal justice.",
    "published": "2025-09-27T21:06:17Z",
    "link": "http://arxiv.org/pdf/2509.23497v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Bruno M. Henrique",
      "Eugene Santos Jr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23494v1",
    "title": "Revisiting Multivariate Time Series Forecasting with Missing Values",
    "summary": "Missing values are common in real-world time series, and multivariate time\nseries forecasting with missing values (MTSF-M) has become a crucial area of\nresearch for ensuring reliable predictions. To address the challenge of missing\ndata, current approaches have developed an imputation-then-prediction framework\nthat uses imputation modules to fill in missing values, followed by forecasting\non the imputed data. However, this framework overlooks a critical issue: there\nis no ground truth for the missing values, making the imputation process\nsusceptible to errors that can degrade prediction accuracy. In this paper, we\nconduct a systematic empirical study and reveal that imputation without direct\nsupervision can corrupt the underlying data distribution and actively degrade\nprediction accuracy. To address this, we propose a paradigm shift that moves\naway from imputation and directly predicts from the partially observed time\nseries. We introduce Consistency-Regularized Information Bottleneck (CRIB), a\nnovel framework built on the Information Bottleneck principle. CRIB combines a\nunified-variate attention mechanism with a consistency regularization scheme to\nlearn robust representations that filter out noise introduced by missing values\nwhile preserving essential predictive signals. Comprehensive experiments on\nfour real-world datasets demonstrate the effectiveness of CRIB, which predicts\naccurately even under high missing rates. Our code is available in\nhttps://github.com/Muyiiiii/CRIB.",
    "published": "2025-09-27T20:57:48Z",
    "link": "http://arxiv.org/pdf/2509.23494v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jie Yang",
      "Yifan Hu",
      "Kexin Zhang",
      "Luyang Niu",
      "Yushun Dong",
      "Philip S. Yu",
      "Kaize Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23488v1",
    "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild",
    "summary": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities.",
    "published": "2025-09-27T20:23:13Z",
    "link": "http://arxiv.org/pdf/2509.23488v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Siyang Wu",
      "Honglin Bao",
      "Sida Li",
      "Ari Holtzman",
      "James A. Evans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23486v1",
    "title": "Text-Based Approaches to Item Difficulty Modeling in Large-Scale\n  Assessments: A Systematic Review",
    "summary": "Item difficulty plays a crucial role in test performance, interpretability of\nscores, and equity for all test-takers, especially in large-scale assessments.\nTraditional approaches to item difficulty modeling rely on field testing and\nclassical test theory (CTT)-based item analysis or item response theory (IRT)\ncalibration, which can be time-consuming and costly. To overcome these\nchallenges, text-based approaches leveraging machine learning and language\nmodels, have emerged as promising alternatives. This paper reviews and\nsynthesizes 37 articles on automated item difficulty prediction in large-scale\nassessment settings published through May 2025. For each study, we delineate\nthe dataset, difficulty parameter, subject domain, item type, number of items,\ntraining and test data split, input, features, model, evaluation criteria, and\nmodel performance outcomes. Results showed that although classic machine\nlearning models remain relevant due to their interpretability, state-of-the-art\nlanguage models, using both small and large transformer-based architectures,\ncan capture syntactic and semantic patterns without the need for manual feature\nengineering. Uniquely, model performance outcomes were summarized to serve as a\nbenchmark for future research and overall, text-based methods have the\npotential to predict item difficulty with root mean square error (RMSE) as low\nas 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.\nThe review concludes by discussing implications for practice and outlining\nfuture research directions for automated item difficulty modeling.",
    "published": "2025-09-27T20:19:39Z",
    "link": "http://arxiv.org/pdf/2509.23486v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "I.2.7",
      "I.2.7"
    ],
    "authors": [
      "Sydney Peters",
      "Nan Zhang",
      "Hong Jiao",
      "Ming Li",
      "Tianyi Zhou",
      "Robert Lissitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23484v1",
    "title": "Accurate Predictions in Education with Discrete Variational Inference",
    "summary": "One of the largest drivers of social inequality is unequal access to personal\ntutoring, with wealthier individuals able to afford it, while the majority\ncannot. Affordable, effective AI tutors offer a scalable solution. We focus on\nadaptive learning, predicting whether a student will answer a question\ncorrectly, a key component of any effective tutoring system. Yet many platforms\nstruggle to achieve high prediction accuracy, especially in data-sparse\nsettings. To address this, we release the largest open dataset of\nprofessionally marked formal mathematics exam responses to date. We introduce a\nprobabilistic modelling framework rooted in Item Response Theory (IRT) that\nachieves over 80 percent accuracy, setting a new benchmark for mathematics\nprediction accuracy of formal exam papers. Extending this, our collaborative\nfiltering models incorporate topic-level skill profiles, but reveal a\nsurprising and educationally significant finding, a single latent ability\nparameter alone is needed to achieve the maximum predictive accuracy. Our main\ncontribution though is deriving and implementing a novel discrete variational\ninference framework, achieving our highest prediction accuracy in low-data\nsettings and outperforming all classical IRT and matrix factorisation\nbaselines.",
    "published": "2025-09-27T20:13:02Z",
    "link": "http://arxiv.org/pdf/2509.23484v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tom Quilter",
      "Anastasia Ilick",
      "Anastasia Ilick",
      "Richard Turner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23482v1",
    "title": "GeoBS: Information-Theoretic Quantification of Geographic Bias in AI\n  Models",
    "summary": "The widespread adoption of AI models, especially foundation models (FMs), has\nmade a profound impact on numerous domains. However, it also raises significant\nethical concerns, including bias issues. Although numerous efforts have been\nmade to quantify and mitigate social bias in AI models, geographic bias (in\nshort, geo-bias) receives much less attention, which presents unique\nchallenges. While previous work has explored ways to quantify geo-bias, these\nmeasures are model-specific (e.g., mean absolute deviation of LLM ratings) or\nspatially implicit (e.g., average fairness scores of all spatial partitions).\nWe lack a model-agnostic, universally applicable, and spatially explicit\ngeo-bias evaluation framework that allows researchers to fairly compare the\ngeo-bias of different AI models and to understand what spatial factors\ncontribute to the geo-bias. In this paper, we establish an\ninformation-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias\nScores). We demonstrate the generalizability of the proposed framework by\nshowing how to interpret and analyze existing geo-bias measures under this\nframework. Then, we propose three novel geo-bias scores that explicitly take\nintricate spatial factors (multi-scalability, distance decay, and anisotropy)\ninto consideration. Finally, we conduct extensive experiments on 3 tasks, 8\ndatasets, and 8 models to demonstrate that both task-specific GeoAI models and\ngeneral-purpose foundation models may suffer from various types of geo-bias.\nThis framework will not only advance the technical understanding of geographic\nbias but will also establish a foundation for integrating spatial fairness into\nthe design, deployment, and evaluation of AI systems.",
    "published": "2025-09-27T20:07:21Z",
    "link": "http://arxiv.org/pdf/2509.23482v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhangyu Wang",
      "Nemin Wu",
      "Qian Cao",
      "Jiangnan Xia",
      "Zeping Liu",
      "Yiqun Xie",
      "Akshay Nambi",
      "Tanuja Ganu",
      "Ni Lao",
      "Ninghao Liu",
      "Gengchen Mai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23472v1",
    "title": "Memory-Efficient Fine-Tuning via Low-Rank Activation Compression",
    "summary": "The parameter-efficient fine-tuning paradigm has garnered significant\nattention with the advancement of foundation models. Although numerous methods\nhave been proposed to reduce the number of trainable parameters, their\nsubstantial memory overhead remains a critical bottleneck that hinders\npractical deployment. In this paper, we observe that model activations\nconstitute a major source of memory consumption, especially under large batch\nsizes and long context lengths; however, the rank of the activations remains\nconsistently low. Motivated by this insight, we propose a memory-efficient\nfine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior\nwork, LoRAct provides a more flexible and versatile compressing strategy that\ncan be applied online during the forward pass without the need for any\ncalibration data. Moreover, LoRAct incorporates a novel sampling-based\northogonal decomposition algorithm specifically designed for low-rank matrices,\noffering improved computational efficiency and a tighter error bound compared\nto the widely used RSVD. Experiments on both vision and language tasks\ndemonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces\nactivation memory by approximately 80% in comparison with the widely adopted\nLoRA method, while maintaining competitive performance. The source code is\navailable at https://github.com/shijxcs/meft.",
    "published": "2025-09-27T19:48:32Z",
    "link": "http://arxiv.org/pdf/2509.23472v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiang-Xin Shi",
      "Wen-Da Wei",
      "Jin-Fei Qi",
      "Xuanyu Chen",
      "Tong Wei",
      "Yu-Feng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23468v1",
    "title": "Multi-Modal Manipulation via Multi-Modal Policy Consensus",
    "summary": "Effectively integrating diverse sensory modalities is crucial for robotic\nmanipulation. However, the typical approach of feature concatenation is often\nsuboptimal: dominant modalities such as vision can overwhelm sparse but\ncritical signals like touch in contact-rich tasks, and monolithic architectures\ncannot flexibly incorporate new or missing modalities without retraining. Our\nmethod factorizes the policy into a set of diffusion models, each specialized\nfor a single representation (e.g., vision or touch), and employs a router\nnetwork that learns consensus weights to adaptively combine their\ncontributions, enabling incremental of new representations. We evaluate our\napproach on simulated manipulation tasks in {RLBench}, as well as real-world\ntasks such as occluded object picking, in-hand spoon reorientation, and puzzle\ninsertion, where it significantly outperforms feature-concatenation baselines\non scenarios requiring multimodal reasoning. Our policy further demonstrates\nrobustness to physical perturbations and sensor corruption. We further conduct\nperturbation-based importance analysis, which reveals adaptive shifts between\nmodalities.",
    "published": "2025-09-27T19:43:04Z",
    "link": "http://arxiv.org/pdf/2509.23468v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Haonan Chen",
      "Jiaming Xu",
      "Hongyu Chen",
      "Kaiwen Hong",
      "Binghao Huang",
      "Chaoqi Liu",
      "Jiayuan Mao",
      "Yunzhu Li",
      "Yilun Du",
      "Katherine Driggs-Campbell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23465v1",
    "title": "ViTSP: A Vision Language Models Guided Framework for Large-Scale\n  Traveling Salesman Problems",
    "summary": "Solving Traveling Salesman Problem (TSP) is NP-hard yet fundamental for wide\nreal-world applications. Classical exact methods face challenges in scaling,\nand heuristic methods often require domain-specific parameter calibration.\nWhile learning-based approaches have shown promise, they suffer from poor\ngeneralization and limited scalability due to fixed training data. This work\nproposes ViTSP, a novel framework that leverages pre-trained vision language\nmodels (VLMs) to visually guide the solution process for large-scale TSPs. The\nVLMs function to identify promising small-scale subproblems from a visualized\nTSP instance, which are then efficiently optimized using an off-the-shelf\nsolver to improve the global solution. ViTSP bypasses the dedicated model\ntraining at the user end while maintaining effectiveness across diverse\ninstances. Experiments on real-world TSP instances ranging from 1k to 88k nodes\ndemonstrate that ViTSP consistently achieves solutions with average optimality\ngaps below 0.2%, outperforming existing learning-based methods. Under the same\nruntime budget, it surpasses the best-performing heuristic solver, LKH-3, by\nreducing its gaps by 12% to 100%, particularly on very-large-scale instances\nwith more than 10k nodes. Our framework offers a new perspective in hybridizing\npre-trained generative models and operations research solvers in solving\ncombinatorial optimization problems, with practical implications for\nintegration into more complex logistics systems. The code is available at\nhttps://anonymous.4open.science/r/ViTSP_codes-6683.",
    "published": "2025-09-27T19:27:24Z",
    "link": "http://arxiv.org/pdf/2509.23465v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhuoli Yin",
      "Yi Ding",
      "Reem Khir",
      "Hua Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23462v1",
    "title": "Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free\n  Multi-Agent Learning",
    "summary": "Scalable multi-agent reinforcement learning (MARL) remains a central\nchallenge for AI. Existing population-based methods, like Policy-Space Response\nOracles, PSRO, require storing explicit policy populations and constructing\nfull payoff matrices, incurring quadratic computation and linear memory costs.\nWe present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free\nframework that replaces explicit populations with a compact set of latent\nanchors and a single amortized generator. Instead of exhaustively constructing\nthe payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,\nmultiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB\noracle to adaptively expand the policy set. Best responses are trained within\nthe generator using an advantage-based trust-region objective, eliminating the\nneed to store and train separate actors. We evaluated GEMS in a variety of\nTwo-player and Multi-Player games such as the Deceptive Messages Game, Kuhn\nPoker and Multi-Particle environment. We find that GEMS is up to ~6x faster,\nhas 1.3x less memory usage than PSRO, while also reaps higher rewards\nsimultaneously. These results demonstrate that GEMS retains the game theoretic\nguarantees of PSRO, while overcoming its fundamental inefficiencies, hence\nenabling scalable multi-agent learning in multiple domains.",
    "published": "2025-09-27T19:23:38Z",
    "link": "http://arxiv.org/pdf/2509.23462v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alakh Sharma",
      "Gaurish Trivedi",
      "Kartikey Bhandari",
      "Yash Sinha",
      "Dhruv Kumar",
      "Pratik Narang",
      "Jagat Sesh Challa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23461v1",
    "title": "Data-Efficient Training by Evolved Sampling",
    "summary": "Data selection is designed to accelerate learning with preserved performance.\nTo achieve this, a fundamental thought is to identify informative data samples\nwith significant contributions to the training. In this work, we propose\n\\textbf{Evolved Sampling} (\\textbf{ES}), a simple yet effective framework for\n\\emph{dynamic} sampling along the training process. This method conducts \\em\nbatch \\em level data selection based on the dynamics of losses and augmented\n\\emph{loss differences}, which enables flexible \\emph{frequency tuning}, and\nhence significantly reduces the back propagation time with maintained model\nperformance. Due to its conciseness, ES is also readily extensible to\nincorporate \\em set \\em level data selection (to form ES with pruning,\n\\textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP)\nconsistently achieves lossless training accelerations across various\npre-training and post-training tasks, saving up to nearly 45\\% wall-clock time.\nOur results motivate further investigations on the data efficiency aspect of\nmodern large-scale machine learning.",
    "published": "2025-09-27T19:19:16Z",
    "link": "http://arxiv.org/pdf/2509.23461v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Ziheng Cheng",
      "Zhong Li",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23454v1",
    "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN\n  Architecture for Robust Phonocardiogram Classification",
    "summary": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training.",
    "published": "2025-09-27T18:52:50Z",
    "link": "http://arxiv.org/pdf/2509.23454v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.SP"
    ],
    "authors": [
      "Md. Saiful Bari Siddiqui",
      "Utsab Saha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23449v1",
    "title": "Beyond Embeddings: Interpretable Feature Extraction for Binary Code\n  Similarity",
    "summary": "Binary code similarity detection is a core task in reverse engineering. It\nsupports malware analysis and vulnerability discovery by identifying\nsemantically similar code in different contexts. Modern methods have progressed\nfrom manually engineered features to vector representations. Hand-crafted\nstatistics (e.g., operation ratios) are interpretable, but shallow and fail to\ngeneralize. Embedding-based methods overcome this by learning robust\ncross-setting representations, but these representations are opaque vectors\nthat prevent rapid verification. They also face a scalability-accuracy\ntrade-off, since high-dimensional nearest-neighbor search requires\napproximations that reduce precision. Current approaches thus force a\ncompromise between interpretability, generalizability, and scalability.\n  We bridge these gaps using a language model-based agent to conduct structured\nreasoning analysis of assembly code and generate features such as input/output\ntypes, side effects, notable constants, and algorithmic intent. Unlike\nhand-crafted features, they are richer and adaptive. Unlike embeddings, they\nare human-readable, maintainable, and directly searchable with inverted or\nrelational indexes. Without any matching training, our method respectively\nachieves 42% and 62% for recall@1 in cross-architecture and cross-optimization\ntasks, comparable to embedding methods with training (39% and 34%). Combined\nwith embeddings, it significantly outperforms the state-of-the-art,\ndemonstrating that accuracy, scalability, and interpretability can coexist.",
    "published": "2025-09-27T18:34:32Z",
    "link": "http://arxiv.org/pdf/2509.23449v1.pdf",
    "category": [
      "cs.AI",
      "cs.CR",
      "cs.SE"
    ],
    "authors": [
      "Charles E. Gagnon",
      "Steven H. H. Ding",
      "Philippe Charland",
      "Benjamin C. M. Fung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23443v1",
    "title": "Factor Decorrelation Enhanced Data Removal from Deep Predictive Models",
    "summary": "The imperative of user privacy protection and regulatory compliance\nnecessitates sensitive data removal in model training, yet this process often\ninduces distributional shifts that undermine model performance-particularly in\nout-of-distribution (OOD) scenarios. We propose a novel data removal approach\nthat enhances deep predictive models through factor decorrelation and loss\nperturbation. Our approach introduces: (1) a discriminative-preserving factor\ndecorrelation module employing dynamic adaptive weight adjustment and iterative\nrepresentation updating to reduce feature redundancy and minimize inter-feature\ncorrelations. (2) a smoothed data removal mechanism with loss perturbation that\ncreates information-theoretic safeguards against data leakage during removal\noperations. Extensive experiments on five benchmark datasets show that our\napproach outperforms other baselines and consistently achieves high predictive\naccuracy and robustness even under significant distribution shifts. The results\nhighlight its superior efficiency and adaptability in both in-distribution and\nout-of-distribution scenarios.",
    "published": "2025-09-27T18:23:21Z",
    "link": "http://arxiv.org/pdf/2509.23443v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wenhao Yang",
      "Lin Li",
      "Xiaohui Tao",
      "Kaize Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23442v1",
    "title": "S$^3$F-Net: A Multi-Modal Approach to Medical Image Classification via\n  Spatial-Spectral Summarizer Fusion Network",
    "summary": "Convolutional Neural Networks have become a cornerstone of medical image\nanalysis due to their proficiency in learning hierarchical spatial features.\nHowever, this focus on a single domain is inefficient at capturing global,\nholistic patterns and fails to explicitly model an image's frequency-domain\ncharacteristics. To address these challenges, we propose the Spatial-Spectral\nSummarizer Fusion Network (S$^3$F-Net), a dual-branch framework that learns\nfrom both spatial and spectral representations simultaneously. The S$^3$F-Net\nperforms a fusion of a deep spatial CNN with our proposed shallow spectral\nencoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer,\nwhich leverages the Convolution Theorem by applying a bank of learnable filters\ndirectly to an image's full Fourier spectrum via a computation-efficient\nelement-wise multiplication. This allows the SpectralFilter layer to attain a\nglobal receptive field instantaneously, with its output being distilled by a\nlightweight summarizer network. We evaluate S$^3$F-Net across four medical\nimaging datasets spanning different modalities to validate its efficacy and\ngeneralizability. Our framework consistently and significantly outperforms its\nstrong spatial-only baseline in all cases, with accuracy improvements of up to\n5.13%. With a powerful Bilinear Fusion, S$^3$F-Net achieves a SOTA competitive\naccuracy of 98.76% on the BRISC2025 dataset. Concatenation Fusion performs\nbetter on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11%\naccuracy, surpassing many top-performing, much deeper models. Our\nexplainability analysis also reveals that the S$^3$F-Net learns to dynamically\nadjust its reliance on each branch based on the input pathology. These results\nverify that our dual-domain approach is a powerful and generalizable paradigm\nfor medical image analysis.",
    "published": "2025-09-27T18:18:39Z",
    "link": "http://arxiv.org/pdf/2509.23442v1.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Md. Saiful Bari Siddiqui",
      "Mohammed Imamul Hassan Bhuiyan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23435v1",
    "title": "AudioRole: An Audio Dataset for Character Role-Playing in Large Language\n  Models",
    "summary": "The creation of high-quality multimodal datasets remains fundamental for\nadvancing role-playing capabilities in large language models (LLMs). While\nexisting works predominantly focus on text-based persona simulation, Audio\nRole-Playing (ARP) presents unique challenges due to the need for synchronized\nalignment of semantic content and vocal characteristics. To address this gap,\nwe propose AudioRole, a meticulously curated dataset from 13 TV series spanning\n1K+ hours with 1M+ character-grounded dialogues, providing synchronized\naudio-text pairs annotated with speaker identities and contextual metadata. In\naddition, to demonstrate the effectiveness of the dataset, we introduced\nARP-Eval, a dual-aspect evaluation framework that assesses both response\nquality and role fidelity. Empirical validation showing GLM-4-Voice trained on\nAudioRole (which we called ARP-Model) achieve an average Acoustic\nPersonalization score of 0.31, significantly outperforming the original\nGLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically\nsupports role-playing in one-shot scenarios. The ARP-Model also achieves a\nContent Personalization score of 0.36, surpassing the untrained original model\nby about 38% and maintaining the same level as MiniCPM-O-2.6.\n  AudioRole features dialogues from over 115 main characters, 6 trained\nARP-Models that role-play different characters, and evaluation protocols.\nTogether, they provide an essential resource for advancing audio-grounded\nrole-playing research.",
    "published": "2025-09-27T18:08:51Z",
    "link": "http://arxiv.org/pdf/2509.23435v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Wenyu Li",
      "Xiaoqi Jiao",
      "Yi Chang",
      "Guangyan Zhang",
      "Yiwen Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23434v1",
    "title": "NeuroBridge: Using Generative AI to Bridge Cross-neurotype Communication\n  Differences through Neurotypical Perspective-taking",
    "summary": "Communication challenges between autistic and neurotypical individuals stem\nfrom a mutual lack of understanding of each other's distinct, and often\ncontrasting, communication styles. Yet, autistic individuals are expected to\nadapt to neurotypical norms, making interactions inauthentic and mentally\nexhausting for them. To help redress this imbalance, we build NeuroBridge, an\nonline platform that utilizes large language models (LLMs) to simulate: (a) an\nAI character that is direct and literal, a style common among many autistic\nindividuals, and (b) four cross-neurotype communication scenarios in a\nfeedback-driven conversation between this character and a neurotypical user.\nThrough NeuroBridge, neurotypical individuals gain a firsthand look at autistic\ncommunication, and reflect on their role in shaping cross-neurotype\ninteractions. In a user study with 12 neurotypical participants, we find that\nNeuroBridge improved their understanding of how autistic people may interpret\nlanguage differently, with all describing autism as a social difference that\n\"needs understanding by others\" after completing the simulation. Participants\nvalued its personalized, interactive format and described AI-generated feedback\nas \"constructive\", \"logical\" and \"non-judgmental\". Most perceived the portrayal\nof autism in the simulation as accurate, suggesting that users may readily\naccept AI-generated (mis)representations of disabilities. To conclude, we\ndiscuss design implications for disability representation in AI, the need for\nmaking NeuroBridge more personalized, and LLMs' limitations in modeling complex\nsocial scenarios.",
    "published": "2025-09-27T18:05:41Z",
    "link": "http://arxiv.org/pdf/2509.23434v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Rukhshan Haroon",
      "Kyle Wigdor",
      "Katie Yang",
      "Nicole Toumanios",
      "Eileen T. Crehan",
      "Fahad Dogar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24007v1",
    "title": "Sequential Diffusion Language Models",
    "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
    "published": "2025-09-28T17:59:15Z",
    "link": "http://arxiv.org/pdf/2509.24007v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Yangzhou Liu",
      "Yue Cao",
      "Hao Li",
      "Gen Luo",
      "Zhe Chen",
      "Weiyun Wang",
      "Xiaobo Liang",
      "Biqing Qi",
      "Lijun Wu",
      "Changyao Tian",
      "Yanting Zhang",
      "Yuqiang Li",
      "Tong Lu",
      "Yu Qiao",
      "Jifeng Dai",
      "Wenhai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23979v1",
    "title": "ByteSized32Refactored: Towards an Extensible Interactive Text Games\n  Corpus for LLM World Modeling and Evaluation",
    "summary": "Simulating interactive world models remains a core challenge in Large\nLanguage Models(LLMs). In this work, we introduce the ByteSized32Refactored, a\nrefactored, modular, and extensible implementation of the original ByteSized32\ncorpus to explore the task of text game generation. We further optimize the\ncode structure of each text game and create the GameBasic.py foundation\nlibrary, which centralizes common logic across all 32 games by abstracting 7\nbase classes (GameObject, etc.) into reusable modules, thereby reducing from\n20k to 10k total lines of Python code compared to the original Bytesized32. Our\nrefactored implementation enables extendability - with our centralized design,\nByteSized32Refactored can be more efficiently extended to include text games of\nnew scenarios and specifications by reusing the shared logic and\nfunctionalities. Extensive experiments with GPT-4o demonstrate a mix of\nperformance - with Bytesized32Refactored, the generated text games for unseen\nscenarios showcase quality improvements on two of the four evaluation\ndimensions while decreases on the other two, indicating that the hierarchical\nstructure of the refactored code presents new challenges for LLMs. Overall, we\nhighlight that our extensible code structure, centered on the foundation\nlibrary and the modular optimization, not only facilitates LLM adaptation to\nenvironment specifications but also establishes a scalable environment that\nsupports future extensions.",
    "published": "2025-09-28T17:07:54Z",
    "link": "http://arxiv.org/pdf/2509.23979v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Haonan Wang",
      "Junfeng Sun",
      "Xingdi Yuan",
      "Ruoyao Wang",
      "Ziang Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23967v1",
    "title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs",
    "summary": "Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)\nreasoning to improve accuracy on complex tasks. However, always generating\nlengthy reasoning traces is inefficient, leading to excessive token usage and\nhigher inference costs. This paper introduces the Hybrid Policy Optimization\n(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to\nselectively decide when to engage in detailed reasoning (Think-on) and when to\nrespond directly (Think-off). Specifically, HiPO combines a hybrid data\npipelineproviding paired Think-on and Think-off responseswith a hybrid\nreinforcement learning reward system that balances accuracy and efficiency\nwhile avoiding over-reliance on detailed reasoning. Experiments across\nmathematics and coding benchmarks demonstrate that HiPO can substantially\nreduce token length while maintaining or improving accuracy. Finally, we hope\nHiPO a can be a principled approach for efficient adaptive reasoning, advancing\nthe deployment of reasoning-oriented LLMs in real-world, resource-sensitive\nsettings.",
    "published": "2025-09-28T16:46:12Z",
    "link": "http://arxiv.org/pdf/2509.23967v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ken Deng",
      "Zizheng Zhan",
      "Wen Xiang",
      "Wenqiang Zhu",
      "Tianhao Peng",
      "Xinping Lei",
      "Weihao Li",
      "Jingxuan Xu",
      "Kun Wu",
      "Yifan Yao",
      "Haoyang Huang",
      "Huaixi Tang",
      "Kepeng Lei",
      "Zhiyi Lai",
      "Songwei Yu",
      "Zongxian Feng",
      "Zuchen Gao",
      "Weihao Xie",
      "Chenchen Zhang",
      "Yanan Wu",
      "Yuanxing Zhang",
      "Lecheng Huang",
      "Yuqun Zhang",
      "Jie Liu",
      "Zhaoxiang Zhang",
      "Haotian Zhang",
      "Bin Chen",
      "Jiaheng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23964v1",
    "title": "Detecting and Rectifying Noisy Labels: A Similarity-based Approach",
    "summary": "Label noise in datasets could damage the performance of neural net training.\nAs the size of modern deep networks grows, there is a growing demand for\nautomated tools for detecting such errors. In this paper, we propose post-hoc,\nmodel-agnostic error detection and rectification methods utilizing the\npenultimate feature from a neural network. Our idea is based on the observation\nthat the similarity between the penultimate feature of a mislabeled data point\nand its true class data points is higher than that for data points from other\nclasses, making the probability of label occurrence within a tight, similar\ncluster informative for detecting and rectifying errors. Extensive experiments\nshow our method not only demonstrates high performance across various noises\nbut also automatically rectifies these errors to improve the quality of\ndatasets.",
    "published": "2025-09-28T16:41:56Z",
    "link": "http://arxiv.org/pdf/2509.23964v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Dang Huu-Tien",
      "Naoya Inoue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23936v1",
    "title": "Assessing Large Language Models in Updating Their Forecasts with New\n  Information",
    "summary": "Prior work has largely treated future event prediction as a static task,\nfailing to consider how forecasts and the confidence in them should evolve as\nnew evidence emerges. To address this gap, we introduce EVOLVECAST, a framework\nfor evaluating whether large language models appropriately revise their\npredictions in response to new information. In particular, EVOLVECAST assesses\nwhether LLMs adjust their forecasts when presented with information released\nafter their training cutoff. We use human forecasters as a comparative\nreference to analyze prediction shifts and confidence calibration under updated\ncontexts. While LLMs demonstrate some responsiveness to new information, their\nupdates are often inconsistent or overly conservative. We further find that\nneither verbalized nor logits-based confidence estimates consistently\noutperform the other, and both remain far from the human reference standard.\nAcross settings, models tend to express conservative bias, underscoring the\nneed for more robust approaches to belief updating.",
    "published": "2025-09-28T15:16:20Z",
    "link": "http://arxiv.org/pdf/2509.23936v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhangdie Yuan",
      "Zifeng Ding",
      "Andreas Vlachos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23933v1",
    "title": "Beyond Benchmarks: Understanding Mixture-of-Experts Models through\n  Internal Mechanisms",
    "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising direction,\noffering efficiency and scalability by activating only a subset of parameters\nduring inference. However, current research remains largely\nperformance-centric, with limited understanding of its internal mechanisms,\nthereby constraining broader progress. In this work, we use an internal metric\nto investigate the mechanisms of MoE architecture by explicitly incorporating\nrouting mechanisms and analyzing expert-level behaviors. Through systematic\nanalyses of a wide range of publicly available MoE models, we uncover several\nfindings: (1) neuron utilization decreases as models evolve, reflecting\nstronger generalization; (2) training exhibits a dynamic trajectory, where\nbenchmark performance alone provides limited signal while MUI reveals deeper\ninsights; (3) task completion emerges from collaborative contributions of\nmultiple experts, with shared experts driving concentration; and (4) activation\npatterns at the neuron level provide a fine-grained proxy for data diversity.\nTogether, these results demonstrate the potential of MUI as a complementary\nindicator to benchmark performance, offering new insights into the capacity,\ndynamics, and specialization of MoE models. Our project can be found at\nhttps://yingjiahao14.github.io/MoE-MUI/.",
    "published": "2025-09-28T15:13:38Z",
    "link": "http://arxiv.org/pdf/2509.23933v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Jiahao Ying",
      "Mingbao Lin",
      "Qianru Sun",
      "Yixin Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23883v1",
    "title": "DocPruner: A Storage-Efficient Framework for Multi-Vector Visual\n  Document Retrieval via Adaptive Patch-Level Embedding Pruning",
    "summary": "Visual Document Retrieval (VDR), the task of retrieving visually-rich\ndocument pages using queries that combine visual and textual cues, is crucial\nfor numerous real-world applications. Recent state-of-the-art methods leverage\nLarge Vision-Language Models (LVLMs) in a multi-vector paradigm, representing\neach document as patch-level embeddings to capture fine-grained details. While\nhighly effective, this approach introduces a critical challenge: prohibitive\nstorage overhead, as storing hundreds of vectors per page makes large-scale\ndeployment costly and impractical. To address this, we introduce DocPruner, the\nfirst framework to employ adaptive patch-level embedding pruning for VDR to\neffectively reduce the storage overhead. DocPruner leverages the intra-document\npatch attention distribution to dynamically identify and discard redundant\nembeddings for each document. This adaptive mechanism enables a significant\n50-60% reduction in storage for leading multi-vector VDR models with negligible\ndegradation in document retrieval performance. Extensive experiments across\nmore than ten representative datasets validate that DocPruner offers a robust,\nflexible, and effective solution for building storage-efficient, large-scale\nVDR systems.",
    "published": "2025-09-28T13:47:24Z",
    "link": "http://arxiv.org/pdf/2509.23883v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Yibo Yan",
      "Guangwei Xu",
      "Xin Zou",
      "Shuliang Liu",
      "James Kwok",
      "Xuming Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23873v1",
    "title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token\n  Pruning for Efficient Supervised Fine-Tuning",
    "summary": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step\ninto a compute-intensive phase rivaling mid-training in scale, data efficiency\nhas become critical for aligning large language models (LLMs) under tight\nbudgets. Existing data pruning methods suffer from a fragmented design: they\noperate either at the sample level or the token level in isolation, failing to\njointly optimize both dimensions. This disconnect leads to significant\ninefficiencies--high-value samples may still contain redundant tokens, while\ntoken-level pruning often discards crucial instructional or corrective signals\nembedded in individual examples. To address this bottleneck, we introduce the\nError-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes\nthe heterogeneous utility of training data across samples and tokens. Guided by\nthis insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework\nthat strategically coordinates sample pruning and token pruning. Q-Tuning\nemploys a two-stage strategy: first, it performs sample-level triage to retain\nexamples rich in informative misconceptions or calibration signals; second, it\napplies an asymmetric token-pruning policy, using a context-aware scoring\nmechanism to trim less salient tokens exclusively from misconception samples\nwhile preserving calibration samples in their entirety. Our method sets a new\nstate of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,\nQ-Tuning achieves a +38\\% average improvement over the full-data SFT baseline\nusing only 12.5\\% of the original training data. As the first dynamic pruning\napproach to consistently outperform full-data training, Q-Tuning provides a\npractical and scalable blueprint for maximizing data utilization in\nbudget-constrained LLM SFT.",
    "published": "2025-09-28T13:27:38Z",
    "link": "http://arxiv.org/pdf/2509.23873v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shaobo Wang",
      "Jiaming Wang",
      "Jiajun Zhang",
      "Cong Wang",
      "Yue Min",
      "Zichen Wen",
      "Fei Huang",
      "Huiqiang Jiang",
      "Junyang Lin",
      "Dayiheng Liu",
      "Linfeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23863v1",
    "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context\n  Language Models",
    "summary": "Progress in long-context reasoning for large language models (LLMs) has\nlagged behind other recent advances. This gap arises not only from the\nintrinsic difficulty of processing long texts, but also from the scarcity of\nreliable human annotations and programmatically verifiable reward signals. In\nthis paper, we propose SPELL, a multi-role self-play reinforcement learning\nframework that enables scalable, label-free optimization for long-context\nreasoning. SPELL integrates three cyclical roles-questioner, responder, and\nverifier-within a single model to enable continual self-improvement. The\nquestioner generates questions from raw documents paired with reference\nanswers; the responder learns to solve these questions based on the documents;\nand the verifier evaluates semantic equivalence between the responder's output\nand the questioner's reference answer, producing reward signals to guide\ncontinual training. To stabilize training, we introduce an automated curriculum\nthat gradually increases document length and a reward function that adapts\nquestion difficulty to the model's evolving capabilities. Extensive experiments\non six long-context benchmarks show that SPELL consistently improves\nperformance across diverse LLMs and outperforms equally sized models fine-tuned\non large-scale annotated data. Notably, SPELL achieves an average 7.6-point\ngain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising\nits performance ceiling and showing promise for scaling to even more capable\nmodels.",
    "published": "2025-09-28T13:08:10Z",
    "link": "http://arxiv.org/pdf/2509.23863v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ziyi Yang",
      "Weizhou Shen",
      "Ruijun Chen",
      "Chenliang Li",
      "Fanqi Wan",
      "Ming Yan",
      "Xiaojun Quan",
      "Fei Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23808v1",
    "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR",
    "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
    "published": "2025-09-28T11:14:58Z",
    "link": "http://arxiv.org/pdf/2509.23808v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Fanding Huang",
      "Guanbo Huang",
      "Xiao Fan",
      "Yi He",
      "Xiao Liang",
      "Xiao Chen",
      "Qinting Jiang",
      "Faisal Nadeem Khan",
      "Jingyan Jiang",
      "Zhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23805v1",
    "title": "Open-DeBias: Toward Mitigating Open-Set Bias in Language Models",
    "summary": "Large Language Models (LLMs) have achieved remarkable success on question\nanswering (QA) tasks, yet they often encode harmful biases that compromise\nfairness and trustworthiness. Most existing bias mitigation approaches are\nrestricted to predefined categories, limiting their ability to address novel or\ncontext-specific emergent biases. To bridge this gap, we tackle the novel\nproblem of open-set bias detection and mitigation in text-based QA. We\nintroduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases\nacross a wide range of categories and subgroups, encompassing both known and\npreviously unseen biases. Additionally, we propose Open-DeBias, a novel,\ndata-efficient, and parameter-efficient debiasing method that leverages adapter\nmodules to mitigate existing social and stereotypical biases while generalizing\nto unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias\nimproves QA accuracy on BBQ dataset by nearly $48\\%$ on ambiguous subsets and\n$6\\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction\nof the training data. Remarkably, the same adapters, in a zero-shot transfer to\nKorean BBQ, achieve $84\\%$ accuracy, demonstrating robust language-agnostic\ngeneralization. Through extensive evaluation, we also validate the\neffectiveness of Open-DeBias across a broad range of NLP tasks, including\nStereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,\nand suitability for general-purpose, open-domain bias mitigation. The project\npage is available at: https://sites.google.com/view/open-debias25",
    "published": "2025-09-28T11:08:39Z",
    "link": "http://arxiv.org/pdf/2509.23805v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Arti Rani",
      "Shweta Singh",
      "Nihar Ranjan Sahoo",
      "Gaurav Kumar Nayak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23793v1",
    "title": "Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented\n  Generation for Islamic Knowledge Question Answering",
    "summary": "This paper presents our submission to the QIAS 2025 shared task on Islamic\nknowledge understanding and reasoning. We developed a hybrid\nretrieval-augmented generation (RAG) system that combines sparse and dense\nretrieval methods with cross-encoder reranking to improve large language model\n(LLM) performance. Our three-stage pipeline incorporates BM25 for initial\nretrieval, a dense embedding retrieval model for semantic matching, and\ncross-encoder reranking for precise content retrieval. We evaluate our approach\non both subtasks using two LLMs, Fanar and Mistral, demonstrating that the\nproposed RAG pipeline enhances performance across both, with accuracy\nimprovements up to 25%, depending on the task and model configuration. Our best\nconfiguration is achieved with Fanar, yielding accuracy scores of 45% in\nSubtask 1 and 80% in Subtask 2.",
    "published": "2025-09-28T10:27:08Z",
    "link": "http://arxiv.org/pdf/2509.23793v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Muhammad Abu Ahmad",
      "Mohamad Ballout",
      "Raia Abu Ahmad",
      "Elia Bruni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23782v1",
    "title": "Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice\n  Questions",
    "summary": "Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)\ndespite demonstrating correct knowledge in other contexts, such as free-form\ngeneration. To investigate the mechanism underlying this knowledge-prediction\ngap on MCQs and alleviate it, we conduct a probing analysis and find that\nresidual streams in certain layers contain a subspace spanned by two important\nbases: a \\emph{knowledge basis} that encodes the probability of the\nground-truth answer for a given MCQ and a \\emph{prediction basis} that encodes\nthe probability of the answer choice predicted by the model. We observe that\nincorrect predictions arise from a misalignment of the model's hidden states\nalong these two bases. Hence, we introduce \\textbf{KAPPA} (Knowledge-Aligned\nPrediction through Projection-based Adjustment), a parameter-free intervention\nthat transforms the hidden states to align the prediction coordinate with the\nknowledge coordinate within this subspace. Experiments on binary-choice\nreformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA\nsubstantially improves accuracy and consistently outperforms baselines. While\noptimal subspaces differ across tasks, subspaces generalize to some extent, as\nsupported by cross-dataset experiments. Moreover, KAPPA extends its\neffectiveness to free-form questions beyond MCQs. Our work provides a new\ngeometric understanding of the knowledge-prediction gap and offers a practical\nmethod for better aligning model behavior with its latent knowledge.",
    "published": "2025-09-28T09:57:24Z",
    "link": "http://arxiv.org/pdf/2509.23782v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yoonah Park",
      "Haesung Pyun",
      "Yohan Jo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23753v1",
    "title": "Anchored Supervised Fine-Tuning",
    "summary": "Post-training of large language models involves a fundamental trade-off\nbetween supervised fine-tuning (SFT), which efficiently mimics demonstrations\nbut tends to memorize, and reinforcement learning (RL), which achieves better\ngeneralization at higher computational cost. Dynamic Fine-Tuning (DFT) recently\nemerged as a promising middle ground, reweighting SFT objectives with token\nprobabilities and achieving improvements in certain reasoning domains, though\nit exhibits instability in other tasks. We provide a analysis of DFT through\nthe reward-weighted regression (RWR) framework, revealing that it corresponds\nto a specific auxiliary distribution choice that yields provably tighter RL\nbounds than standard SFT. However, our analysis also uncovers a critical\nlimitation: this construction lacks distributional anchoring, leading to\nprogressive drift that undermines training stability. To address this, we\npropose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's\nreweighting with lightweight KL regularization to preserve tightness while\nensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT\nacross mathematical reasoning, medical knowledge grounding, and code\ngeneration, achieving substantial improvements with minimal computational\noverhead. Our RWR framework provides a systematic lens for understanding\npost-training methods and demonstrates that principled theoretical analysis\nleads to both stronger guarantees and practical gains.",
    "published": "2025-09-28T08:58:12Z",
    "link": "http://arxiv.org/pdf/2509.23753v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "He Zhu",
      "Junyou Su",
      "Peng Lai",
      "Ren Ma",
      "Wenjia Zhang",
      "Linyi Yang",
      "Guanhua Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23747v1",
    "title": "Beyond Game Theory Optimal: Profit-Maximizing Poker Agents for No-Limit\n  Holdem",
    "summary": "Game theory has grown into a major field over the past few decades, and poker\nhas long served as one of its key case studies. Game-Theory-Optimal (GTO)\nprovides strategies to avoid loss in poker, but pure GTO does not guarantee\nmaximum profit. To this end, we aim to develop a model that outperforms GTO\nstrategies to maximize profit in No Limit Holdem, in heads-up (two-player) and\nmulti-way (more than two-player) situations. Our model finds the GTO foundation\nand goes further to exploit opponents. The model first navigates toward many\nsimulated poker hands against itself and keeps adjusting its decisions until no\naction can reliably beat it, creating a strong baseline that is close to the\ntheoretical best strategy. Then, it adapts by observing opponent behavior and\nadjusting its strategy to capture extra value accordingly. Our results indicate\nthat Monte-Carlo Counterfactual Regret Minimization (CFR) performs best in\nheads-up situations and CFR remains the strongest method in most multi-way\nsituations. By combining the defensive strength of GTO with real-time\nexploitation, our approach aims to show how poker agents can move from merely\nnot losing to consistently winning against diverse opponents.",
    "published": "2025-09-28T08:51:57Z",
    "link": "http://arxiv.org/pdf/2509.23747v1.pdf",
    "category": [
      "cs.GT",
      "cs.CL"
    ],
    "authors": [
      "SeungHyun Yi",
      "Seungjun Yi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23715v1",
    "title": "Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and\n  Fine-Tuned Question Answering",
    "summary": "Ensuring that both new and experienced drivers master current traffic rules\nis critical to road safety. This paper evaluates Large Language Models (LLMs)\non Romanian driving-law QA with explanation generation. We release a\n1{,}208-question dataset (387 multimodal) and compare text-only and multimodal\nSOTA systems, then measure the impact of domain-specific fine-tuning for Llama\n3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but\nfine-tuned 8B models are competitive. Textual descriptions of images outperform\ndirect visual input. Finally, an LLM-as-a-Judge assesses explanation quality,\nrevealing self-preference bias. The study informs explainable QA for\nless-resourced languages.",
    "published": "2025-09-28T07:58:00Z",
    "link": "http://arxiv.org/pdf/2509.23715v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Eduard Barbu",
      "Adrian Marius Dumitran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23714v1",
    "title": "Collaboration of Fusion and Independence: Hypercomplex-driven Robust\n  Multi-Modal Knowledge Graph Completion",
    "summary": "Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts\nin multi-modal knowledge graphs (MMKGs) by leveraging both structural\nrelationships and diverse modality information of entities. Existing MMKGC\nmethods follow two multi-modal paradigms: fusion-based and ensemble-based.\nFusion-based methods employ fixed fusion strategies, which inevitably leads to\nthe loss of modality-specific information and a lack of flexibility to adapt to\nvarying modality relevance across contexts. In contrast, ensemble-based methods\nretain modality independence through dedicated sub-models but struggle to\ncapture the nuanced, context-dependent semantic interplay between modalities.\nTo overcome these dual limitations, we propose a novel MMKGC method M-Hyper,\nwhich achieves the coexistence and collaboration of fused and independent\nmodality representations. Our method integrates the strengths of both\nparadigms, enabling effective cross-modal interactions while maintaining\nmodality-specific information. Inspired by ``quaternion'' algebra, we utilize\nits four orthogonal bases to represent multiple independent modalities and\nemploy the Hamilton product to efficiently model pair-wise interactions among\nthem. Specifically, we introduce a Fine-grained Entity Representation\nFactorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)\nmodule to obtain robust representations for three independent modalities and\none fused modality. The resulting four modality representations are then mapped\nto the four orthogonal bases of a biquaternion (a hypercomplex extension of\nquaternion) for comprehensive modality interaction. Extensive experiments\nindicate its state-of-the-art performance, robustness, and computational\nefficiency.",
    "published": "2025-09-28T07:55:01Z",
    "link": "http://arxiv.org/pdf/2509.23714v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhiqiang Liu",
      "Yichi Zhang",
      "Mengshu Sun",
      "Lei Liang",
      "Wen Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23698v1",
    "title": "VIVA+: Human-Centered Situational Decision-Making",
    "summary": "Multimodal Large Language Models (MLLMs) show promising results for embodied\nagents in operating meaningfully in complex, human-centered environments. Yet,\nevaluating their capacity for nuanced, human-like reasoning and decision-making\nremains challenging. In this work, we introduce VIVA+, a cognitively grounded\nbenchmark for evaluating the reasoning and decision-making of MLLMs in\nhuman-centered situations. VIVA+ consists of 1,317 real-world situations paired\nwith 6,373 multiple-choice questions, targeting three core abilities for\ndecision-making: (1) Foundational Situation Comprehension, (2) Context-Driven\nAction Justification, and (3) Reflective Reasoning. Together, these dimensions\nprovide a systematic framework for assessing a model's ability to perceive,\nreason, and act in socially meaningful ways. We evaluate the latest commercial\nand open-source models on VIVA+, where we reveal distinct performance patterns\nand highlight significant challenges. We further explore targeted training and\nmulti-step reasoning strategies, which yield consistent performance\nimprovements. Finally, our in-depth analysis highlights current model\nlimitations and provides actionable insights for advancing MLLMs toward more\nrobust, context-aware, and socially adept decision-making in real-world\nsettings.",
    "published": "2025-09-28T07:13:11Z",
    "link": "http://arxiv.org/pdf/2509.23698v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhe Hu",
      "Yixiao Ren",
      "Guanzhong Liu",
      "Jing Li",
      "Yu Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23690v1",
    "title": "HomeSafeBench: A Benchmark for Embodied Vision-Language Models in\n  Free-Exploration Home Safety Inspection",
    "summary": "Embodied agents can identify and report safety hazards in the home\nenvironments. Accurately evaluating their capabilities in home safety\ninspection tasks is curcial, but existing benchmarks suffer from two key\nlimitations. First, they oversimplify safety inspection tasks by using textual\ndescriptions of the environment instead of direct visual information, which\nhinders the accurate evaluation of embodied agents based on Vision-Language\nModels (VLMs). Second, they use a single, static viewpoint for environmental\nobservation, which restricts the agents' free exploration and cause the\nomission of certain safety hazards, especially those that are occluded from a\nfixed viewpoint. To alleviate these issues, we propose HomeSafeBench, a\nbenchmark with 12,900 data points covering five common home safety hazards:\nfire, electric shock, falling object, trips, and child safety. HomeSafeBench\nprovides dynamic first-person perspective images from simulated home\nenvironments, enabling the evaluation of VLM capabilities for home safety\ninspection. By allowing the embodied agents to freely explore the room,\nHomeSafeBench provides multiple dynamic perspectives in complex environments\nfor a more thorough inspection. Our comprehensive evaluation of mainstream VLMs\non HomeSafeBench reveals that even the best-performing model achieves an\nF1-score of only 10.23%, demonstrating significant limitations in current VLMs.\nThe models particularly struggle with identifying safety hazards and selecting\neffective exploration strategies. We hope HomeSafeBench will provide valuable\nreference and support for future research related to home security inspections.\nOur dataset and code will be publicly available soon.",
    "published": "2025-09-28T07:01:27Z",
    "link": "http://arxiv.org/pdf/2509.23690v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Siyuan Gao",
      "Jiashu Yao",
      "Haoyu Wen",
      "Yuhang Guo",
      "Zeming Liu",
      "Heyan Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23686v1",
    "title": "TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in\n  System F",
    "summary": "Large Language Models (LLMs) are increasingly integrated into the software\nengineering ecosystem. Their test-time compute (TTC) reasoning capabilities\nshow significant potential for understanding program logic and semantics beyond\nmere token recognition. However, current benchmarks for code reasoning lack a\nformal, program-centric deductive framework to ensure sound evaluation, and are\nincapable of assessing whether models genuinely reason about program semantics\nor merely exploit superficial associations between natural language and code\ntokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to\nevaluate LLM reasoning based on type inference in System F, a task we refer to\nas program semantics reasoning. By employing verified transformations to remove\nsemantically irrelevant natural language, we construct TF-Bench_pure, a purely\nsemantics-driven variant of TF-Bench. Our analysis reveals substantial\nlimitations in state-of-the-art LLMs, with the best-performing LLM\n(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.\nAdditionally, we propose two novel metrics to assess robustness and the\neffectiveness of test-time reasoning, underscoring critical limitations in\ncurrent LLM capabilities and highlighting essential directions for future\nresearch.",
    "published": "2025-09-28T06:57:42Z",
    "link": "http://arxiv.org/pdf/2509.23686v1.pdf",
    "category": [
      "cs.CL",
      "cs.PL",
      "cs.SE"
    ],
    "authors": [
      "Yifeng He",
      "Luning Yang",
      "Christopher Castro Gaw Gonzalo",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23657v1",
    "title": "Beyond English-Centric Training: How Reinforcement Learning Improves\n  Cross-Lingual Reasoning in LLMs",
    "summary": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs)\nattracts widespread attention. While reinforcement learning (RL) has shown\nsuperior performance for improving complex reasoning, its impact on\ncross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains\nunexplored. We present the first systematic investigation into cross-lingual\nreasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation\nmodel, we conduct experiments on diverse multilingual reasoning benchmarks,\nincluding math reasoning, commonsense reasoning, and scientific reasoning. Our\ninvestigation yields two significant findings: (1) Tuning with RL not only\nachieves higher accuracy but also demonstrates substantially stronger\ncross-lingual generalization capabilities compared to SFT. (2) RL training on\nnon-English data yields better overall performance and generalization than\ntraining on English data, which is not observed with SFT. Furthermore, through\ncomprehensive mechanistic analyses, we explore the underlying factors of RL's\nsuperiority and generalization across languages. Our results provide compelling\nevidence that RL enables the model with more robust reasoning strategies,\noffering crucial guidance for more equitable and effective multilingual\nreasoning.",
    "published": "2025-09-28T05:48:39Z",
    "link": "http://arxiv.org/pdf/2509.23657v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shulin Huang",
      "Yiran Ding",
      "Junshu Pan",
      "Yue Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23653v1",
    "title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language\n  Models",
    "summary": "Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect\ntokens: once a token is generated, it typically remains fixed. The key\nchallenge is to identify potential errors in the inputs. In this paper, we\npropose \\emph{\\underline{Rem}asking-\\underline{e}nabled \\underline{Di}ffusion\nLanguage Model (RemeDi}, a mask-based DLM that introduces \\emph{remasking} as\nanother fundamental mechanism, enabling more flexible text refinement in\ndiffusion-based text generation. To achieve this, RemeDi jointly predicts token\ndistributions and per-token confidence scores at each step. The confidence\nscores determine which tokens to be unmasked after the current step, allowing\nthe model to identify tokens with low quality and remask them. These remasked\ntokens can be resampled with richer context in subsequent steps. We design a\nremask-aware pipeline to train this ability, including supervised fine-tuning\nwhich teaches the model to detect and remask incorrect tokens in addition to\npredict mask tokens, and reinforcement learning which optimizes full generation\ntrajectories toward higher rewards. Experiments show that RemeDi achieves the\nstate-of-the-art results among open-source DLMs on multiple datasets.",
    "published": "2025-09-28T05:39:49Z",
    "link": "http://arxiv.org/pdf/2509.23653v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zemin Huang",
      "Yuhang Wang",
      "Zhiyang Chen",
      "Guo-Jun Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23649v1",
    "title": "From Past To Path: Masked History Learning for Next-Item Prediction in\n  Generative Recommendation",
    "summary": "Generative recommendation, which directly generates item identifiers, has\nemerged as a promising paradigm for recommendation systems. However, its\npotential is fundamentally constrained by the reliance on purely autoregressive\ntraining. This approach focuses solely on predicting the next item while\nignoring the rich internal structure of a user's interaction history, thus\nfailing to grasp the underlying intent. To address this limitation, we propose\nMasked History Learning (MHL), a novel training framework that shifts the\nobjective from simple next-step prediction to deep comprehension of history.\nMHL augments the standard autoregressive objective with an auxiliary task of\nreconstructing masked historical items, compelling the model to understand\n``why'' an item path is formed from the user's past behaviors, rather than just\n``what'' item comes next. We introduce two key contributions to enhance this\nframework: (1) an entropy-guided masking policy that intelligently targets the\nmost informative historical items for reconstruction, and (2) a curriculum\nlearning scheduler that progressively transitions from history reconstruction\nto future prediction. Experiments on three public datasets show that our method\nsignificantly outperforms state-of-the-art generative models, highlighting that\na comprehensive understanding of the past is crucial for accurately predicting\na user's future path. The code will be released to the public.",
    "published": "2025-09-28T05:22:19Z",
    "link": "http://arxiv.org/pdf/2509.23649v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "KaiWen Wei",
      "Kejun He",
      "Xiaomian Kang",
      "Jie Zhang",
      "Yuming Yang",
      "Jiang Zhong",
      "He Bai",
      "Junnan Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23633v1",
    "title": "Fast Thinking for Large Language Models",
    "summary": "Reasoning-oriented Large Language Models (LLMs) often rely on generating\nexplicit tokens step by step, and their effectiveness typically hinges on\nlarge-scale supervised fine-tuning or reinforcement learning. While\nChain-of-Thought (CoT) techniques substantially enhance performance on complex\nreasoning tasks, they remain inefficient, requiring long reasoning traces that\nincrease latency and token usage. In this work, we introduce Latent Codebooks\nfor Fast Thinking, a framework that uses concise CoT sketches only during\ntraining to learn a codebook of discrete strategy priors. At inference, the\nmodel conditions on a handful of continuous thinking vectors distilled from the\ncodebook in a single pass, enabling strategy-level guidance without producing\nexplicit reasoning tokens. To complement this design, we propose GainRouter, a\nlightweight routing mechanism that adaptively switches between fast codebook\nguided inference and slow explicit reasoning, thereby suppressing overthinking\nand reducing unnecessary token generation. Experiments across multiple\nreasoning benchmarks show that our approach achieves competitive or superior\naccuracy while substantially lowering inference cost, offering a practical path\ntoward efficient and controllable reasoning in large language models.",
    "published": "2025-09-28T04:19:48Z",
    "link": "http://arxiv.org/pdf/2509.23633v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Haoyu Zheng",
      "Zhuonan Wang",
      "Yuqian Yuan",
      "Tianwei Lin",
      "Wenqiao Zhang",
      "Zheqi Lv",
      "Juncheng Li",
      "Siliang Tang",
      "Yueting Zhuang",
      "Hongyang He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23580v1",
    "title": "LLM Hallucination Detection: HSAD",
    "summary": "Although Large Language Models have demonstrated powerful capabilities in a\nwide range of tasks such as language understanding and code generation, the\nfrequent occurrence of hallucinations during the generation process has become\na significant impediment to their deployment in critical application scenarios.\nCurrent mainstream hallucination detection methods rely on factual consistency\nverification or static hidden layer features. The former is constrained by the\nscope of knowledge coverage, while the latter struggles to capture reasoning\nbiases during the inference process. To address these issues, and inspired by\nsignal analysis methods in cognitive neuroscience, this paper proposes a\nhallucination detection method based on the frequency-domain analysis of hidden\nlayer temporal signals, named HSAD (\\textbf{H}idden \\textbf{S}ignal\n\\textbf{A}nalysis-based \\textbf{D}etection). First, by treating the LLM's\nreasoning process as a cognitive journey that unfolds over time, we propose\nmodeling and simulating the human process of signal perception and\ndiscrimination in a deception-detection scenario through hidden layer temporal\nsignals. Next, The Fast Fourier Transform is applied to map these temporal\nsignals into the frequency domain to construct spectral features, which are\nused to capture anomalies that arise during the reasoning process; analysis\nexperiments on these spectral features have proven the effectiveness of this\napproach. Finally, a hallucination detection algorithm is designed based on\nthese spectral features to identify hallucinations in the generated content. By\neffectively combining the modeling of the reasoning process with\nfrequency-domain feature extraction, the HSAD method overcomes the limitations\nof existing approaches in terms of knowledge coverage and the detection of\nreasoning biases, demonstrating higher detection accuracy and robustness.",
    "published": "2025-09-28T02:25:34Z",
    "link": "http://arxiv.org/pdf/2509.23580v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "JinXin Li",
      "Gang Tu",
      "JunJie Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23579v1",
    "title": "Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language\n  Models on Text-to-JQL Tasks",
    "summary": "Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter\nissues from Jira. Yet, to our knowledge, there is no open, real-world,\nexecution-based benchmark for mapping natural language queries to JQL. We\nintroduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000\nnatural language (NL) requests paired with validated JQL queries and\nexecution-based results on a live Jira instance with over 200,000 issues. To\nreflect real-world usage, each JQL query is associated with four types of user\nrequests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)\nSemantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,\ntogether with an execution-based scoring toolkit, and a static snapshot of the\nevaluated Jira instance for reproducibility. We report text-to-JQL results on\n23 Large Language Models (LLMs) spanning parameter sizes, open and closed\nsource models, across execution accuracy, exact match, and canonical exact\nmatch. In this paper, we report results on Jackal-5K, a 5,000-pair subset of\nJackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only\n60.3% execution accuracy averaged equally across four user request types.\nPerformance varies significantly across user request types: (i) Long NL\n(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)\nSemantically Exact (99.3%). By benchmarking LLMs on their ability to produce\ncorrect and executable JQL queries, Jackal exposes the limitations of current\nstate-of-the-art LLMs and sets a new, execution-based challenge for future\nresearch in Jira enterprise data.",
    "published": "2025-09-28T02:23:22Z",
    "link": "http://arxiv.org/pdf/2509.23579v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kevin Frank",
      "Anmol Gulati",
      "Elias Lumer",
      "Sindy Campagna",
      "Vamse Kumar Subbiah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23504v1",
    "title": "AraS2P: Arabic Speech-to-Phonemes System",
    "summary": "This paper describes AraS2P, our speech-to-phonemes system submitted to the\nIqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training\nstrategy. In the first stage, task-adaptive continue pretraining was performed\non large-scale Arabic speech-phonemes datasets, which were generated by\nconverting the Arabic text using the MSA Phonetiser. In the second stage, the\nmodel was fine-tuned on the official shared task data, with additional\naugmentation from XTTS-v2-synthesized recitations featuring varied Ayat\nsegments, speaker embeddings, and textual perturbations to simulate possible\nhuman errors. The system ranked first on the official leaderboard,\ndemonstrating that phoneme-aware pretraining combined with targeted\naugmentation yields strong performance in phoneme-level mispronunciation\ndetection.",
    "published": "2025-09-27T21:25:20Z",
    "link": "http://arxiv.org/pdf/2509.23504v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Bassam Matar",
      "Mohamed Fayed",
      "Ayman Khalafallah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23499v1",
    "title": "Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional",
    "summary": "Understanding the interplay between intra-modality dependencies (the\ncontribution of an individual modality to a target task) and inter-modality\ndependencies (the relationships between modalities and the target task) is\nfundamental to advancing multi-modal learning. However, the nature of and\ninteraction between these dependencies within current benchmark evaluations\nremains poorly characterized. In this work, we present a large-scale empirical\nstudy to quantify these dependencies across 23 visual question-answering\nbenchmarks using multi-modal large language models (MLLMs) covering domains\nsuch as general and expert knowledge reasoning, optical character recognition,\nand document understanding. Our findings show that the reliance on vision,\nquestion (text), and their interaction varies significantly, both across and\nwithin benchmarks. We discover that numerous benchmarks intended to mitigate\ntext-only biases have inadvertently amplified image-only dependencies. This\ncharacterization persists across model sizes, as larger models often use these\nintra-modality dependencies to achieve high performance that mask an underlying\nlack of multi-modal reasoning. We provide a quantitative characterization of\nmulti-modal datasets, enabling a principled approach to multi-modal benchmark\ndesign and evaluation.",
    "published": "2025-09-27T21:13:29Z",
    "link": "http://arxiv.org/pdf/2509.23499v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Divyam Madaan",
      "Varshan Muhunthan",
      "Kyunghyun Cho",
      "Sumit Chopra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23487v1",
    "title": "Temporal Generalization: A Reality Check",
    "summary": "Machine learning (ML) models often struggle to maintain performance under\ndistribution shifts, leading to inaccurate predictions on unseen future data.\nIn this work, we investigate whether and under what conditions models can\nachieve such a generalization when relying solely on past data. We explore two\nprimary approaches: convex combinations of past model parameters\n(\\emph{parameter interpolation}) and explicit extrapolation beyond the convex\nhull of past parameters (\\emph{parameter extrapolation}). We benchmark several\nmethods within these categories on a diverse set of temporal tasks, including\nlanguage modeling, news summarization, news tag prediction, academic paper\ncategorization, satellite image-based land use classification over time, and\nhistorical yearbook photo gender prediction. Our empirical findings show that\nnone of the evaluated methods consistently outperforms the simple baseline of\nusing the latest available model parameters in all scenarios. In the absence of\naccess to future data or robust assumptions about the underlying\ndata-generating process, these results underscore the inherent difficulties of\ngeneralizing and extrapolating to future data and warrant caution when\nevaluating claims of such generalization.",
    "published": "2025-09-27T20:20:44Z",
    "link": "http://arxiv.org/pdf/2509.23487v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Divyam Madaan",
      "Sumit Chopra",
      "Kyunghyun Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23459v1",
    "title": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction",
    "summary": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy.",
    "published": "2025-09-27T19:07:50Z",
    "link": "http://arxiv.org/pdf/2509.23459v1.pdf",
    "category": [
      "cs.CR",
      "cs.CL"
    ],
    "authors": [
      "Sepideh Abedini",
      "Shubhankar Mohapatra",
      "D. B. Emerson",
      "Masoumeh Shafieinejad",
      "Jesse C. Cresswell",
      "Xi He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23452v1",
    "title": "FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based\n  Diffusion Editing",
    "summary": "Frame of Reference (FoR) is a fundamental concept in spatial reasoning that\nhumans utilize to comprehend and describe space. With the rapid progress in\nMultimodal Language models, the moment has come to integrate this\nlong-overlooked dimension into these models. In particular, in text-to-image\n(T2I) generation, even state-of-the-art models exhibit a significant\nperformance gap when spatial descriptions are provided from perspectives other\nthan the camera. To address this limitation, we propose Frame of\nReference-guided Spatial Adjustment in LLM-based Diffusion Editing (FoR-SALE),\nan extension of the Self-correcting LLM-controlled Diffusion (SLD) framework\nfor T2I. For-Sale evaluates the alignment between a given text and an initially\ngenerated image, and refines the image based on the Frame of Reference\nspecified in the spatial expressions. It employs vision modules to extract the\nspatial configuration of the image, while simultaneously mapping the spatial\nexpression to a corresponding camera perspective. This unified perspective\nenables direct evaluation of alignment between language and vision. When\nmisalignment is detected, the required editing operations are generated and\napplied. FoR-SALE applies novel latent-space operations to adjust the facing\ndirection and depth of the generated images. We evaluate FoR-SALE on two\nbenchmarks specifically designed to assess spatial understanding with FoR. Our\nframework improves the performance of state-of-the-art T2I models by up to 5.3%\nusing only a single round of correction.",
    "published": "2025-09-27T18:42:04Z",
    "link": "http://arxiv.org/pdf/2509.23452v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Tanawan Premsri",
      "Parisa Kordjamshidi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23441v1",
    "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language\n  Models",
    "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit\nharmful behaviors. Current alignment strategies typically embed safety into\nmodel weights, making these controls implicit, static, and difficult to modify.\nThis paper introduces Cognition-of-Thought (CooT), a novel decoding-time\nframework that equips LLMs with an explicit cognitive self-monitoring loop.\nCooT couples a standard text Generator with a cognitive Perceiver that\ncontinuously monitors the unfolding sequence. The Perceiver uses a structured,\nprecedence-based hierarchy of principles (e.g., safety over obedience) to\ndetect potential misalignments as they arise. When violations are flagged, CooT\nintervenes by rolling back the generation to the point of error and\nregenerating under injected guidance that combines universal social priors with\ncontext-specific warnings. CooT thus transforms alignment from a fixed property\ninto an explicit, dynamic, and auditable process active during inference,\nallowing for flexible policy updates without retraining the model. Extensive\nexperiments across multiple benchmarks and model families confirm that CooT\nconsistently improves safety and social reasoning performance.",
    "published": "2025-09-27T18:16:57Z",
    "link": "http://arxiv.org/pdf/2509.23441v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xuanming Zhang",
      "Yuxuan Chen",
      "Min-Hsuan Yeh",
      "Yixuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23433v1",
    "title": "SPIKE-RL: Video-LLMs meet Bayesian Surprise",
    "summary": "Real-world videos often show routine activities punctuated by memorable,\nsurprising events. However, most Video-LLMs process videos by sampling frames\nuniformly, likely missing critical moments that define a video's narrative. We\nintroduce SPIKE, an inference-time framework that quantifies Bayesian Surprise\nas the belief update triggered by new visual evidence in the video stream,\nidentifying moments where new visual evidence conflicts with prior beliefs.\nSPIKE effectively localizes surprise in videos, strongly correlated with humans\non positive (FunQA) and negative (Oops!) surprise benchmarks. Since the beliefs\nof zero-shot Video-LLMs are often suboptimal, we develop SPIKE-RL, which\nleverages GRPO to optimize belief hypotheses based on a reward signal from the\nvideo caption. SPIKE and SPIKE-RL guide query-agnostic surprise-weighted frame\nsampling, which allocates more frames to interesting moments in the video. With\nthis strategy, we achieve consistent performance gains on five downstream\nbenchmarks over uniform sampling. By enabling Video-LLMs to track beliefs and\nregister surprise, our work paves the way for more robust models that can\nrevise their understanding in response to new information.",
    "published": "2025-09-27T18:02:23Z",
    "link": "http://arxiv.org/pdf/2509.23433v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Sahithya Ravi",
      "Aditya Chinchure",
      "Raymond T. Ng",
      "Leonid Sigal",
      "Vered Shwartz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24004v1",
    "title": "SIE3D: Single-image Expressive 3D Avatar generation via Semantic\n  Embedding and Perceptual Expression Loss",
    "summary": "Generating high-fidelity 3D head avatars from a single image is challenging,\nas current methods lack fine-grained, intuitive control over expressions via\ntext. This paper proposes SIE3D, a framework that generates expressive 3D\navatars from a single image and descriptive text. SIE3D fuses identity features\nfrom the image with semantic embedding from text through a novel conditioning\nscheme, enabling detailed control. To ensure generated expressions accurately\nmatch the text, it introduces an innovative perceptual expression loss\nfunction. This loss uses a pre-trained expression classifier to regularize the\ngeneration process, guaranteeing expression accuracy. Extensive experiments\nshow SIE3D significantly improves controllability and realism, outperforming\ncompetitive methods in identity preservation and expression fidelity on a\nsingle consumer-grade GPU. Project page:\nhttps://blazingcrystal1747.github.io/SIE3D/",
    "published": "2025-09-28T17:56:42Z",
    "link": "http://arxiv.org/pdf/2509.24004v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhiqi Huang",
      "Dulongkai Cui",
      "Jinglu Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24001v1",
    "title": "Gaze Estimation for Human-Robot Interaction: Analysis Using the NICO\n  Platform",
    "summary": "This paper evaluates the current gaze estimation methods within an HRI\ncontext of a shared workspace scenario. We introduce a new, annotated dataset\ncollected with the NICO robotic platform. We evaluate four state-of-the-art\ngaze estimation models. The evaluation shows that the angular errors are close\nto those reported on general-purpose benchmarks. However, when expressed in\nterms of distance in the shared workspace the best median error is 16.48 cm\nquantifying the practical limitations of current methods. We conclude by\ndiscussing these limitations and offering recommendations on how to best\nintegrate gaze estimation as a modality in HRI systems.",
    "published": "2025-09-28T17:49:27Z",
    "link": "http://arxiv.org/pdf/2509.24001v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO",
      "I.4.9"
    ],
    "authors": [
      "Matej Palider",
      "Omar Eldardeer",
      "Viktor Kocur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23999v1",
    "title": "TREAT-Net: Tabular-Referenced Echocardiography Analysis for Acute\n  Coronary Syndrome Treatment Prediction",
    "summary": "Coronary angiography remains the gold standard for diagnosing Acute Coronary\nSyndrome (ACS). However, its resource-intensive and invasive nature can expose\npatients to procedural risks and diagnostic delays, leading to postponed\ntreatment initiation. In this work, we introduce TREAT-Net, a multimodal deep\nlearning framework for ACS treatment prediction that leverages non-invasive\nmodalities, including echocardiography videos and structured clinical records.\nTREAT-Net integrates tabular-guided cross-attention to enhance video\ninterpretation, along with a late fusion mechanism to align predictions across\nmodalities. Trained on a dataset of over 9000 ACS cases, the model outperforms\nunimodal and non-fused baselines, achieving a balanced accuracy of 67.6% and an\nAUROC of 71.1%. Cross-modality agreement analysis demonstrates 88.6% accuracy\nfor intervention prediction. These findings highlight the potential of\nTREAT-Net as a non-invasive tool for timely and accurate patient triage,\nparticularly in underserved populations with limited access to coronary\nangiography.",
    "published": "2025-09-28T17:45:01Z",
    "link": "http://arxiv.org/pdf/2509.23999v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Diane Kim",
      "Minh Nguyen Nhat To",
      "Sherif Abdalla",
      "Teresa S. M. Tsang",
      "Purang Abolmaesumi",
      "and Christina Luong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23993v1",
    "title": "Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement\n  Fine-Tuning",
    "summary": "Scalable and realistic simulation of multi-agent traffic behavior is critical\nfor advancing autonomous driving technologies. Although existing data-driven\nsimulators have made significant strides in this domain, they predominantly\nrely on supervised learning to align simulated distributions with real-world\ndriving scenarios. A persistent challenge, however, lies in the distributional\nshift that arises between training and testing, which often undermines model\ngeneralization in unseen environments. To address this limitation, we propose\nSMART-R1, a novel R1-style reinforcement fine-tuning paradigm tailored for\nnext-token prediction models to better align agent behavior with human\npreferences and evaluation metrics. Our approach introduces a metric-oriented\npolicy optimization algorithm to improve distribution alignment and an\niterative \"SFT-RFT-SFT\" training strategy that alternates between Supervised\nFine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) to maximize performance\ngains. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) validate the effectiveness of this simple yet powerful R1-style training\nframework in enhancing foundation models. The results on the Waymo Open Sim\nAgents Challenge (WOSAC) showcase that SMART-R1 achieves state-of-the-art\nperformance with an overall realism meta score of 0.7858, ranking first on the\nleaderboard at the time of submission.",
    "published": "2025-09-28T17:36:13Z",
    "link": "http://arxiv.org/pdf/2509.23993v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Muleilan Pei",
      "Shaoshuai Shi",
      "Shaojie Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23991v1",
    "title": "RPG360: Robust 360 Depth Estimation with Perspective Foundation Models\n  and Graph Optimization",
    "summary": "The increasing use of 360 images across various domains has emphasized the\nneed for robust depth estimation techniques tailored for omnidirectional\nimages. However, obtaining large-scale labeled datasets for 360 depth\nestimation remains a significant challenge. In this paper, we propose RPG360, a\ntraining-free robust 360 monocular depth estimation method that leverages\nperspective foundation models and graph optimization. Our approach converts 360\nimages into six-face cubemap representations, where a perspective foundation\nmodel is employed to estimate depth and surface normals. To address depth scale\ninconsistencies across different faces of the cubemap, we introduce a novel\ndepth scale alignment technique using graph-based optimization, which\nparameterizes the predicted depth and normal maps while incorporating an\nadditional per-face scale parameter. This optimization ensures depth scale\nconsistency across the six-face cubemap while preserving 3D structural\nintegrity. Furthermore, as foundation models exhibit inherent robustness in\nzero-shot settings, our method achieves superior performance across diverse\ndatasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate\nthe versatility of our depth estimation approach by validating its benefits in\ndownstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion\n0.2 ~ 9.7% in AUC@5.",
    "published": "2025-09-28T17:33:12Z",
    "link": "http://arxiv.org/pdf/2509.23991v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Yonghan Lee",
      "Dinesh Manocha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23980v1",
    "title": "Towards Redundancy Reduction in Diffusion Models for Efficient Video\n  Super-Resolution",
    "summary": "Diffusion models have recently shown promising results for video\nsuper-resolution (VSR). However, directly adapting generative diffusion models\nto VSR can result in redundancy, since low-quality videos already preserve\nsubstantial content information. Such redundancy leads to increased\ncomputational overhead and learning burden, as the model performs superfluous\noperations and must learn to filter out irrelevant information. To address this\nproblem, we propose OASIS, an efficient $\\textbf{o}$ne-step diffusion model\nwith $\\textbf{a}$ttention $\\textbf{s}$pecialization for real-world\nv$\\textbf{i}$deo $\\textbf{s}$uper-resolution. OASIS incorporates an attention\nspecialization routing that assigns attention heads to different patterns\naccording to their intrinsic behaviors. This routing mitigates redundancy while\neffectively preserving pretrained knowledge, allowing diffusion models to\nbetter adapt to VSR and achieve stronger performance. Moreover, we propose a\nsimple yet effective progressive training strategy, which starts with\ntemporally consistent degradations and then shifts to inconsistent settings.\nThis strategy facilitates learning under complex degradations. Extensive\nexperiments demonstrate that OASIS achieves state-of-the-art performance on\nboth synthetic and real-world datasets. OASIS also provides superior inference\nspeed, offering a $\\textbf{6.2$\\times$}$ speedup over one-step diffusion\nbaselines such as SeedVR2. The code will be available at\n\\href{https://github.com/jp-guo/OASIS}{https://github.com/jp-guo/OASIS}.",
    "published": "2025-09-28T17:08:51Z",
    "link": "http://arxiv.org/pdf/2509.23980v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jinpei Guo",
      "Yifei Ji",
      "Zheng Chen",
      "Yufei Wang",
      "Sizhuo Ma",
      "Yong Guo",
      "Yulun Zhang",
      "Jian Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23971v1",
    "title": "VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic\n  Diffusion",
    "summary": "Modern diffusion models generate realistic traffic simulations but\nsystematically violate physical constraints. In a large-scale evaluation of\nSceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of\ngenerated trajectories violate basic physical laws - vehicles collide, drive\noff roads, and spawn inside buildings. This reveals a fundamental limitation:\ncurrent models treat physical validity as an emergent property rather than an\narchitectural requirement. We propose Validity-First Spatial Intelligence\n(VFSI), which enforces constraints through energy-based guidance during\ndiffusion sampling, without model retraining. By incorporating collision\navoidance and kinematic constraints as energy functions, we guide the denoising\nprocess toward physically valid trajectories. Across 200 urban scenarios from\nthe Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to\n8.1%) and improves overall validity by 87% (50.3% to 94.2%), while\nsimultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our\nmodel-agnostic approach demonstrates that explicit constraint enforcement\nduring inference is both necessary and sufficient for physically valid traffic\nsimulation.",
    "published": "2025-09-28T16:48:49Z",
    "link": "http://arxiv.org/pdf/2509.23971v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kargi Chauhan",
      "Leilani H. Gilpin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23968v1",
    "title": "A Novel Hybrid Deep Learning and Chaotic Dynamics Approach for Thyroid\n  Cancer Classification",
    "summary": "Timely and accurate diagnosis is crucial in addressing the global rise in\nthyroid cancer, ensuring effective treatment strategies and improved patient\noutcomes. We present an intelligent classification method that couples an\nAdaptive Convolutional Neural Network (CNN) with Cohen-Daubechies-Feauveau\n(CDF9/7) wavelets whose detail coefficients are modulated by an n-scroll\nchaotic system to enrich discriminative features. We evaluate on the public\nDDTI thyroid ultrasound dataset (n = 1,638 images; 819 malignant / 819 benign)\nusing 5-fold cross-validation, where the proposed method attains 98.17%\naccuracy, 98.76% sensitivity, 97.58% specificity, 97.55% F1-score, and an AUC\nof 0.9912. A controlled ablation shows that adding chaotic modulation to CDF9/7\nimproves accuracy by +8.79 percentage points over a CDF9/7-only CNN (from\n89.38% to 98.17%). To objectively position our approach, we trained\nstate-of-the-art backbones on the same data and splits: EfficientNetV2-S\n(96.58% accuracy; AUC 0.987), Swin-T (96.41%; 0.986), ViT-B/16 (95.72%; 0.983),\nand ConvNeXt-T (96.94%; 0.987). Our method outperforms the best of these by\n+1.23 points in accuracy and +0.0042 in AUC, while remaining computationally\nefficient (28.7 ms per image; 1,125 MB peak VRAM). Robustness is further\nsupported by cross-dataset testing on TCIA (accuracy 95.82%) and transfer to an\nISIC skin-lesion subset (n = 28 unique images, augmented to 2,048; accuracy\n97.31%). Explainability analyses (Grad-CAM, SHAP, LIME) highlight clinically\nrelevant regions. Altogether, the wavelet-chaos-CNN pipeline delivers\nstate-of-the-art thyroid ultrasound classification with strong generalization\nand practical runtime characteristics suitable for clinical integration.",
    "published": "2025-09-28T16:46:31Z",
    "link": "http://arxiv.org/pdf/2509.23968v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nada Bouchekout",
      "Abdelkrim Boukabou",
      "Morad Grimes",
      "Yassine Habchi",
      "Yassine Himeur",
      "Hamzah Ali Alkhazaleh",
      "Shadi Atalla",
      "Wathiq Mansoor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23958v1",
    "title": "Reinforcement Learning with Inverse Rewards for World Model\n  Post-training",
    "summary": "World models simulate dynamic environments, enabling agents to interact with\ndiverse input modalities. Although recent advances have improved the visual\nquality and temporal consistency of video world models, their ability of\naccurately modeling human-specified actions remains under-explored.\nReinforcement learning presents a promising approach for directly improving the\nsuboptimal action-following capability of pre-trained models, assuming that an\nappropriate reward function can be defined. However, transferring reinforcement\nlearning post-training methods to world model is impractical due to the\nprohibitive cost of large-scale preference annotations and the infeasibility of\nconstructing rule-based video verifiers. To address this gap, we propose\nReinforcement Learning with Inverse Rewards (RLIR), a post-training framework\nthat derives verifiable reward signals by recovering input actions from\ngenerated videos using an Inverse Dynamics Model. By mapping high-dimensional\nvideo modality to a low-dimensional action space, RLIR provides an objective\nand verifiable reward for optimization via Group Relative Policy Optimization.\nExperiments across autoregressive and diffusion paradigms demonstrate 5-10%\ngains in action-following, up to 10% improvements in visual quality, and higher\nhuman preference scores, establishing RLIR as the first post-training method\nspecifically designed to enhance action-following in video world models.",
    "published": "2025-09-28T16:27:47Z",
    "link": "http://arxiv.org/pdf/2509.23958v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yang Ye",
      "Tianyu He",
      "Shuo Yang",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23955v1",
    "title": "ColLab: A Collaborative Spatial Progressive Data Engine for Referring\n  Expression Comprehension and Generation",
    "summary": "Referring Expression Comprehension (REC) and Referring Expression Generation\n(REG) are fundamental tasks in multimodal understanding, supporting precise\nobject localization through natural language. However, existing REC and REG\ndatasets rely heavily on manual annotation, which is labor-intensive and\ndifficult to scale. In this paper, we propose ColLab, a collaborative spatial\nprogressive data engine that enables fully automated REC and REG data\ngeneration without human supervision. Specifically, our method introduces a\nCollaborative Multimodal Model Interaction (CMMI) strategy, which leverages the\nsemantic understanding of multimodal large language models (MLLMs) and large\nlanguage models (LLMs) to generate descriptions. Furthermore, we design a\nmodule termed Spatial Progressive Augmentation (SPA) to enhance spatial\nexpressiveness among duplicate instances. Experiments demonstrate that ColLab\nsignificantly accelerates the annotation process of REC and REG while improving\nthe quality and discriminability of the generated expressions. In addition to\nthe core methodological contribution, our framework was partially adopted in\nthe data generation pipeline of the ICCV 2025 MARS2 Challenge on Multimodal\nReasoning, enriching the dataset with diverse and challenging samples that\nbetter reflect real-world reasoning demands.",
    "published": "2025-09-28T16:21:29Z",
    "link": "http://arxiv.org/pdf/2509.23955v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shilan Zhang",
      "Jirui Huang",
      "Ruilin Yao",
      "Cong Wang",
      "Yaxiong Chen",
      "Peng Xu",
      "Shengwu Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23951v1",
    "title": "HunyuanImage 3.0 Technical Report",
    "summary": "We present HunyuanImage 3.0, a native multimodal model that unifies\nmultimodal understanding and generation within an autoregressive framework,\nwith its image generation module publicly available. The achievement of\nHunyuanImage 3.0 relies on several key components, including meticulous data\ncuration, advanced architecture design, a native Chain-of-Thoughts schema,\nprogressive model pre-training, aggressive model post-training, and an\nefficient infrastructure that enables large-scale training and inference. With\nthese advancements, we successfully trained a Mixture-of-Experts (MoE) model\ncomprising over 80 billion parameters in total, with 13 billion parameters\nactivated per token during inference, making it the largest and most powerful\nopen-source image generative model to date. We conducted extensive experiments\nand the results of automatic and human evaluation of text-image alignment and\nvisual quality demonstrate that HunyuanImage 3.0 rivals previous\nstate-of-the-art models. By releasing the code and weights of HunyuanImage 3.0,\nwe aim to enable the community to explore new ideas with a state-of-the-art\nfoundation model, fostering a dynamic and vibrant multimodal ecosystem. All\nopen source assets are publicly available at\nhttps://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
    "published": "2025-09-28T16:14:10Z",
    "link": "http://arxiv.org/pdf/2509.23951v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Siyu Cao",
      "Hangting Chen",
      "Peng Chen",
      "Yiji Cheng",
      "Yutao Cui",
      "Xinchi Deng",
      "Ying Dong",
      "Kipper Gong",
      "Tianpeng Gu",
      "Xiusen Gu",
      "Tiankai Hang",
      "Duojun Huang",
      "Jie Jiang",
      "Zhengkai Jiang",
      "Weijie Kong",
      "Changlin Li",
      "Donghao Li",
      "Junzhe Li",
      "Xin Li",
      "Yang Li",
      "Zhenxi Li",
      "Zhimin Li",
      "Jiaxin Lin",
      " Linus",
      "Lucaz Liu",
      "Shu Liu",
      "Songtao Liu",
      "Yu Liu",
      "Yuhong Liu",
      "Yanxin Long",
      "Fanbin Lu",
      "Qinglin Lu",
      "Yuyang Peng",
      "Yuanbo Peng",
      "Xiangwei Shen",
      "Yixuan Shi",
      "Jiale Tao",
      "Yangyu Tao",
      "Qi Tian",
      "Pengfei Wan",
      "Chunyu Wang",
      "Kai Wang",
      "Lei Wang",
      "Linqing Wang",
      "Lucas Wang",
      "Qixun Wang",
      "Weiyan Wang",
      "Hao Wen",
      "Bing Wu",
      "Jianbing Wu",
      "Yue Wu",
      "Senhao Xie",
      "Fang Yang",
      "Miles Yang",
      "Xiaofeng Yang",
      "Xuan Yang",
      "Zhantao Yang",
      "Jingmiao Yu",
      "Zheng Yuan",
      "Chao Zhang",
      "Jian-Wei Zhang",
      "Peizhen Zhang",
      "Shi-Xue Zhang",
      "Tao Zhang",
      "Weigang Zhang",
      "Yepeng Zhang",
      "Yingfang Zhang",
      "Zihao Zhang",
      "Zijian Zhang",
      "Penghao Zhao",
      "Zhiyuan Zhao",
      "Xuefei Zhe",
      "Jianchen Zhu",
      "Zhao Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23947v1",
    "title": "CrashSplat: 2D to 3D Vehicle Damage Segmentation in Gaussian Splatting",
    "summary": "Automatic car damage detection has been a topic of significant interest for\nthe auto insurance industry as it promises faster, accurate, and cost-effective\ndamage assessments. However, few works have gone beyond 2D image analysis to\nleverage 3D reconstruction methods, which have the potential to provide a more\ncomprehensive and geometrically accurate representation of the damage.\nMoreover, recent methods employing 3D representations for novel view synthesis,\nparticularly 3D Gaussian Splatting (3D-GS), have demonstrated the ability to\ngenerate accurate and coherent 3D reconstructions from a limited number of\nviews. In this work we introduce an automatic car damage detection pipeline\nthat performs 3D damage segmentation by up-lifting 2D masks. Additionally, we\npropose a simple yet effective learning-free approach for single-view 3D-GS\nsegmentation. Specifically, Gaussians are projected onto the image plane using\ncamera parameters obtained via Structure from Motion (SfM). They are then\nfiltered through an algorithm that utilizes Z-buffering along with a normal\ndistribution model of depth and opacities. Through experiments we found that\nthis method is particularly effective for challenging scenarios like car damage\ndetection, where target objects (e.g., scratches, small dents) may only be\nclearly visible in a single view, making multi-view consistency approaches\nimpractical or impossible. The code is publicly available at:\nhttps://github.com/DragosChileban/CrashSplat.",
    "published": "2025-09-28T15:49:33Z",
    "link": "http://arxiv.org/pdf/2509.23947v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dragoş-Andrei Chileban",
      "Andrei-Ştefan Bulzan",
      "Cosmin Cernǎzanu-Glǎvan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23931v1",
    "title": "AutoPrune: Each Complexity Deserves a Pruning Policy",
    "summary": "The established redundancy in visual tokens within large vision-language\nmodels allows pruning to effectively reduce their substantial computational\ndemands. Previous methods typically employ heuristic layer-specific pruning\nstrategies where, although the number of tokens removed may differ across\ndecoder layers, the overall pruning schedule is fixed and applied uniformly to\nall input samples and tasks, failing to align token elimination with the\nmodel's holistic reasoning trajectory. Cognitive science indicates that human\nvisual processing often begins with broad exploration to accumulate evidence\nbefore narrowing focus as the target becomes distinct. Our experiments reveal\nan analogous pattern in these models. This observation suggests that neither a\nfixed pruning schedule nor a heuristic layer-wise strategy can optimally\naccommodate the diverse complexities inherent in different inputs. To overcome\nthis limitation, we introduce Complexity-Adaptive Pruning (AutoPrune), a\ntraining-free, plug-and-play framework that tailors pruning policies to varying\nsample and task complexities. Specifically, AutoPrune quantifies the mutual\ninformation between visual and textual tokens, then projects this signal to a\nbudget-constrained logistic retention curve. Each such logistic curve, defined\nby its unique shape, corresponds to the specific complexity of different tasks\nand can guarantee adherence to predefined computational constraints. We\nevaluate AutoPrune on standard vision-language tasks and on\nVision-Language-Action models for autonomous driving. Notably, when applied to\nLLaVA-1.5-7B, our method prunes 89% of visual tokens and reduces inference\nFLOPs by 76.8% while retaining 96.7% of the original accuracy averaged over all\ntasks. This corresponds to a 9.1% improvement over the recent work PDrop,\ndemonstrating the effectiveness. Code is available at\nhttps://github.com/AutoLab-SAI-SJTU/AutoPrune.",
    "published": "2025-09-28T15:09:00Z",
    "link": "http://arxiv.org/pdf/2509.23931v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hanshi Wang",
      "Yuhao Xu",
      "Zekun Xu",
      "Jin Gao",
      "Yufan Liu",
      "Weiming Hu",
      "Ke Wang",
      "Zhipeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23930v1",
    "title": "A University of Texas Medical Branch Case Study on Aortic Calcification\n  Detection",
    "summary": "This case study details The University of Texas Medical Branch (UTMB)'s\npartnership with Zauron Labs, Inc. to enhance detection and coding of aortic\ncalcifications (ACs) using chest radiographs. ACs are often underreported\ndespite their significant prognostic value for cardiovascular disease, and UTMB\npartnered with Zauron to apply its advanced AI tools, including a\nhigh-performing image model (AUC = 0.938) and a fine-tuned language model based\non Meta's Llama 3.2, to retrospectively analyze imaging and report data. The\neffort identified 495 patients out of 3,988 unique patients assessed (5,000\ntotal exams) whose reports contained indications of aortic calcifications that\nwere not properly coded for reimbursement (12.4% miscode rate) as well as an\nadditional 84 patients who had aortic calcifications that were missed during\ninitial review (2.1% misdiagnosis rate). Identification of these patients\nprovided UTMB with the potential to impact clinical care for these patients and\npursue $314k in missed annual revenue. These findings informed UTMB's decision\nto adopt Zauron's Guardian Pro software system-wide to ensure accurate,\nAI-enhanced peer review and coding, improving both patient care and financial\nsolvency. This study is covered under University of Texas Health San Antonio's\nInstitutional Review Board Study ID 00001887.",
    "published": "2025-09-28T15:08:53Z",
    "link": "http://arxiv.org/pdf/2509.23930v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "92C55"
    ],
    "authors": [
      "Eric Walser",
      "Peter McCaffrey",
      "Kal Clark",
      "Nicholas Czarnek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23927v1",
    "title": "SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing",
    "summary": "Cross-modal artificial intelligence has garnered widespread attention in\nrecent years, achieving significant progress in the study of natural images.\nHowever, existing methods are mostly designed for RGB imagery, leaving a\nsignificant gap in modeling synthetic aperture radar (SAR) imagery. SAR, with\nits all-day, all-weather imaging capabilities, plays an irreplaceable role in\nremote sensing scene understanding. To address this gap, this paper proposes\nSAR-KnowLIP, the first universal SAR multimodal foundational model, along with\nreusable data and evaluation baselines. Specifically: (1) This work introduces\nthe critical yet long-overlooked attribute of geographic information into\nremote sensing research, constructing SAR-GEOVL-1M (the first large-scale SAR\ndataset with complete geographic projection properties), covering multiple\nsatellite platforms, 120,000 images, and 135 cities. (2) Aligned structured\ntext is generated through a hierarchical cognitive chain-of-thought (HCoT),\nproviding more than one million multi-dimensional semantic annotations of\nlandforms, regional functions, target attributes, and spatial relationships.\n(3) We design a Self-Consistent Iterative Optimization mechanism that\ncontinuously enhances cross-modal alignment through a self-supervised closed\nloop of contrastive, matching, and reconstruction learning on a transferable\nmultimodal encoder. (4) A unified evaluation benchmark is established across 11\nrepresentative downstream vision and vision-language tasks, with comparisons\nagainst 14 leading foundation models, where SAR-KnowLIP demonstrates leading\nperformance, particularly in object counting and land-cover classification. We\nexpect that SAR-KnowLIP's large-scale multimodal data, transferable model\narchitecture, and comprehensive experimental benchmark will significantly\nadvance the development of SAR multimodal baseline models.",
    "published": "2025-09-28T15:03:25Z",
    "link": "http://arxiv.org/pdf/2509.23927v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yi Yang",
      "Xiaokun Zhang",
      "Qingchen Fang",
      "Ziqi Ye",
      "Rui Li",
      "Li Liu",
      "Haipeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23926v1",
    "title": "Learning Encoding-Decoding Direction Pairs to Unveil Concepts of\n  Influence in Deep Vision Networks",
    "summary": "Empirical evidence shows that deep vision networks represent concepts as\ndirections in latent space, vectors we call concept embeddings. Each concept\nhas a latent factor-a scalar-indicating its presence in an input patch. For a\ngiven patch, multiple latent factors are encoded into a compact representation\nby linearly combining concept embeddings, with the factors as coefficients.\nSince these embeddings enable such encoding, we call them encoding directions.\nA latent factor can be recovered via the inner product with a filter, a vector\nwe call a decoding direction. These encoding-decoding direction pairs are not\ndirectly accessible, but recovering them helps open the black box of deep\nnetworks, enabling understanding, debugging, and improving models. Decoder\ndirections attribute meaning to latent codes, while encoding directions assess\nconcept influence on predictions, with both enabling model correction by\nunlearning irrelevant concepts. Unlike prior matrix decomposition, autoencoder,\nor dictionary learning methods that rely on feature reconstruction, we propose\na new perspective: decoding directions are identified via directional\nclustering of activations, and encoding directions are estimated with signal\nvectors under a probabilistic view. We further leverage network weights through\na novel technique, Uncertainty Region Alignment, which reveals interpretable\ndirections affecting predictions. Our analysis shows that (a) on synthetic\ndata, our method recovers ground-truth direction pairs; (b) on real data,\ndecoding directions map to monosemantic, interpretable concepts and outperform\nunsupervised baselines; and (c) signal vectors faithfully estimate encoding\ndirections, validated via activation maximization. Finally, we demonstrate\napplications in understanding global model behavior, explaining individual\npredictions, and intervening to produce counterfactuals or correct errors.",
    "published": "2025-09-28T15:02:34Z",
    "link": "http://arxiv.org/pdf/2509.23926v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alexandros Doumanoglou",
      "Kurt Driessens",
      "Dimitrios Zarpalas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23922v1",
    "title": "DriveE2E: Closed-Loop Benchmark for End-to-End Autonomous Driving\n  through Real-to-Simulation",
    "summary": "Closed-loop evaluation is increasingly critical for end-to-end autonomous\ndriving. Current closed-loop benchmarks using the CARLA simulator rely on\nmanually configured traffic scenarios, which can diverge from real-world\nconditions, limiting their ability to reflect actual driving performance. To\naddress these limitations, we introduce a simple yet challenging closed-loop\nevaluation framework that closely integrates real-world driving scenarios into\nthe CARLA simulator with infrastructure cooperation. Our approach involves\nextracting 800 dynamic traffic scenarios selected from a comprehensive 100-hour\nvideo dataset captured by high-mounted infrastructure sensors, and creating\nstatic digital twin assets for 15 real-world intersections with consistent\nvisual appearance. These digital twins accurately replicate the traffic and\nenvironmental characteristics of their real-world counterparts, enabling more\nrealistic simulations in CARLA. This evaluation is challenging due to the\ndiversity of driving behaviors, locations, weather conditions, and times of day\nat complex urban intersections. In addition, we provide a comprehensive\nclosed-loop benchmark for evaluating end-to-end autonomous driving models.\nProject URL:\n\\href{https://github.com/AIR-THU/DriveE2E}{https://github.com/AIR-THU/DriveE2E}.",
    "published": "2025-09-28T14:55:14Z",
    "link": "http://arxiv.org/pdf/2509.23922v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Haibao Yu",
      "Wenxian Yang",
      "Ruiyang Hao",
      "Chuanye Wang",
      "Jiaru Zhong",
      "Ping Luo",
      "Zaiqing Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23919v1",
    "title": "Token Painter: Training-Free Text-Guided Image Inpainting via Mask\n  Autoregressive Models",
    "summary": "Text-guided image inpainting aims to inpaint masked image regions based on a\ntextual prompt while preserving the background. Although diffusion-based\nmethods have become dominant, their property of modeling the entire image in\nlatent space makes it challenging for the results to align well with prompt\ndetails and maintain a consistent background. To address these issues, we\nexplore Mask AutoRegressive (MAR) models for this task. MAR naturally supports\nimage inpainting by generating latent tokens corresponding to mask regions,\nenabling better local controllability without altering the background. However,\ndirectly applying MAR to this task makes the inpainting content either ignore\nthe prompts or be disharmonious with the background context. Through analysis\nof the attention maps from the inpainting images, we identify the impact of\nbackground tokens on text tokens during the MAR generation, and leverage this\nto design \\textbf{Token Painter}, a training-free text-guided image inpainting\nmethod based on MAR. Our approach introduces two key components: (1)\nDual-Stream Encoder Information Fusion (DEIF), which fuses the semantic and\ncontext information from text and background in frequency domain to produce\nnovel guidance tokens, allowing MAR to generate text-faithful inpainting\ncontent while keeping harmonious with background context. (2) Adaptive Decoder\nAttention Score Enhancing (ADAE), which adaptively enhances attention scores on\nguidance tokens and inpainting tokens to further enhance the alignment of\nprompt details and the content visual quality. Extensive experiments\ndemonstrate that our training-free method outperforms prior state-of-the-art\nmethods across almost all metrics and delivers superior visual results. Codes\nwill be released.",
    "published": "2025-09-28T14:48:52Z",
    "link": "http://arxiv.org/pdf/2509.23919v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Longtao Jiang",
      "Mingfei Han",
      "Lei Chen",
      "Yongqiang Yu",
      "Feng Zhao",
      "Xiaojun Chang",
      "Zhihui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23917v1",
    "title": "Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP\n  and Its Derivatives",
    "summary": "As a general-purpose vision-language pretraining model, CLIP demonstrates\nstrong generalization ability in image-text alignment tasks and has been widely\nadopted in downstream applications such as image classification and image-text\nretrieval. However, it struggles with fine-grained tasks such as object\ndetection and semantic segmentation. While many variants aim to improve CLIP on\nthese tasks, its robustness to adversarial perturbations remains underexplored.\nUnderstanding how adversarial examples transfer across tasks is key to\nassessing CLIP's generalization limits and security risks. In this work, we\nconduct a systematic empirical analysis of the cross-task transfer behavior of\nCLIP-based models on image-text retrieval, object detection, and semantic\nsegmentation under adversarial perturbations. We find that adversarial examples\ngenerated from fine-grained tasks (e.g., object detection and semantic\nsegmentation) often exhibit stronger transfer potential than those from\ncoarse-grained tasks, enabling more effective attacks against the original CLIP\nmodel. Motivated by this observation, we propose a novel framework, Multi-Task\nAdversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature\naggregation loss and generates perturbations with enhanced cross-task\ngeneralization capability. This design strengthens the attack effectiveness of\nfine-grained task models on the shared CLIP backbone. Experimental results on\nmultiple public datasets show that MT-AdvCLIP significantly improves the\nadversarial transfer success rate (The average attack success rate across\nmultiple tasks is improved by over 39%.) against various CLIP-derived models,\nwithout increasing the perturbation budget. This study reveals the transfer\nmechanism of adversarial examples in multi-task CLIP models, offering new\ninsights into multi-task robustness evaluation and adversarial example design.",
    "published": "2025-09-28T14:46:52Z",
    "link": "http://arxiv.org/pdf/2509.23917v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kuanrong Liu",
      "Siyuan Liang",
      "Cheng Qian",
      "Ming Zhang",
      "Xiaochun Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23915v1",
    "title": "Revisit the Imbalance Optimization in Multi-task Learning: An\n  Experimental Analysis",
    "summary": "Multi-task learning (MTL) aims to build general-purpose vision systems by\ntraining a single network to perform multiple tasks jointly. While promising,\nits potential is often hindered by \"unbalanced optimization\", where task\ninterference leads to subpar performance compared to single-task models. To\nfacilitate research in MTL, this paper presents a systematic experimental\nanalysis to dissect the factors contributing to this persistent problem. Our\ninvestigation confirms that the performance of existing optimization methods\nvaries inconsistently across datasets, and advanced architectures still rely on\ncostly grid-searched loss weights. Furthermore, we show that while powerful\nVision Foundation Models (VFMs) provide strong initialization, they do not\ninherently resolve the optimization imbalance, and merely increasing data\nquantity offers limited benefits. A crucial finding emerges from our analysis:\na strong correlation exists between the optimization imbalance and the norm of\ntask-specific gradients. We demonstrate that this insight is directly\napplicable, showing that a straightforward strategy of scaling task losses\naccording to their gradient norms can achieve performance comparable to that of\nan extensive and computationally expensive grid search. Our comprehensive\nanalysis suggests that understanding and controlling gradient dynamics is a\nmore direct path to stable MTL than developing increasingly complex methods.",
    "published": "2025-09-28T14:40:06Z",
    "link": "http://arxiv.org/pdf/2509.23915v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yihang Guo",
      "Tianyuan Yu",
      "Liang Bai",
      "Yanming Guo",
      "Yirun Ruan",
      "William Li",
      "Weishi Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23911v1",
    "title": "MoReact: Generating Reactive Motion from Textual Descriptions",
    "summary": "Modeling and generating human reactions poses a significant challenge with\nbroad applications for computer vision and human-computer interaction. Existing\nmethods either treat multiple individuals as a single entity, directly\ngenerating interactions, or rely solely on one person's motion to generate the\nother's reaction, failing to integrate the rich semantic information that\nunderpins human interactions. Yet, these methods often fall short in adaptive\nresponsiveness, i.e., the ability to accurately respond to diverse and dynamic\ninteraction scenarios. Recognizing this gap, our work introduces an approach\ntailored to address the limitations of existing models by focusing on\ntext-driven human reaction generation. Our model specifically generates\nrealistic motion sequences for individuals that responding to the other's\nactions based on a descriptive text of the interaction scenario. The goal is to\nproduce motion sequences that not only complement the opponent's movements but\nalso semantically fit the described interactions. To achieve this, we present\nMoReact, a diffusion-based method designed to disentangle the generation of\nglobal trajectories and local motions sequentially. This approach stems from\nthe observation that generating global trajectories first is crucial for\nguiding local motion, ensuring better alignment with given action and text.\nFurthermore, we introduce a novel interaction loss to enhance the realism of\ngenerated close interactions. Our experiments, utilizing data adapted from a\ntwo-person motion dataset, demonstrate the efficacy of our approach for this\nnovel task, which is capable of producing realistic, diverse, and controllable\nreactions that not only closely match the movements of the counterpart but also\nadhere to the textual guidance. Please find our webpage at\nhttps://xiyan-xu.github.io/MoReactWebPage.",
    "published": "2025-09-28T14:31:41Z",
    "link": "http://arxiv.org/pdf/2509.23911v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiyan Xu",
      "Sirui Xu",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23909v1",
    "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling",
    "summary": "Instruction-guided image editing has achieved remarkable progress, yet\ncurrent models still face challenges with complex instructions and often\nrequire multiple samples to produce a desired result. Reinforcement Learning\n(RL) offers a promising solution, but its adoption in image editing has been\nseverely hindered by the lack of a high-fidelity, efficient reward signal. In\nthis work, we present a comprehensive methodology to overcome this barrier,\ncentered on the development of a state-of-the-art, specialized reward model. We\nfirst introduce EditReward-Bench, a comprehensive benchmark to systematically\nevaluate reward models on editing quality. Building on this benchmark, we\ndevelop EditScore, a series of reward models (7B-72B) for evaluating the\nquality of instruction-guided image editing. Through meticulous data curation\nand filtering, EditScore effectively matches the performance of learning\nproprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy\ntailored for the generative nature of EditScore, our largest variant even\nsurpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity\nreward model is the key to unlocking online RL for image editing. Our\nexperiments show that, while even the largest open-source VLMs fail to provide\nan effective learning signal, EditScore enables efficient and robust policy\noptimization. Applying our framework to a strong base model, OmniGen2, results\nin a final model that shows a substantial and consistent performance uplift.\nOverall, this work provides the first systematic path from benchmarking to\nreward modeling to RL training in image editing, showing that a high-fidelity,\ndomain-specialized reward model is the key to unlocking the full potential of\nRL in this domain.",
    "published": "2025-09-28T14:28:24Z",
    "link": "http://arxiv.org/pdf/2509.23909v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xin Luo",
      "Jiahao Wang",
      "Chenyuan Wu",
      "Shitao Xiao",
      "Xiyan Jiang",
      "Defu Lian",
      "Jiajun Zhang",
      "Dong Liu",
      "Zheng liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23907v1",
    "title": "Adversarial Versus Federated: An Adversarial Learning based\n  Multi-Modality Cross-Domain Federated Medical Segmentation",
    "summary": "Federated learning enables collaborative training of machine learning models\namong different clients while ensuring data privacy, emerging as the mainstream\nfor breaking data silos in the healthcare domain. However, the imbalance of\nmedical resources, data corruption or improper data preservation may lead to a\nsituation where different clients possess medical images of different modality.\nThis heterogeneity poses a significant challenge for cross-domain medical image\nsegmentation within the federated learning framework. To address this\nchallenge, we propose a new Federated Domain Adaptation (FedDA) segmentation\ntraining framework. Specifically, we propose a feature-level adversarial\nlearning among clients by aligning feature maps across clients through\nembedding an adversarial training mechanism. This design can enhance the\nmodel's generalization on multiple domains and alleviate the negative impact\nfrom domain-shift. Comprehensive experiments on three medical image datasets\ndemonstrate that our proposed FedDA substantially achieves cross-domain\nfederated aggregation, endowing single modality client with cross-modality\nprocessing capabilities, and consistently delivers robust performance compared\nto state-of-the-art federated aggregation algorithms in objective and\nsubjective assessment. Our code are available at\nhttps://github.com/GGbond-study/FedDA.",
    "published": "2025-09-28T14:26:04Z",
    "link": "http://arxiv.org/pdf/2509.23907v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "You Zhou",
      "Lijiang Chen",
      "Shuchang Lyu",
      "Guangxia Cui",
      "Wenpei Bai",
      "Zheng Zhou",
      "Meng Li",
      "Guangliang Cheng",
      "Huiyu Zhou",
      "Qi Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23900v1",
    "title": "LifeCLEF Plant Identification Task 2014",
    "summary": "The LifeCLEFs plant identification task provides a testbed for a\nsystem-oriented evaluation of plant identification about 500 species trees and\nherbaceous plants. Seven types of image content are considered: scan and\nscan-like pictures of leaf, and 6 kinds of detailed views with unconstrained\nconditions, directly photographed on the plant: flower, fruit, stem & bark,\nbranch, leaf and entire view. The main originality of this data is that it was\nspecifically built through a citizen sciences initiative conducted by Tela\nBotanica, a French social network of amateur and expert botanists. This makes\nthe task closer to the conditions of a real-world application. This overview\npresents more precisely the resources and assessments of task, summarizes the\nretrieval approaches employed by the participating groups, and provides an\nanalysis of the main evaluation results. With a total of ten groups from six\ncountries and with a total of twenty seven submitted runs, involving distinct\nand original methods, this fourth year task confirms Image & Multimedia\nRetrieval community interest for biodiversity and botany, and highlights\nfurther challenging studies in plant identification.",
    "published": "2025-09-28T14:16:15Z",
    "link": "http://arxiv.org/pdf/2509.23900v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Herve Goeau",
      "Alexis Joly",
      "Pierre Bonnet",
      "Souheil Selmi",
      "Jean-Francois Molino",
      "Daniel Barthelemy",
      "Nozha Boujemaa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23899v1",
    "title": "Q-FSRU: Quantum-Augmented Frequency-Spectral For Medical Visual Question\n  Answering",
    "summary": "Solving tough clinical questions that require both image and text\nunderstanding is still a major challenge in healthcare AI. In this work, we\npropose Q-FSRU, a new model that combines Frequency Spectrum Representation and\nFusion (FSRU) with a method called Quantum Retrieval-Augmented Generation\n(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in\nfeatures from medical images and related text, then shifts them into the\nfrequency domain using Fast Fourier Transform (FFT). This helps it focus on\nmore meaningful data and filter out noise or less useful information. To\nimprove accuracy and ensure that answers are based on real knowledge, we add a\nquantum inspired retrieval system. It fetches useful medical facts from\nexternal sources using quantum-based similarity techniques. These details are\nthen merged with the frequency-based features for stronger reasoning. We\nevaluated our model using the VQA-RAD dataset, which includes real radiology\nimages and questions. The results showed that Q-FSRU outperforms earlier\nmodels, especially on complex cases needing image text reasoning. The mix of\nfrequency and quantum information improves both performance and explainability.\nOverall, this approach offers a promising way to build smart, clear, and\nhelpful AI tools for doctors.",
    "published": "2025-09-28T14:09:00Z",
    "link": "http://arxiv.org/pdf/2509.23899v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rakesh Thakur",
      "Yusra Tariq",
      "Rakesh Chandra Joshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23891v1",
    "title": "LifeCLEF Plant Identification Task 2015",
    "summary": "The LifeCLEF plant identification challenge aims at evaluating plant\nidentification methods and systems at a very large scale, close to the\nconditions of a real-world biodiversity monitoring scenario. The 2015\nevaluation was actually conducted on a set of more than 100K images\nillustrating 1000 plant species living in West Europe. The main originality of\nthis dataset is that it was built through a large-scale participatory sensing\nplateform initiated in 2011 and which now involves tens of thousands of\ncontributors. This overview presents more precisely the resources and\nassessments of the challenge, summarizes the approaches and systems employed by\nthe participating research groups, and provides an analysis of the main\noutcomes.",
    "published": "2025-09-28T13:53:35Z",
    "link": "http://arxiv.org/pdf/2509.23891v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Herve Goeau",
      "Pierre Bonnet",
      "Alexis Joly"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23888v1",
    "title": "AssemblyHands-X: Modeling 3D Hand-Body Coordination for Understanding\n  Bimanual Human Activities",
    "summary": "Bimanual human activities inherently involve coordinated movements of both\nhands and body. However, the impact of this coordination in activity\nunderstanding has not been systematically evaluated due to the lack of suitable\ndatasets. Such evaluation demands kinematic-level annotations (e.g., 3D pose)\nfor the hands and body, yet existing 3D activity datasets typically annotate\neither hand or body pose. Another line of work employs marker-based motion\ncapture to provide full-body pose, but the physical markers introduce visual\nartifacts, thereby limiting models' generalization to natural, markerless\nvideos. To address these limitations, we present AssemblyHands-X, the first\nmarkerless 3D hand-body benchmark for bimanual activities, designed to study\nthe effect of hand-body coordination for action recognition. We begin by\nconstructing a pipeline for 3D pose annotation from synchronized multi-view\nvideos. Our approach combines multi-view triangulation with SMPL-X mesh\nfitting, yielding reliable 3D registration of hands and upper body. We then\nvalidate different input representations (e.g., video, hand pose, body pose, or\nhand-body pose) across recent action recognition models based on graph\nconvolution or spatio-temporal attention. Our extensive experiments show that\npose-based action inference is more efficient and accurate than video\nbaselines. Moreover, joint modeling of hand and body cues improves action\nrecognition over using hands or upper body alone, highlighting the importance\nof modeling interdependent hand-body dynamics for a holistic understanding of\nbimanual activities.",
    "published": "2025-09-28T13:52:14Z",
    "link": "http://arxiv.org/pdf/2509.23888v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tatsuro Banno",
      "Takehiko Ohkawa",
      "Ruicong Liu",
      "Ryosuke Furuta",
      "Yoichi Sato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23880v1",
    "title": "Learning Adaptive Pseudo-Label Selection for Semi-Supervised 3D Object\n  Detection",
    "summary": "Semi-supervised 3D object detection (SS3DOD) aims to reduce costly 3D\nannotations utilizing unlabeled data. Recent studies adopt pseudo-label-based\nteacher-student frameworks and demonstrate impressive performance. The main\nchallenge of these frameworks is in selecting high-quality pseudo-labels from\nthe teacher's predictions. Most previous methods, however, select pseudo-labels\nby comparing confidence scores over thresholds manually set. The latest works\ntackle the challenge either by dynamic thresholding or refining the quality of\npseudo-labels. Such methods still overlook contextual information e.g. object\ndistances, classes, and learning states, and inadequately assess the\npseudo-label quality using partial information available from the networks. In\nthis work, we propose a novel SS3DOD framework featuring a learnable\npseudo-labeling module designed to automatically and adaptively select\nhigh-quality pseudo-labels. Our approach introduces two networks at the teacher\noutput level. These networks reliably assess the quality of pseudo-labels by\nthe score fusion and determine context-adaptive thresholds, which are\nsupervised by the alignment of pseudo-labels over GT bounding boxes.\nAdditionally, we introduce a soft supervision strategy that can learn robustly\nunder pseudo-label noises. This helps the student network prioritize cleaner\nlabels over noisy ones in semi-supervised learning. Extensive experiments on\nthe KITTI and Waymo datasets demonstrate the effectiveness of our method. The\nproposed method selects high-precision pseudo-labels while maintaining a wider\ncoverage of contexts and a higher recall rate, significantly improving relevant\nSS3DOD methods.",
    "published": "2025-09-28T13:40:48Z",
    "link": "http://arxiv.org/pdf/2509.23880v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Taehun Kong",
      "Tae-Kyun Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23867v1",
    "title": "Sim-DETR: Unlock DETR for Temporal Sentence Grounding",
    "summary": "Temporal sentence grounding aims to identify exact moments in a video that\ncorrespond to a given textual query, typically addressed with detection\ntransformer (DETR) solutions. However, we find that typical strategies designed\nto enhance DETR do not improve, and may even degrade, its performance in this\ntask. We systematically analyze and identify the root causes of this abnormal\nbehavior: (1) conflicts between queries from similar target moments and (2)\ninternal query conflicts due to the tension between global semantics and local\nlocalization. Building on these insights, we propose a simple yet powerful\nbaseline, Sim-DETR, which extends the standard DETR with two minor\nmodifications in the decoder layers: (1) constraining self-attention between\nqueries based on their semantic and positional overlap and (2) adding\nquery-to-frame alignment to bridge the global and local contexts. Experiments\ndemonstrate that Sim-DETR unlocks the full potential of DETR for temporal\nsentence grounding, offering a strong baseline for future research.",
    "published": "2025-09-28T13:21:10Z",
    "link": "http://arxiv.org/pdf/2509.23867v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiajin Tang",
      "Zhengxuan Wei",
      "Yuchen Zhu",
      "Cheng Shi",
      "Guanbin Li",
      "Liang Lin",
      "Sibei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23859v1",
    "title": "FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for\n  Fair and Explainable Facial Beauty Prediction",
    "summary": "Facial Beauty Prediction (FBP) has made significant strides with the\napplication of deep learning, yet state-of-the-art models often exhibit\ncritical limitations, including architectural constraints, inherent demographic\nbiases, and a lack of transparency. Existing methods, primarily based on\nConvolutional Neural Networks (CNNs), excel at capturing local texture but\nstruggle with global facial harmony, while Vision Transformers (ViTs)\neffectively model long-range dependencies but can miss fine-grained details.\nFurthermore, models trained on benchmark datasets can inadvertently learn and\nperpetuate societal biases related to protected attributes like ethnicity. To\naddress these interconnected challenges, we propose \\textbf{FairViT-GAN}, a\nnovel hybrid framework that synergistically integrates a CNN branch for local\nfeature extraction and a ViT branch for global context modeling. More\nsignificantly, we introduce an adversarial debiasing mechanism where the\nfeature extractor is explicitly trained to produce representations that are\ninvariant to protected attributes, thereby actively mitigating algorithmic\nbias. Our framework's transparency is enhanced by visualizing the distinct\nfocus of each architectural branch. Extensive experiments on the SCUT-FBP5500\nbenchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in\npredictive accuracy, achieving a Pearson Correlation of \\textbf{0.9230} and\nreducing RMSE to \\textbf{0.2650}, but also excels in fairness. Our analysis\nreveals a remarkable \\textbf{82.9\\% reduction in the performance gap} between\nethnic subgroups, with the adversary's classification accuracy dropping to\nnear-random chance (52.1\\%). We believe FairViT-GAN provides a robust,\ntransparent, and significantly fairer blueprint for developing responsible AI\nsystems for subjective visual assessment.",
    "published": "2025-09-28T12:55:31Z",
    "link": "http://arxiv.org/pdf/2509.23859v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Djamel Eddine Boukhari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23849v1",
    "title": "CE-FAM: Concept-Based Explanation via Fusion of Activation Maps",
    "summary": "Although saliency maps can highlight important regions to explain the\nreasoning behind image classification in artificial intelligence (AI), the\nmeaning of these regions is left to the user's interpretation. In contrast,\nconceptbased explanations decompose AI predictions into humanunderstandable\nconcepts, clarifying their contributions. However, few methods can\nsimultaneously reveal what concepts an image classifier learns, which regions\nare associated with them, and how they contribute to predictions. We propose a\nnovel concept-based explanation method, Concept-based Explanation via Fusion of\nActivation Maps (CE-FAM). It employs a branched network that shares activation\nmaps with an image classifier and learns to mimic the embeddings of a Vision\nand Language Model (VLM). The branch network predicts concepts in an image, and\ntheir corresponding regions are represented by a weighted sum of activation\nmaps, with weights given by the gradients of the concept prediction scores.\nTheir contributions are quantified based on their impact on the image\nclassification score. Our method provides a general framework for identifying\nthe concept regions and their contributions while leveraging VLM knowledge to\nhandle arbitrary concepts without requiring an annotated dataset. Furthermore,\nwe introduce a novel evaluation metric to assess the accuracy of the concept\nregions. Our qualitative and quantitative evaluations demonstrate our method\noutperforms existing approaches and excels in zero-shot inference for unseen\nconcepts.",
    "published": "2025-09-28T12:40:53Z",
    "link": "http://arxiv.org/pdf/2509.23849v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Michihiro Kuroki",
      "Toshihiko Yamasaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23841v1",
    "title": "Towards Fine-Grained Text-to-3D Quality Assessment: A Benchmark and A\n  Two-Stage Rank-Learning Metric",
    "summary": "Recent advances in Text-to-3D (T23D) generative models have enabled the\nsynthesis of diverse, high-fidelity 3D assets from textual prompts. However,\nexisting challenges restrict the development of reliable T23D quality\nassessment (T23DQA). First, existing benchmarks are outdated, fragmented, and\ncoarse-grained, making fine-grained metric training infeasible. Moreover,\ncurrent objective metrics exhibit inherent design limitations, resulting in\nnon-representative feature extraction and diminished metric robustness. To\naddress these limitations, we introduce T23D-CompBench, a comprehensive\nbenchmark for compositional T23D generation. We define five components with\ntwelve sub-components for compositional prompts, which are used to generate\n3,600 textured meshes from ten state-of-the-art generative models. A\nlarge-scale subjective experiment is conducted to collect 129,600 reliable\nhuman ratings across different perspectives. Based on T23D-CompBench, we\nfurther propose Rank2Score, an effective evaluator with two-stage training for\nT23DQA. Rank2Score enhances pairwise training via supervised contrastive\nregression and curriculum learning in the first stage, and subsequently refines\npredictions using mean opinion scores to achieve closer alignment with human\njudgments in the second stage. Extensive experiments and downstream\napplications demonstrate that Rank2Score consistently outperforms existing\nmetrics across multiple dimensions and can additionally serve as a reward\nfunction to optimize generative models. The project is available at\nhttps://cbysjtu.github.io/Rank2Score/.",
    "published": "2025-09-28T12:30:47Z",
    "link": "http://arxiv.org/pdf/2509.23841v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bingyang Cui",
      "Yujie Zhang",
      "Qi Yang",
      "Zhu Li",
      "Yiling Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23838v1",
    "title": "2nd Place Report of MOSEv2 Challenge 2025: Concept Guided Video Object\n  Segmentation via SeC",
    "summary": "Semi-supervised Video Object Segmentation aims to segment a specified target\nthroughout a video sequence, initialized by a first-frame mask. Previous\nmethods rely heavily on appearance-based pattern matching and thus exhibit\nlimited robustness against challenges such as drastic visual changes,\nocclusions, and scene shifts. This failure is often attributed to a lack of\nhigh-level conceptual understanding of the target. The recently proposed\nSegment Concept (SeC) framework mitigated this limitation by using a Large\nVision-Language Model (LVLM) to establish a deep semantic understanding of the\nobject for more persistent segmentation. In this work, we evaluate its\nzero-shot performance on the challenging coMplex video Object SEgmentation v2\n(MOSEv2) dataset. Without any fine-tuning on the training set, SeC achieved\n39.7 \\JFn on the test set and ranked 2nd place in the Complex VOS track of the\n7th Large-scale Video Object Segmentation Challenge.",
    "published": "2025-09-28T12:26:03Z",
    "link": "http://arxiv.org/pdf/2509.23838v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhixiong Zhang",
      "Shuangrui Ding",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Jiaqi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23833v1",
    "title": "AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset\n  with Speech Recognition Baselines",
    "summary": "Whisper speech recognition is crucial not only for ensuring privacy in\nsensitive communications but also for providing a critical communication bridge\nfor patients under vocal restraint and enabling discrete interaction in\nnoise-sensitive environments. The development of Chinese mandarin audio-visual\nwhisper speech recognition is hindered by the lack of large-scale datasets. We\npresent AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech\ndataset, featuring 30 hours each of whisper speech and parallel normal speech,\nwith synchronized frontal facial videos. Moreover, we propose an audio-visual\nspeech recognition (AVSR) baseline based on the Whisper-Flamingo framework,\nwhich integrates a parallel training strategy to align embeddings across speech\ntypes, and employs a projection layer to adapt to whisper speech's spectral\nproperties. The model achieves a Character Error Rate (CER) of 4.13% for\nwhisper speech and 1.11% for normal speech in the test set of our dataset, and\nestablishes new state-of-the-art results on the wTIMIT benchmark. The dataset\nand the AVSR baseline codes are open-sourced at\nhttps://zutm.github.io/AISHELL6-Whisper.",
    "published": "2025-09-28T12:14:06Z",
    "link": "http://arxiv.org/pdf/2509.23833v1.pdf",
    "category": [
      "eess.AS",
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "authors": [
      "Cancan Li",
      "Fei Su",
      "Juan Liu",
      "Hui Bu",
      "Yulong Wan",
      "Hongbin Suo",
      "Ming Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23828v1",
    "title": "Uni4D-LLM: A Unified SpatioTemporal-Aware VLM for 4D Understanding and\n  Generation",
    "summary": "Vision-language models (VLMs) have demonstrated strong performance in 2D\nscene understanding and generation, but extending this unification to the\nphysical world remains an open challenge. Existing 3D and 4D approaches\ntypically embed scene geometry into autoregressive model for semantic\nunderstanding and diffusion model for content generation. This paradigm gap\nprevents a single model from jointly handling both tasks, especially in dynamic\n4D settings where spatiotemporal modeling is critical. We propose Uni4D-LLM,\nthe first unified VLM framework with spatiotemporal awareness for 4D scene\nunderstanding and generation. Our design is guided by two key insights: 1)\nUnification requires a shared representation. We extract semantic features for\nunderstanding and noisy-injected appearance features for generation,\nincorporate 4D geometric cues, and fuse them into a spatiotemporal-aware visual\nrepresentation through adaptive cross-attention. 2) Unification requires a\nshared architecture. Both autoregression and diffusion are built on Transformer\nbackbones, and this enables integration into a single LLM with task-specific\nheads. By aligning visual and linguistic representations, our Uni4D-LLM\nproduces predictions for both understanding and generation within one\nTransformer-based framework. We further apply instruction fine-tuning on\ndiverse 4D vision-language datasets to improve generalization across tasks.\nExtensive experiments on multiple benchmarks demonstrate that Uni4D-LLM\nachieves competitive or superior results compared to state-of-the-art models\nand offers the first true unification of 4D scene understanding and generation.",
    "published": "2025-09-28T12:06:54Z",
    "link": "http://arxiv.org/pdf/2509.23828v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hanyu Zhou",
      "Gim Hee Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23827v1",
    "title": "Assessing Visual Privacy Risks in Multimodal AI: A Novel\n  Taxonomy-Grounded Evaluation of Vision-Language Models",
    "summary": "Artificial Intelligence have profoundly transformed the technological\nlandscape in recent years. Large Language Models (LLMs) have demonstrated\nimpressive abilities in reasoning, text comprehension, contextual pattern\nrecognition, and integrating language with visual understanding. While these\nadvances offer significant benefits, they also reveal critical limitations in\nthe models' ability to grasp the notion of privacy. There is hence substantial\ninterest in determining if and how these models can understand and enforce\nprivacy principles, particularly given the lack of supporting resources to test\nsuch a task. In this work, we address these challenges by examining how legal\nframeworks can inform the capabilities of these emerging technologies. To this\nend, we introduce a comprehensive, multi-level Visual Privacy Taxonomy that\ncaptures a wide range of privacy issues, designed to be scalable and adaptable\nto existing and future research needs. Furthermore, we evaluate the\ncapabilities of several state-of-the-art Vision-Language Models (VLMs),\nrevealing significant inconsistencies in their understanding of contextual\nprivacy. Our work contributes both a foundational taxonomy for future research\nand a critical benchmark of current model limitations, demonstrating the urgent\nneed for more robust, privacy-aware AI systems.",
    "published": "2025-09-28T12:04:54Z",
    "link": "http://arxiv.org/pdf/2509.23827v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Efthymios Tsaprazlis",
      "Tiantian Feng",
      "Anil Ramakrishna",
      "Rahul Gupta",
      "Shrikanth Narayanan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23804v1",
    "title": "Controllable Generation of Large-Scale 3D Urban Layouts with Semantic\n  and Structural Guidance",
    "summary": "Urban modeling is essential for city planning, scene synthesis, and gaming.\nExisting image-based methods generate diverse layouts but often lack geometric\ncontinuity and scalability, while graph-based methods capture structural\nrelations yet overlook parcel semantics. We present a controllable framework\nfor large-scale 3D vector urban layout generation, conditioned on both geometry\nand semantics. By fusing geometric and semantic attributes, introducing edge\nweights, and embedding building height in the graph, our method extends 2D\nlayouts to realistic 3D structures. It also enables users to directly control\nthe output by modifying semantic attributes. Experiments show that it produces\nvalid, large-scale urban models, offering an effective tool for data-driven\nplanning and design.",
    "published": "2025-09-28T11:08:17Z",
    "link": "http://arxiv.org/pdf/2509.23804v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mengyuan Niu",
      "Xinxin Zhuo",
      "Ruizhe Wang",
      "Yuyue Huang",
      "Junyan Yang",
      "Qiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23774v1",
    "title": "Texture Vector-Quantization and Reconstruction Aware Prediction for\n  Generative Super-Resolution",
    "summary": "Vector-quantized based models have recently demonstrated strong potential for\nvisual prior modeling. However, existing VQ-based methods simply encode visual\nfeatures with nearest codebook items and train index predictor with code-level\nsupervision. Due to the richness of visual signal, VQ encoding often leads to\nlarge quantization error. Furthermore, training predictor with code-level\nsupervision can not take the final reconstruction errors into consideration,\nresult in sub-optimal prior modeling accuracy. In this paper we address the\nabove two issues and propose a Texture Vector-Quantization and a Reconstruction\nAware Prediction strategy. The texture vector-quantization strategy leverages\nthe task character of super-resolution and only introduce codebook to model the\nprior of missing textures. While the reconstruction aware prediction strategy\nmakes use of the straight-through estimator to directly train index predictor\nwith image-level supervision. Our proposed generative SR model (TVQ&RAP) is\nable to deliver photo-realistic SR results with small computational cost.",
    "published": "2025-09-28T09:40:38Z",
    "link": "http://arxiv.org/pdf/2509.23774v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qifan Li",
      "Jiale Zou",
      "Jinhua Zhang",
      "Wei Long",
      "Xinyu Zhou",
      "Shuhang Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23772v1",
    "title": "A Modality-Tailored Graph Modeling Framework for Urban Region\n  Representation via Contrastive Learning",
    "summary": "Graph-based models have emerged as a powerful paradigm for modeling\nmultimodal urban data and learning region representations for various\ndownstream tasks. However, existing approaches face two major limitations. (1)\nThey typically employ identical graph neural network architectures across all\nmodalities, failing to capture modality-specific structures and\ncharacteristics. (2) During the fusion stage, they often neglect spatial\nheterogeneity by assuming that the aggregation weights of different modalities\nremain invariant across regions, resulting in suboptimal representations. To\naddress these issues, we propose MTGRR, a modality-tailored graph modeling\nframework for urban region representation, built upon a multimodal dataset\ncomprising point of interest (POI), taxi mobility, land use, road element,\nremote sensing, and street view images. (1) MTGRR categorizes modalities into\ntwo groups based on spatial density and data characteristics: aggregated-level\nand point-level modalities. For aggregated-level modalities, MTGRR employs a\nmixture-of-experts (MoE) graph architecture, where each modality is processed\nby a dedicated expert GNN to capture distinct modality-specific\ncharacteristics. For the point-level modality, a dual-level GNN is constructed\nto extract fine-grained visual semantic features. (2) To obtain effective\nregion representations under spatial heterogeneity, a spatially-aware\nmultimodal fusion mechanism is designed to dynamically infer region-specific\nmodality fusion weights. Building on this graph modeling framework, MTGRR\nfurther employs a joint contrastive learning strategy that integrates region\naggregated-level, point-level, and fusion-level objectives to optimize region\nrepresentations. Experiments on two real-world datasets across six modalities\nand three tasks demonstrate that MTGRR consistently outperforms\nstate-of-the-art baselines, validating its effectiveness.",
    "published": "2025-09-28T09:38:08Z",
    "link": "http://arxiv.org/pdf/2509.23772v1.pdf",
    "category": [
      "cs.CV",
      "stat.AP"
    ],
    "authors": [
      "Yaya Zhao",
      "Kaiqi Zhao",
      "Zixuan Tang",
      "Zhiyuan Liu",
      "Xiaoling Lu",
      "Yalei Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23770v1",
    "title": "GenView++: Unifying Adaptive View Generation and Quality-Driven\n  Supervision for Contrastive Representation Learning",
    "summary": "The success of contrastive learning depends on the construction and\nutilization of high-quality positive pairs. However, current methods face\ncritical limitations on two fronts: on the construction side, both handcrafted\nand generative augmentations often suffer from limited diversity and risk\nsemantic corruption; on the learning side, the absence of a quality assessment\nmechanism leads to suboptimal supervision where all pairs are treated equally.\nTo tackle these challenges, we propose GenView++, a unified framework that\naddresses both fronts by introducing two synergistic innovations. To improve\npair construction, GenView++ introduces a multi-source adaptive view generation\nmechanism to synthesize diverse yet semantically coherent views by dynamically\nmodulating generative parameters across image-conditioned, text-conditioned,\nand image-text-conditioned strategies. Second, a quality-driven contrastive\nlearning mechanism assesses each pair's semantic alignment and diversity to\ndynamically reweight their training contribution, prioritizing high-quality\npairs while suppressing redundant or misaligned pairs. Extensive experiments\ndemonstrate the effectiveness of GenView++ across both vision and\nvision-language tasks. For vision representation learning, it improves MoCov2\nby +2.5% on ImageNet linear classification. For vision-language learning, it\nraises the average zero-shot classification accuracy by +12.31% over CLIP and\n+5.31% over SLIP across ten datasets, and further improves Flickr30k text\nretrieval R@5 by +3.2%. The code is available at\nhttps://github.com/xiaojieli0903/GenViewPlusPlus.",
    "published": "2025-09-28T09:35:37Z",
    "link": "http://arxiv.org/pdf/2509.23770v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaojie Li",
      "Bei Wang",
      "Jianlong Wu",
      "Yue Yu",
      "Liqiang Nie",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23769v1",
    "title": "ReLumix: Extending Image Relighting to Video via Video Diffusion Models",
    "summary": "Controlling illumination during video post-production is a crucial yet\nelusive goal in computational photography. Existing methods often lack\nflexibility, restricting users to certain relighting models. This paper\nintroduces ReLumix, a novel framework that decouples the relighting algorithm\nfrom temporal synthesis, thereby enabling any image relighting technique to be\nseamlessly applied to video. Our approach reformulates video relighting into a\nsimple yet effective two-stage process: (1) an artist relights a single\nreference frame using any preferred image-based technique (e.g., Diffusion\nModels, physics-based renderers); and (2) a fine-tuned stable video diffusion\n(SVD) model seamlessly propagates this target illumination throughout the\nsequence. To ensure temporal coherence and prevent artifacts, we introduce a\ngated cross-attention mechanism for smooth feature blending and a temporal\nbootstrapping strategy that harnesses SVD's powerful motion priors. Although\ntrained on synthetic data, ReLumix shows competitive generalization to\nreal-world videos. The method demonstrates significant improvements in visual\nfidelity, offering a scalable and versatile solution for dynamic lighting\ncontrol.",
    "published": "2025-09-28T09:35:33Z",
    "link": "http://arxiv.org/pdf/2509.23769v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Lezhong Wang",
      "Shutong Jin",
      "Ruiqi Cui",
      "Anders Bjorholm Dahl",
      "Jeppe Revall Frisvad",
      "Siavash Bigdeli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23760v1",
    "title": "UniAlignment: Semantic Alignment for Unified Image Generation,\n  Understanding, Manipulation and Perception",
    "summary": "The remarkable success of diffusion models in text-to-image generation has\nsparked growing interest in expanding their capabilities to a variety of\nmulti-modal tasks, including image understanding, manipulation, and perception.\nThese tasks require advanced semantic comprehension across both visual and\ntextual modalities, especially in scenarios involving complex semantic\ninstructions. However, existing approaches often rely heavily on\nvision-language models (VLMs) or modular designs for semantic guidance, leading\nto fragmented architectures and computational inefficiency. To address these\nchallenges, we propose UniAlignment, a unified multimodal generation framework\nwithin a single diffusion transformer. UniAlignment introduces a dual-stream\ndiffusion training strategy that incorporates both intrinsic-modal semantic\nalignment and cross-modal semantic alignment, thereby enhancing the model's\ncross-modal consistency and instruction-following robustness. Additionally, we\npresent SemGen-Bench, a new benchmark specifically designed to evaluate\nmultimodal semantic consistency under complex textual instructions. Extensive\nexperiments across multiple tasks and benchmarks demonstrate that UniAlignment\noutperforms existing baselines, underscoring the significant potential of\ndiffusion models in unified multimodal generation.",
    "published": "2025-09-28T09:11:30Z",
    "link": "http://arxiv.org/pdf/2509.23760v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinyang Song",
      "Libin Wang",
      "Weining Wang",
      "Shaozhen Liu",
      "Dandan Zheng",
      "Jingdong Chen",
      "Qi Li",
      "Zhenan Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23742v1",
    "title": "GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling\n  for Large-Scale Data",
    "summary": "To effectively handle clustering task for large-scale datasets, we propose a\nnovel scalable skeleton clustering algorithm, namely GBSK, which leverages the\ngranular-ball technique to capture the underlying structure of data. By\nmulti-sampling the dataset and constructing multi-grained granular-balls, GBSK\nprogressively uncovers a statistical \"skeleton\" -- a spatial abstraction that\napproximates the essential structure and distribution of the original data.\nThis strategy enables GBSK to dramatically reduce computational overhead while\nmaintaining high clustering accuracy. In addition, we introduce an adaptive\nversion, AGBSK, with simplified parameter settings to enhance usability and\nfacilitate deployment in real-world scenarios. Extensive experiments conducted\non standard computing hardware demonstrate that GBSK achieves high efficiency\nand strong clustering performance on large-scale datasets, including one with\nup to 100 million instances across 256 dimensions. Our implementation and\nexperimental results are available at: https://github.com/XFastDataLab/GBSK/.",
    "published": "2025-09-28T08:41:15Z",
    "link": "http://arxiv.org/pdf/2509.23742v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Yewang Chen",
      "Junfeng Li",
      "Shuyin Xia",
      "Qinghong Lai",
      "Xinbo Gao",
      "Guoyin Wang",
      "Dongdong Cheng",
      "Yi Liu",
      "Yi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23741v1",
    "title": "ResAD++: Towards Class Agnostic Anomaly Detection via Residual Feature\n  Learning",
    "summary": "This paper explores the problem of class-agnostic anomaly detection (AD),\nwhere the objective is to train one class-agnostic AD model that can generalize\nto detect anomalies in diverse new classes from different domains without any\nretraining or fine-tuning on the target data. When applied for new classes, the\nperformance of current single- and multi-class AD methods is still\nunsatisfactory. One fundamental reason is that representation learning in\nexisting methods is still class-related, namely, feature correlation. To\naddress this issue, we propose residual features and construct a simple but\neffective framework, termed ResAD. Our core insight is to learn the residual\nfeature distribution rather than the initial feature distribution. Residual\nfeatures are formed by matching and then subtracting normal reference features.\nIn this way, we can effectively realize feature decorrelation. Even in new\nclasses, the distribution of normal residual features would not remarkably\nshift from the learned distribution. In addition, we think that residual\nfeatures still have one issue: scale correlation. To this end, we propose a\nfeature hypersphere constraining approach, which learns to constrain initial\nnormal residual features into a spatial hypersphere for enabling the feature\nscales of different classes as consistent as possible. Furthermore, we propose\na novel logbarrier bidirectional contraction OCC loss and vector quantization\nbased feature distribution matching module to enhance ResAD, leading to the\nimproved version of ResAD (ResAD++). Comprehensive experiments on eight\nreal-world AD datasets demonstrate that our ResAD++ can achieve remarkable AD\nresults when directly used in new classes, outperforming state-of-the-art\ncompeting methods and also surpassing ResAD. The code is available at\nhttps://github.com/xcyao00/ResAD.",
    "published": "2025-09-28T08:41:05Z",
    "link": "http://arxiv.org/pdf/2509.23741v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xincheng Yao",
      "Chao Shi",
      "Muming Zhao",
      "Guangtao Zhai",
      "Chongyang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23737v1",
    "title": "GRS-SLAM3R: Real-Time Dense SLAM with Gated Recurrent State",
    "summary": "DUSt3R-based end-to-end scene reconstruction has recently shown promising\nresults in dense visual SLAM. However, most existing methods only use image\npairs to estimate pointmaps, overlooking spatial memory and global\nconsistency.To this end, we introduce GRS-SLAM3R, an end-to-end SLAM framework\nfor dense scene reconstruction and pose estimation from RGB images without any\nprior knowledge of the scene or camera parameters. Unlike existing DUSt3R-based\nframeworks, which operate on all image pairs and predict per-pair point maps in\nlocal coordinate frames, our method supports sequentialized input and\nincrementally estimates metric-scale point clouds in the global coordinate. In\norder to improve consistent spatial correlation, we use a latent state for\nspatial memory and design a transformer-based gated update module to reset and\nupdate the spatial memory that continuously aggregates and tracks relevant 3D\ninformation across frames. Furthermore, we partition the scene into submaps,\napply local alignment within each submap, and register all submaps into a\ncommon world frame using relative constraints, producing a globally consistent\nmap. Experiments on various datasets show that our framework achieves superior\nreconstruction accuracy while maintaining real-time performance.",
    "published": "2025-09-28T08:33:34Z",
    "link": "http://arxiv.org/pdf/2509.23737v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Guole Shen",
      "Tianchen Deng",
      "Yanbo Wang",
      "Yongtao Chen",
      "Yilin Shen",
      "Jiuming Liu",
      "Jingchuan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23733v1",
    "title": "FastViDAR: Real-Time Omnidirectional Depth Estimation via Alternative\n  Hierarchical Attention",
    "summary": "In this paper we propose FastViDAR, a novel framework that takes four fisheye\ncamera inputs and produces a full $360^\\circ$ depth map along with per-camera\ndepth, fusion depth, and confidence estimates. Our main contributions are: (1)\nWe introduce Alternative Hierarchical Attention (AHA) mechanism that\nefficiently fuses features across views through separate intra-frame and\ninter-frame windowed self-attention, achieving cross-view feature mixing with\nreduced overhead. (2) We propose a novel ERP fusion approach that projects\nmulti-view depth estimates to a shared equirectangular coordinate system to\nobtain the final fusion depth. (3) We generate ERP image-depth pairs using HM3D\nand 2D3D-S datasets for comprehensive evaluation, demonstrating competitive\nzero-shot performance on real datasets while achieving up to 20 FPS on NVIDIA\nOrin NX embedded hardware. Project page:\n\\href{https://3f7dfc.github.io/FastVidar/}{https://3f7dfc.github.io/FastVidar/}",
    "published": "2025-09-28T08:25:27Z",
    "link": "http://arxiv.org/pdf/2509.23733v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Hangtian Zhao",
      "Xiang Chen",
      "Yizhe Li",
      "Qianhao Wang",
      "Haibo Lu",
      "Fei Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23723v1",
    "title": "DiffPCN: Latent Diffusion Model Based on Multi-view Depth Images for\n  Point Cloud Completion",
    "summary": "Latent diffusion models (LDMs) have demonstrated remarkable generative\ncapabilities across various low-level vision tasks. However, their potential\nfor point cloud completion remains underexplored due to the unstructured and\nirregular nature of point clouds. In this work, we propose DiffPCN, a novel\ndiffusion-based coarse-to-fine framework for point cloud completion. Our\napproach comprises two stages: an initial stage for generating coarse point\nclouds, and a refinement stage that improves their quality through point\ndenoising and upsampling. Specifically, we first project the unordered and\nirregular partial point cloud into structured depth images, which serve as\nconditions for a well-designed DepthLDM to synthesize completed multi-view\ndepth images that are used to form coarse point clouds. In this way, our\nDiffPCN can yield high-quality and high-completeness coarse point clouds by\nleveraging LDM' s powerful generation and comprehension capabilities. Then,\nsince LDMs inevitably introduce outliers into the generated depth maps, we\ndesign a Point Denoising Network to remove artifacts from the coarse point\ncloud by predicting a per-point distance score. Finally, we devise an\nAssociation-Aware Point Upsampler, which guides the upsampling process by\nleveraging local association features between the input point cloud and the\ncorresponding coarse points, further yielding a dense and high-fidelity output.\nExperimental results demonstrate that our DiffPCN achieves state-of-the-art\nperformance in geometric accuracy and shape completeness, significantly\nimproving the robustness and consistency of point cloud completion.",
    "published": "2025-09-28T08:05:43Z",
    "link": "http://arxiv.org/pdf/2509.23723v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zijun Li",
      "Hongyu Yan",
      "Shijie Li",
      "Kunming Luo",
      "Li Lu",
      "Xulei Yang",
      "Weisi Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23719v1",
    "title": "PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary\n  Diagnosis of Parkinson's Disease",
    "summary": "Parkinson's disease (PD) is a common neurodegenerative disorder that severely\ndiminishes patients' quality of life. Its global prevalence has increased\nmarkedly in recent decades. Current diagnostic workflows are complex and\nheavily reliant on neurologists' expertise, often resulting in delays in early\ndetection and missed opportunities for timely intervention. To address these\nissues, we propose an end-to-end automated diagnostic method for PD, termed\nPD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly\nfrom raw MRI scans. This framework first introduces an MRI Pre-processing\nModule (MRI-Processor) to mitigate inter-subject and inter-scanner variability\nby flexibly integrating established medical imaging preprocessing tools. It\nthen incorporates two forms of clinical prior knowledge: (1)\nBrain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions\nstrongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior),\nwhich reflects the accelerated aging typically observed in PD-associated\nregions. Building on these priors, we design two dedicated modules: the\nRelevance-Prior Guided Feature Aggregation Module (Aggregator), which guides\nthe model to focus on PD-associated regions at the inter-subject level, and the\nAge-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps\nas auxiliary constraints at the intra-subject level to enhance diagnostic\naccuracy and clinical interpretability. Furthermore, we collected external test\ndata from our collaborating hospital. Experimental results show that\nPD-Diag-Net achieves 86\\% accuracy on external tests and over 96% accuracy in\nearly-stage diagnosis, outperforming existing advanced methods by more than\n20%.",
    "published": "2025-09-28T08:00:03Z",
    "link": "http://arxiv.org/pdf/2509.23719v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuai Shao",
      "Shu Jiang",
      "Shiyuan Zhao",
      "Di Yang",
      "Yan Wang",
      "Yutong Bai",
      "Jianguo Zhang",
      "Jiangtao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23718v1",
    "title": "Diff-3DCap: Shape Captioning with Diffusion Models",
    "summary": "The task of 3D shape captioning occupies a significant place within the\ndomain of computer graphics and has garnered considerable interest in recent\nyears. Traditional approaches to this challenge frequently depend on the\nutilization of costly voxel representations or object detection techniques, yet\noften fail to deliver satisfactory outcomes. To address the above challenges,\nin this paper, we introduce Diff-3DCap, which employs a sequence of projected\nviews to represent a 3D object and a continuous diffusion model to facilitate\nthe captioning process. More precisely, our approach utilizes the continuous\ndiffusion model to perturb the embedded captions during the forward phase by\nintroducing Gaussian noise and then predicts the reconstructed annotation\nduring the reverse phase. Embedded within the diffusion framework is a\ncommitment to leveraging a visual embedding obtained from a pre-trained\nvisual-language model, which naturally allows the embedding to serve as a\nguiding signal, eliminating the need for an additional classifier. Extensive\nresults of our experiments indicate that Diff-3DCap can achieve performance\ncomparable to that of the current state-of-the-art methods.",
    "published": "2025-09-28T07:59:22Z",
    "link": "http://arxiv.org/pdf/2509.23718v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zhenyu Shu",
      "Jiawei Wen",
      "Shiyang Li",
      "Shiqing Xin",
      "Ligang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23709v1",
    "title": "StrucADT: Generating Structure-controlled 3D Point Clouds with Adjacency\n  Diffusion Transformer",
    "summary": "In the field of 3D point cloud generation, numerous 3D generative models have\ndemonstrated the ability to generate diverse and realistic 3D shapes. However,\nthe majority of these approaches struggle to generate controllable 3D point\ncloud shapes that meet user-specific requirements, hindering the large-scale\napplication of 3D point cloud generation. To address the challenge of lacking\ncontrol in 3D point cloud generation, we are the first to propose controlling\nthe generation of point clouds by shape structures that comprise part\nexistences and part adjacency relationships. We manually annotate the adjacency\nrelationships between the segmented parts of point cloud shapes, thereby\nconstructing a StructureGraph representation. Based on this StructureGraph\nrepresentation, we introduce StrucADT, a novel structure-controllable point\ncloud generation model, which consists of StructureGraphNet module to extract\nstructure-aware latent features, cCNF Prior module to learn the distribution of\nthe latent features controlled by the part adjacency, and Diffusion Transformer\nmodule conditioned on the latent features and part adjacency to generate\nstructure-consistent point cloud shapes. Experimental results demonstrate that\nour structure-controllable 3D point cloud generation method produces\nhigh-quality and diverse point cloud shapes, enabling the generation of\ncontrollable point clouds based on user-specified shape structures and\nachieving state-of-the-art performance in controllable point cloud generation\non the ShapeNet dataset.",
    "published": "2025-09-28T07:45:51Z",
    "link": "http://arxiv.org/pdf/2509.23709v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zhenyu Shu",
      "Jiajun Shen",
      "Zhongui Chen",
      "Xiaoguang Han",
      "Shiqing Xin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23703v1",
    "title": "DFG-PCN: Point Cloud Completion with Degree-Flexible Point Graph",
    "summary": "Point cloud completion is a vital task focused on reconstructing complete\npoint clouds and addressing the incompleteness caused by occlusion and limited\nsensor resolution. Traditional methods relying on fixed local region\npartitioning, such as k-nearest neighbors, which fail to account for the highly\nuneven distribution of geometric complexity across different regions of a\nshape. This limitation leads to inefficient representation and suboptimal\nreconstruction, especially in areas with fine-grained details or structural\ndiscontinuities. This paper proposes a point cloud completion framework called\nDegree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns\nnode degrees using a detail-aware metric that combines feature variation and\ncurvature, focusing on structurally important regions. We further introduce a\ngeometry-aware graph integration module that uses Manhattan distance for edge\naggregation and detail-guided fusion of local and global features to enhance\nrepresentation. Extensive experiments on multiple benchmark datasets\ndemonstrate that our method consistently outperforms state-of-the-art\napproaches.",
    "published": "2025-09-28T07:28:42Z",
    "link": "http://arxiv.org/pdf/2509.23703v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zhenyu Shu",
      "Jian Yao",
      "Shiqing Xin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23700v1",
    "title": "INSTINCT: Instance-Level Interaction Architecture for Query-Based\n  Collaborative Perception",
    "summary": "Collaborative perception systems overcome single-vehicle limitations in\nlong-range detection and occlusion scenarios by integrating multi-agent sensory\ndata, improving accuracy and safety. However, frequent cooperative interactions\nand real-time requirements impose stringent bandwidth constraints. Previous\nworks proves that query-based instance-level interaction reduces bandwidth\ndemands and manual priors, however, LiDAR-focused implementations in\ncollaborative perception remain underdeveloped, with performance still trailing\nstate-of-the-art approaches. To bridge this gap, we propose INSTINCT\n(INSTance-level INteraCtion ArchiTecture), a novel collaborative perception\nframework featuring three core components: 1) a quality-aware filtering\nmechanism for high-quality instance feature selection; 2) a dual-branch\ndetection routing scheme to decouple collaboration-irrelevant and\ncollaboration-relevant instances; and 3) a Cross Agent Local Instance Fusion\nmodule to aggregate local hybrid instance features. Additionally, we enhance\nthe ground truth (GT) sampling technique to facilitate training with diverse\nhybrid instance features. Extensive experiments across multiple datasets\ndemonstrate that INSTINCT achieves superior performance. Specifically, our\nmethod achieves an improvement in accuracy 13.23%/33.08% in DAIR-V2X and\nV2V4Real while reducing the communication bandwidth to 1/281 and 1/264 compared\nto state-of-the-art methods. The code is available at\nhttps://github.com/CrazyShout/INSTINCT.",
    "published": "2025-09-28T07:16:32Z",
    "link": "http://arxiv.org/pdf/2509.23700v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yunjiang Xu",
      "Lingzhi Li",
      "Jin Wang",
      "Yupeng Ouyang",
      "Benyuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23697v1",
    "title": "Confidence Aware SSD Ensemble with Weighted Boxes Fusion for Weapon\n  Detection",
    "summary": "The safety and security of public spaces is of vital importance, driving the\nneed for sophisticated surveillance systems capable of accurately detecting\nweapons, which are often hampered by issues like partial occlusion, varying\nlighting, and cluttered backgrounds. While single-model detectors are advanced,\nthey often lack robustness in these challenging conditions. This paper presents\nthe hypothesis that ensemble of Single Shot Multibox Detector (SSD) models with\ndiverse feature extraction backbones can significantly enhance detection\nrobustness. To leverage diverse feature representations, individual SSD models\nwere trained using a selection of backbone networks: VGG16, ResNet50,\nEfficientNet, and MobileNetV3. The study is conducted on a dataset consisting\nof images of three distinct weapon classes: guns, heavy weapons and knives. The\npredictions from these models are combined using the Weighted Boxes Fusion\n(WBF) method, an ensemble technique designed to optimize bounding box accuracy.\nOur key finding is that the fusion strategy is as critical as the ensemble's\ndiversity, a WBF approach using a 'max' confidence scoring strategy achieved a\nmean Average Precision (mAP) of 0.838. This represents a 2.948% relative\nimprovement over the best-performing single model and consistently outperforms\nother fusion heuristics. This research offers a robust approach to enhancing\nreal-time weapon detection capabilities in surveillance applications by\ndemonstrating that confidence-aware fusion is a key mechanism for improving\naccuracy metrics of ensembles.",
    "published": "2025-09-28T07:08:48Z",
    "link": "http://arxiv.org/pdf/2509.23697v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Atharva Jadhav",
      "Arush Karekar",
      "Manas Divekar",
      "Shachi Natu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23681v1",
    "title": "QuantSparse: Comprehensively Compressing Video Diffusion Transformer\n  with Model Quantization and Attention Sparsification",
    "summary": "Diffusion transformers exhibit remarkable video generation capability, yet\ntheir prohibitive computational and memory costs hinder practical deployment.\nModel quantization and attention sparsification are two promising directions\nfor compression, but each alone suffers severe performance degradation under\naggressive compression. Combining them promises compounded efficiency gains,\nbut naive integration is ineffective. The sparsity-induced information loss\nexacerbates quantization noise, leading to amplified attention shifts. To\naddress this, we propose \\textbf{QuantSparse}, a unified framework that\nintegrates model quantization with attention sparsification. Specifically, we\nintroduce \\textit{Multi-Scale Salient Attention Distillation}, which leverages\nboth global structural guidance and local salient supervision to mitigate\nquantization-induced bias. In addition, we develop \\textit{Second-Order Sparse\nAttention Reparameterization}, which exploits the temporal stability of\nsecond-order residuals to efficiently recover information lost under sparsity.\nExperiments on HunyuanVideo-13B demonstrate that QuantSparse achieves 20.88\nPSNR, substantially outperforming the state-of-the-art quantization baseline\nQ-VDiT (16.85 PSNR), while simultaneously delivering a \\textbf{3.68$\\times$}\nreduction in storage and \\textbf{1.88$\\times$} acceleration in end-to-end\ninference. Our code will be released in\nhttps://github.com/wlfeng0509/QuantSparse.",
    "published": "2025-09-28T06:49:44Z",
    "link": "http://arxiv.org/pdf/2509.23681v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Weilun Feng",
      "Chuanguang Yang",
      "Haotong Qin",
      "Mingqiang Wu",
      "Yuqi Li",
      "Xiangqi Li",
      "Zhulin An",
      "Libo Huang",
      "Yulun Zhang",
      "Michele Magno",
      "Yongjun Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23677v1",
    "title": "MSD-KMamba: Bidirectional Spatial-Aware Multi-Modal 3D Brain\n  Segmentation via Multi-scale Self-Distilled Fusion Strategy",
    "summary": "Numerous CNN-Transformer hybrid models rely on high-complexity global\nattention mechanisms to capture long-range dependencies, which introduces\nnon-linear computational complexity and leads to significant resource\nconsumption. Although knowledge distillation and sparse attention mechanisms\ncan improve efficiency, they often fall short of delivering the high\nsegmentation accuracy necessary for complex tasks. Balancing model performance\nwith computational efficiency remains a critical challenge. In this work, we\npropose a novel 3D multi-modal image segmentation framework, termed MSD-KMamba,\nwhich integrates bidirectional spatial perception with multi-scale\nself-distillation. The bidirectional spatial aware branch effectively captures\nlong-range spatial context dependencies across brain regions, while also\nincorporating a powerful nonlinear feature extraction mechanism that further\nenhances the model's ability to learn complex and heterogeneous patterns. In\naddition, the proposed multi-scale self-distilled fusion strategy strengthens\nhierarchical feature representations and improves the transfer of semantic\ninformation at different resolution levels. By jointly leveraging the\nbidirectional spatial perception branch and the multi-scale self-distilled\nfusion strategy, our framework effectively mitigates the bottleneck of\nquadratic computational complexity in volumetric segmentation, while\nsimultaneously addressing the limitation of insufficient global perception.\nExtensive experiments on multiple standard benchmark datasets demonstrate that\nMSD-KMamba consistently outperforms state-of-the-art methods in segmentation\naccuracy, robustness, and generalization, while maintaining high computational\nefficiency and favorable scalability. The source code of MSD-KMamba is publicly\navailable at https://github.com/daimao-zhang/MSD-KMamba.",
    "published": "2025-09-28T06:34:01Z",
    "link": "http://arxiv.org/pdf/2509.23677v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dayu Tan",
      "Ziwei Zhang",
      "Yansan Su",
      "Xin Peng",
      "Yike Dai",
      "Chunhou Zheng",
      "Weimin Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23672v1",
    "title": "Token Merging via Spatiotemporal Information Mining for Surgical Video\n  Understanding",
    "summary": "Vision Transformer models have shown impressive effectiveness in the surgical\nvideo understanding tasks through long-range dependency modeling. However,\ncurrent methods suffer from prohibitive computational costs due to processing\nmassive spatiotemporal tokens across video frames. While prior work on token\nmerging has advanced model efficiency, they fail to adequately consider the\ninherent spatiotemporal structure of video data and overlook the heterogeneous\nnature of information distribution, leading to suboptimal performance. In this\npaper, we propose a spatiotemporal information mining token merging (STIM-TM)\nmethod, representing the first dedicated approach for surgical video\nunderstanding. STIM-TM introduces a decoupled strategy that reduces token\nredundancy along temporal and spatial dimensions independently. Specifically,\nthe temporal component merges spatially corresponding tokens from consecutive\nframes using saliency weighting, preserving critical sequential information and\nmaintaining continuity. Meanwhile, the spatial component prioritizes merging\nstatic tokens through temporal stability analysis, protecting dynamic regions\ncontaining essential surgical information. Operating in a training-free manner,\nSTIM-TM achieves significant efficiency gains with over $65\\%$ GFLOPs reduction\nwhile preserving competitive accuracy across comprehensive surgical video\ntasks. Our method also supports efficient training of long-sequence surgical\nvideos, addressing computational bottlenecks in surgical applications.",
    "published": "2025-09-28T06:24:57Z",
    "link": "http://arxiv.org/pdf/2509.23672v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xixi Jiang",
      "Chen Yang",
      "Dong Zhang",
      "Pingcheng Dong",
      "Xin Yang",
      "Kwang-Ting Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23663v1",
    "title": "HIVTP: A Training-Free Method to Improve VLMs Efficiency via\n  Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score",
    "summary": "Vision-Language Models (VLMs) have shown strong capabilities on diverse\nmultimodal tasks. However, the large number of visual tokens output by the\nvision encoder severely hinders inference efficiency, and prior studies have\nshown that many of these tokens are not important and can therefore be safely\npruned. In this work, we propose HIVTP, a training-free method to improve VLMs\nefficiency via hierarchical visual token pruning using a novel\nmiddle-layer-based importance score. Specifically, we utilize attention maps\nextracted from the middle layers of the vision encoder, which better reflect\nfine-grained and object-level attention, to estimate visual token importance.\nBased on this, we propose a hierarchical visual token pruning method to retain\nboth globally and locally important visual tokens. Specifically, we reshape the\n1-D visual token sequence output by the vision encoder into a 2-D spatial\nlayout. In the global retaining stage, we divide the image into regions and\nretain tokens with higher importance scores in each region; in the local\nretaining stage, we then divide the image into small windows and retain the\nmost important token in each local window. Experimental results show that our\nproposed method, HIVTP, can reduce the time-to-first-token (TTFT) of\nLLaVA-v1.5-7B and LLaVA-Next-7B by up to 50.0% and 55.1%, respectively, and\nimprove the token generation throughput by up to 60.9% and 47.3%, without\nsacrificing accuracy, and even achieving improvements on certain benchmarks.\nCompared with prior works, HIVTP achieves better accuracy while offering higher\ninference efficiency.",
    "published": "2025-09-28T05:53:39Z",
    "link": "http://arxiv.org/pdf/2509.23663v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingqi Xu",
      "Jingxi Lu",
      "Chenghao Li",
      "Sreetama Sarkar",
      "Peter A. Beerel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23661v1",
    "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal\n  Training",
    "summary": "We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models\n(LMMs) that achieve state-of-the-art performance with significantly reduced\ncomputational and financial costs. Different from the existing works,\nLLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for\nbuilding high-quality vision-language models entirely from scratch. The\nLLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale\nCurated Datasets: We construct an 85M concept-balanced pretraining dataset\nLLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction\ndataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressed\nmultimodal tokens. (2) Efficient Training Framework: We develop a complete\nend-to-end efficient training framework leveraging an offline parallel data\npacking strategy to facilitate the training of LLaVA-OneVision-1.5 within a\n$16,000 budget. (3) State-of-the-art Performance: Experimental results\ndemonstrate that LLaVA-OneVision1.5 yields exceptionally competitive\nperformance across a broad range of downstream tasks. Specifically,\nLLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and\nLLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. We\nanticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community\nto await further updates.",
    "published": "2025-09-28T05:52:55Z",
    "link": "http://arxiv.org/pdf/2509.23661v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiang An",
      "Yin Xie",
      "Kaicheng Yang",
      "Wenkang Zhang",
      "Xiuwei Zhao",
      "Zheng Cheng",
      "Yirui Wang",
      "Songcen Xu",
      "Changrui Chen",
      "Chunsheng Wu",
      "Huajie Tan",
      "Chunyuan Li",
      "Jing Yang",
      "Jie Yu",
      "Xiyao Wang",
      "Bin Qin",
      "Yumeng Wang",
      "Zizhen Yan",
      "Ziyong Feng",
      "Ziwei Liu",
      "Bo Li",
      "Jiankang Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23647v1",
    "title": "Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of\n  Cluttered Objects on Edge Devices",
    "summary": "Robust 6D pose estimation of novel objects under challenging illumination\nremains a significant challenge, often requiring a trade-off between accurate\ninitial pose estimation and efficient real-time tracking. We present a unified\nframework explicitly designed for efficient execution on edge devices, which\nsynergizes a robust initial estimation module with a fast motion-based tracker.\nThe key to our approach is a shared, lighting-invariant color-pair feature\nrepresentation that forms a consistent foundation for both stages. For initial\nestimation, this feature facilitates robust registration between the live RGB-D\nview and the object's 3D mesh. For tracking, the same feature logic validates\ntemporal correspondences, enabling a lightweight model to reliably regress the\nobject's motion. Extensive experiments on benchmark datasets demonstrate that\nour integrated approach is both effective and robust, providing competitive\npose estimation accuracy while maintaining high-fidelity tracking even through\nabrupt pose changes.",
    "published": "2025-09-28T05:07:49Z",
    "link": "http://arxiv.org/pdf/2509.23647v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Xingjian Yang",
      "Ashis G. Banerjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23646v1",
    "title": "Sparse-Up: Learnable Sparse Upsampling for 3D Generation with\n  High-Fidelity Textures",
    "summary": "The creation of high-fidelity 3D assets is often hindered by a 'pixel-level\npain point': the loss of high-frequency details. Existing methods often trade\noff one aspect for another: either sacrificing cross-view consistency,\nresulting in torn or drifting textures, or remaining trapped by the resolution\nceiling of explicit voxels, forfeiting fine texture detail. In this work, we\npropose Sparse-Up, a memory-efficient, high-fidelity texture modeling framework\nthat effectively preserves high-frequency details. We use sparse voxels to\nguide texture reconstruction and ensure multi-view consistency, while\nleveraging surface anchoring and view-domain partitioning to break through\nresolution constraints. Surface anchoring employs a learnable upsampling\nstrategy to constrain voxels to the mesh surface, eliminating over 70% of\nredundant voxels present in traditional voxel upsampling. View-domain\npartitioning introduces an image patch-guided voxel partitioning scheme,\nsupervising and back-propagating gradients only on visible local patches.\nThrough these two strategies, we can significantly reduce memory consumption\nduring high-resolution voxel training without sacrificing geometric\nconsistency, while preserving high-frequency details in textures.",
    "published": "2025-09-28T05:06:03Z",
    "link": "http://arxiv.org/pdf/2509.23646v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lu Xiao",
      "Jiale Zhang",
      "Yang Liu",
      "Taicheng Huang",
      "Xin Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23643v1",
    "title": "Griffin: Generative Reference and Layout Guided Image Composition",
    "summary": "Text-to-image models have achieved a level of realism that enables the\ngeneration of highly convincing images. However, text-based control can be a\nlimiting factor when more explicit guidance is needed. Defining both the\ncontent and its precise placement within an image is crucial for achieving\nfiner control. In this work, we address the challenge of multi-image layout\ncontrol, where the desired content is specified through images rather than\ntext, and the model is guided on where to place each element. Our approach is\ntraining-free, requires a single image per reference, and provides explicit and\nsimple control for object and part-level composition. We demonstrate its\neffectiveness across various image composition tasks.",
    "published": "2025-09-28T04:54:06Z",
    "link": "http://arxiv.org/pdf/2509.23643v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Aryan Mikaeili",
      "Amirhossein Alimohammadi",
      "Negar Hassanpour",
      "Ali Mahdavi-Amiri",
      "Andrea Tagliasacchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23641v1",
    "title": "From Static to Dynamic: a Survey of Topology-Aware Perception in\n  Autonomous Driving",
    "summary": "The key to achieving autonomous driving lies in topology-aware perception,\nthe structured understanding of the driving environment with an emphasis on\nlane topology and road semantics. This survey systematically reviews four core\nresearch directions under this theme: vectorized map construction, topological\nstructure modeling, prior knowledge fusion, and language model-based\nperception. Across these directions, we observe a unifying trend: a paradigm\nshift from static, pre-built maps to dynamic, sensor-driven perception.\nSpecifically, traditional static maps have provided semantic context for\nautonomous systems. However, they are costly to construct, difficult to update\nin real time, and lack generalization across regions, limiting their\nscalability. In contrast, dynamic representations leverage on-board sensor data\nfor real-time map construction and topology reasoning. Each of the four\nresearch directions contributes to this shift through compact spatial modeling,\nsemantic relational reasoning, robust domain knowledge integration, and\nmultimodal scene understanding powered by pre-trained language models.\nTogether, they pave the way for more adaptive, scalable, and explainable\nautonomous driving systems.",
    "published": "2025-09-28T04:47:33Z",
    "link": "http://arxiv.org/pdf/2509.23641v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Yixiao Chen",
      "Ruining Yang",
      "Xin Chen",
      "Jia He",
      "Dongliang Xu",
      "Yue Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23640v1",
    "title": "EfficientMIL: Efficient Linear-Complexity MIL Method for WSI\n  Classification",
    "summary": "Whole slide images (WSIs) classification represents a fundamental challenge\nin computational pathology, where multiple instance learning (MIL) has emerged\nas the dominant paradigm. Current state-of-the-art (SOTA) MIL methods rely on\nattention mechanisms, achieving good performance but requiring substantial\ncomputational resources due to quadratic complexity when processing hundreds of\nthousands of patches. To address this computational bottleneck, we introduce\nEfficientMIL, a novel linear-complexity MIL approach for WSIs classification\nwith the patches selection module Adaptive Patch Selector (APS) that we\ndesigned, replacing the quadratic-complexity self-attention mechanisms in\nTransformer-based MIL methods with efficient sequence models including\nRNN-based GRU, LSTM, and State Space Model (SSM) Mamba. EfficientMIL achieves\nsignificant computational efficiency improvements while outperforming other MIL\nmethods across multiple histopathology datasets. On TCGA-Lung dataset,\nEfficientMIL-Mamba achieved AUC of 0.976 and accuracy of 0.933, while on\nCAMELYON16 dataset, EfficientMIL-GRU achieved AUC of 0.990 and accuracy of\n0.975, surpassing previous state-of-the-art methods. Extensive experiments\ndemonstrate that APS is also more effective for patches selection than\nconventional selection strategies.",
    "published": "2025-09-28T04:47:11Z",
    "link": "http://arxiv.org/pdf/2509.23640v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chengying She",
      "Ben Wang",
      "Xinran Zhang",
      "Dongjie Fan",
      "Jialu Zhang",
      "Chengwei Chen",
      "Lizhuang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23635v1",
    "title": "MotionVerse: A Unified Multimodal Framework for Motion Comprehension,\n  Generation and Editing",
    "summary": "This paper proposes MotionVerse, a unified framework that harnesses the\ncapabilities of Large Language Models (LLMs) to comprehend, generate, and edit\nhuman motion in both single-person and multi-person scenarios. To efficiently\nrepresent motion data, we employ a motion tokenizer with residual quantization,\nwhich converts continuous motion sequences into multi-stream discrete tokens.\nFurthermore, we introduce a \\textit{Delay Parallel} Modeling strategy, which\ntemporally staggers the encoding of residual token streams. This design enables\nLLMs to effectively capture inter-stream dependencies while maintaining\ncomputational efficiency comparable to single-stream modeling. Moreover, to\nalleviate modality interference between motion and language, we design a\n\\textit{dual-tower architecture} with modality-specific parameters, ensuring\nstable integration of motion information for both comprehension and generation\ntasks. Comprehensive ablation studies demonstrate the effectiveness of each\ncomponent in MotionVerse, and extensive experiments showcase its superior\nperformance across a wide range of motion-relevant tasks.",
    "published": "2025-09-28T04:20:56Z",
    "link": "http://arxiv.org/pdf/2509.23635v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ruibing Hou",
      "Mingshuang Luo",
      "Hongyu Pan",
      "Hong Chang",
      "Shiguang Shan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23626v1",
    "title": "Efficient Domain-Adaptive Multi-Task Dense Prediction with Vision\n  Foundation Models",
    "summary": "Multi-task dense prediction, which aims to jointly solve tasks like semantic\nsegmentation and depth estimation, is crucial for robotics applications but\nsuffers from domain shift when deploying models in new environments. While\nunsupervised domain adaptation (UDA) addresses this challenge for single tasks,\nexisting multi-task UDA methods primarily rely on adversarial learning\napproaches that are less effective than recent self-training techniques. In\nthis paper, we introduce FAMDA, a simple yet effective UDA framework that\nbridges this gap by leveraging Vision Foundation Models (VFMs) as powerful\nteachers. Our approach integrates Segmentation and Depth foundation models into\na self-training paradigm to generate high-quality pseudo-labels for the target\ndomain, effectively distilling their robust generalization capabilities into a\nsingle, efficient student network. Extensive experiments show that FAMDA\nachieves state-of-the-art (SOTA) performance on standard synthetic-to-real UDA\nmulti-task learning (MTL) benchmarks and a challenging new day-to-night\nadaptation task. Our framework enables the training of highly efficient models;\na lightweight variant achieves SOTA accuracy while being more than 10$\\times$\nsmaller than foundation models, highlighting FAMDA's suitability for creating\ndomain-adaptive and efficient models for resource-constrained robotics\napplications.",
    "published": "2025-09-28T04:02:36Z",
    "link": "http://arxiv.org/pdf/2509.23626v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Beomseok Kang",
      "Niluthpol Chowdhury Mithun",
      "Mikhail Sizintsev",
      "Han-Pang Chiu",
      "Supun Samarasekera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23624v1",
    "title": "DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to\n  Online Handwriting Generation",
    "summary": "Deep generative models have advanced text-to-online handwriting generation\n(TOHG), which aims to synthesize realistic pen trajectories conditioned on\ntextual input and style references. However, most existing methods still\nprimarily focus on character- or word-level generation, resulting in\ninefficiency and a lack of holistic structural modeling when applied to full\ntext lines. To address these issues, we propose DiffInk, the first latent\ndiffusion Transformer framework for full-line handwriting generation. We first\nintroduce InkVAE, a novel sequential variational autoencoder enhanced with two\ncomplementary latent-space regularization losses: (1) an OCR-based loss\nenforcing glyph-level accuracy, and (2) a style-classification loss preserving\nwriting style. This dual regularization yields a semantically structured latent\nspace where character content and writer styles are effectively disentangled.\nWe then introduce InkDiT, a novel latent diffusion Transformer that integrates\ntarget text and reference styles to generate coherent pen trajectories.\nExperimental results demonstrate that DiffInk outperforms existing\nstate-of-the-art methods in both glyph accuracy and style fidelity, while\nsignificantly improving generation efficiency. Code will be made publicly\navailable.",
    "published": "2025-09-28T03:58:15Z",
    "link": "http://arxiv.org/pdf/2509.23624v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wei Pan",
      "Huiguo He",
      "Hiuyi Cheng",
      "Yilin Shi",
      "Lianwen Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23610v1",
    "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention",
    "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/.",
    "published": "2025-09-28T03:25:34Z",
    "link": "http://arxiv.org/pdf/2509.23610v1.pdf",
    "category": [
      "cs.SD",
      "cs.CV"
    ],
    "authors": [
      "Kai Li",
      "Kejun Gao",
      "Xiaolin Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23608v1",
    "title": "FlowLUT: Efficient Image Enhancement via Differentiable LUTs and\n  Iterative Flow Matching",
    "summary": "Deep learning-based image enhancement methods face a fundamental trade-off\nbetween computational efficiency and representational capacity. For example,\nalthough a conventional three-dimensional Look-Up Table (3D LUT) can process a\ndegraded image in real time, it lacks representational flexibility and depends\nsolely on a fixed prior. To address this problem, we introduce FlowLUT, a novel\nend-to-end model that integrates the efficiency of LUTs, multiple priors, and\nthe parameter-independent characteristic of flow-matched reconstructed images.\nSpecifically, firstly, the input image is transformed in color space by a\ncollection of differentiable 3D LUTs (containing a large number of 3D LUTs with\ndifferent priors). Subsequently, a lightweight content-aware dynamically\npredicts fusion weights, enabling scene-adaptive color correction with\n$\\mathcal{O}(1)$ complexity. Next, a lightweight fusion prediction network runs\non multiple 3D LUTs, with $\\mathcal{O}(1)$ complexity for scene-adaptive color\ncorrection.Furthermore, to address the inherent representation limitations of\nLUTs, we design an innovative iterative flow matching method to restore local\nstructural details and eliminate artifacts. Finally, the entire model is\njointly optimized under a composite loss function enforcing perceptual and\nstructural fidelity. Extensive experimental results demonstrate the\neffectiveness of our method on three benchmarks.",
    "published": "2025-09-28T03:22:01Z",
    "link": "http://arxiv.org/pdf/2509.23608v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Liubing Hu",
      "Chen Wu",
      "Anrui Wang",
      "Dianjie Lu",
      "Guijuan Zhang",
      "Zhuoran Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23607v1",
    "title": "ZeroScene: A Zero-Shot Framework for 3D Scene Generation from a Single\n  Image and Controllable Texture Editing",
    "summary": "In the field of 3D content generation, single image scene reconstruction\nmethods still struggle to simultaneously ensure the quality of individual\nassets and the coherence of the overall scene in complex environments, while\ntexture editing techniques often fail to maintain both local continuity and\nmulti-view consistency. In this paper, we propose a novel system ZeroScene,\nwhich leverages the prior knowledge of large vision models to accomplish both\nsingle image-to-3D scene reconstruction and texture editing in a zero-shot\nmanner. ZeroScene extracts object-level 2D segmentation and depth information\nfrom input images to infer spatial relationships within the scene. It then\njointly optimizes 3D and 2D projection losses of the point cloud to update\nobject poses for precise scene alignment, ultimately constructing a coherent\nand complete 3D scene that encompasses both foreground and background.\nMoreover, ZeroScene supports texture editing of objects in the scene. By\nimposing constraints on the diffusion model and introducing a mask-guided\nprogressive image generation strategy, we effectively maintain texture\nconsistency across multiple viewpoints and further enhance the realism of\nrendered results through Physically Based Rendering (PBR) material estimation.\nExperimental results demonstrate that our framework not only ensures the\ngeometric and appearance accuracy of generated assets, but also faithfully\nreconstructs scene layouts and produces highly detailed textures that closely\nalign with text prompts.",
    "published": "2025-09-28T03:21:12Z",
    "link": "http://arxiv.org/pdf/2509.23607v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Xiang Tang",
      "Ruotong Li",
      "Xiaopeng Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23605v1",
    "title": "VMDiff: Visual Mixing Diffusion for Limitless Cross-Object Synthesis",
    "summary": "Creating novel images by fusing visual cues from multiple sources is a\nfundamental yet underexplored problem in image-to-image generation, with broad\napplications in artistic creation, virtual reality and visual media. Existing\nmethods often face two key challenges: coexistent generation, where multiple\nobjects are simply juxtaposed without true integration, and bias generation,\nwhere one object dominates the output due to semantic imbalance. To address\nthese issues, we propose Visual Mixing Diffusion (VMDiff), a simple yet\neffective diffusion-based framework that synthesizes a single, coherent object\nby integrating two input images at both noise and latent levels. Our approach\ncomprises: (1) a hybrid sampling process that combines guided denoising,\ninversion, and spherical interpolation with adjustable parameters to achieve\nstructure-aware fusion, mitigating coexistent generation; and (2) an efficient\nadaptive adjustment module, which introduces a novel similarity-based score to\nautomatically and adaptively search for optimal parameters, countering semantic\nbias. Experiments on a curated benchmark of 780 concept pairs demonstrate that\nour method outperforms strong baselines in visual quality, semantic\nconsistency, and human-rated creativity.",
    "published": "2025-09-28T03:17:58Z",
    "link": "http://arxiv.org/pdf/2509.23605v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zeren Xiong",
      "Yue Yu",
      "Zedong Zhang",
      "Shuo Chen",
      "Jian Yang",
      "Jun Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23603v1",
    "title": "MAN: Latent Diffusion Enhanced Multistage Anti-Noise Network for\n  Efficient and High-Quality Low-Dose CT Image Denoising",
    "summary": "While diffusion models have set a new benchmark for quality in Low-Dose\nComputed Tomography (LDCT) denoising, their clinical adoption is critically\nhindered by extreme computational costs, with inference times often exceeding\nthousands of seconds per scan. To overcome this barrier, we introduce MAN, a\nLatent Diffusion Enhanced Multistage Anti-Noise Network for Efficient and\nHigh-Quality Low-Dose CT Image Denoising task. Our method operates in a\ncompressed latent space via a perceptually-optimized autoencoder, enabling an\nattention-based conditional U-Net to perform the fast, deterministic\nconditional denoising diffusion process with drastically reduced overhead. On\nthe LDCT and Projection dataset, our model achieves superior perceptual\nquality, surpassing CNN/GAN-based methods while rivaling the reconstruction\nfidelity of computationally heavy diffusion models like DDPM and Dn-Dp. Most\ncritically, in the inference stage, our model is over 60x faster than\nrepresentative pixel space diffusion denoisers, while remaining competitive on\nPSNR/SSIM scores. By bridging the gap between high fidelity and clinical\nviability, our work demonstrates a practical path forward for advanced\ngenerative models in medical imaging.",
    "published": "2025-09-28T03:13:39Z",
    "link": "http://arxiv.org/pdf/2509.23603v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tangtangfang Fang",
      "Jingxi Hu",
      "Xiangjian He",
      "Jiaqi Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23602v1",
    "title": "Deep Taxonomic Networks for Unsupervised Hierarchical Prototype\n  Discovery",
    "summary": "Inspired by the human ability to learn and organize knowledge into\nhierarchical taxonomies with prototypes, this paper addresses key limitations\nin current deep hierarchical clustering methods. Existing methods often tie the\nstructure to the number of classes and underutilize the rich prototype\ninformation available at intermediate hierarchical levels. We introduce deep\ntaxonomic networks, a novel deep latent variable approach designed to bridge\nthese gaps. Our method optimizes a large latent taxonomic hierarchy,\nspecifically a complete binary tree structured mixture-of-Gaussian prior within\na variational inference framework, to automatically discover taxonomic\nstructures and associated prototype clusters directly from unlabeled data\nwithout assuming true label sizes. We analytically show that optimizing the\nELBO of our method encourages the discovery of hierarchical relationships among\nprototypes. Empirically, our learned models demonstrate strong hierarchical\nclustering performance, outperforming baselines across diverse image\nclassification datasets using our novel evaluation mechanism that leverages\nprototype clusters discovered at all hierarchical levels. Qualitative results\nfurther reveal that deep taxonomic networks discover rich and interpretable\nhierarchical taxonomies, capturing both coarse-grained semantic categories and\nfine-grained visual distinctions.",
    "published": "2025-09-28T03:13:32Z",
    "link": "http://arxiv.org/pdf/2509.23602v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zekun Wang",
      "Ethan Haarer",
      "Zhiyi Dai",
      "Tianyi Zhu",
      "Christopher J. MacLellan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23601v1",
    "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
    "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
    "published": "2025-09-28T03:12:43Z",
    "link": "http://arxiv.org/pdf/2509.23601v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Han Hu",
      "Zhuoran Zheng",
      "Liang Li",
      "Chen Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23594v1",
    "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
    "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks.",
    "published": "2025-09-28T02:51:35Z",
    "link": "http://arxiv.org/pdf/2509.23594v1.pdf",
    "category": [
      "cs.CR",
      "cs.CV"
    ],
    "authors": [
      "Yixu Wang",
      "Yan Teng",
      "Yingchun Wang",
      "Xingjun Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23584v1",
    "title": "VividFace: High-Quality and Efficient One-Step Diffusion For Video Face\n  Enhancement",
    "summary": "Video Face Enhancement (VFE) seeks to reconstruct high-quality facial regions\nfrom degraded video sequences, a capability that underpins numerous\napplications including video conferencing, film restoration, and surveillance.\nDespite substantial progress in the field, current methods that primarily rely\non video super-resolution and generative frameworks continue to face three\nfundamental challenges: (1) faithfully modeling intricate facial textures while\npreserving temporal consistency; (2) restricted model generalization due to the\nlack of high-quality face video training data; and (3) low efficiency caused by\nrepeated denoising steps during inference. To address these challenges, we\npropose VividFace, a novel and efficient one-step diffusion framework for video\nface enhancement. Built upon the pretrained WANX video generation model, our\nmethod leverages powerful spatiotemporal priors through a single-step flow\nmatching paradigm, enabling direct mapping from degraded inputs to high-quality\noutputs with significantly reduced inference time. To further boost efficiency,\nwe propose a Joint Latent-Pixel Face-Focused Training strategy that employs\nstochastic switching between facial region optimization and global\nreconstruction, providing explicit supervision in both latent and pixel spaces\nthrough a progressive two-stage training process. Additionally, we introduce an\nMLLM-driven data curation pipeline for automated selection of high-quality\nvideo face datasets, enhancing model generalization. Extensive experiments\ndemonstrate that VividFace achieves state-of-the-art results in perceptual\nquality, identity preservation, and temporal stability, while offering\npractical resources for the research community.",
    "published": "2025-09-28T02:39:48Z",
    "link": "http://arxiv.org/pdf/2509.23584v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shulian Zhang",
      "Yong Guo",
      "Long Peng",
      "Ziyang Wang",
      "Ye Chen",
      "Wenbo Li",
      "Xiao Zhang",
      "Yulun Zhang",
      "Jian Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23582v1",
    "title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization",
    "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor image generation, demonstrating superior scalability and performance over\nU-Net architectures. However, their practical deployment is hindered by\nsubstantial computational and memory costs. While Quantization-Aware Training\n(QAT) has shown promise for U-Nets, its application to DiTs faces unique\nchallenges, primarily due to the sensitivity and distributional complexity of\nactivations. In this work, we identify activation quantization as the primary\nbottleneck for pushing DiTs to extremely low-bit settings. To address this, we\npropose a systematic QAT framework for DiTs, named RobuQ. We start by\nestablishing a strong ternary weight (W1.58A4) DiT baseline. Building upon\nthis, we propose RobustQuantizer to achieve robust activation quantization. Our\ntheoretical analyses show that the Hadamard transform can convert unknown\nper-token distributions into per-token normal distributions, providing a strong\nfoundation for this method. Furthermore, we propose AMPN, the first\nActivation-only Mixed-Precision Network pipeline for DiTs. This method applies\nternary weights across the entire network while allocating different activation\nprecisions to each layer to eliminate information bottlenecks. Through\nextensive experiments on unconditional and conditional image generation, our\nRobuQ framework achieves state-of-the-art performance for DiT quantization in\nsub-4-bit quantization configuration. To the best of our knowledge, RobuQ is\nthe first achieving stable and competitive image generation on large datasets\nlike ImageNet-1K with activations quantized to average 2 bits. The code and\nmodels will be available at https://github.com/racoonykc/RobuQ .",
    "published": "2025-09-28T02:35:12Z",
    "link": "http://arxiv.org/pdf/2509.23582v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kaicheng Yang",
      "Xun Zhang",
      "Haotong Qin",
      "Yucheng Lin",
      "Kaisen Yang",
      "Xianglong Yan",
      "Yulun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23572v1",
    "title": "Automated design of compound lenses with discrete-continuous\n  optimization",
    "summary": "We introduce a method that automatically and jointly updates both continuous\nand discrete parameters of a compound lens design, to improve its performance\nin terms of sharpness, speed, or both. Previous methods for compound lens\ndesign use gradient-based optimization to update continuous parameters (e.g.,\ncurvature of individual lens elements) of a given lens topology, requiring\nextensive expert intervention to realize topology changes. By contrast, our\nmethod can additionally optimize discrete parameters such as number and type\n(e.g., singlet or doublet) of lens elements. Our method achieves this\ncapability by combining gradient-based optimization with a tailored Markov\nchain Monte Carlo sampling algorithm, using transdimensional mutation and\nparaxial projection operations for efficient global exploration. We show\nexperimentally on a variety of lens design tasks that our method effectively\nexplores an expanded design space of compound lenses, producing better designs\nthan previous methods and pushing the envelope of speed-sharpness tradeoffs\nachievable by automated lens design.",
    "published": "2025-09-28T02:08:23Z",
    "link": "http://arxiv.org/pdf/2509.23572v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV",
      "physics.app-ph"
    ],
    "authors": [
      "Arjun Teh",
      "Delio Vicini",
      "Bernd Bickel",
      "Ioannis Gkioulekas",
      "Matthew O'Toole"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23566v1",
    "title": "Towards Interpretable Visual Decoding with Attention to Brain\n  Representations",
    "summary": "Recent work has demonstrated that complex visual stimuli can be decoded from\nhuman brain activity using deep generative models, helping brain science\nresearchers interpret how the brain represents real-world scenes. However, most\ncurrent approaches leverage mapping brain signals into intermediate image or\ntext feature spaces before guiding the generative process, masking the effect\nof contributions from different brain areas on the final reconstruction output.\nIn this work, we propose NeuroAdapter, a visual decoding framework that\ndirectly conditions a latent diffusion model on brain representations,\nbypassing the need for intermediate feature spaces. Our method demonstrates\ncompetitive visual reconstruction quality on public fMRI datasets compared to\nprior work, while providing greater transparency into how brain signals shape\nthe generation process. To this end, we contribute an Image-Brain\nBI-directional interpretability framework (IBBI) which investigates\ncross-attention mechanisms across diffusion denoising steps to reveal how\ndifferent cortical areas influence the unfolding generative trajectory. Our\nresults highlight the potential of end-to-end brain-to-image decoding and\nestablish a path toward interpreting diffusion models through the lens of\nvisual neuroscience.",
    "published": "2025-09-28T01:55:55Z",
    "link": "http://arxiv.org/pdf/2509.23566v1.pdf",
    "category": [
      "cs.CV",
      "I.2.0; I.4.9"
    ],
    "authors": [
      "Pinyuan Feng",
      "Hossein Adeli",
      "Wenxuan Guo",
      "Fan Cheng",
      "Ethan Hwang",
      "Nikolaus Kriegeskorte"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23555v1",
    "title": "From Fields to Splats: A Cross-Domain Survey of Real-Time Neural Scene\n  Representations",
    "summary": "Neural scene representations such as Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have transformed how 3D environments are modeled,\nrendered, and interpreted. NeRF introduced view-consistent photorealism via\nvolumetric rendering; 3DGS has rapidly emerged as an explicit, efficient\nalternative that supports high-quality rendering, faster optimization, and\nintegration into hybrid pipelines for enhanced photorealism and task-driven\nscene understanding. This survey examines how 3DGS is being adopted across\nSLAM, telepresence and teleoperation, robotic manipulation, and 3D content\ngeneration. Despite their differences, these domains share common goals:\nphotorealistic rendering, meaningful 3D structure, and accurate downstream\ntasks. We organize the review around unified research questions that explain\nwhy 3DGS is increasingly displacing NeRF-based approaches: What technical\nadvantages drive its adoption? How does it adapt to different input modalities\nand domain-specific constraints? What limitations remain? By systematically\ncomparing domain-specific pipelines, we show that 3DGS balances photorealism,\ngeometric fidelity, and computational efficiency. The survey offers a roadmap\nfor leveraging neural rendering not only for image synthesis but also for\nperception, interaction, and content creation across real and virtual\nenvironments.",
    "published": "2025-09-28T01:30:50Z",
    "link": "http://arxiv.org/pdf/2509.23555v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Javed Ahmad",
      "Penggang Gao",
      "Donatien Delehelle",
      "Mennuti Canio",
      "Nikhil Deshpande",
      "Jesús Ortiz",
      "Darwin G. Caldwell",
      "Yonas Teodros Tefera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23541v1",
    "title": "OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D\n  Reconstruction",
    "summary": "In this paper, we propose a training scheme called OVSeg3R to learn\nopen-vocabulary 3D instance segmentation from well-studied 2D perception models\nwith the aid of 3D reconstruction. OVSeg3R directly adopts reconstructed scenes\nfrom 2D videos as input, avoiding costly manual adjustment while aligning input\nwith real-world applications. By exploiting the 2D to 3D correspondences\nprovided by 3D reconstruction models, OVSeg3R projects each view's 2D instance\nmask predictions, obtained from an open-vocabulary 2D model, onto 3D to\ngenerate annotations for the view's corresponding sub-scene. To avoid\nincorrectly introduced false positives as supervision due to partial\nannotations from 2D to 3D, we propose a View-wise Instance Partition algorithm,\nwhich partitions predictions to their respective views for supervision,\nstabilizing the training process. Furthermore, since 3D reconstruction models\ntend to over-smooth geometric details, clustering reconstructed points into\nrepresentative super-points based solely on geometry, as commonly done in\nmainstream 3D segmentation methods, may overlook geometrically non-salient\nobjects. We therefore introduce 2D Instance Boundary-aware Superpoint, which\nleverages 2D masks to constrain the superpoint clustering, preventing\nsuperpoints from violating instance boundaries. With these designs, OVSeg3R not\nonly extends a state-of-the-art closed-vocabulary 3D instance segmentation\nmodel to open-vocabulary, but also substantially narrows the performance gap\nbetween tail and head classes, ultimately leading to an overall improvement of\n+2.3 mAP on the ScanNet200 benchmark. Furthermore, under the standard\nopen-vocabulary setting, OVSeg3R surpasses previous methods by about +7.1 mAP\non the novel classes, further validating its effectiveness.",
    "published": "2025-09-28T00:41:22Z",
    "link": "http://arxiv.org/pdf/2509.23541v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hongyang Li",
      "Jinyuan Qu",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23535v1",
    "title": "Calibrated and Resource-Aware Super-Resolution for Reliable Driver\n  Behavior Analysis",
    "summary": "Driver monitoring systems require not just high accuracy but reliable,\nwell-calibrated confidence scores for safety-critical deployment. While direct\nlow-resolution training yields high overall accuracy, it produces poorly\ncalibrated predictions that can be dangerous in safety-critical scenarios. We\npropose a resource-aware adaptive super-resolution framework that optimizes for\nmodel calibration and high precision-recall on critical events. Our approach\nachieves state-of-the-art performance on safety-centric metrics: best\ncalibration (ECE of 5.8\\% vs 6.2\\% for LR-trained baselines), highest AUPR for\ndrowsiness detection (0.78 vs 0.74), and superior precision-recall for phone\nuse detection (0.74 vs 0.71). A lightweight artifact detector (0.3M parameters,\n5.2ms overhead) provides additional safety by filtering SR-induced\nhallucinations. While LR-trained video models serve as strong general-purpose\nbaselines, our adaptive framework represents the state-of-the-art solution for\nsafety-critical applications where reliability is paramount.",
    "published": "2025-09-28T00:08:44Z",
    "link": "http://arxiv.org/pdf/2509.23535v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ibne Farabi Shihab",
      "Weiheng Chai",
      "Jiyang Wang",
      "Sanjeda Akter",
      "Senem Velipasalar Gursoy",
      "Anuj Sharma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23492v1",
    "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual\n  Videos",
    "summary": "We present Orientation-anchored Gaussian Splatting (OriGS), a novel framework\nfor high-quality 4D reconstruction from casually captured monocular videos.\nWhile recent advances extend 3D Gaussian Splatting to dynamic scenes via\nvarious motion anchors, such as graph nodes or spline control points, they\noften rely on low-rank assumptions and fall short in modeling complex,\nregion-specific deformations inherent to unconstrained dynamics. OriGS\naddresses this by introducing a hyperdimensional representation grounded in\nscene orientation. We first estimate a Global Orientation Field that propagates\nprincipal forward directions across space and time, serving as stable\nstructural guidance for dynamic modeling. Built upon this, we propose\nOrientation-aware Hyper-Gaussian, a unified formulation that embeds time,\nspace, geometry, and orientation into a coherent probabilistic state. This\nenables inferring region-specific deformation through principled conditioned\nslicing, adaptively capturing diverse local dynamics in alignment with global\nmotion intent. Experiments demonstrate the superior reconstruction fidelity of\nOriGS over mainstream methods in challenging real-world dynamic scenes.",
    "published": "2025-09-27T20:43:43Z",
    "link": "http://arxiv.org/pdf/2509.23492v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junyi Wu",
      "Jiachen Tao",
      "Haoxuan Wang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Yan Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23480v1",
    "title": "RestoRect: Degraded Image Restoration via Latent Rectified Flow &\n  Feature Distillation",
    "summary": "Current approaches for restoration of degraded images face a critical\ntrade-off: high-performance models are too slow for practical use, while fast\nmodels produce poor results. Knowledge distillation transfers teacher knowledge\nto students, but existing static feature matching methods cannot capture how\nmodern transformer architectures dynamically generate features. We propose\n'RestoRect', a novel Latent Rectified Flow Feature Distillation method for\nrestoring degraded images. We apply rectified flow to reformulate feature\ndistillation as a generative process where students learn to synthesize\nteacher-quality features through learnable trajectories in latent space. Our\nframework combines Retinex theory for physics-based decomposition with\nlearnable anisotropic diffusion constraints, and trigonometric color space\npolarization. We introduce a Feature Layer Extraction loss for robust knowledge\ntransfer between different network architectures through cross-normalized\ntransformer feature alignment with percentile-based outlier detection.\nRestoRect achieves better training stability, and faster convergence and\ninference while preserving restoration quality. We demonstrate superior results\nacross 15 image restoration datasets, covering 4 tasks, on 8 metrics.",
    "published": "2025-09-27T20:04:41Z",
    "link": "http://arxiv.org/pdf/2509.23480v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shourya Verma",
      "Mengbo Wang",
      "Nadia Atallah Lanman",
      "Ananth Grama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23475v1",
    "title": "Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling\n  Missing Modalities, Noisy Pseudo-Labels, and Model Degradation",
    "summary": "Recent multi-modal face anti-spoofing (FAS) methods have investigated the\npotential of leveraging multiple modalities to distinguish live and spoof\nfaces. However, pre-adapted multi-modal FAS models often fail to detect unseen\nattacks from new target domains. Although a more realistic domain adaptation\n(DA) scenario has been proposed for single-modal FAS to learn specific spoof\nattacks during inference, DA remains unexplored in multi-modal FAS methods. In\nthis paper, we propose a novel framework, MFAS-DANet, to address three major\nchallenges in multi-modal FAS under the DA scenario: missing modalities, noisy\npseudo labels, and model degradation. First, to tackle the issue of missing\nmodalities, we propose extracting complementary features from other modalities\nto substitute missing modality features or enhance existing ones. Next, to\nreduce the impact of noisy pseudo labels during model adaptation, we propose\nderiving reliable pseudo labels by leveraging prediction uncertainty across\ndifferent modalities. Finally, to prevent model degradation, we design an\nadaptive mechanism that decreases the loss weight during unstable adaptations\nand increasing it during stable ones. Extensive experiments demonstrate the\neffectiveness and state-of-the-art performance of our proposed MFAS-DANet.",
    "published": "2025-09-27T19:52:31Z",
    "link": "http://arxiv.org/pdf/2509.23475v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ming-Tsung Hsu",
      "Fang-Yu Hsu",
      "Yi-Ting Lin",
      "Kai-Heng Chien",
      "Jun-Ren Chen",
      "Cheng-Hsiang Su",
      "Yi-Chen Ou",
      "Chiou-Ting Hsu",
      "Pei-Kai Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23457v1",
    "title": "No Concept Left Behind: Test-Time Optimization for Compositional\n  Text-to-Image Generation",
    "summary": "Despite recent advances in text-to-image (T2I) models, they often fail to\nfaithfully render all elements of complex prompts, frequently omitting or\nmisrepresenting specific objects and attributes. Test-time optimization has\nemerged as a promising approach to address this limitation by refining\ngeneration without the need for retraining. In this paper, we propose a\nfine-grained test-time optimization framework that enhances compositional\nfaithfulness in T2I generation. Unlike most of prior approaches that rely\nsolely on a global image/text similarity score, our method decomposes the input\nprompt into semantic concepts and evaluates alignment at both the global and\nconcept levels. A fine-grained variant of CLIP is used to compute concept-level\ncorrespondence, producing detailed feedback on missing or inaccurate concepts.\nThis feedback is fed into an iterative prompt refinement loop, enabling the\nlarge language model to propose improved prompts. Experiments on DrawBench and\nCompBench prompts demonstrate that our method significantly improves concept\ncoverage and human-judged faithfulness over both standard test-time\noptimization and the base T2I model. Code is available at:\nhttps://github.com/AmirMansurian/NoConceptLeftBehind",
    "published": "2025-09-27T18:59:49Z",
    "link": "http://arxiv.org/pdf/2509.23457v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohammad Hossein Sameti",
      "Amir M. Mansourian",
      "Arash Marioriyad",
      "Soheil Fadaee Oshyani",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23455v1",
    "title": "3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D\n  Kinematic Analysis from Monocular RGB cameras",
    "summary": "Monocular 3D pose estimators produce camera-centered skeletons, creating\nview-dependent kinematic signals that complicate comparative analysis in\napplications such as health and sports science. We present 3DPCNet, a compact,\nestimator-agnostic module that operates directly on 3D joint coordinates to\nrectify any input pose into a consistent, body-centered canonical frame. Its\nhybrid encoder fuses local skeletal features from a graph convolutional network\nwith global context from a transformer via a gated cross-attention mechanism.\nFrom this representation, the model predicts a continuous 6D rotation that is\nmapped to an $SO(3)$ matrix to align the pose. We train the model in a\nself-supervised manner on the MM-Fi dataset using synthetically rotated poses,\nguided by a composite loss ensuring both accurate rotation and pose\nreconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error\nfrom over 20$^{\\circ}$ to 3.4$^{\\circ}$ and the Mean Per Joint Position Error\nfrom ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations\non the TotalCapture dataset further demonstrate that our method produces\nacceleration signals from video that show strong visual correspondence to\nground-truth IMU sensor data, confirming that our module removes viewpoint\nvariability to enable physically plausible motion analysis.",
    "published": "2025-09-27T18:55:21Z",
    "link": "http://arxiv.org/pdf/2509.23455v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Tharindu Ekanayake",
      "Constantino Álvarez Casado",
      "Miguel Bordallo López"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23438v1",
    "title": "FM-SIREN & FM-FINER: Nyquist-Informed Frequency Multiplier for Implicit\n  Neural Representation with Periodic Activation",
    "summary": "Existing periodic activation-based implicit neural representation (INR)\nnetworks, such as SIREN and FINER, suffer from hidden feature redundancy, where\nneurons within a layer capture overlapping frequency components due to the use\nof a fixed frequency multiplier. This redundancy limits the expressive capacity\nof multilayer perceptrons (MLPs). Drawing inspiration from classical signal\nprocessing methods such as the Discrete Sine Transform (DST), we propose\nFM-SIREN and FM-FINER, which assign Nyquist-informed, neuron-specific frequency\nmultipliers to periodic activations. Unlike existing approaches, our design\nintroduces frequency diversity without requiring hyperparameter tuning or\nadditional network depth. This simple yet principled modification reduces the\nredundancy of features by nearly 50% and consistently improves signal\nreconstruction across diverse INR tasks, including fitting 1D audio, 2D image\nand 3D shape, and synthesis of neural radiance fields (NeRF), outperforming\ntheir baseline counterparts while maintaining efficiency.",
    "published": "2025-09-27T18:14:47Z",
    "link": "http://arxiv.org/pdf/2509.23438v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohammed Alsakabi",
      "Wael Mobeirek",
      "John M. Dolan",
      "Ozan K. Tonguz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23852v1",
    "title": "SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation\n  Involving How, When and Where",
    "summary": "The accompanying actions and gestures in dialogue are often closely linked to\ninteractions with the environment, such as looking toward the interlocutor or\nusing gestures to point to the described target at appropriate moments. Speech\nand semantics guide the production of gestures by determining their timing\n(WHEN) and style (HOW), while the spatial locations of interactive objects\ndictate their directional execution (WHERE). Existing approaches either rely\nsolely on descriptive language to generate motions or utilize audio to produce\nnon-interactive gestures, thereby lacking the characterization of interactive\ntiming and spatial intent. This significantly limits the applicability of\nconversational gesture generation, whether in robotics or in the fields of game\nand animation production. To address this gap, we present a full-stack\nsolution. We first established a unique data collection method to\nsimultaneously capture high-precision human motion and spatial intent. We then\ndeveloped a generation model driven by audio, language, and spatial data,\nalongside dedicated metrics for evaluating interaction timing and spatial\naccuracy. Finally, we deployed the solution on a humanoid robot, enabling rich,\ncontext-aware physical interactions.",
    "published": "2025-09-28T12:43:09Z",
    "link": "http://arxiv.org/pdf/2509.23852v1.pdf",
    "category": [
      "cs.GR",
      "cs.MM",
      "cs.RO"
    ],
    "authors": [
      "Yiheng Huang",
      "Junran Peng",
      "Silei Shen",
      "Jingwei Yang",
      "ZeJi Wei",
      "ChenCheng Bai",
      "Yonghao He",
      "Wei Sui",
      "Muyi Sun",
      "Yan Liu",
      "Xu-Cheng Yin",
      "Man Zhang",
      "Zhaoxiang Zhang",
      "Chuanchen Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23489v1",
    "title": "Modeling and Exploiting the Time Course of Chromatic Adaptation for\n  Display Power Optimizations in Virtual Reality",
    "summary": "We introduce a gaze-tracking--free method to reduce OLED display power\nconsumption in VR with minimal perceptual impact. This technique exploits the\ntime course of chromatic adaptation, the human visual system's ability to\nmaintain stable color perception under changing illumination. To that end, we\npropose a novel psychophysical paradigm that models how human adaptation state\nchanges with the scene illuminant. We exploit this model to compute an optimal\nilluminant shift trajectory, controlling the rate and extent of illumination\nchange, to reduce display power under a given perceptual loss budget. Our\ntechnique significantly improves the perceptual quality over prior work that\napplies illumination shifts instantaneously. Our technique can also be combined\nwith prior work on luminance dimming to reduce display power by 31% with no\nstatistical loss of perceptual quality.",
    "published": "2025-09-27T20:30:15Z",
    "link": "http://arxiv.org/pdf/2509.23489v1.pdf",
    "category": [
      "cs.GR",
      "cs.HC",
      "I.3.6"
    ],
    "authors": [
      "Ethan Chen",
      "Sushant Kondguli",
      "Carl Marshall",
      "Yuhao Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23829v1",
    "title": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for\n  Dexterous Manipulation",
    "summary": "Dexterous manipulation is critical for advancing robot capabilities in\nreal-world applications, yet diverse and high-quality datasets remain scarce.\nExisting data collection methods either rely on human teleoperation or require\nsignificant human engineering, or generate data with limited diversity, which\nrestricts their scalability and generalization. In this paper, we introduce\nDexFlyWheel, a scalable data generation framework that employs a self-improving\ncycle to continuously enrich data diversity. Starting from efficient seed\ndemonstrations warmup, DexFlyWheel expands the dataset through iterative\ncycles. Each cycle follows a closed-loop pipeline that integrates Imitation\nLearning (IL), residual Reinforcement Learning (RL), rollout trajectory\ncollection, and data augmentation. Specifically, IL extracts human-like\nbehaviors from demonstrations, and residual RL enhances policy generalization.\nThe learned policy is then used to generate trajectories in simulation, which\nare further augmented across diverse environments and spatial configurations\nbefore being fed back into the next cycle. Over successive iterations, a\nself-improving data flywheel effect emerges, producing datasets that cover\ndiverse scenarios and thereby scaling policy performance. Experimental results\ndemonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across\nfour challenging tasks. Policies trained on our dataset achieve an average\nsuccess rate of 81.9\\% on the challenge test sets and successfully transfer to\nthe real world through digital twin, achieving a 78.3\\% success rate on\ndual-arm lift tasks.",
    "published": "2025-09-28T12:07:02Z",
    "link": "http://arxiv.org/pdf/2509.23829v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kefei Zhu",
      "Fengshuo Bai",
      "YuanHao Xiang",
      "Yishuai Cai",
      "Xinglin Chen",
      "Ruochong Li",
      "Xingtao Wang",
      "Hao Dong",
      "Yaodong Yang",
      "Xiaopeng Fan",
      "Yuanpei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23823v1",
    "title": "Control Your Robot: A Unified System for Robot Control and Policy\n  Deployment",
    "summary": "Cross-platform robot control remains difficult because hardware interfaces,\ndata formats, and control paradigms vary widely, which fragments toolchains and\nslows deployment. To address this, we present Control Your Robot, a modular,\ngeneral-purpose framework that unifies data collection and policy deployment\nacross diverse platforms. The system reduces fragmentation through a\nstandardized workflow with modular design, unified APIs, and a closed-loop\narchitecture. It supports flexible robot registration, dual-mode control with\nteleoperation and trajectory playback, and seamless integration from multimodal\ndata acquisition to inference. Experiments on single-arm and dual-arm systems\nshow efficient, low-latency data collection and effective support for policy\nlearning with imitation learning and vision-language-action models. Policies\ntrained on data gathered by Control Your Robot match expert demonstrations\nclosely, indicating that the framework enables scalable and reproducible robot\nlearning across platforms.",
    "published": "2025-09-28T11:51:29Z",
    "link": "http://arxiv.org/pdf/2509.23823v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Tian Nian",
      "Weijie Ke",
      "Yao Mu",
      "Tianxing Chen",
      "Shaolong Zhu",
      "Bingshan Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23821v1",
    "title": "Fostering Robots: A Governance-First Conceptual Framework for Domestic,\n  Curriculum-Based Trajectory Collection",
    "summary": "We propose a conceptual, empirically testable framework for Robot Fostering,\n-a curriculum-driven, governance-first approach to domestic robot deployments,\nemphasizing long-term, curated interaction trajectories. We formalize\ntrajectory quality with quantifiable metrics and evaluation protocols aligned\nwith EU-grade governance standards, delineating a low-resource empirical\nroadmap to enable rigorous validation through future pilot studies.",
    "published": "2025-09-28T11:50:40Z",
    "link": "http://arxiv.org/pdf/2509.23821v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Federico Pablo-Marti",
      "Carlos Mir Fernandez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23801v1",
    "title": "High-Precision Climbing Robot Localization Using Planar Array\n  UWB/GPS/IMU/Barometer Integration",
    "summary": "To address the need for high-precision localization of climbing robots in\ncomplex high-altitude environments, this paper proposes a multi-sensor fusion\nsystem that overcomes the limitations of single-sensor approaches. Firstly, the\nlocalization scenarios and the problem model are analyzed. An integrated\narchitecture of Attention Mechanism-based Fusion Algorithm (AMFA) incorporating\nplanar array Ultra-Wideband (UWB), GPS, Inertial Measurement Unit (IMU), and\nbarometer is designed to handle challenges such as GPS occlusion and UWB\nNon-Line-of-Sight (NLOS) problem. Then, End-to-end neural network inference\nmodels for UWB and barometer are developed, along with a multimodal attention\nmechanism for adaptive data fusion. An Unscented Kalman Filter (UKF) is applied\nto refine the trajectory, improving accuracy and robustness. Finally,\nreal-world experiments show that the method achieves 0.48 m localization\naccuracy and lower MAX error of 1.50 m, outperforming baseline algorithms such\nas GPS/INS-EKF and demonstrating stronger robustness.",
    "published": "2025-09-28T10:55:23Z",
    "link": "http://arxiv.org/pdf/2509.23801v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shuning Zhang",
      "Renjing Xu",
      "Zhanchen Zhu",
      "Xiangyu Chen",
      "Yunheng Wang",
      "Xu Jiang",
      "Peibo Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23721v1",
    "title": "DA-MMP: Learning Coordinated and Accurate Throwing with Dynamics-Aware\n  Motion Manifold Primitives",
    "summary": "Dynamic manipulation is a key capability for advancing robot performance,\nenabling skills such as tossing. While recent learning-based approaches have\npushed the field forward, most methods still rely on manually designed action\nparameterizations, limiting their ability to produce the highly coordinated\nmotions required in complex tasks. Motion planning can generate feasible\ntrajectories, but the dynamics gap-stemming from control inaccuracies, contact\nuncertainties, and aerodynamic effects-often causes large deviations between\nplanned and executed trajectories. In this work, we propose Dynamics-Aware\nMotion Manifold Primitives (DA-MMP), a motion generation framework for\ngoal-conditioned dynamic manipulation, and instantiate it on a challenging\nreal-world ring-tossing task. Our approach extends motion manifold primitives\nto variable-length trajectories through a compact parametrization and learns a\nhigh-quality manifold from a large-scale dataset of planned motions. Building\non this manifold, a conditional flow matching model is trained in the latent\nspace with a small set of real-world trials, enabling the generation of\nthrowing trajectories that account for execution dynamics. Experiments show\nthat our method can generate coordinated and smooth motion trajectories for the\nring-tossing task. In real-world evaluations, it achieves high success rates\nand even surpasses the performance of trained human experts. Moreover, it\ngeneralizes to novel targets beyond the training range, indicating that it\nsuccessfully learns the underlying trajectory-dynamics mapping.",
    "published": "2025-09-28T08:03:23Z",
    "link": "http://arxiv.org/pdf/2509.23721v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chi Chu",
      "Huazhe Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23705v1",
    "title": "MDCPP: Multi-robot Dynamic Coverage Path Planning for Workload\n  Adaptation",
    "summary": "Multi-robot Coverage Path Planning (MCPP) addresses the problem of computing\npaths for multiple robots to effectively cover a large area of interest.\nConventional approaches to MCPP typically assume that robots move at fixed\nvelocities, which is often unrealistic in real-world applications where robots\nmust adapt their speeds based on the specific coverage tasks assigned to\nthem.Consequently, conventional approaches often lead to imbalanced workload\ndistribution among robots and increased completion time for coverage tasks. To\naddress this, we introduce a novel Multi-robot Dynamic Coverage Path Planning\n(MDCPP) algorithm for complete coverage in two-dimensional environments. MDCPP\ndynamically estimates each robot's remaining workload by approximating the\ntarget distribution with Gaussian mixture models, and assigns coverage regions\nusing a capacity-constrained Voronoi diagram. We further develop a distributed\nimplementation of MDCPP for range-constrained robotic networks. Simulation\nresults validate the efficacy of MDCPP, showing qualitative improvements and\nsuperior performance compared to an existing sweeping algorithm, and a\nquantifiable impact of communication range on coverage efficiency.",
    "published": "2025-09-28T07:34:43Z",
    "link": "http://arxiv.org/pdf/2509.23705v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jun Chen",
      "Mingjia Chen",
      "Shinkyu Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23656v1",
    "title": "Certifiably Optimal State Estimation and Robot Calibration Using\n  Trace-Constrained SDP",
    "summary": "Many nonconvex problems in robotics can be relaxed into convex formulations\nvia semidefinite programming (SDP), which offers the advantage of global\noptimality. The practical quality of these solutions, however, critically\ndepends on achieving rank-1 matrices, a condition that typically requires\nadditional tightening. In this work, we focus on trace-constrained SDPs, where\nthe decision variables are positive semidefinite (PSD) matrices with fixed\ntrace values. These additional constraints not only capture important\nstructural properties but also facilitate first-order methods for recovering\nrank-1 solutions. We introduce customized fixed-trace variables and constraints\nto represent common robotic quantities such as rotations and translations,\nwhich can be exactly recovered when the corresponding variables are rank-1. To\nfurther improve practical performance, we develop a gradient-based refinement\nprocedure that projects relaxed SDP solutions toward rank-1, low-cost\ncandidates, which can then be certified for global optimality via the dual\nproblem. We demonstrate that many robotics tasks can be expressed within this\ntrace-constrained SDP framework, and showcase its effectiveness through\nsimulations in perspective-n-point (PnP) estimation, hand-eye calibration, and\ndual-robot system calibration. To support broader use, we also introduce a\nmodular ``virtual robot'' abstraction that simplifies modeling across different\nproblem settings.",
    "published": "2025-09-28T05:46:35Z",
    "link": "http://arxiv.org/pdf/2509.23656v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Liangting Wu",
      "Roberto Tron"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23651v1",
    "title": "HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod\n  Robot",
    "summary": "Robots in real-world environments are often required to move/manipulate\nobjects comparable in weight to their own bodies. Compared to grasping and\ncarrying, pushing provides a more straightforward and efficient non-prehensile\nmanipulation strategy, avoiding complex grasp design while leveraging direct\ncontact to regulate an object's pose. Achieving effective pushing, however,\ndemands both sufficient manipulation forces and the ability to maintain\nstability, which is particularly challenging when dealing with heavy or\nirregular objects. To address these challenges, we propose HeLoM, a\nlearning-based hierarchical whole-body manipulation framework for a hexapod\nrobot that exploits coordinated multi-limb control. Inspired by the cooperative\nstrategies of multi-legged insects, our framework leverages redundant contact\npoints and high degrees of freedom to enable dynamic redistribution of contact\nforces. HeLoM's high-level planner plans pushing behaviors and target object\nposes, while its low-level controller maintains locomotion stability and\ngenerates dynamically consistent joint actions. Our policies trained in\nsimulation are directly deployed on real robots without additional fine-tuning.\nThis design allows the robot to maintain balance while exerting continuous and\ncontrollable pushing forces through coordinated foreleg interaction and\nsupportive hind-leg propulsion. We validate the effectiveness of HeLoM through\nboth simulation and real-world experiments. Results show that our framework can\nstably push boxes of varying sizes and unknown physical properties to\ndesignated goal poses in the real world.",
    "published": "2025-09-28T05:34:39Z",
    "link": "http://arxiv.org/pdf/2509.23651v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xinrong Yang",
      "Peizhuo Li",
      "Hongyi Li",
      "Junkai Lu",
      "Linnan Chang",
      "Yuhong Cao",
      "Yifeng Zhang",
      "Ge Sun",
      "Guillaume Sartoretti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23650v1",
    "title": "KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe\n  Egocentric Legged Locomotion",
    "summary": "Vision-based locomotion has shown great promise in enabling legged robots to\nperceive and adapt to complex environments. However, visual information is\ninherently fragile, being vulnerable to occlusions, reflections, and lighting\nchanges, which often cause instability in locomotion. Inspired by animal\nsensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial\nintegration framework, where kinesthetics encodes proprioceptive sensing of\nbody motion and visuospatial reasoning captures visual perception of\nsurrounding terrain. Specifically, KiVi separates these pathways, leveraging\nproprioception as a stable backbone while selectively incorporating vision for\nterrain awareness and obstacle avoidance. This modality-balanced, yet\nintegrative design, combined with memory-enhanced attention, allows the robot\nto robustly interpret visual cues while maintaining fallback stability through\nproprioception. Extensive experiments show that our method enables quadruped\nrobots to stably traverse diverse terrains and operate reliably in unstructured\noutdoor environments, remaining robust to out-of-distribution (OOD) visual\nnoise and occlusion unseen during training, thereby highlighting its\neffectiveness and applicability to real-world legged locomotion.",
    "published": "2025-09-28T05:31:07Z",
    "link": "http://arxiv.org/pdf/2509.23650v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Peizhuo Li",
      "Hongyi Li",
      "Yuxuan Ma",
      "Linnan Chang",
      "Xinrong Yang",
      "Ruiqi Yu",
      "Yifeng Zhang",
      "Yuhong Cao",
      "Qiuguo Zhu",
      "Guillaume Sartoretti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23623v1",
    "title": "Encoding Material Safety using Control Barrier Functions for Soft\n  Actuator Control",
    "summary": "Until recently, the concept of soft robot safety was an informal notion,\noften attributed solely to the fact that soft robots are less likely to damage\ntheir operating environment than rigid robots. As the field moves toward\nfeedback control for practical applications, it becomes increasingly important\nto define what safety means and to characterize how soft robots can become\nunsafe. The unifying theme of soft robotics is to achieve useful functionality\nthrough deformation. Consequently, limitations in constitutive model accuracy\nand risks of material failure are inherent to all soft robots and pose a key\nchallenge in designing provably safe controllers. This work introduces a formal\ndefinition of material safety based on strain energy functions and provides a\ncontroller that enforces it. We characterize safe and unsafe sets of an\nincompressible hyperelastic material and demonstrate that safety can be\nenforced using a high-order control barrier function (HOCBF) with quadratic\nprogram-based feedback control. As a case study, we consider a pressurized\nhyperelastic tube with inertial effects, first-order viscous effects, and\nfull-state feedback. Simulation results verify that the proposed methodology\ncan enforce the material safety specification.",
    "published": "2025-09-28T03:57:51Z",
    "link": "http://arxiv.org/pdf/2509.23623v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nicholas Pagliocca",
      "Behrad Koohbor",
      "Mitja Trkov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23575v1",
    "title": "Generalizable Coarse-to-Fine Robot Manipulation via Language-Aligned 3D\n  Keypoints",
    "summary": "Hierarchical coarse-to-fine policy, where a coarse branch predicts a region\nof interest to guide a fine-grained action predictor, has demonstrated\nsignificant potential in robotic 3D manipulation tasks by especially enhancing\nsample efficiency and enabling more precise manipulation. However, even\naugmented with pre-trained models, these hierarchical policies still suffer\nfrom generalization issues. To enhance generalization to novel instructions and\nenvironment variations, we propose Coarse-to-fine Language-Aligned manipulation\nPolicy (CLAP), a framework that integrates three key components: 1) task\ndecomposition, 2) VLM fine-tuning for 3D keypoint prediction, and 3) 3D-aware\nrepresentation. Through comprehensive experiments in simulation and on a real\nrobot, we demonstrate its superior generalization capability. Specifically, on\nGemBench, a benchmark designed for evaluating generalization, our approach\nachieves a 12\\% higher average success rate than the SOTA method while using\nonly 1/5 of the training trajectories. In real-world experiments, our policy,\ntrained on only 10 demonstrations, successfully generalizes to novel\ninstructions and environments.",
    "published": "2025-09-28T02:11:47Z",
    "link": "http://arxiv.org/pdf/2509.23575v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jianshu Hu",
      "Lidi Wang",
      "Shujia Li",
      "Yunpeng Jiang",
      "Xiao Li",
      "Paul Weng",
      "Yutong Ban"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23567v1",
    "title": "GES-UniGrasp: A Two-Stage Dexterous Grasping Strategy With\n  Geometry-Based Expert Selection",
    "summary": "Robust and human-like dexterous grasping of general objects is a critical\ncapability for advancing intelligent robotic manipulation in real-world\nscenarios. However, existing reinforcement learning methods guided by grasp\npriors often result in unnatural behaviors. In this work, we present\n\\textit{ContactGrasp}, a robotic dexterous pre-grasp and grasp dataset that\nexplicitly accounts for task-relevant wrist orientation and thumb-index\npinching coordination. The dataset covers 773 objects in 82 categories,\nproviding a rich foundation for training human-like grasp strategies. Building\nupon this dataset, we perform geometry-based clustering to group objects by\nshape, enabling a two-stage Geometry-based Expert Selection (GES) framework\nthat selects among specialized experts for grasping diverse object geometries,\nthereby enhancing adaptability to diverse shapes and generalization across\ncategories. Our approach demonstrates natural grasp postures and achieves high\nsuccess rates of 99.4\\% and 96.3\\% on the train and test sets, respectively,\nshowcasing strong generalization and high-quality grasp execution.",
    "published": "2025-09-28T01:56:34Z",
    "link": "http://arxiv.org/pdf/2509.23567v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Fangting Xu",
      "Jilin Zhu",
      "Xiaoming Gu",
      "Jianzhong Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23561v1",
    "title": "High Torque Density PCB Axial Flux Permanent Magnet Motor for Micro\n  Robots",
    "summary": "Quasi-direct-drive (QDD) actuation is transforming legged and manipulator\nrobots by eliminating high-ratio gearboxes, yet it demands motors that deliver\nvery high torque at low speed within a thin, disc-shaped joint envelope.\nAxial-flux permanent-magnet (AFPM) machines meet these geometric and torque\nrequirements, but scaling them below a 20mm outer diameter is hampered by poor\ncopper fill in conventional wound stators, inflating resistance and throttling\ncontinuous torque. This paper introduces a micro-scale AFPM motor that\novercomes these limitations through printed-circuit-board (PCB) windings\nfabricated with advanced IC-substrate high-density interconnect (HDI)\ntechnology. The resulting 48-layer stator-formed by stacking four 12-layer HDI\nmodules-achieves a record 45\\% copper fill in a package only 5mm thick and 19mm\nin diameter. We perform comprehensive electromagnetic and thermal analyses to\ninform the motor design, then fabricate a prototype whose performance\ncharacteristics are experimentally verified.",
    "published": "2025-09-28T01:40:22Z",
    "link": "http://arxiv.org/pdf/2509.23561v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jianren Wang",
      "Jie Han",
      "Abhinav Gupta",
      "Deepak Pathak",
      "Yang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23556v1",
    "title": "Zero-shot Whole-Body Manipulation with a Large-Scale Soft Robotic Torso\n  via Guided Reinforcement Learning",
    "summary": "Whole-body manipulation is a powerful yet underexplored approach that enables\nrobots to interact with large, heavy, or awkward objects using more than just\ntheir end-effectors. Soft robots, with their inherent passive compliance, are\nparticularly well-suited for such contact-rich manipulation tasks, but their\nuncertainties in kinematics and dynamics pose significant challenges for\nsimulation and control. In this work, we address this challenge with a\nsimulation that can run up to 350x real time on a single thread in MuJoCo and\nprovide a detailed analysis of the critical tradeoffs between speed and\naccuracy for this simulation. Using this framework, we demonstrate a successful\nzero-shot sim-to-real transfer of a learned whole-body manipulation policy,\nachieving an 88% success rate on the Baloo hardware platform. We show that\nguiding RL with a simple motion primitive is critical to this success where\nstandard reward shaping methods struggled to produce a stable and successful\npolicy for whole-body manipulation. Furthermore, our analysis reveals that the\nlearned policy does not simply mimic the motion primitive. It exhibits\nbeneficial reactive behavior, such as re-grasping and perturbation recovery. We\nanalyze and contrast this learned policy against an open-loop baseline to show\nthat the policy can also exhibit aggressive over-corrections under\nperturbation. To our knowledge, this is the first demonstration of forceful,\nsix-DoF whole-body manipulation using two continuum soft arms on a large-scale\nplatform (10 kg payloads), with zero-shot policy transfer.",
    "published": "2025-09-28T01:32:52Z",
    "link": "http://arxiv.org/pdf/2509.23556v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Curtis C. Johnson",
      "Carlo Alessi",
      "Egidio Falotico",
      "Marc D. Killpack"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23506v1",
    "title": "Ask, Reason, Assist: Decentralized Robot Collaboration via Language and\n  Logic",
    "summary": "Increased robot deployment, such as in warehousing, has revealed a need for\nseamless collaboration among heterogeneous robot teams to resolve unforeseen\nconflicts. To address this challenge, we propose a novel decentralized\nframework that enables robots to request and provide help. The process begins\nwhen a robot detects a conflict and uses a Large Language Model (LLM) to decide\nwhether external assistance is required. If so, it crafts and broadcasts a\nnatural language (NL) help request. Potential helper robots reason over the\nrequest and respond with offers of assistance, including information about the\neffect on their ongoing tasks. Helper reasoning is implemented via an LLM\ngrounded in Signal Temporal Logic (STL) using a Backus-Naur Form (BNF) grammar,\nensuring syntactically valid NL-to-STL translations, which are then solved as a\nMixed Integer Linear Program (MILP). Finally, the requester robot selects a\nhelper by reasoning over the expected increase in system-level total task\ncompletion time. We evaluated our framework through experiments comparing\ndifferent helper-selection strategies and found that considering multiple\noffers allows the requester to minimize added makespan. Our approach\nsignificantly outperforms heuristics such as selecting the nearest available\ncandidate helper robot, and achieves performance comparable to a centralized\n\"Oracle\" baseline but without heavy information demands.",
    "published": "2025-09-27T21:28:08Z",
    "link": "http://arxiv.org/pdf/2509.23506v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Dan BW Choe",
      "Sundhar Vinodh Sangeetha",
      "Steven Emanuel",
      "Chih-Yuan Chiu",
      "Samuel Coogan",
      "Shreyas Kousik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23456v1",
    "title": "Robust Orientation Estimation with TRIAD-aided Manifold EKF",
    "summary": "The manifold extended Kalman filter (Manifold EKF) has found extensive\napplication for attitude determination. Magnetometers employed as sensors for\nsuch attitude determination are easily prone to disturbances by their\nsensitivity to calibration and external magnetic fields. The TRIAD (Tri-Axial\nAttitude Determination) algorithm is well known as a sub-optimal attitude\nestimator. In this article, we incorporate this sub-optimal feature of the\nTRIAD in mitigating the influence of the magnetometer reading in the pitch and\nroll axis determination in the Manifold EKF algorithm. We substantiate our\nresults with experiments.",
    "published": "2025-09-27T18:59:40Z",
    "link": "http://arxiv.org/pdf/2509.23456v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Arjun Sadananda",
      "Ravi Banavar",
      "Kavi Arya"
    ]
  }
]