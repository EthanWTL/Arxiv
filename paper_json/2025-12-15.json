[
  {
    "id": "http://arxiv.org/abs/2512.13690v1",
    "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
    "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
    "published": "2025-12-15T18:59:57Z",
    "updated": "2025-12-15T18:59:57Z",
    "link": "http://arxiv.org/pdf/2512.13690v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "authors": [
      "Susung Hong",
      "Chongjian Ge",
      "Zhifei Zhang",
      "Jui-Hsien Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13678v1",
    "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D",
    "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/",
    "published": "2025-12-15T18:58:55Z",
    "updated": "2025-12-15T18:58:55Z",
    "link": "http://arxiv.org/pdf/2512.13678v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ziqi Ma",
      "Hongqiao Chen",
      "Yisong Yue",
      "Georgia Gkioxari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.08919v2",
    "title": "Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions",
    "summary": "High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.",
    "published": "2025-05-13T19:31:01Z",
    "updated": "2025-12-15T18:56:33Z",
    "link": "http://arxiv.org/pdf/2505.08919v2.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Kangxian Xie",
      "Yufei Zhu",
      "Kaiming Kuang",
      "Li Zhang",
      "Hongwei Bran Li",
      "Mingchen Gao",
      "Jiancheng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13658v1",
    "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
    "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.",
    "published": "2025-12-15T18:51:00Z",
    "updated": "2025-12-15T18:51:00Z",
    "link": "http://arxiv.org/pdf/2512.13658v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Mohammadreza Molavi",
      "Mohammad Moein",
      "Mohammadreza Tavakoli",
      "Abdolali Faraji",
      "Stefan T. Mol",
      "Gábor Kismihók"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13654v1",
    "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases",
    "summary": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.",
    "published": "2025-12-15T18:47:48Z",
    "updated": "2025-12-15T18:47:48Z",
    "link": "http://arxiv.org/pdf/2512.13654v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.IR"
    ],
    "authors": [
      "John E. Ortega",
      "Dhruv D. Joshi",
      "Matt P. Borkowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13644v1",
    "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
    "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.",
    "published": "2025-12-15T18:37:12Z",
    "updated": "2025-12-15T18:37:12Z",
    "link": "http://arxiv.org/pdf/2512.13644v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Raktim Gautam Goswami",
      "Amir Bar",
      "David Fan",
      "Tsung-Yen Yang",
      "Gaoyue Zhou",
      "Prashanth Krishnamurthy",
      "Michael Rabbat",
      "Farshad Khorrami",
      "Yann LeCun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13641v1",
    "title": "From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves",
    "summary": "The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.",
    "published": "2025-12-15T18:36:48Z",
    "updated": "2025-12-15T18:36:48Z",
    "link": "http://arxiv.org/pdf/2512.13641v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Gabriel Vitorino de Andrade",
      "Saulo Roberto dos Santos",
      "Itallo Patrick Castro Alves da Silva",
      "Emanuel Adler Medeiros Pereira",
      "Erick de Andrade Barboza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03979v2",
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "summary": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/Blur-Diffusion-Model/.",
    "published": "2025-12-03T17:10:44Z",
    "updated": "2025-12-15T18:15:43Z",
    "link": "http://arxiv.org/pdf/2512.03979v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jin-Ting He",
      "Fu-Jen Tsai",
      "Yan-Tsung Peng",
      "Min-Hung Chen",
      "Chia-Wen Lin",
      "Yen-Yu Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20130v4",
    "title": "AI Copilots for Reproducibility in Science: A Case Study",
    "summary": "Open science initiatives seek to make research outputs more transparent, accessible, and reusable, but ensuring that published findings can be independently reproduced remains a persistent challenge. In this paper we describe an AI-driven \"Reproducibility Copilot\" that analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or \"rote\", reproducibility. Our initial results suggest that the copilot has the potential to substantially reduce reproduction time (in one case from over 30 hours to about 1 hour) while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing values for hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets. Although preliminary, these findings suggest that AI tools can meaningfully reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication.",
    "published": "2025-06-25T04:56:28Z",
    "updated": "2025-12-15T18:11:39Z",
    "link": "http://arxiv.org/pdf/2506.20130v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Adrien Bibal",
      "Steven N. Minton",
      "Deborah Khider",
      "Yolanda Gil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13607v1",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
    "published": "2025-12-15T18:02:35Z",
    "updated": "2025-12-15T18:02:35Z",
    "link": "http://arxiv.org/pdf/2512.13607v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Boxin Wang",
      "Chankyu Lee",
      "Nayeon Lee",
      "Sheng-Chieh Lin",
      "Wenliang Dai",
      "Yang Chen",
      "Yangyi Chen",
      "Zhuolin Yang",
      "Zihan Liu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22621v2",
    "title": "IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning",
    "summary": "Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICL's internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICL's rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICL's activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and two model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.",
    "published": "2025-09-26T17:46:32Z",
    "updated": "2025-12-15T18:01:10Z",
    "link": "http://arxiv.org/pdf/2509.22621v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Aayush Mishra",
      "Daniel Khashabi",
      "Anqi Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13600v1",
    "title": "DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides",
    "summary": "Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.",
    "published": "2025-12-15T17:53:18Z",
    "updated": "2025-12-15T17:53:18Z",
    "link": "http://arxiv.org/pdf/2512.13600v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haoyue Zhang",
      "Meera Chappidi",
      "Erolcan Sayar",
      "Helen Richards",
      "Zhijun Chen",
      "Lucas Liu",
      "Roxanne Wadia",
      "Peter A Humphrey",
      "Fady Ghali",
      "Alberto Contreras-Sanz",
      "Peter Black",
      "Jonathan Wright",
      "Stephanie Harmon",
      "Michael Haffner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10600v4",
    "title": "Faster Results from a Smarter Schedule: Reframing Collegiate Cross Country through Analysis of the National Running Club Database",
    "summary": "Collegiate cross country teams often build their season schedules on intuition rather than evidence, partly because large-scale performance datasets are not publicly accessible. To address this limitation, we introduce the National Running Club Database (NRCD), the first openly available dataset to aggregate 23,725 race results from 7,594 collegiate club athletes across the 2023-2025 seasons. Unlike existing resources, NRCD includes detailed course metadata, allowing us to develop two standardized performance metrics: Converted Only (distance correction) and Standardized (distance, weather, and elevation adjusted). Using these standardized measures, we find that athletes with slower initial performances exhibit the greatest improvement within a season, and that race frequency is the strongest predictor of improvement. Using six machine learning models, random forest achieves the highest accuracy (r squared equals 0.92), revealing that athletes who race more frequently progress significantly faster than those who do not. At the team level, programs whose athletes race at least four times during the regular season have substantially higher odds of placing in the top 15 at nationals (chi-squared less than 0.01). These results challenge common coaching practices that favor minimal racing before championship meets. Our findings demonstrate that a data-informed scheduling strategy improves both individual development and team competitiveness. The NRCD provides a new foundation for evidence-based decision-making in collegiate cross country and opens opportunities for further research on standardized, longitudinal athlete performance modeling.",
    "published": "2025-09-12T17:50:23Z",
    "updated": "2025-12-15T17:45:12Z",
    "link": "http://arxiv.org/pdf/2509.10600v4.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jonathan A. Karr",
      "Ryan M. Fryer",
      "Ben Darden",
      "Nicholas Pell",
      "Kayla Ambrose",
      "Evan Hall",
      "Ramzi K. Bualuan",
      "Nitesh V. Chawla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13586v1",
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
    "published": "2025-12-15T17:41:19Z",
    "updated": "2025-12-15T17:41:19Z",
    "link": "http://arxiv.org/pdf/2512.13586v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jia-Nan Li",
      "Jian Guan",
      "Wei Wu",
      "Chongxuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13583v1",
    "title": "DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication",
    "summary": "In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}δ \\right)}/(\\sqrt{n}Jε) \\right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\\left(ε, δ\\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.",
    "published": "2025-12-15T17:37:02Z",
    "updated": "2025-12-15T17:37:02Z",
    "link": "http://arxiv.org/pdf/2512.13583v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zehan Zhu",
      "Heng Zhao",
      "Yan Huang",
      "Joey Tianyi Zhou",
      "Shouling Ji",
      "Jinming Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03005v2",
    "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?",
    "summary": "The rapid advancement of large language models (LLMs) has opened new possibilities for AI for good applications. As LLMs increasingly mediate online communication, their potential to foster empathy and constructive dialogue becomes an important frontier for responsible AI research. This work explores whether LLMs can serve not only as moderators that detect harmful content, but as mediators capable of understanding and de-escalating online conflicts. Our framework decomposes mediation into two subtasks: judgment, where an LLM evaluates the fairness and emotional dynamics of a conversation, and steering, where it generates empathetic, de-escalatory messages to guide participants toward resolution. To assess mediation quality, we construct a large Reddit-based dataset and propose a multi-stage evaluation pipeline combining principle-based scoring, user simulation, and human comparison. Experiments show that API-based models outperform open-source counterparts in both reasoning and intervention alignment when doing mediation. Our findings highlight both the promise and limitations of current LLMs as emerging agents for online social mediation.",
    "published": "2025-12-02T18:31:18Z",
    "updated": "2025-12-15T17:31:26Z",
    "link": "http://arxiv.org/pdf/2512.03005v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Dawei Li",
      "Abdullah Alnaibari",
      "Muhammad Arslan",
      "Manny Sandoval",
      "Deborah Hall",
      "Yasin Silva",
      "Huan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13568v1",
    "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
    "summary": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.",
    "published": "2025-12-15T17:25:39Z",
    "updated": "2025-12-15T17:25:39Z",
    "link": "http://arxiv.org/pdf/2512.13568v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Leonard Bereska",
      "Zoe Tzifa-Kratira",
      "Reza Samavi",
      "Efstratios Gavves"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25531v4",
    "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources",
    "summary": "We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance. MixtureVitae follows a permissive-first, risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources). MixtureVitae adopts a simple, single-stage pretraining recipe that integrates a large proportion of permissive synthetic instruction and reasoning data-signals typically introduced during post-training and generally scarce in permissive web corpora. We categorize all sources into a three-tier scheme that reflects varying risk levels and provide shard-level provenance metadata to enable risk-aware usage. In controlled experiments using the open-sci-ref training protocol (fixed architectures and hyperparameters; 50B and 300B token budgets across 130M-1.7B parameters), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B-parameters/300B-tokens setting, they surpass FineWeb-Edu and approach DCLM late in training. Performance is particularly strong on MMLU and on math and code benchmarks: a 1.7B model pretrained on 300B MixtureVitae tokens matches or exceeds a strong 1.7B instruction-tuned baseline on GSM8K, HumanEval, and MBPP, despite using over 36 times fewer tokens (300B vs. ~11T). Supported by a thorough decontamination analysis, these results show that permissive-first data with high instruction and reasoning density, tiered by licensing and provenance-related risk, can provide a practical and risk-mitigated foundation for training capable LLMs, reducing reliance on broad web scrapes without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae",
    "published": "2025-09-29T21:40:10Z",
    "updated": "2025-12-15T17:24:36Z",
    "link": "http://arxiv.org/pdf/2509.25531v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Huu Nguyen",
      "Victor May",
      "Harsh Raj",
      "Marianna Nezhurina",
      "Yishan Wang",
      "Yanqi Luo",
      "Minh Chien Vu",
      "Taishi Nakamura",
      "Ken Tsui",
      "Van Khue Nguyen",
      "David Salinas",
      "Aleksandra Krasnodębska",
      "Christoph Schuhmann",
      "Mats Leon Richter",
      " Xuan-Son",
      " Vu",
      "Jenia Jitsev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01983v2",
    "title": "Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification",
    "summary": "Identification of antimicrobial peptides is an important and necessary issue in today's era. Antimicrobial peptides are essential as an alternative to antibiotics for biomedical applications and many other practical applications. These oligopeptides are useful in drug design and cause innate immunity against microorganisms. Artificial intelligence algorithms have played a significant role in the ease of identifying these peptides.This research is improved by improving proposed method in the field of antimicrobial peptides prediction. Suggested method is improved by combining the best coding method from different perspectives, In the following a deep neural network to balance the imbalanced combined datasets. The results of this research show that the proposed method have a significant improvement in the accuracy and efficiency of the prediction of antimicrobial peptides and are able to provide the best results compared to the existing methods. These development in the field of prediction and classification of antimicrobial peptides, basically in the fields of medicine and pharmaceutical industries, have high effectiveness and application.",
    "published": "2025-05-16T16:11:42Z",
    "updated": "2025-12-15T17:22:43Z",
    "link": "http://arxiv.org/pdf/2506.01983v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Reyhaneh Keshavarzpour",
      "Eghbal Mansoori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13564v1",
    "title": "Memory in the Age of AI Agents",
    "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
    "published": "2025-12-15T17:22:34Z",
    "updated": "2025-12-15T17:22:34Z",
    "link": "http://arxiv.org/pdf/2512.13564v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuyang Hu",
      "Shichun Liu",
      "Yanwei Yue",
      "Guibin Zhang",
      "Boyang Liu",
      "Fangyi Zhu",
      "Jiahang Lin",
      "Honglin Guo",
      "Shihan Dou",
      "Zhiheng Xi",
      "Senjie Jin",
      "Jiejun Tan",
      "Yanbin Yin",
      "Jiongnan Liu",
      "Zeyu Zhang",
      "Zhongxiang Sun",
      "Yutao Zhu",
      "Hao Sun",
      "Boci Peng",
      "Zhenrong Cheng",
      "Xuanbo Fan",
      "Jiaxin Guo",
      "Xinlei Yu",
      "Zhenhong Zhou",
      "Zewen Hu",
      "Jiahao Huo",
      "Junhao Wang",
      "Yuwei Niu",
      "Yu Wang",
      "Zhenfei Yin",
      "Xiaobin Hu",
      "Yue Liao",
      "Qiankun Li",
      "Kun Wang",
      "Wangchunshu Zhou",
      "Yixin Liu",
      "Dawei Cheng",
      "Qi Zhang",
      "Tao Gui",
      "Shirui Pan",
      "Yan Zhang",
      "Philip Torr",
      "Zhicheng Dou",
      "Ji-Rong Wen",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13559v1",
    "title": "Verifying Rumors via Stance-Aware Structural Modeling",
    "summary": "Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.",
    "published": "2025-12-15T17:16:56Z",
    "updated": "2025-12-15T17:16:56Z",
    "link": "http://arxiv.org/pdf/2512.13559v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Gibson Nkhata",
      "Uttamasha Anjally Oyshi",
      "Quan Mai",
      "Susan Gauch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21184v3",
    "title": "Can Language Models Discover Scaling Laws?",
    "summary": "Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate seven diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.",
    "published": "2025-07-27T05:45:26Z",
    "updated": "2025-12-15T17:03:31Z",
    "link": "http://arxiv.org/pdf/2507.21184v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Haowei Lin",
      "Haotian Ye",
      "Wenzheng Feng",
      "Quzhe Huang",
      "Yujun Li",
      "Hubert Lim",
      "Zhengrui Li",
      "Xiangyu Wang",
      "Jianzhu Ma",
      "James Zou",
      "Yitao Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.20179v2",
    "title": "Human-computer interactions predict mental health",
    "summary": "Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode mental health with state-of-the-art biomarker precision. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA to predict 1.3 million mental-health self-reports from 20,000 cursor and touchscreen recordings recorded in 9,000 online participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, identifies individuals living with mental illness, and achieves near-ceiling accuracy when predicting group-level mental health. By extracting non-verbal signatures of psychological function that have so far remained untapped, MAILA represents a key step toward foundation models for mental health. The ability to decode mental states at zero marginal cost creates new opportunities in neuroscience, medicine, and public health, while raising urgent questions about privacy, agency, and autonomy online.",
    "published": "2025-11-25T11:00:39Z",
    "updated": "2025-12-15T16:47:48Z",
    "link": "http://arxiv.org/pdf/2511.20179v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Veith Weilnhammer",
      "Jefferson Ortega",
      "David Whitney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11656v3",
    "title": "MALLM: Multi-Agent Large Language Models Framework",
    "summary": "Multi-agent debate (MAD) has demonstrated the ability to augment collective intelligence by scaling test-time compute and leveraging expertise. Current frameworks for multi-agent debate are often designed towards tool use, lack integrated evaluation, or provide limited configurability of agent personas, response generators, discussion paradigms, and decision protocols. We introduce MALLM (Multi-Agent Large Language Models), an open-source framework that enables systematic analysis of MAD components. MALLM offers more than 144 unique configurations of MAD, including (1) agent personas (e.g., Expert, Personality), (2) response generators (e.g., Critical, Reasoning), (3) discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g., Voting, Consensus). MALLM uses simple configuration files to define a debate. Furthermore, MALLM can load any textual Hugging Face dataset (e.g., MMLU-Pro, WinoGrande) and provides an evaluation pipeline for easy comparison of MAD configurations. MALLM enables researchers to systematically configure, run, and evaluate debates for their problems, facilitating the understanding of the components and their interplay.",
    "published": "2025-09-15T07:48:02Z",
    "updated": "2025-12-15T16:45:40Z",
    "link": "http://arxiv.org/pdf/2509.11656v3.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jonas Becker",
      "Lars Benedikt Kaesberg",
      "Niklas Bauer",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.07665v2",
    "title": "FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing",
    "summary": "Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.",
    "published": "2025-11-10T22:19:37Z",
    "updated": "2025-12-15T16:45:22Z",
    "link": "http://arxiv.org/pdf/2511.07665v2.pdf",
    "category": [
      "cs.AR",
      "cs.AI"
    ],
    "authors": [
      "Yuzhe Fu",
      "Changchun Zhou",
      "Hancheng Ye",
      "Bowen Duan",
      "Qiyu Huang",
      "Chiyue Wei",
      "Cong Guo",
      "Hai \"Helen'' Li",
      "Yiran Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.09810v3",
    "title": "Towards a Common Framework for Autoformalization",
    "summary": "Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.",
    "published": "2025-09-11T19:28:56Z",
    "updated": "2025-12-15T16:43:14Z",
    "link": "http://arxiv.org/pdf/2509.09810v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Agnieszka Mensfelt",
      "David Tena Cucala",
      "Santiago Franco",
      "Angeliki Koutsoukou-Argyraki",
      "Vince Trencsenyi",
      "Kostas Stathis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13510v1",
    "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
    "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
    "published": "2025-12-15T16:38:46Z",
    "updated": "2025-12-15T16:38:46Z",
    "link": "http://arxiv.org/pdf/2512.13510v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Linjie Mu",
      "Yannian Gu",
      "Zhongzhen Huang",
      "Yakun Zhu",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04146v2",
    "title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models",
    "summary": "Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and code generation. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. While these models have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency in next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output tokens in parallel, mitigating the limitations of sequential decoding. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive study of the performance characteristics of ARMs and DLMs, combining theoretical analysis with empirical profiling to characterize the trade-offs between these approaches. We show that although DLMs can achieve higher arithmetic intensity than ARMs by leveraging parallelism across token positions, they fail to scale effectively with longer contexts. We then explore block-wise decoding for DLMs, which decouples arithmetic intensity from sequence length and enables better scaling to long contexts (similar to ARMs). We also examine batched inference and find that ARMs exhibit superior throughput as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, emphasizing that reducing the number of sampling steps is key for open-source DLMs to achieve lower latency relative to ARMs.",
    "published": "2025-10-05T10:50:52Z",
    "updated": "2025-12-15T16:36:36Z",
    "link": "http://arxiv.org/pdf/2510.04146v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Minseo Kim",
      "Coleman Hooper",
      "Aditya Tomar",
      "Chenfeng Xu",
      "Mehrdad Farajtabar",
      "Michael W. Mahoney",
      "Kurt Keutzer",
      "Amir Gholami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13505v1",
    "title": "Defending the Hierarchical Result Models of Precedential Constraint",
    "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.",
    "published": "2025-12-15T16:33:33Z",
    "updated": "2025-12-15T16:33:33Z",
    "link": "http://arxiv.org/pdf/2512.13505v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Henry Prakken",
      "Wijnand van Woerkom"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13501v1",
    "title": "Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS",
    "summary": "Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.\n  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.\n  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.",
    "published": "2025-12-15T16:29:23Z",
    "updated": "2025-12-15T16:29:23Z",
    "link": "http://arxiv.org/pdf/2512.13501v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Sabrine Ennaji",
      "Elhadj Benkhelifa",
      "Luigi Vincenzo Mancini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13494v1",
    "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping",
    "summary": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.",
    "published": "2025-12-15T16:25:55Z",
    "updated": "2025-12-15T16:25:55Z",
    "link": "http://arxiv.org/pdf/2512.13494v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yu-Chen Lu",
      "Sheng-Feng Yu",
      "Hui-Hsien Weng",
      "Pei-Shuo Wang",
      "Yu-Fang Hu",
      "Liang Hung-Chun",
      "Hung-Yueh Chiang",
      "Kai-Chiang Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13481v1",
    "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings",
    "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.",
    "published": "2025-12-15T16:17:12Z",
    "updated": "2025-12-15T16:17:12Z",
    "link": "http://arxiv.org/pdf/2512.13481v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Ojas Pungalia",
      "Rashi Upadhyay",
      "Abhishek Mishra",
      "Abhiram H",
      "Tejasvi Alladi",
      "Sujan Yenuganti",
      "Dhruv Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03682v2",
    "title": "How PARTs assemble into wholes: Learning the relative composition of images",
    "summary": "The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.",
    "published": "2025-06-04T08:12:18Z",
    "updated": "2025-12-15T16:15:57Z",
    "link": "http://arxiv.org/pdf/2506.03682v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Melika Ayoughi",
      "Samira Abnar",
      "Chen Huang",
      "Chris Sandino",
      "Sayeri Lala",
      "Eeshan Gunesh Dhekane",
      "Dan Busbridge",
      "Shuangfei Zhai",
      "Vimal Thilak",
      "Josh Susskind",
      "Pascal Mettes",
      "Paul Groth",
      "Hanlin Goh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13478v1",
    "title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models",
    "summary": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.",
    "published": "2025-12-15T16:14:32Z",
    "updated": "2025-12-15T16:14:32Z",
    "link": "http://arxiv.org/pdf/2512.13478v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kei Saito"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13458v1",
    "title": "SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy",
    "summary": "Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.",
    "published": "2025-12-15T15:56:04Z",
    "updated": "2025-12-15T15:56:04Z",
    "link": "http://arxiv.org/pdf/2512.13458v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Yici Liu",
      "Qi Wei Oung",
      "Hoi Leong Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13438v1",
    "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
    "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
    "published": "2025-12-15T15:34:06Z",
    "updated": "2025-12-15T15:34:06Z",
    "link": "http://arxiv.org/pdf/2512.13438v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Dezhi Ran",
      "Zhi Gong",
      "Yuzhe Guo",
      "Mengzhou Wu",
      "Yuan Cao",
      "Haochuan Lu",
      "Hengyu Zhang",
      "Xia Zeng",
      "Gang Cao",
      "Liangchao Yao",
      "Yuetang Deng",
      "Wei Yang",
      "Tao Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.00319v2",
    "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs",
    "summary": "The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.",
    "published": "2025-11-29T04:47:14Z",
    "updated": "2025-12-15T15:32:09Z",
    "link": "http://arxiv.org/pdf/2512.00319v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Ruike Hu",
      "Shulei Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16634v5",
    "title": "Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations",
    "summary": "Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN",
    "published": "2025-08-16T03:14:07Z",
    "updated": "2025-12-15T15:02:16Z",
    "link": "http://arxiv.org/pdf/2508.16634v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhendong Yang",
      "Jie Wang",
      "Liansong Zong",
      "Xiaorong Liu",
      "Quan Qian",
      "Shiqian Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.20302v5",
    "title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning",
    "summary": "Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many views treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary modifications, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and less explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignment(s) from OM to reduce the number of matching candidates and improve overall OV performance.",
    "published": "2024-09-30T14:00:04Z",
    "updated": "2025-12-15T14:59:48Z",
    "link": "http://arxiv.org/pdf/2409.20302v5.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Zhangcheng Qiang",
      "Kerry Taylor",
      "Weiqing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13402v1",
    "title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery",
    "summary": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.",
    "published": "2025-12-15T14:53:20Z",
    "updated": "2025-12-15T14:53:20Z",
    "link": "http://arxiv.org/pdf/2512.13402v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Lorenzo Pettinari",
      "Sidaty El Hadramy",
      "Michael Wehrli",
      "Philippe C. Cattin",
      "Daniel Studer",
      "Carol C. Hasler",
      "Maria Licci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13399v1",
    "title": "Differentiable Evolutionary Reinforcement Learning",
    "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.",
    "published": "2025-12-15T14:50:08Z",
    "updated": "2025-12-15T14:50:08Z",
    "link": "http://arxiv.org/pdf/2512.13399v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Sitao Cheng",
      "Tianle Li",
      "Xuhan Huang",
      "Xunjian Yin",
      "Difan Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13374v1",
    "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection",
    "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.",
    "published": "2025-12-15T14:28:35Z",
    "updated": "2025-12-15T14:28:35Z",
    "link": "http://arxiv.org/pdf/2512.13374v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Francesca Da Ros",
      "Luca Di Gaspero",
      "Kevin Roitero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13363v1",
    "title": "Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers",
    "summary": "This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.",
    "published": "2025-12-15T14:18:12Z",
    "updated": "2025-12-15T14:18:12Z",
    "link": "http://arxiv.org/pdf/2512.13363v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shibani Sankpal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.12041v3",
    "title": "Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning",
    "summary": "We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning",
    "published": "2025-05-24T08:22:34Z",
    "updated": "2025-12-15T14:12:36Z",
    "link": "http://arxiv.org/pdf/2506.12041v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Yewei Liu",
      "Xiyuan Wang",
      "Muhan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13356v1",
    "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)",
    "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.",
    "published": "2025-12-15T14:10:04Z",
    "updated": "2025-12-15T14:10:04Z",
    "link": "http://arxiv.org/pdf/2512.13356v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zeyad Gamal",
      "Youssef Mahran",
      "Ayman El-Badawy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.15407v2",
    "title": "IPR-1: Interactive Physical Reasoner",
    "summary": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.",
    "published": "2025-11-19T13:04:44Z",
    "updated": "2025-12-15T14:03:42Z",
    "link": "http://arxiv.org/pdf/2511.15407v2.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Mingyu Zhang",
      "Lifeng Zhuo",
      "Tianxi Tan",
      "Guocan Xie",
      "Xian Nie",
      "Yan Li",
      "Renjie Zhao",
      "Zizhu He",
      "Ziyu Wang",
      "Jiting Cai",
      "Yong-Lu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16840v2",
    "title": "In-context Learning of Evolving Data Streams with Tabular Foundational Models",
    "summary": "State-of-the-art data stream mining has long drawn from ensembles of the Very Fast Decision Tree, a seminal algorithm honored with the 2015 KDD Test-of-Time Award. However, the emergence of large tabular models, i.e., transformers designed for structured numerical data, marks a significant paradigm shift. These models move beyond traditional weight updates, instead employing in-context learning through prompt tuning. By using on-the-fly sketches to summarize unbounded streaming data, one can feed this information into a pre-trained model for efficient processing. This work bridges advancements from both areas, highlighting how transformers' implicit meta-learning abilities, pre-training on drifting natural data, and reliance on context optimization directly address the core challenges of adaptive learning in dynamic environments. Exploring real-time model adaptation, this research demonstrates that TabPFN, coupled with a simple sliding memory strategy, consistently outperforms ensembles of Hoeffding trees, such as Adaptive Random Forest, and Streaming Random Patches, across all non-stationary benchmarks.",
    "published": "2025-02-24T04:52:35Z",
    "updated": "2025-12-15T13:59:16Z",
    "link": "http://arxiv.org/pdf/2502.16840v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Afonso Lourenço",
      "João Gama",
      "Eric P. Xing",
      "Goreti Marreiros"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.09907v3",
    "title": "Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models",
    "summary": "Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.",
    "published": "2025-11-13T03:08:51Z",
    "updated": "2025-12-15T13:55:27Z",
    "link": "http://arxiv.org/pdf/2511.09907v3.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Yongxian Wei",
      "Yilin Zhao",
      "Li Shen",
      "Xinrui Chen",
      "Runxi Cheng",
      "Sinan Du",
      "Hao Yu",
      "Gang Liu",
      "Jiahong Yan",
      "Chun Yuan",
      "Dian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13330v1",
    "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
    "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
    "published": "2025-12-15T13:41:41Z",
    "updated": "2025-12-15T13:41:41Z",
    "link": "http://arxiv.org/pdf/2512.13330v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Joona Kytöniemi",
      "Jousia Piha",
      "Akseli Reunamo",
      "Fedor Vitiugin",
      "Farrokh Mehryary",
      "Sampo Pyysalo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13325v1",
    "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
    "summary": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
    "published": "2025-12-15T13:40:00Z",
    "updated": "2025-12-15T13:40:00Z",
    "link": "http://arxiv.org/pdf/2512.13325v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Malte Hellmeier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13323v1",
    "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
    "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
    "published": "2025-12-15T13:39:14Z",
    "updated": "2025-12-15T13:39:14Z",
    "link": "http://arxiv.org/pdf/2512.13323v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Árpád Pándy",
      "Róbert Lakatos",
      "András Hajdu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07874v2",
    "title": "Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction",
    "summary": "Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.",
    "published": "2025-03-10T21:46:57Z",
    "updated": "2025-12-15T13:35:29Z",
    "link": "http://arxiv.org/pdf/2503.07874v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chenyu Zhang",
      "Yihao Luo",
      "Lei Zhu",
      "Martyn G Boutelle",
      "Choon Hwai Yap",
      "Guang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13317v1",
    "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion",
    "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.",
    "published": "2025-12-15T13:35:28Z",
    "updated": "2025-12-15T13:35:28Z",
    "link": "http://arxiv.org/pdf/2512.13317v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mikhail Zakharov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13316v1",
    "title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning",
    "summary": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.\n  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures",
    "published": "2025-12-15T13:35:27Z",
    "updated": "2025-12-15T13:35:27Z",
    "link": "http://arxiv.org/pdf/2512.13316v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mayank Gulati",
      "Benedikt Groß",
      "Gerhard Wunder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.19929v2",
    "title": "DynamiX: Large-Scale Dynamic Social Network Simulator",
    "summary": "Understanding the intrinsic mechanisms of social platforms is an urgent demand to maintain social stability. The rise of large language models provides significant potential for social network simulations to capture attitude dynamics and reproduce collective behaviors. However, existing studies mainly focus on scaling up agent populations, neglecting the dynamic evolution of social relationships. To address this gap, we introduce DynamiX, a novel large-scale social network simulator dedicated to dynamic social network modeling. DynamiX uses a dynamic hierarchy module for selecting core agents with key characteristics at each timestep, enabling accurate alignment of real-world adaptive switching of user roles. Furthermore, we design distinct dynamic social relationship modeling strategies for different user types. For opinion leaders, we propose an information-stream-based link prediction method recommending potential users with similar stances, simulating homogeneous connections, and autonomous behavior decisions. For ordinary users, we construct an inequality-oriented behavior decision-making module, effectively addressing unequal social interactions and capturing the patterns of relationship adjustments driven by multi-dimensional factors. Experimental results demonstrate that DynamiX exhibits marked improvements in attitude evolution simulation and collective behavior analysis compared to static networks. Besides, DynamiX opens a new theoretical perspective on follower growth prediction, providing empirical evidence for opinion leaders cultivation.",
    "published": "2025-07-26T12:13:30Z",
    "updated": "2025-12-15T13:32:12Z",
    "link": "http://arxiv.org/pdf/2507.19929v2.pdf",
    "category": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "authors": [
      "Yanhui Sun",
      "Wu Liu",
      "Wentao Wang",
      "Hantao Yao",
      "Jiebo Luo",
      "Yongdong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10611v2",
    "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
    "summary": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
    "published": "2025-12-11T13:04:44Z",
    "updated": "2025-12-15T13:25:12Z",
    "link": "http://arxiv.org/pdf/2512.10611v2.pdf",
    "category": [
      "cs.AI",
      "cs.NE"
    ],
    "authors": [
      "Minghao LI",
      "Ruihang Wang",
      "Rui Tan",
      "Yonggang Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13300v1",
    "title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction",
    "summary": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.",
    "published": "2025-12-15T13:14:20Z",
    "updated": "2025-12-15T13:14:20Z",
    "link": "http://arxiv.org/pdf/2512.13300v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Qinglin Jia",
      "Zhaocheng Du",
      "Chuhan Wu",
      "Huifeng Guo",
      "Ruiming Tang",
      "Shuting Shi",
      "Muyu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13298v1",
    "title": "MiniLingua: A Small Open-Source LLM for European Languages",
    "summary": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.",
    "published": "2025-12-15T13:12:42Z",
    "updated": "2025-12-15T13:12:42Z",
    "link": "http://arxiv.org/pdf/2512.13298v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Anna Aksenova",
      "Boris Zverkov",
      "Nicola Dainese",
      "Alexander Nikitin",
      "Pekka Marttinen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13297v1",
    "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data",
    "summary": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.",
    "published": "2025-12-15T13:10:42Z",
    "updated": "2025-12-15T13:10:42Z",
    "link": "http://arxiv.org/pdf/2512.13297v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zhenghao Zhu",
      "Chuxue Cao",
      "Sirui Han",
      "Yuanfeng Song",
      "Xing Chen",
      "Caleb Chen Cao",
      "Yike Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18375v3",
    "title": "Progressive Localisation in Localist LLMs",
    "summary": "This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models (LLMs) while preserving performance. Through systematic experimentation with GPT-2 fine-tuned on The Psychology of Artificial Superintelligence, we evaluate five locality configurations: two uniform baselines (fully distributed and fully localist) and three progressive polynomial schedules. We investigate whether interpretability constraints can be aligned with natural semantic structure while being applied strategically across network depth. We demonstrate that progressive semantic localization, combining adaptive semantic block partitioning with steep polynomial locality schedules, achieves near-baseline language modeling performance while providing interpretable attention patterns. Multiple independent training runs with different random seeds establish that results are statistically robust and highly reproducible. The approach dramatically outperforms both fixed-window localization and naive uniform locality constraints. Analysis reveals that maintaining flexibility through low-fidelity constraints preserves model capacity while providing interpretability benefits, and that steep schedules concentrating locality in decision-critical final layers while preserving distributed learning in early layers achieve near-baseline attention distribution characteristics. These findings demonstrate that interpretability mechanisms should align with semantic structure to achieve practical performance-interpretability tradeoffs for trustworthy AI systems.",
    "published": "2025-11-23T09:49:13Z",
    "updated": "2025-12-15T13:10:19Z",
    "link": "http://arxiv.org/pdf/2511.18375v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Joachim Diederich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.03494v3",
    "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
    "summary": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy distribution generate complex time series data crucial for Prognostics and Health Management (PHM). While Deep Learning (DL) methods have demonstrated strong forecasting capabilities, their adoption in industrial CPS remains limited due insufficient robustness. Existing robustness evaluations primarily focus on formal verification or adversarial perturbations, inadequately representing the complexities encountered in real-world CPS scenarios. To address this, we introduce a practical robustness definition grounded in distributional robustness, explicitly tailored to industrial CPS, and propose a systematic framework for robustness evaluation. Our framework simulates realistic disturbances, such as sensor drift, noise and irregular sampling, enabling thorough robustness analyses of forecasting models on real-world CPS datasets. The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets, assisting in informed model selection and architecture design. Through extensive empirical studies evaluating prominent DL architectures (including recurrent, convolutional, attention-based, modular, and structured state-space models) we demonstrate the applicability and effectiveness of our approach. We publicly release our robustness benchmark to encourage further research and reproducibility.",
    "published": "2025-04-04T14:50:48Z",
    "updated": "2025-12-15T13:07:25Z",
    "link": "http://arxiv.org/pdf/2504.03494v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alexander Windmann",
      "Henrik Steude",
      "Daniel Boschmann",
      "Oliver Niggemann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13293v1",
    "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
    "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",
    "published": "2025-12-15T13:03:08Z",
    "updated": "2025-12-15T13:03:08Z",
    "link": "http://arxiv.org/pdf/2512.13293v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Hao Fua",
      "Wei Liu",
      "Shuai Zhoua"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13290v1",
    "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models",
    "summary": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.",
    "published": "2025-12-15T12:59:59Z",
    "updated": "2025-12-15T12:59:59Z",
    "link": "http://arxiv.org/pdf/2512.13290v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shu Yu",
      "Chaochao Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19609v2",
    "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling",
    "summary": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.",
    "published": "2025-05-26T07:22:39Z",
    "updated": "2025-12-15T12:58:53Z",
    "link": "http://arxiv.org/pdf/2505.19609v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hongtao Xu",
      "Wenting Shen",
      "Yuanxin Wei",
      "Ang Wang",
      "Guo Runfan",
      "Tianxing Wang",
      "Yong Li",
      "Mingzhen Li",
      "Weile Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15811v2",
    "title": "From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System",
    "summary": "Generative query suggestion using large language models offers a powerful way to enhance conversational systems, but aligning outputs with nuanced user preferences remains a critical challenge. To address this, we introduce a multi-stage framework designed for progressive alignment between the generation policy and user intent. Our pipeline begins with prompt engineering as a cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we introduce a distillation method on click logs to create a robust foundational model. To better model user preferences while capturing their inherent uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user preferences as probability distributions rather than point estimates. Finally, we employ reinforcement learning to align the generation policy with these preferences, guided by a composite reward function that integrates GaRM with auxiliary heuristics to mitigate reward hacking. To maintain training stability, this process is enhanced by a novel out-of-distribution regularization method and a two-stage reward fusion technique. Extensive experiments demonstrate that our framework significantly outperforms baselines on both automatic and human evaluations and yields a 34\\% relative increase in user engagement as measured by click-through rate in live A/B tests.",
    "published": "2025-08-15T10:17:01Z",
    "updated": "2025-12-15T12:51:55Z",
    "link": "http://arxiv.org/pdf/2508.15811v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Junhao Yin",
      "Haolin Wang",
      "Peng Bao",
      "Ju Xu",
      "Yongliang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.06069v2",
    "title": "Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations",
    "summary": "Deep Gaussian process models typically employ discrete hierarchies, but recent advancements in differential Gaussian processes (DiffGPs) have extended these models to infinite depths. However, existing DiffGP approaches often overlook the uncertainty in kernel hyperparameters by treating them as fixed and time-invariant, which degrades the model's predictive performance and neglects the posterior distribution. In this work, we introduce a fully Bayesian framework that models kernel hyperparameters as random variables and utilizes coupled stochastic differential equations (SDEs) to jointly learn their posterior distributions alongside those of inducing points. By incorporating the estimation uncertainty of hyperparameters, our method significantly enhances model flexibility and adaptability to complex dynamic systems. Furthermore, we employ a black-box adaptive SDE solver with a neural network to achieve realistic, time varying posterior approximations, thereby improving the expressiveness of the variational posterior. Comprehensive experimental evaluations demonstrate that our approach outperforms traditional methods in terms of flexibility, accuracy, and other key performance metrics. This work not only provides a robust Bayesian extension to DiffGP models but also validates its effectiveness in handling intricate dynamic behaviors, thereby advancing the applicability of Gaussian process models in diverse real-world scenarios.",
    "published": "2024-08-12T11:41:07Z",
    "updated": "2025-12-15T12:27:16Z",
    "link": "http://arxiv.org/pdf/2408.06069v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jian Xu",
      "Zhiqi Lin",
      "Min Chen",
      "Junmei Yang",
      "Delu Zeng",
      "John Paisley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10019v2",
    "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-Scale Model Reasoning",
    "summary": "Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., up to 1.5B parameters) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.",
    "published": "2025-08-07T01:13:30Z",
    "updated": "2025-12-15T12:12:19Z",
    "link": "http://arxiv.org/pdf/2508.10019v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Li Wang",
      "Changhao Zhang",
      "Zengqi Xiu",
      "Kai Lu",
      "Xin Yu",
      "Kui Zhang",
      "Wenjun Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13240v1",
    "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection",
    "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.",
    "published": "2025-12-15T11:55:55Z",
    "updated": "2025-12-15T11:55:55Z",
    "link": "http://arxiv.org/pdf/2512.13240v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zihui Zhao",
      "Zechang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.14507v3",
    "title": "From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion",
    "summary": "Structured pruning of Generative Pre-trained Transformers (GPTs) offers a promising path to efficiency but often suffers from irreversible performance degradation due to the discarding of transformer blocks. In this paper, we introduce FuseGPT, a compression paradigm that reframes structured pruning as iterative knowledge grafting rather than simple removal. Motivated by the observation that linear block merging fails to capture non-linear feature disparities and that block importance fluctuates dynamically during pruning, FuseGPT employs a dual-strategy pipeline. First, we propose Macro Influence (MI), a dynamic fusion-aware metric that continuously re-evaluates block redundancy as the network topology evolves. Second, instead of rigid parameter averaging, we introduce a learnable low-rank fusion mechanism that adaptively grafts the knowledge of pruned blocks onto surviving layers via lightweight local distillation. Extensive experiments on LLaMA, Mistral, Qwen, and Phi families demonstrate that FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier: at 25\\% sparsity, FuseGPT achieves lower perplexity than prior methods at 20\\% sparsity, improves zero-shot reasoning by up to 4.5 points, and delivers 1.33$\\times$ inference speedup with 25\\% memory reduction. Furthermore, FuseGPT is orthogonal to quantization, achieving 52.1\\% total compression with negligible quality loss when combined with 4-bit GPTQ. We make our code publicly available at https://github.com/JarvisPei/FuseGPT.",
    "published": "2024-11-21T09:49:28Z",
    "updated": "2025-12-15T11:49:40Z",
    "link": "http://arxiv.org/pdf/2411.14507v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zehua Pei",
      "Hui-Ling Zhen",
      "Xianzhi Yu",
      "Sinno Jialin Pan",
      "Mingxuan Yuan",
      "Bei Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13235v1",
    "title": "CORE: Contrastive Masked Feature Reconstruction on Graphs",
    "summary": "In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.",
    "published": "2025-12-15T11:48:48Z",
    "updated": "2025-12-15T11:48:48Z",
    "link": "http://arxiv.org/pdf/2512.13235v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jianyuan Bo",
      "Yuan Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03584v2",
    "title": "Decoding and Engineering the Phytobiome Communication for Smart Agriculture",
    "summary": "Smart agriculture applications, integrating technologies like the Internet of Things and machine learning/artificial intelligence (ML/AI) into agriculture, hold promise to address modern challenges of rising food demand, environmental pollution, and water scarcity. Alongside the concept of the phytobiome, which defines the area including the plant, its environment, and associated organisms, and the recent emergence of molecular communication (MC), there exists an important opportunity to advance agricultural science and practice using communication theory. In this article, we motivate to use the communication engineering perspective for developing a holistic understanding of the phytobiome communication and bridge the gap between the phytobiome communication and smart agriculture. Firstly, an overview of phytobiome communication via molecular and electrophysiological signals is presented and a multi-scale framework modeling the phytobiome as a communication network is conceptualized. Then, how this framework is used to model electrophysiological signals is demonstrated with plant experiments. Furthermore, possible smart agriculture applications, such as smart irrigation and targeted delivery of agrochemicals, through engineering the phytobiome communication are proposed. These applications merge ML/AI methods with the Internet of Bio-Nano-Things enabled by MC and pave the way towards more efficient, sustainable, and eco-friendly agricultural production. Finally, the implementation challenges, open research issues, and industrial outlook for these applications are discussed.",
    "published": "2025-08-05T15:50:19Z",
    "updated": "2025-12-15T11:34:13Z",
    "link": "http://arxiv.org/pdf/2508.03584v2.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "cs.ET",
      "cs.NI",
      "q-bio.MN"
    ],
    "authors": [
      "Fatih Gulec",
      "Hamdan Awan",
      "Nigel Wallbridge",
      "Andrew W. Eckford"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05019v2",
    "title": "Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints",
    "summary": "The generation of synthetic clinical trial data offers a promising approach to mitigating privacy concerns and data accessibility limitations in medical research. However, ensuring that synthetic datasets maintain high fidelity, utility, and adherence to domain-specific constraints remains a key challenge. While hyperparameter optimization (HPO) improves generative model performance, the effectiveness of different optimization strategies for synthetic clinical data remains unclear. This study systematically evaluates four HPO objectives across nine generative models, comparing single-metric to compound metric optimization. Our results demonstrate that HPO consistently improves synthetic data quality, with Tab DDPM achieving the largest relative gains, followed by TVAE (60%), CTGAN (39%), and CTAB-GAN+ (38%). Compound metric optimization outperformed single-metric objectives, producing more generalizable synthetic datasets. Despite improving overall quality, HPO alone fails to prevent violations of essential clinical survival constraints. Preprocessing and postprocessing played a crucial role in reducing these violations, as models lacking robust processing steps produced invalid data in up to 61% of cases. These findings underscore the necessity of integrating explicit domain knowledge alongside HPO to generate high-quality synthetic datasets. Our study provides actionable recommendations for improving synthetic data generation, with future work needed to refine metric selection and validate findings on larger datasets.",
    "published": "2025-05-08T07:51:36Z",
    "updated": "2025-12-15T11:25:28Z",
    "link": "http://arxiv.org/pdf/2505.05019v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Waldemar Hahn",
      "Jan-Niklas Eckardt",
      "Christoph Röllig",
      "Martin Sedlmayr",
      "Jan Moritz Middeke",
      "Markus Wolfien"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.00858v4",
    "title": "Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach",
    "summary": "In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power of deep networks. We generate a 3D vision-and-language synthetic dataset of tabletop scenes in a simulation environment to train our approach and perform extensive evaluations in both synthetic and real-world scenes. Results showcase the benefits of our approach in terms of accuracy, sample-efficiency, and robustness to the user's vocabulary, while being transferable to real-world scenes with few-shot visual fine-tuning. Finally, we integrate our method with a robot framework and demonstrate how it can serve as an interpretable solution for an interactive object-picking task, achieving an average success rate of 80.2\\%, both in simulation and with a real robot. We make supplementary material available in https://gtziafas.github.io/neurosymbolic-manipulation.",
    "published": "2022-10-03T12:21:45Z",
    "updated": "2025-12-15T11:12:03Z",
    "link": "http://arxiv.org/pdf/2210.00858v4.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Georgios Tziafas",
      "Hamidreza Kasaei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13194v1",
    "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models",
    "summary": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \\(1 - \\max(P_{\\mathrm{target}})\\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.",
    "published": "2025-12-15T11:08:56Z",
    "updated": "2025-12-15T11:08:56Z",
    "link": "http://arxiv.org/pdf/2512.13194v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chendong Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13190v1",
    "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
    "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
    "published": "2025-12-15T10:55:20Z",
    "updated": "2025-12-15T10:55:20Z",
    "link": "http://arxiv.org/pdf/2512.13190v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jin Sob Kim",
      "Hyun Joon Park",
      "Wooseok Shin",
      "Dongil Park",
      "Sung Won Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13186v1",
    "title": "PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning",
    "summary": "Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.",
    "published": "2025-12-15T10:50:48Z",
    "updated": "2025-12-15T10:50:48Z",
    "link": "http://arxiv.org/pdf/2512.13186v1.pdf",
    "category": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "authors": [
      "Khalid Ferji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.06268v3",
    "title": "A Collectivist, Economic Perspective on AI",
    "summary": "Information technology is in the midst of a revolution in which omnipresent data collection and machine learning are impacting the human world as never before. The word ``intelligence'' is being used as a North Star for the development of this technology, with human cognition viewed as a baseline. This view neglects the fact that humans are social animals and that much of our intelligence is social and cultural in origin. Moreover, failing to properly situate aspects of intelligence at the social level contributes to the treatment of the societal consequences of technology as an afterthought. The path forward is not merely more data and compute, and not merely more attention paid to cognitive or symbolic representations, but a thorough blending of economic and social concepts with computational and inferential concepts at the level of algorithm design.",
    "published": "2025-07-08T03:07:43Z",
    "updated": "2025-12-15T10:39:01Z",
    "link": "http://arxiv.org/pdf/2507.06268v3.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Michael I. Jordan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13174v1",
    "title": "Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments",
    "summary": "Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.",
    "published": "2025-12-15T10:35:44Z",
    "updated": "2025-12-15T10:35:44Z",
    "link": "http://arxiv.org/pdf/2512.13174v1.pdf",
    "category": [
      "econ.GN",
      "cs.AI"
    ],
    "authors": [
      "Francesco Salvi",
      "Giuseppe Russo",
      "Adam Barla",
      "Vincent Moreau",
      "Robert West"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13168v1",
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
    "published": "2025-12-15T10:28:45Z",
    "updated": "2025-12-15T10:28:45Z",
    "link": "http://arxiv.org/pdf/2512.13168v1.pdf",
    "category": [
      "cs.AI",
      "cs.CE",
      "cs.IR",
      "cs.MA"
    ],
    "authors": [
      "Haoyu Dong",
      "Pengkun Zhang",
      "Yan Gao",
      "Xuanyu Dong",
      "Yilin Cheng",
      "Mingzhe Lu",
      "Adina Yakefu",
      "Shuxin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13165v1",
    "title": "SACn: Soft Actor-Critic with n-step Returns",
    "summary": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.",
    "published": "2025-12-15T10:23:13Z",
    "updated": "2025-12-15T10:23:13Z",
    "link": "http://arxiv.org/pdf/2512.13165v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jakub Łyskawa",
      "Jakub Lewandowski",
      "Paweł Wawrzyński"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13164v1",
    "title": "A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis",
    "summary": "The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.",
    "published": "2025-12-15T10:22:43Z",
    "updated": "2025-12-15T10:22:43Z",
    "link": "http://arxiv.org/pdf/2512.13164v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xianchao Guan",
      "Zhiyuan Fan",
      "Yifeng Wang",
      "Fuqiang Chen",
      "Yanjiang Zhou",
      "Zengyang Che",
      "Hongxue Meng",
      "Xin Li",
      "Yaowei Wang",
      "Hongpeng Wang",
      "Min Zhang",
      "Heng Tao Shen",
      "Zheng Zhang",
      "Yongbing Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07084v3",
    "title": "HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting",
    "summary": "Transformer-based methods have achieved impressive results in time series forecasting. However, existing Transformers still exhibit limitations in sequence modeling as they tend to overemphasize temporal dependencies. This incurs additional computational overhead without yielding corresponding performance gains. We find that the performance of Transformers is highly dependent on the embedding method used to learn effective representations. To address this issue, we extract multivariate features to augment the effective information captured in the embedding layer, yielding multidimensional embeddings that convey richer and more meaningful sequence representations. These representations enable Transformer-based forecasters to better understand the series. Specifically, we introduce Hybrid Temporal and Multivariate Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature extraction module with a carefully designed multivariate feature extraction module to provide complementary features, thereby achieving a balance between model complexity and performance. By combining HTME with the Transformer architecture, we present HTMformer, leveraging the enhanced feature extraction capability of the HTME extractor to build a lightweight forecaster. Experiments conducted on eight real-world datasets demonstrate that our approach outperforms existing baselines in both accuracy and efficiency.",
    "published": "2025-10-08T14:40:42Z",
    "updated": "2025-12-15T10:16:46Z",
    "link": "http://arxiv.org/pdf/2510.07084v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Tan Wang",
      "Yun Wei Dong",
      "Qi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13159v1",
    "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning",
    "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.",
    "published": "2025-12-15T10:08:53Z",
    "updated": "2025-12-15T10:08:53Z",
    "link": "http://arxiv.org/pdf/2512.13159v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Emre Can Acikgoz",
      "Jinoh Oh",
      "Jie Hao",
      "Joo Hyuk Jeon",
      "Heng Ji",
      "Dilek Hakkani-Tür",
      "Gokhan Tur",
      "Xiang Li",
      "Chengyuan Ma",
      "Xing Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06522v3",
    "title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance",
    "summary": "Recent work on large language models (LLMs) has increasingly focused on post-training and alignment with datasets curated to enhance instruction following, world knowledge, and specialized skills. However, most post-training datasets used in leading open- and closed-source LLMs remain inaccessible to the public, with limited information about their construction process. This lack of transparency has motivated the recent development of open-source post-training corpora. While training on these open alternatives can yield performance comparable to that of leading models, systematic comparisons remain challenging due to the significant computational cost of conducting them rigorously at scale, and are therefore largely absent. As a result, it remains unclear how specific samples, task types, or curation strategies influence downstream performance when assessing data quality. In this work, we conduct the first comprehensive side-by-side analysis of two prominent open post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie framework, we annotate each sample with detailed quality metrics, including turn structure (single-turn vs. multi-turn), task category, input quality, and response quality, and we derive statistics that reveal structural and qualitative similarities and differences between the two datasets. Based on these insights, we design a principled curation recipe that produces a new data mixture, TuluTalk, which contains 14% fewer samples than either source dataset while matching or exceeding their performance on key benchmarks. Our findings offer actionable insights for constructing more effective post-training datasets that improve model performance within practical resource limits. To support future research, we publicly release both the annotated source datasets and our curated TuluTalk mixture.",
    "published": "2025-06-06T20:34:06Z",
    "updated": "2025-12-15T10:08:14Z",
    "link": "http://arxiv.org/pdf/2506.06522v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Aladin Djuhera",
      "Swanand Ravindra Kadhe",
      "Syed Zawad",
      "Farhan Ahmed",
      "Heiko Ludwig",
      "Holger Boche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13157v1",
    "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction",
    "summary": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.",
    "published": "2025-12-15T10:05:59Z",
    "updated": "2025-12-15T10:05:59Z",
    "link": "http://arxiv.org/pdf/2512.13157v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Peter Kocsis",
      "Lukas Höllein",
      "Matthias Nießner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13154v1",
    "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations",
    "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.",
    "published": "2025-12-15T10:02:50Z",
    "updated": "2025-12-15T10:02:50Z",
    "link": "http://arxiv.org/pdf/2512.13154v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Emre Can Acikgoz",
      "Jinoh Oh",
      "Joo Hyuk Jeon",
      "Jie Hao",
      "Heng Ji",
      "Dilek Hakkani-Tür",
      "Gokhan Tur",
      "Xiang Li",
      "Chengyuan Ma",
      "Xing Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13142v1",
    "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels",
    "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.",
    "published": "2025-12-15T09:50:00Z",
    "updated": "2025-12-15T09:50:00Z",
    "link": "http://arxiv.org/pdf/2512.13142v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Anika Sharma",
      "Malavika Mampally",
      "Chidaksh Ravuru",
      "Kandyce Brennan",
      "Neil Gaikwad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13131v1",
    "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning",
    "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.",
    "published": "2025-12-15T09:43:08Z",
    "updated": "2025-12-15T09:43:08Z",
    "link": "http://arxiv.org/pdf/2512.13131v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.MM",
      "cs.SD"
    ],
    "authors": [
      "Xin Guo",
      "Yifan Zhao",
      "Jia Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21720v2",
    "title": "A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue",
    "summary": "The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted framework designed to bridge the gap between isolated predictive modeling and an interactive system for psychological analysis. The methodology encompasses a rigorous, end-to-end development lifecycle. First, foundational performance benchmarks were established on four diverse psychological datasets using classical machine learning techniques. Second, state-of-the-art transformer models were fine-tuned, a process that necessitated the development of effective solutions to overcome critical engineering challenges, including the resolution of numerical instability in regression tasks and the creation of a systematic workflow for conducting large-scale training under severe resource constraints. Third, a generative large language model (LLM) was fine-tuned using parameter-efficient techniques to function as an interactive \"Personality Brain.\" Finally, the entire suite of predictive and generative models was architected and deployed as a robust, scalable microservices ecosystem. Key findings include the successful stabilization of transformer-based regression models for affective computing, showing meaningful predictive performance where standard approaches failed, and the development of a replicable methodology for democratizing large-scale AI research. The significance of this work lies in its holistic approach, demonstrating a complete research-to-deployment pipeline that integrates predictive analysis with generative dialogue, thereby providing a practical model for future research in computational psychology and human-AI interaction.",
    "published": "2025-09-16T13:33:40Z",
    "updated": "2025-12-15T09:34:04Z",
    "link": "http://arxiv.org/pdf/2510.21720v2.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Anant Pareek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13122v1",
    "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass",
    "summary": "Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R",
    "published": "2025-12-15T09:21:28Z",
    "updated": "2025-12-15T09:21:28Z",
    "link": "http://arxiv.org/pdf/2512.13122v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Vivek Alumootil",
      "Tuan-Anh Vu",
      "M. Khalid Jawed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13111v1",
    "title": "From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network",
    "summary": "In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.",
    "published": "2025-12-15T09:08:42Z",
    "updated": "2025-12-15T09:08:42Z",
    "link": "http://arxiv.org/pdf/2512.13111v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hayk Amirkhanian",
      "Marco F. Huber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13109v1",
    "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
    "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
    "published": "2025-12-15T09:04:06Z",
    "updated": "2025-12-15T09:04:06Z",
    "link": "http://arxiv.org/pdf/2512.13109v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zewen Qiang",
      "Sendong Zhao",
      "Haochun Wang",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13107v1",
    "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather",
    "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.",
    "published": "2025-12-15T09:03:46Z",
    "updated": "2025-12-15T09:03:46Z",
    "link": "http://arxiv.org/pdf/2512.13107v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhijian He",
      "Feifei Liu",
      "Yuwei Li",
      "Zhanpeng Liu",
      "Jintao Cheng",
      "Xieyuanli Chen",
      "Xiaoyu Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13106v1",
    "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.",
    "published": "2025-12-15T09:03:45Z",
    "updated": "2025-12-15T09:03:45Z",
    "link": "http://arxiv.org/pdf/2512.13106v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Shenzhi Yang",
      "Guangcheng Zhu",
      "Xing Zheng",
      "Yingfan MA",
      "Zhongqi Chen",
      "Bowen Song",
      "Weiqiang Wang",
      "Junbo Zhao",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13102v1",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "published": "2025-12-15T08:59:19Z",
    "updated": "2025-12-15T08:59:19Z",
    "link": "http://arxiv.org/pdf/2512.13102v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Rajeev Bhatt Ambati",
      "Tianyi Niu",
      "Aashu Singh",
      "Shlok Mishra",
      "Shashank Srivastava",
      "Snigdha Chaturvedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13101v1",
    "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation",
    "summary": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.",
    "published": "2025-12-15T08:57:49Z",
    "updated": "2025-12-15T08:57:49Z",
    "link": "http://arxiv.org/pdf/2512.13101v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wenjing Lu",
      "Yi Hong",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13100v1",
    "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning",
    "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.",
    "published": "2025-12-15T08:57:15Z",
    "updated": "2025-12-15T08:57:15Z",
    "link": "http://arxiv.org/pdf/2512.13100v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Guanhua Ji",
      "Harsha Polavaram",
      "Lawrence Yunliang Chen",
      "Sandeep Bajamahal",
      "Zehan Ma",
      "Simeon Adebola",
      "Chenfeng Xu",
      "Ken Goldberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13094v1",
    "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation",
    "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.",
    "published": "2025-12-15T08:50:23Z",
    "updated": "2025-12-15T08:50:23Z",
    "link": "http://arxiv.org/pdf/2512.13094v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Xiang Li",
      "Gang Liu",
      "Weitao Zhou",
      "Hongyi Zhu",
      "Zhong Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13089v1",
    "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era",
    "summary": "Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.",
    "published": "2025-12-15T08:42:23Z",
    "updated": "2025-12-15T08:42:23Z",
    "link": "http://arxiv.org/pdf/2512.13089v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ziqiang Zhu",
      "Bowei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.01054v2",
    "title": "Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs",
    "summary": "Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.\n  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.\n  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.\n  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.",
    "published": "2025-11-30T19:57:49Z",
    "updated": "2025-12-15T08:37:27Z",
    "link": "http://arxiv.org/pdf/2512.01054v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "MohammadParsa Dini",
      "Human Jafari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13074v1",
    "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval",
    "summary": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.\n  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.",
    "published": "2025-12-15T08:11:24Z",
    "updated": "2025-12-15T08:11:24Z",
    "link": "http://arxiv.org/pdf/2512.13074v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Huimu Wang",
      "Yiming Qiu",
      "Xingzhi Yao",
      "Zhiguo Chen",
      "Guoyu Tang",
      "Songlin Wang",
      "Sulong Xu",
      "Mingming Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06256v2",
    "title": "SpectrumFM: A Foundation Model for Intelligent Spectrum Management",
    "summary": "Intelligent spectrum management is crucial for improving spectrum efficiency and achieving secure utilization of spectrum resources. However, existing intelligent spectrum management methods, typically based on small-scale models, suffer from notable limitations in recognition accuracy, convergence speed, and generalization, particularly in the complex and dynamic spectrum environments. To address these challenges, this paper proposes a novel spectrum foundation model, termed SpectrumFM, establishing a new paradigm for spectrum management. SpectrumFM features an innovative encoder architecture that synergistically exploits the convolutional neural networks and the multi-head self-attention mechanisms to enhance feature extraction and enable robust representation learning. The model is pre-trained via two novel self-supervised learning tasks, namely masked reconstruction and next-slot signal prediction, which leverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive and transferable spectrum representations. Furthermore, a parameter-efficient fine-tuning strategy is proposed to enable SpectrumFM to adapt to various downstream spectrum management tasks, including automatic modulation classification (AMC), wireless technology classification (WTC), spectrum sensing (SS), and anomaly detection (AD). Extensive experiments demonstrate that SpectrumFM achieves superior performance in terms of accuracy, robustness, adaptability, few-shot learning efficiency, and convergence speed, consistently outperforming conventional methods across multiple benchmarks. Specifically, SpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%, achieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise ratio (SNR), and enhances AD performance by over 10%.",
    "published": "2025-05-02T04:06:39Z",
    "updated": "2025-12-15T08:08:20Z",
    "link": "http://arxiv.org/pdf/2505.06256v2.pdf",
    "category": [
      "eess.SP",
      "cs.AI"
    ],
    "authors": [
      "Fuhui Zhou",
      "Chunyu Liu",
      "Hao Zhang",
      "Wei Wu",
      "Qihui Wu",
      "Tony Q. S. Quek",
      "Chan-Byoung Chae"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13070v1",
    "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization",
    "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.",
    "published": "2025-12-15T08:07:23Z",
    "updated": "2025-12-15T08:07:23Z",
    "link": "http://arxiv.org/pdf/2512.13070v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Bizhe Bai",
      "Hongming Wu",
      "Peng Ye",
      "Tao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13063v1",
    "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators",
    "summary": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.",
    "published": "2025-12-15T07:50:09Z",
    "updated": "2025-12-15T07:50:09Z",
    "link": "http://arxiv.org/pdf/2512.13063v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Cheril Shah",
      "Akshit Agarwal",
      "Kanak Garg",
      "Mourad Heddaya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11047v2",
    "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control",
    "summary": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To execute the desired locomotion commands more precisely, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.",
    "published": "2025-12-11T19:07:31Z",
    "updated": "2025-12-15T07:46:35Z",
    "link": "http://arxiv.org/pdf/2512.11047v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Haoran Jiang",
      "Jin Chen",
      "Qingwen Bu",
      "Li Chen",
      "Modi Shi",
      "Yanjie Zhang",
      "Delong Li",
      "Chuanzhe Suo",
      "Chuang Wang",
      "Zhihui Peng",
      "Hongyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11506v2",
    "title": "EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection",
    "summary": "As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.",
    "published": "2025-12-12T12:06:36Z",
    "updated": "2025-12-15T07:14:59Z",
    "link": "http://arxiv.org/pdf/2512.11506v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Georgios Kaoukis",
      "Ioannis Aris Koufopoulos",
      "Eleni Psaroudaki",
      "Danae Pla Karidi",
      "Evaggelia Pitoura",
      "George Papastefanatos",
      "Panayiotis Tsaparas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13043v1",
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
    "published": "2025-12-15T07:11:56Z",
    "updated": "2025-12-15T07:11:56Z",
    "link": "http://arxiv.org/pdf/2512.13043v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Tong Wei",
      "Yijun Yang",
      "Changhao Zhang",
      "Junliang Xing",
      "Yuanchun Shi",
      "Zongqing Lu",
      "Deheng Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13033v1",
    "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism",
    "summary": "The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes",
    "published": "2025-12-15T07:03:24Z",
    "updated": "2025-12-15T07:03:24Z",
    "link": "http://arxiv.org/pdf/2512.13033v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Jongwook Kim",
      "Sangheon Yun",
      "Sukjin Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.07293v2",
    "title": "Fast Wrong-way Cycling Detection in CCTV Videos: Sparse Sampling is All You Need",
    "summary": "Effective monitoring of unusual transportation behaviors, such as wrong-way cycling (i.e., riding a bicycle or e-bike against designated traffic flow), is crucial for optimizing law enforcement deployment and traffic planning. However, accurately recording all wrong-way cycling events is both unnecessary and infeasible in resource-constrained environments, as it requires high-resolution cameras for evidence collection and event detection. To address this challenge, we propose WWC-Predictor, a novel method for efficiently estimating the wrong-way cycling ratio, defined as the proportion of wrong-way cycling events relative to the total number of cycling movements over a given time period. The core innovation of our method lies in accurately detecting wrong-way cycling events in sparsely sampled frames using a light-weight detector, then estimating the overall ratio using an autoregressive moving average model. To evaluate the effectiveness of our method, we construct a benchmark dataset consisting of 35 minutes of video sequences with minute-level annotations.Our method achieves an average error rate of a mere 1.475\\% while consuming only 19.12\\% GPU time required by conventional tracking methods, validating its effectiveness in estimating the wrong-way cycling ratio. Our source code is publicly available at: https://github.com/VICA-Lab-HKUST-GZ/WWC-Predictor.",
    "published": "2024-05-12T14:16:05Z",
    "updated": "2025-12-15T06:34:41Z",
    "link": "http://arxiv.org/pdf/2405.07293v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jing Xu",
      "Wentao Shi",
      "Sheng Ren",
      "Lijuan Zhang",
      "Weikai Yang",
      "Pan Gao",
      "Jie Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.13426v2",
    "title": "ALIGN: Word Association Learning for Cultural Alignment in Large Language Models",
    "summary": "Large language models (LLMs) exhibit cultural bias from overrepresented viewpoints in training data, yet cultural alignment remains a challenge due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient and cognitively grounded method: fine-tuning LLMs on native speakers' word-association norms, leveraging cognitive psychology findings that such associations capture cultural knowledge. Using word association datasets from native speakers in the US (English) and China (Mandarin), we train Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning and preference optimization. We evaluate models' cultural alignment through a two-tier evaluation framework that spans lexical associations and cultural value alignment using the World Values Survey. Results show significant improvements in lexical alignment (16-20% English, 43-165% Mandarin on Precision@5) and high-level cultural value shifts. On a subset of 50 questions where US and Chinese respondents diverge most, fine-tuned Qwen nearly doubles its response alignment with Chinese values (13 to 25). Remarkably, our trained 7-8B models match or exceed vanilla 70B baselines, demonstrating that a few million of culture-grounded associations achieve value alignment without expensive retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.",
    "published": "2025-08-19T00:55:20Z",
    "updated": "2025-12-15T06:22:52Z",
    "link": "http://arxiv.org/pdf/2508.13426v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chunhua Liu",
      "Kabir Manandhar Shrestha",
      "Sukai Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19700v4",
    "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models",
    "summary": "The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \\textit{Residual Alignment Model} (\\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.",
    "published": "2025-05-26T08:53:02Z",
    "updated": "2025-12-15T06:09:56Z",
    "link": "http://arxiv.org/pdf/2505.19700v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yi Liu",
      "Dianqing Liu",
      "Mingye Zhu",
      "Junbo Guo",
      "Yongdong Zhang",
      "Zhendong Mao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.07770v2",
    "title": "Layer-aware TDNN: Speaker Recognition Using Multi-Layer Features from Pre-Trained Models",
    "summary": "Recent advances in self-supervised learning (SSL) on Transformers have significantly improved speaker verification (SV) by providing domain-general speech representations. However, existing approaches have underutilized the multi-layered nature of SSL encoders. To address this limitation, we propose the layer-aware time-delay neural network (L-TDNN), which directly performs layer/frame-wise processing on the layer-wise hidden state outputs from pre-trained models, extracting fixed-size speaker vectors. L-TDNN comprises a layer-aware convolutional network, a frame-adaptive layer aggregation, and attentive statistic pooling, explicitly modeling of the recognition and processing of previously overlooked layer dimension. We evaluated L-TDNN across multiple speech SSL Transformers and diverse speech-speaker corpora against other approaches for leveraging pre-trained encoders. L-TDNN consistently demonstrated robust verification performance, achieving the lowest error rates throughout the experiments. Concurrently, it stood out in terms of model compactness and exhibited inference efficiency comparable to the existing systems. These results highlight the advantages derived from the proposed layer-aware processing approach. Future work includes exploring joint training with SSL frontends and the incorporation of score calibration to further enhance state-of-the-art verification performance.",
    "published": "2024-09-12T05:55:32Z",
    "updated": "2025-12-15T05:44:38Z",
    "link": "http://arxiv.org/pdf/2409.07770v2.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Jin Sob Kim",
      "Hyun Joon Park",
      "Wooseok Shin",
      "Juan Yun",
      "Sung Won Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12997v1",
    "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP",
    "summary": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.",
    "published": "2025-12-15T05:41:08Z",
    "updated": "2025-12-15T05:41:08Z",
    "link": "http://arxiv.org/pdf/2512.12997v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wenjing lu",
      "Zerui Tao",
      "Dongping Zhang",
      "Yuning Qiu",
      "Yang Yang",
      "Qibin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12987v1",
    "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning",
    "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.",
    "published": "2025-12-15T05:23:23Z",
    "updated": "2025-12-15T05:23:23Z",
    "link": "http://arxiv.org/pdf/2512.12987v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Amin Jalal Aghdasian",
      "Farzaneh Abdollahi",
      "Ali Kamali Iglie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13685v1",
    "title": "Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech",
    "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.",
    "published": "2025-12-15T18:59:49Z",
    "updated": "2025-12-15T18:59:49Z",
    "link": "http://arxiv.org/pdf/2512.13685v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Dylan Phelps",
      "Rodrigo Wilkens",
      "Edward Gow-Smith",
      "Lilian Hubner",
      "Bárbara Malcorra",
      "César Rennó-Costa",
      "Marco Idiart",
      "Maria-Cruz Villa-Uriol",
      "Aline Villavicencio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13676v1",
    "title": "Towards Effective Model Editing for LLM Personalization",
    "summary": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.",
    "published": "2025-12-15T18:58:15Z",
    "updated": "2025-12-15T18:58:15Z",
    "link": "http://arxiv.org/pdf/2512.13676v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Baixiang Huang",
      "Limeng Cui",
      "Jiapeng Liu",
      "Haoran Wang",
      "Jiawei Xu",
      "Zhuiyue Tan",
      "Yutong Chen",
      "Chen Luo",
      "Yi Liu",
      "Kai Shu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13674v1",
    "title": "Towards Interactive Intelligence for Digital Humans",
    "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.",
    "published": "2025-12-15T18:57:35Z",
    "updated": "2025-12-15T18:57:35Z",
    "link": "http://arxiv.org/pdf/2512.13674v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.GR",
      "cs.HC"
    ],
    "authors": [
      "Yiyi Cai",
      "Xuangeng Chu",
      "Xiwei Gao",
      "Sitong Gong",
      "Yifei Huang",
      "Caixin Kang",
      "Kunhang Li",
      "Haiyang Liu",
      "Ruicong Liu",
      "Yun Liu",
      "Dianwen Ng",
      "Zixiong Su",
      "Erwin Wu",
      "Yuhan Wu",
      "Dingkun Yan",
      "Tianyu Yan",
      "Chang Zeng",
      "Bo Zheng",
      "You Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13667v1",
    "title": "A stylometric analysis of speaker attribution from speech transcripts",
    "summary": "Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.",
    "published": "2025-12-15T18:55:25Z",
    "updated": "2025-12-15T18:55:25Z",
    "link": "http://arxiv.org/pdf/2512.13667v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Cristina Aggazzotti",
      "Elizabeth Allyn Smith"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13655v1",
    "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
    "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.",
    "published": "2025-12-15T18:48:42Z",
    "updated": "2025-12-15T18:48:42Z",
    "link": "http://arxiv.org/pdf/2512.13655v1.pdf",
    "category": [
      "cs.CL",
      "cs.SE"
    ],
    "authors": [
      "Richard J. Young"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13618v1",
    "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models",
    "summary": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.",
    "published": "2025-12-15T18:10:51Z",
    "updated": "2025-12-15T18:10:51Z",
    "link": "http://arxiv.org/pdf/2512.13618v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Zefang Liu",
      "Nam Nguyen",
      "Yinzhu Quan",
      "Austin Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13598v1",
    "title": "Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization",
    "summary": "A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.",
    "published": "2025-12-15T17:52:16Z",
    "updated": "2025-12-15T17:52:16Z",
    "link": "http://arxiv.org/pdf/2512.13598v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Daniel Melcer",
      "Qi Chen",
      "Wen-Hao Chiang",
      "Shweta Garg",
      "Pranav Garg",
      "Christian Bock"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13552v1",
    "title": "PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation",
    "summary": "This work introduces {\\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.",
    "published": "2025-12-15T17:11:31Z",
    "updated": "2025-12-15T17:11:31Z",
    "link": "http://arxiv.org/pdf/2512.13552v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hour Kaing",
      "Raj Dabre",
      "Haiyue Song",
      "Van-Hien Tran",
      "Hideki Tanaka",
      "Masao Utiyama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13515v1",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "summary": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "published": "2025-12-15T16:42:51Z",
    "updated": "2025-12-15T16:42:51Z",
    "link": "http://arxiv.org/pdf/2512.13515v1.pdf",
    "category": [
      "cs.SE",
      "cs.CL",
      "cs.LO"
    ],
    "authors": [
      "Oleg Grynets",
      "Vasyl Lyashkevych",
      "Dmytro Baran",
      "Maksym Orliansky",
      "Taras Zelenyy",
      "Markiian Leshchyshyn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13488v1",
    "title": "SIGMA: An AI-Empowered Training Stack on Early-Life Hardware",
    "summary": "An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.",
    "published": "2025-12-15T16:24:32Z",
    "updated": "2025-12-15T16:24:32Z",
    "link": "http://arxiv.org/pdf/2512.13488v1.pdf",
    "category": [
      "cs.DC",
      "cs.CL"
    ],
    "authors": [
      "Lei Qu",
      "Lianhai Ren",
      "Peng Cheng",
      "Rui Gao",
      "Ruizhe Wang",
      "Tianyu Chen",
      "Xiao Liu",
      "Xingjian Zhang",
      "Yeyun Gong",
      "Yifan Xiong",
      "Yucheng Ding",
      "Yuting Jiang",
      "Zhenghao Lin",
      "Zhongxin Guo",
      "Ziyue Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13487v1",
    "title": "Advancing Bangla Machine Translation Through Informal Datasets",
    "summary": "Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.",
    "published": "2025-12-15T16:22:45Z",
    "updated": "2025-12-15T16:22:45Z",
    "link": "http://arxiv.org/pdf/2512.13487v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ayon Roy",
      "Risat Rahaman",
      "Sadat Shibly",
      "Udoy Saha Joy",
      "Abdulla Al Kafi",
      "Farig Yousuf Sadeque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.03888v3",
    "title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize",
    "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.",
    "published": "2025-09-04T05:15:55Z",
    "updated": "2025-12-15T16:08:21Z",
    "link": "http://arxiv.org/pdf/2509.03888v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Cheng Wang",
      "Zeming Wei",
      "Qin Liu",
      "Muhao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13472v1",
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
    "published": "2025-12-15T16:07:34Z",
    "updated": "2025-12-15T16:07:34Z",
    "link": "http://arxiv.org/pdf/2512.13472v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jian Yang",
      "Shawn Guo",
      "Lin Jing",
      "Wei Zhang",
      "Aishan Liu",
      "Chuan Hao",
      "Zhoujun Li",
      "Wayne Xin Zhao",
      "Xianglong Liu",
      "Weifeng Lv",
      "Bryan Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13441v1",
    "title": "Large language models are not about language",
    "summary": "Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.",
    "published": "2025-12-15T15:36:42Z",
    "updated": "2025-12-15T15:36:42Z",
    "link": "http://arxiv.org/pdf/2512.13441v1.pdf",
    "category": [
      "cs.CL",
      "q-bio.NC"
    ],
    "authors": [
      "Johan J. Bolhuis",
      "Andrea Moro",
      "Stephen Crain",
      "Sandiway Fong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08768v4",
    "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP",
    "summary": "Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at https://github.com/gufranSabri/deepseek-evals",
    "published": "2025-06-10T13:10:31Z",
    "updated": "2025-12-15T15:16:44Z",
    "link": "http://arxiv.org/pdf/2506.08768v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ahmed Hasanaath",
      "Aisha Alansari",
      "Ahmed Ashraf",
      "Chafik Salmane",
      "Hamzah Luqman",
      "Saad Ezzini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13352v1",
    "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models",
    "summary": "Large Language Models (LLMs) are prone to mem- orizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA bench- marks, allowing us to evaluate their practical utility in real-world extraction scenarios.",
    "published": "2025-12-15T14:05:49Z",
    "updated": "2025-12-15T14:05:49Z",
    "link": "http://arxiv.org/pdf/2512.13352v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Ali Al Sahili",
      "Ali Chehab",
      "Razane Tajeddine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.04683v2",
    "title": "Geschlechtsübergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden",
    "summary": "This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.",
    "published": "2025-12-04T11:27:22Z",
    "updated": "2025-12-15T13:52:40Z",
    "link": "http://arxiv.org/pdf/2512.04683v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Carolin Mueller-Spitzer",
      "Samira Ochs",
      "Jan Oliver Ruediger",
      "Sascha Wolfer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19108v2",
    "title": "Are most sentences unique? An empirical examination of Chomskyan claims",
    "summary": "A repeated claim in linguistics is that the majority of linguistic utterances are unique. For example, Pinker (1994: 10), summarizing an argument by Noam Chomsky, states that \"virtually every sentence that a person utters or understands is a brand-new combination of words, appearing for the first time in the history of the universe.\" With the increased availability of large corpora, this is a claim that can be empirically investigated. The current paper addresses the question by using the NLTK Python library to parse corpora of different genres, providing counts of exact string matches in each. Results show that while completely unique sentences are often the majority of corpora, this is highly constrained by genre, and that duplicate sentences are not an insignificant part of any individual corpus.",
    "published": "2025-09-23T14:54:19Z",
    "updated": "2025-12-15T13:34:11Z",
    "link": "http://arxiv.org/pdf/2509.19108v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hiram Ring"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.01948v2",
    "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
    "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
    "published": "2025-12-01T17:58:59Z",
    "updated": "2025-12-15T13:10:06Z",
    "link": "http://arxiv.org/pdf/2512.01948v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Dingling Zhang",
      "He Zhu",
      "Jincheng Ren",
      "Kangqi Song",
      "Xinran Zhou",
      "Boyu Feng",
      "Shudong Liu",
      "Jiabin Luo",
      "Weihao Xie",
      "Zhaohui Wang",
      "Tianrui Qin",
      "King Zhu",
      "Yuqing Wang",
      "Qianben Chen",
      "Yuchen Eleanor Jiang",
      "Wei Wang",
      "Jiaheng Liu",
      "Wangchunshu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13286v1",
    "title": "Integrating Causal Reasoning into Automated Fact-Checking",
    "summary": "In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.",
    "published": "2025-12-15T12:56:00Z",
    "updated": "2025-12-15T12:56:00Z",
    "link": "http://arxiv.org/pdf/2512.13286v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Youssra Rebboud",
      "Pasquale Lisena",
      "Raphael Troncy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13279v1",
    "title": "AIR: Post-training Data Selection for Reasoning via Attention Head Influence",
    "summary": "LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.",
    "published": "2025-12-15T12:38:24Z",
    "updated": "2025-12-15T12:38:24Z",
    "link": "http://arxiv.org/pdf/2512.13279v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jinrui Liu",
      "Jeff Wu",
      "Xuanguang Pan",
      "Gavin Cheung",
      "Shuai Ma",
      "Chongyang Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13278v1",
    "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
    "summary": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
    "published": "2025-12-15T12:38:04Z",
    "updated": "2025-12-15T12:38:04Z",
    "link": "http://arxiv.org/pdf/2512.13278v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jiaru Zou",
      "Ling Yang",
      "Yunzhe Qi",
      "Sirui Chen",
      "Mengting Ai",
      "Ke Shen",
      "Jingrui He",
      "Mengdi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20858v2",
    "title": "A survey of diversity quantification in natural language processing: The why, what, where and how",
    "summary": "The concept of diversity has received increased consideration in Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting and inclusion, approximating human linguistic behavior, and increasing systems' performance. Diversity has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6 years, with \"diversity\" or \"diverse\" in their title. We find a wide range of settings in which diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized approach. We believe that this study paves the way towards a better formalization of diversity in NLP, which should bring a better understanding of this notion and a better comparability between various approaches.",
    "published": "2025-07-28T14:12:34Z",
    "updated": "2025-12-15T11:50:31Z",
    "link": "http://arxiv.org/pdf/2507.20858v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Louis Estève",
      "Marie-Catherine de Marneffe",
      "Nurit Melnik",
      "Agata Savary",
      "Olha Kanishcheva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11297v2",
    "title": "LegalRikai: Open Benchmark - A Benchmark for Complex Japanese Corporate Legal Tasks",
    "summary": "This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.",
    "published": "2025-12-12T05:47:06Z",
    "updated": "2025-12-15T11:07:12Z",
    "link": "http://arxiv.org/pdf/2512.11297v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shogo Fujita",
      "Yuji Naraki",
      "Yiqing Zhu",
      "Shinsuke Mori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.02580v2",
    "title": "From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks",
    "summary": "Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.",
    "published": "2025-12-02T09:48:57Z",
    "updated": "2025-12-15T10:01:01Z",
    "link": "http://arxiv.org/pdf/2512.02580v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Changpeng Yang",
      "Jinyang Wu",
      "Yuchen Liu",
      "Shuai Zhang",
      "Yang Li",
      "Qiliang Liang",
      "Hongzhen Wang",
      "Shuai Nie",
      "Jiaming Xu",
      "Runyu Shi",
      "Ying Huang",
      "Guoquan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19476v2",
    "title": "A Pipeline to Assess Merging Methods via Behavior and Internals",
    "summary": "Merging methods combine the weights of multiple language models (LMs) to leverage their capacities, such as for domain adaptation. While existing studies investigate merged models from a solely behavioral perspective, we offer the first comprehensive view by assessing and connecting their behavior and internals. We present a novel evaluation pipeline that first merges multiple parent LMs, and then evaluates the merged models in comparison to the initial ones based on their behavior on downstream tasks, like MMLU, and the internal encoded linguistic competence. We showcase this pipeline by assessing the merging of instruction fine-tuned with math- and code-adapted LMs from the Qwen2.5 family. Our results show that merging methods impacts behavior and internals differently. While the performance of merged models is typically between that of the two parent models, their encoded information about linguistic phenomena, particularly in morphology and syntax, can surpass the parent models. Moreover, we find weak ranking correlation between this behavior and internal evaluation. With our pipeline and initial results, we emphasize the need for more comprehensive evaluations of model merging methods to gain a faithful understanding of their capabilities and reliability, beyond potential superficial behavioral advances.",
    "published": "2025-09-23T18:37:32Z",
    "updated": "2025-12-15T09:42:31Z",
    "link": "http://arxiv.org/pdf/2509.19476v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yutaro Sigrist",
      "Andreas Waldis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15101v2",
    "title": "Cost-aware LLM-based Online Dataset Annotation",
    "summary": "Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.",
    "published": "2025-05-21T04:49:44Z",
    "updated": "2025-12-15T08:51:21Z",
    "link": "http://arxiv.org/pdf/2505.15101v2.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.IT"
    ],
    "authors": [
      "Eray Can Elumar",
      "Cem Tekin",
      "Osman Yagan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13078v1",
    "title": "Heart Disease Prediction using Case Based Reasoning (CBR)",
    "summary": "This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.",
    "published": "2025-12-15T08:20:47Z",
    "updated": "2025-12-15T08:20:47Z",
    "link": "http://arxiv.org/pdf/2512.13078v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Mohaiminul Islam Bhuiyan",
      "Chan Hue Wah",
      "Nur Shazwani Kamarudin",
      "Nur Hafieza Ismail",
      "Ahmad Fakhri Ab Nasir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13059v1",
    "title": "An Open and Reproducible Deep Research Agent for Long-Form Question Answering",
    "summary": "We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.",
    "published": "2025-12-15T07:37:53Z",
    "updated": "2025-12-15T07:37:53Z",
    "link": "http://arxiv.org/pdf/2512.13059v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ikuya Yamada",
      "Wataru Ikeda",
      "Ko Yoshida",
      "Mengyu Ye",
      "Hinata Sugimoto",
      "Masatoshi Suzuki",
      "Hisanori Ozaki",
      "Jun Suzuki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13040v1",
    "title": "Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection",
    "summary": "Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.",
    "published": "2025-12-15T07:09:11Z",
    "updated": "2025-12-15T07:09:11Z",
    "link": "http://arxiv.org/pdf/2512.13040v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Xuwei Tan",
      "Yao Ma",
      "Xueru Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23065v2",
    "title": "SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services",
    "summary": "With the increasing integration of visual and textual content in Social Networking Services (SNS), evaluating the multimodal capabilities of Large Language Models (LLMs) is crucial for enhancing user experience, content understanding, and platform intelligence. Existing benchmarks primarily focus on text-centric tasks, lacking coverage of the multimodal contexts prevalent in modern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a comprehensive multimodal benchmark designed to assess the performance of Vision-Language LLMs in real-world social media scenarios. SNS-Bench-VL incorporates images and text across 8 multimodal tasks, including note comprehension, user engagement analysis, information retrieval, and personalized recommendation. It comprises 4,001 carefully curated multimodal question-answer pairs, covering single-choice, multiple-choice, and open-ended tasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their performance across tasks. Our findings highlight persistent challenges in multimodal social context comprehension. We hope SNS-Bench-VL will inspire future research towards robust, context-aware, and human-aligned multimodal intelligence for next-generation social networking services.",
    "published": "2025-05-29T04:16:24Z",
    "updated": "2025-12-15T06:50:22Z",
    "link": "http://arxiv.org/pdf/2505.23065v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hongcheng Guo",
      "Zheyong Xie",
      "Shaosheng Cao",
      "Boyang Wang",
      "Weiting Liu",
      "Anjie Le",
      "Lei Li",
      "Zhoujun Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03761v3",
    "title": "Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services",
    "summary": "As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.",
    "published": "2025-06-04T09:25:52Z",
    "updated": "2025-12-15T06:39:13Z",
    "link": "http://arxiv.org/pdf/2506.03761v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hongcheng Guo",
      "Zheyong Xie",
      "Shaosheng Cao",
      "Boyang Wang",
      "Weiting Liu",
      "Zheyu Ye",
      "Zhoujun Li",
      "Zuozhu Liu",
      "Wei Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17234v4",
    "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings",
    "summary": "Automatic generation of radiology reports has the potential to alleviate radiologists' significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) employs controlled decoding that completes \"Findings\" before synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive \"Findings\" section before synthesizing the \"Impression\" and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on clinical efficacy scores.",
    "published": "2025-07-23T05:57:59Z",
    "updated": "2025-12-15T06:22:38Z",
    "link": "http://arxiv.org/pdf/2507.17234v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kyeongkyu Lee",
      "Seonghwan Yoon",
      "Hongki Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00522v3",
    "title": "Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond",
    "summary": "Little research explores the correlation between the expressive ability and generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware Minimization (SAM) improves model generalization for both Convolutional Neural Networks (CNNs) and Transformers by encouraging convergence to locally flat minima. However, the connection between sharpness and generalization has not been fully explored for LoRA due to the lack of tools to either empirically seek flat minima or develop theoretical methods. In this work, we propose Flat Minima LoRA (FMLoRA) and its efficient version, i.e., EFMLoRA, to seek flat minima for LoRA. Concretely, we theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace. This approach eliminates the potential interference introduced by perturbations across multiple matrices in the low-rank subspace. Our extensive experiments on large language models and vision-language models demonstrate that EFMLoRA achieves optimize efficiency comparable to that of LoRA while simultaneously attaining comparable or even better performance. For example, on the GLUE dataset with RoBERTa-large, EFMLoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively. On vision-language models, e.g., Qwen-VL-Chat, there are performance improvements of 1.5% and 1.0% on the SQA and VizWiz datasets, respectively. These empirical results also verify that the generalization of LoRA is closely related to sharpness, which is omitted by previous methods.",
    "published": "2025-08-01T10:59:49Z",
    "updated": "2025-12-15T06:11:29Z",
    "link": "http://arxiv.org/pdf/2508.00522v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jiaxin Deng",
      "Qingcheng Zhu",
      "Junbiao Pang",
      "Linlin Yang",
      "Zhongqian Fu",
      "Baochang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.15781v2",
    "title": "DABL: Detecting Semantic Anomalies in Business Processes Using Large Language Models",
    "summary": "Detecting anomalies in business processes is crucial for ensuring operational success. While many existing methods rely on statistical frequency to detect anomalies, it's important to note that infrequent behavior doesn't necessarily imply undesirability. To address this challenge, detecting anomalies from a semantic viewpoint proves to be a more effective approach. However, current semantic anomaly detection methods treat a trace (i.e., process instance) as multiple event pairs, disrupting long-distance dependencies. In this paper, we introduce DABL, a novel approach for detecting semantic anomalies in business processes using large language models (LLMs). We collect 143,137 real-world process models from various domains. By generating normal traces through the playout of these process models and simulating both ordering and exclusion anomalies, we fine-tune Llama 2 using the resulting log. Through extensive experiments, we demonstrate that DABL surpasses existing state-of-the-art semantic anomaly detection methods in terms of both generalization ability and learning of given processes. Users can directly apply DABL to detect semantic anomalies in their own datasets without the need for additional training. Furthermore, DABL offers the capability to interpret the causes of anomalies in natural language, providing valuable insights into the detected anomalies.",
    "published": "2024-06-22T08:20:19Z",
    "updated": "2025-12-15T05:59:22Z",
    "link": "http://arxiv.org/pdf/2406.15781v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Wei Guan",
      "Jian Cao",
      "Jianqi Gao",
      "Haiyan Zhao",
      "Shiyou Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13689v1",
    "title": "LitePT: Lighter Yet Stronger Point Transformer",
    "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.",
    "published": "2025-12-15T18:59:57Z",
    "updated": "2025-12-15T18:59:57Z",
    "link": "http://arxiv.org/pdf/2512.13689v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuanwen Yue",
      "Damien Robert",
      "Jianyuan Wang",
      "Sunghwan Hong",
      "Jan Dirk Wegner",
      "Christian Rupprecht",
      "Konrad Schindler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13687v1",
    "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation",
    "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.",
    "published": "2025-12-15T18:59:54Z",
    "updated": "2025-12-15T18:59:54Z",
    "link": "http://arxiv.org/pdf/2512.13687v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingfeng Yao",
      "Yuda Song",
      "Yucong Zhou",
      "Xinggang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13684v1",
    "title": "Recurrent Video Masked Autoencoders",
    "summary": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.",
    "published": "2025-12-15T18:59:48Z",
    "updated": "2025-12-15T18:59:48Z",
    "link": "http://arxiv.org/pdf/2512.13684v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Daniel Zoran",
      "Nikhil Parthasarathy",
      "Yi Yang",
      "Drew A Hudson",
      "Joao Carreira",
      "Andrew Zisserman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13683v1",
    "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
    "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/",
    "published": "2025-12-15T18:59:13Z",
    "updated": "2025-12-15T18:59:13Z",
    "link": "http://arxiv.org/pdf/2512.13683v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lu Ling",
      "Yunhao Ge",
      "Yichen Sheng",
      "Aniket Bera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13680v1",
    "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction",
    "summary": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$",
    "published": "2025-12-15T18:59:04Z",
    "updated": "2025-12-15T18:59:04Z",
    "link": "http://arxiv.org/pdf/2512.13680v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianye Ding",
      "Yiming Xie",
      "Yiqing Liang",
      "Moitreya Chatterjee",
      "Pedro Miraldo",
      "Huaizu Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.03726v2",
    "title": "Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames",
    "summary": "Estimating the 6D pose of textureless objects from RGB images is an important problem in robotics. Due to appearance ambiguities, rotational symmetries, and severe occlusions, single-view based 6D pose estimators are still unable to handle a wide range of objects, motivating research towards multi-view pose estimation and next-best-view prediction that addresses these limitations. In this work, we propose a comprehensive active perception framework for estimating the 6D poses of textureless objects using only RGB images. Our approach is built upon a key idea: decoupling the 6D pose estimation into a two-step sequential process can greatly improve both accuracy and efficiency. First, we estimate the 3D translation of each object, resolving scale and depth ambiguities inherent to RGB images. These estimates are then used to simplify the subsequent task of determining the 3D orientation, which we achieve through canonical scale template matching. Building on this formulation, we then introduce an active perception strategy that predicts the next best camera viewpoint to capture an RGB image, effectively reducing object pose uncertainty and enhancing pose accuracy. We evaluate our method on the public ROBI and TOD datasets, as well as on our reconstructed transparent object dataset, T-ROBI. Under the same camera viewpoints, our multi-view pose estimation significantly outperforms state-of-the-art approaches. Furthermore, by leveraging our next-best-view strategy, our approach achieves high pose accuracy with fewer viewpoints than heuristic-based policies across all evaluated datasets. The accompanying video and T-ROBI dataset will be released on our project page: https://trailab.github.io/ActiveODPE.",
    "published": "2025-03-05T18:28:32Z",
    "updated": "2025-12-15T18:58:46Z",
    "link": "http://arxiv.org/pdf/2503.03726v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Jun Yang",
      "Wenjie Xue",
      "Sahar Ghavidel",
      "Steven L. Waslander"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13677v1",
    "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation",
    "summary": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.",
    "published": "2025-12-15T18:58:18Z",
    "updated": "2025-12-15T18:58:18Z",
    "link": "http://arxiv.org/pdf/2512.13677v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaohu Huang",
      "Hao Zhou",
      "Qiangpeng Yang",
      "Shilei Wen",
      "Kai Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13672v1",
    "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
    "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
    "published": "2025-12-15T18:57:07Z",
    "updated": "2025-12-15T18:57:07Z",
    "link": "http://arxiv.org/pdf/2512.13672v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Kunhee Kim",
      "NaHyeon Park",
      "Kibeom Hong",
      "Hyunjung Shim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13671v1",
    "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection",
    "summary": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.",
    "published": "2025-12-15T18:57:04Z",
    "updated": "2025-12-15T18:57:04Z",
    "link": "http://arxiv.org/pdf/2512.13671v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junwen Miao",
      "Penghui Du",
      "Yi Liu",
      "Yu Wang",
      "Yan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13665v1",
    "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency",
    "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.",
    "published": "2025-12-15T18:54:30Z",
    "updated": "2025-12-15T18:54:30Z",
    "link": "http://arxiv.org/pdf/2512.13665v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenhan Chen",
      "Sezer Karaoglu",
      "Theo Gevers"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13660v1",
    "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
    "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.",
    "published": "2025-12-15T18:52:43Z",
    "updated": "2025-12-15T18:52:43Z",
    "link": "http://arxiv.org/pdf/2512.13660v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Enshen Zhou",
      "Cheng Chi",
      "Yibo Li",
      "Jingkun An",
      "Jiayuan Zhang",
      "Shanyu Rong",
      "Yi Han",
      "Yuheng Ji",
      "Mengzhen Liu",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Lu Sheng",
      "Shanghang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13639v1",
    "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All",
    "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.",
    "published": "2025-12-15T18:33:08Z",
    "updated": "2025-12-15T18:33:08Z",
    "link": "http://arxiv.org/pdf/2512.13639v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Michal Nazarczuk",
      "Thomas Tanay",
      "Arthur Moreau",
      "Zhensong Zhang",
      "Eduardo Pérez-Pellitero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13636v1",
    "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
    "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
    "published": "2025-12-15T18:31:32Z",
    "updated": "2025-12-15T18:31:32Z",
    "link": "http://arxiv.org/pdf/2512.13636v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Haoyu Fu",
      "Diankun Zhang",
      "Zongchuang Zhao",
      "Jianfeng Cui",
      "Hongwei Xie",
      "Bing Wang",
      "Guang Chen",
      "Dingkang Liang",
      "Xiang Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13635v1",
    "title": "SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning",
    "summary": "Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST",
    "published": "2025-12-15T18:30:40Z",
    "updated": "2025-12-15T18:30:40Z",
    "link": "http://arxiv.org/pdf/2512.13635v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junchao Zhu",
      "Ruining Deng",
      "Junlin Guo",
      "Tianyuan Yao",
      "Chongyu Qu",
      "Juming Xiong",
      "Siqi Lu",
      "Zhengyi Lu",
      "Yanfan Zhu",
      "Marilyn Lionts",
      "Yuechen Yang",
      "Yalin Zheng",
      "Yu Wang",
      "Shilin Zhao",
      "Haichun Yang",
      "Yuankai Huo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.09397v2",
    "title": "OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS",
    "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.",
    "published": "2025-11-12T15:08:46Z",
    "updated": "2025-12-15T18:12:41Z",
    "link": "http://arxiv.org/pdf/2511.09397v2.pdf",
    "category": [
      "cs.CV",
      "cs.CG",
      "cs.GR",
      "cs.HC"
    ],
    "authors": [
      "Haiyi Li",
      "Qi Chen",
      "Denis Kalkofen",
      "Hsiang-Ting Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13609v1",
    "title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models",
    "summary": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.",
    "published": "2025-12-15T18:03:42Z",
    "updated": "2025-12-15T18:03:42Z",
    "link": "http://arxiv.org/pdf/2512.13609v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shweta Mahajan",
      "Shreya Kadambi",
      "Hoang Le",
      "Munawar Hayat",
      "Fatih Porikli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13608v1",
    "title": "DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis",
    "summary": "Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.\n  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.\n  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.\n  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\\% compared to Dinov2's 77.3\\%.\n  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.",
    "published": "2025-12-15T18:03:09Z",
    "updated": "2025-12-15T18:03:09Z",
    "link": "http://arxiv.org/pdf/2512.13608v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Felix J. Dorfner",
      "Manon A. Dorster",
      "Ryan Connolly",
      "Oscar Gentilhomme",
      "Edward Gibbs",
      "Steven Graham",
      "Seth Wander",
      "Thomas Schultz",
      "Manisha Bahl",
      "Dania Daye",
      "Albert E. Kim",
      "Christopher P. Bridge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13604v1",
    "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
    "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
    "published": "2025-12-15T17:59:58Z",
    "updated": "2025-12-15T17:59:58Z",
    "link": "http://arxiv.org/pdf/2512.13604v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jianxiong Gao",
      "Zhaoxi Chen",
      "Xian Liu",
      "Junhao Zhuang",
      "Chengming Xu",
      "Jianfeng Feng",
      "Yu Qiao",
      "Yanwei Fu",
      "Chenyang Si",
      "Ziwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13597v1",
    "title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation",
    "summary": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.",
    "published": "2025-12-15T17:49:22Z",
    "updated": "2025-12-15T17:49:22Z",
    "link": "http://arxiv.org/pdf/2512.13597v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Christophe Bolduc",
      "Julien Philip",
      "Li Ma",
      "Mingming He",
      "Paul Debevec",
      "Jean-François Lalonde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13592v1",
    "title": "Image Diffusion Preview with Consistency Solver",
    "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.",
    "published": "2025-12-15T17:47:49Z",
    "updated": "2025-12-15T17:47:49Z",
    "link": "http://arxiv.org/pdf/2512.13592v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Fu-Yun Wang",
      "Hao Zhou",
      "Liangzhe Yuan",
      "Sanghyun Woo",
      "Boqing Gong",
      "Bohyung Han",
      "Ming-Hsuan Yang",
      "Han Zhang",
      "Yukun Zhu",
      "Ting Liu",
      "Long Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.08962v3",
    "title": "Explainable Quantum Machine Learning for Multispectral Images Segmentation: Case Study",
    "summary": "The emergence of Big Data changed how we approach information systems engineering. Nowadays, when we can use remote sensing techniques for Big Data acquisition, the issues such data introduce are as important as ever. One of those concerns is the processing of the data. Classical methods often fail to address that problem or are incapable of processing the data in a reasonable time. With that in mind information system engineers are required to investigate different approaches to the data processing. The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to investigate their application to real-life computational problem. This field of study is called quantum (information) systems engineering and usually focuses on technical problems with the contemporary devices. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, less explored side of this medal. Using multispectral image segmentation as a task example, we investigated how difficult it is to run a hybrid quantum-classical model on a real, publicly available quantum device. To quantify how and explain why the performance of our model changed when ran on a real device, we propose new explainability metrics. These metrics introduce new meaning to the explainable quantum machine learning; the explanation of the performance issue comes from the quantum device behavior. We also analyzed the expected money costs of running similar experiment on contemporary quantum devices using standard market prices.",
    "published": "2025-03-11T23:55:10Z",
    "updated": "2025-12-15T17:46:00Z",
    "link": "http://arxiv.org/pdf/2503.08962v3.pdf",
    "category": [
      "quant-ph",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Tomasz Rybotycki",
      "Manish K. Gupta",
      "Piotr Gawron"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09476v3",
    "title": "WakeupUrban: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imagery",
    "summary": "Historical satellite imagery archive, such as Keyhole satellite data, offers rare insights into understanding early urban development and long-term transformation. However, severe quality degradation ($\\textit{e.g.}$, distortion, misalignment, and spectral scarcity) and the absence of annotations have long hindered its analysis. To bridge this gap and enhance understanding of urban development, we introduce $\\textbf{WakeupUrbanBench}$, an annotated segmentation dataset based on historical satellite imagery with the earliest observation time among all existing remote sensing (RS) datasets, along with a framework for unsupervised segmentation tasks, $\\textbf{WakeupUSM}$. First, WakeupUrbanBench serves as a pioneer, expertly annotated dataset built on mid-$20^{\\text{th}}$ century RS imagery, involving four key urban classes and spanning 4 cities across 2 continents with nearly 1000 km$^2$ area of diverse urban morphologies, and additionally introducing one present-day city. Second, WakeupUSM is a novel unsupervised semantic segmentation framework for historical RS imagery. It employs a confidence-aware alignment mechanism and focal-confidence loss based on a self-supervised learning architecture, which generates robust pseudo-labels and adaptively prioritizes prediction difficulty and label reliability to improve unsupervised segmentation on noisy historical data without manual supervision. Comprehensive experiments demonstrate WakeupUSM significantly outperforms existing unsupervised segmentation methods $\\textbf{both WakeupUrbanBench and public dataset}$, promising to pave the way for quantitative studies of long-term urban change using modern computer vision. Our benchmark and codes will be released at https://github.com/Tianxiang-Hao/WakeupUrban.",
    "published": "2025-06-11T07:41:30Z",
    "updated": "2025-12-15T17:33:00Z",
    "link": "http://arxiv.org/pdf/2506.09476v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianxiang Hao",
      "Lixian Zhang",
      "Yingjia Zhang",
      "Mengxuan Chen",
      "Jinxiao Zhang",
      "Runmin Dong",
      "Haohuan Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13573v1",
    "title": "MMhops-R1: Multimodal Multi-hop Reasoning",
    "summary": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.",
    "published": "2025-12-15T17:29:02Z",
    "updated": "2025-12-15T17:29:02Z",
    "link": "http://arxiv.org/pdf/2512.13573v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tao Zhang",
      "Ziqi Zhang",
      "Zongyang Ma",
      "Yuxin Chen",
      "Bing Li",
      "Chunfeng Yuan",
      "Guangting Wang",
      "Fengyun Rao",
      "Ying Shan",
      "Weiming Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13560v1",
    "title": "3D Human-Human Interaction Anomaly Detection",
    "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.",
    "published": "2025-12-15T17:17:55Z",
    "updated": "2025-12-15T17:17:55Z",
    "link": "http://arxiv.org/pdf/2512.13560v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shun Maeda",
      "Chunzhi Gu",
      "Koichiro Kamide",
      "Katsuya Hotta",
      "Shangce Gao",
      "Chao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.00746v2",
    "title": "WALINET: A water and lipid identification convolutional Neural Network for nuisance signal removal in 1H MR Spectroscopic Imaging",
    "summary": "Purpose. Proton Magnetic Resonance Spectroscopic Imaging (1H-MRSI) provides non-invasive spectral-spatial mapping of metabolism. However, long-standing problems in whole-brain 1H-MRSI are spectral overlap of metabolite peaks with large lipid signal from scalp, and overwhelming water signal that distorts spectra. Fast and effective methods are needed for high-resolution 1H-MRSI to accurately remove lipid and water signals while preserving the metabolite signal. The potential of supervised neural networks for this task remains unexplored, despite their success for other MRSI processing.\n  Methods. We introduce a deep-learning method based on a modified Y-NET network for water and lipid removal in whole-brain 1H-MRSI. The WALINET (WAter and LIpid neural NETwork) was compared to conventional methods such as the state-of-the-art lipid L2 regularization and Hankel-Lanczos singular value decomposition (HLSVD) water suppression. Methods were evaluated on simulated and in-vivo whole-brain MRSI using NMRSE, SNR, CRLB, and FWHM metrics.\n  Results. WALINET is significantly faster and needs 8s for high-resolution whole-brain MRSI, compared to 42 minutes for conventional HLSVD+L2. Quantitative analysis shows WALINET has better performance than HLSVD+L2: 1) more lipid removal with 41% lower NRMSE, 2) better metabolite signal preservation with 71% lower NRMSE in simulated data, 155% higher SNR and 50% lower CRLB in in-vivo data. Metabolic maps obtained by WALINET in healthy subjects and patients show better gray/white-matter contrast with more visible structural details.\n  Conclusions. WALINET has superior performance for nuisance signal removal and metabolite quantification on whole-brain 1H-MRSI compared to conventional state-of-the-art techniques. This represents a new application of deep-learning for MRSI processing, with potential for automated high-throughput workflow.",
    "published": "2024-10-01T14:37:55Z",
    "updated": "2025-12-15T17:08:11Z",
    "link": "http://arxiv.org/pdf/2410.00746v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Paul Weiser",
      "Georg Langs",
      "Stanislav Motyka",
      "Wolfgang Bogner",
      "Sébastien Courvoisier",
      "Malte Hoffmann",
      "Antoine Klauser",
      "Ovidiu C. Andronesi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13534v1",
    "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains",
    "summary": "A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.",
    "published": "2025-12-15T17:00:21Z",
    "updated": "2025-12-15T17:00:21Z",
    "link": "http://arxiv.org/pdf/2512.13534v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Marianne Rakic",
      "Siyu Gai",
      "Etienne Chollet",
      "John V. Guttag",
      "Adrian V. Dalca"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04130v2",
    "title": "Deep priors for satellite image restoration with accurate uncertainties",
    "summary": "Satellite optical images, upon their on-ground receipt, offer a distorted view of the observed scene. Their restoration, including denoising, deblurring, and sometimes super-resolution, is required before their exploitation. Moreover, quantifying the uncertainties related to this restoration helps to reduce the risks of misinterpreting the image content. Deep learning methods are now state-of-the-art for satellite image restoration. Among them, direct inversion methods train a specific network for each sensor, and generally provide a point estimation of the restored image without the associated uncertainties. Alternatively, deep regularization (DR) methods learn a deep prior on target images before plugging it, as the regularization term, into a model-based optimization scheme. This allows for restoring images from several sensors with a single network and possibly for estimating associated uncertainties. In this paper, we introduce VBLE-xz, a DR method that solves the inverse problem in the latent space of a variational compressive autoencoder (CAE). We adapt the regularization strength by modulating the bitrate of the trained CAE with a training-free approach. Then, VBLE-xz estimates relevant uncertainties jointly in the latent and in the image spaces by sampling an explicit posterior estimated within variational inference. This enables fast posterior sampling, unlike state-of-the-art DR methods that use Markov chains or diffusion-based approaches. We conduct a comprehensive set of experiments on very high-resolution simulated and real Pléiades images, asserting the performance, robustness and scalability of the proposed method. They demonstrate that VBLE-xz represents a compelling alternative to direct inversion methods when uncertainty quantification is required. The code associated to this paper is available in https://github.com/MaudBqrd/VBLExz.",
    "published": "2024-12-05T12:56:03Z",
    "updated": "2025-12-15T16:43:56Z",
    "link": "http://arxiv.org/pdf/2412.04130v2.pdf",
    "category": [
      "cs.CV",
      "eess.IV",
      "physics.optics"
    ],
    "authors": [
      "Biquard Maud",
      "Marie Chabert",
      "Florence Genin",
      "Christophe Latry",
      "Thomas Oberlin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13511v1",
    "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding",
    "summary": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.",
    "published": "2025-12-15T16:38:59Z",
    "updated": "2025-12-15T16:38:59Z",
    "link": "http://arxiv.org/pdf/2512.13511v1.pdf",
    "category": [
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Piyush Bagad",
      "Andrew Zisserman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13507v1",
    "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
    "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
    "published": "2025-12-15T16:36:52Z",
    "updated": "2025-12-15T16:36:52Z",
    "link": "http://arxiv.org/pdf/2512.13507v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Siyan Chen",
      "Yanfei Chen",
      "Ying Chen",
      "Zhuo Chen",
      "Feng Cheng",
      "Xuyan Chi",
      "Jian Cong",
      "Qinpeng Cui",
      "Qide Dong",
      "Junliang Fan",
      "Jing Fang",
      "Zetao Fang",
      "Chengjian Feng",
      "Han Feng",
      "Mingyuan Gao",
      "Yu Gao",
      "Qiushan Guo",
      "Boyang Hao",
      "Qingkai Hao",
      "Bibo He",
      "Qian He",
      "Tuyen Hoang",
      "Ruoqing Hu",
      "Xi Hu",
      "Weilin Huang",
      "Zhaoyang Huang",
      "Zhongyi Huang",
      "Siqi Jiang",
      "Wei Jiang",
      "Yunpu Jiang",
      "Zhuo Jiang",
      "Ashley Kim",
      "Jianan Kong",
      "Zhichao Lai",
      "Shanshan Lao",
      "Ai Li",
      "Feiya Li",
      "Gen Li",
      "Huixia Li",
      "JiaShi Li",
      "Liang Li",
      "Ming Li",
      "Tao Li",
      "Xian Li",
      "Xiaojie Li",
      "Xiaoyang Li",
      "Xingxing Li",
      "Yameng Li",
      "Yifu Li",
      "Yiying Li",
      "Chao Liang",
      "Ying Liang",
      "Zhiqiang Liang",
      "Wang Liao",
      "Yalin Liao",
      "Heng Lin",
      "Kengyu Lin",
      "Shanchuan Lin",
      "Xi Lin",
      "Zhijie Lin",
      "Feng Ling",
      "Fangfang Liu",
      "Gaohong Liu",
      "Jiawei Liu",
      "Jie Liu",
      "Shouda Liu",
      "Shu Liu",
      "Sichao Liu",
      "Songwei Liu",
      "Xin Liu",
      "Xue Liu",
      "Yibo Liu",
      "Zikun Liu",
      "Zuxi Liu",
      "Junlin Lyu",
      "Lecheng Lyu",
      "Qian Lyu",
      "Han Mu",
      "Xiaonan Nie",
      "Jingzhe Ning",
      "Xitong Pan",
      "Yanghua Peng",
      "Lianke Qin",
      "Xueqiong Qu",
      "Yuxi Ren",
      "Yuchen Shen",
      "Guang Shi",
      "Lei Shi",
      "Yan Song",
      "Yinglong Song",
      "Fan Sun",
      "Li Sun",
      "Renfei Sun",
      "Zeyu Sun",
      "Wenjing Tang",
      "Zirui Tao",
      "Feng Wang",
      "Furui Wang",
      "Jinran Wang",
      "Junkai Wang",
      "Ke Wang",
      "Kexin Wang",
      "Qingyi Wang",
      "Rui Wang",
      "Sen Wang",
      "Shuai Wang",
      "Tingru Wang",
      "Weichen Wang",
      "Xin Wang",
      "Yanhui Wang",
      "Yue Wang",
      "Yuping Wang",
      "Yuxuan Wang",
      "Ziyu Wang",
      "Guoqiang Wei",
      "Wanru Wei",
      "Di Wu",
      "Guohong Wu",
      "Hanjie Wu",
      "Jian Wu",
      "Jie Wu",
      "Ruolan Wu",
      "Xinglong Wu",
      "Yonghui Wu",
      "Ruiqi Xia",
      "Liang Xiang",
      "Fei Xiao",
      "XueFeng Xiao",
      "Pan Xie",
      "Shuangyi Xie",
      "Shuang Xu",
      "Jinlan Xue",
      "Bangbang Yang",
      "Ceyuan Yang",
      "Jiaqi Yang",
      "Runkai Yang",
      "Tao Yang",
      "Yang Yang",
      "Yihang Yang",
      "ZhiXian Yang",
      "Ziyan Yang",
      "Yifan Yao",
      "Zilyu Ye",
      "Bowen Yu",
      "Chujie Yuan",
      "Linxiao Yuan",
      "Sichun Zeng",
      "Weihong Zeng",
      "Xuejiao Zeng",
      "Yan Zeng",
      "Chuntao Zhang",
      "Heng Zhang",
      "Jingjie Zhang",
      "Kuo Zhang",
      "Liang Zhang",
      "Liying Zhang",
      "Manlin Zhang",
      "Ting Zhang",
      "Weida Zhang",
      "Xiaohe Zhang",
      "Xinyan Zhang",
      "Yan Zhang",
      "Yuan Zhang",
      "Zixiang Zhang",
      "Fengxuan Zhao",
      "Huating Zhao",
      "Yang Zhao",
      "Hao Zheng",
      "Jianbin Zheng",
      "Xiaozheng Zheng",
      "Yangyang Zheng",
      "Yijie Zheng",
      "Jiexin Zhou",
      "Kuan Zhu",
      "Shenhan Zhu",
      "Wenjia Zhu",
      "Benhui Zou",
      "Feilong Zuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.14681v3",
    "title": "Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model",
    "summary": "Multiplex imaging is revolutionizing pathology by enabling the simultaneous visualization of multiple biomarkers within tissue samples, providing molecular-level insights that traditional hematoxylin and eosin (H&E) staining cannot provide. However, the complexity and cost of multiplex data acquisition have hindered its widespread adoption. Additionally, most existing large repositories of H&E images lack corresponding multiplex images, limiting opportunities for multimodal analysis. To address these challenges, we leverage recent advances in latent diffusion models (LDMs), which excel at modeling complex data distributions by utilizing their powerful priors for fine-tuning to a target domain. In this paper, we introduce a novel framework for virtual multiplex staining that utilizes pretrained LDM parameters to generate multiplex images from H&E images using a conditional diffusion model. Our approach enables marker-by-marker generation by conditioning the diffusion model on each marker, while sharing the same architecture across all markers. To tackle the challenge of varying pixel value distributions across different marker stains and to improve inference speed, we fine-tune the model for single-step sampling, enhancing both color contrast fidelity and inference efficiency through pixel-level loss functions. We validate our framework on two publicly available datasets, notably demonstrating its effectiveness in generating up to 18 different marker types with improved accuracy, a substantial increase over the 2-3 marker types achieved in previous approaches. This validation highlights the potential of our framework, pioneering virtual multiplex staining. Finally, this paper bridges the gap between H&E and multiplex imaging, potentially enabling retrospective studies and large-scale analyses of existing H&E image repositories.",
    "published": "2025-08-20T12:54:58Z",
    "updated": "2025-12-15T16:32:19Z",
    "link": "http://arxiv.org/pdf/2508.14681v3.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Hyun-Jic Oh",
      "Junsik Kim",
      "Zhiyi Shi",
      "Yichen Wu",
      "Yu-An Chen",
      "Peter K Sorger",
      "Hanspeter Pfister",
      "Won-Ki Jeong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13497v1",
    "title": "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing",
    "summary": "In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.",
    "published": "2025-12-15T16:27:23Z",
    "updated": "2025-12-15T16:27:23Z",
    "link": "http://arxiv.org/pdf/2512.13497v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Haoyu Ren",
      "Kay Koehle",
      "Kirill Dorofeev",
      "Darko Anicic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13495v1",
    "title": "Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation",
    "summary": "We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/",
    "published": "2025-12-15T16:25:56Z",
    "updated": "2025-12-15T16:25:56Z",
    "link": "http://arxiv.org/pdf/2512.13495v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiangning Zhang",
      "Junwei Zhu",
      "Zhenye Gan",
      "Donghao Luo",
      "Chuming Lin",
      "Feifan Xu",
      "Xu Peng",
      "Jianlong Hu",
      "Yuansen Liu",
      "Yijia Hong",
      "Weijian Cao",
      "Han Feng",
      "Xu Chen",
      "Chencan Fu",
      "Keke He",
      "Xiaobin Hu",
      "Chengjie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13492v1",
    "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$",
    "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video",
    "published": "2025-12-15T16:25:39Z",
    "updated": "2025-12-15T16:25:39Z",
    "link": "http://arxiv.org/pdf/2512.13492v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiangning Zhang",
      "Junwei Zhu",
      "Teng Hu",
      "Yabiao Wang",
      "Donghao Luo",
      "Weijian Cao",
      "Zhenye Gan",
      "Xiaobin Hu",
      "Zhucun Xue",
      "Chengjie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10576v4",
    "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs",
    "summary": "While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks.Furthermore, grounded in the observation that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, we posit that reasoning ability serves as the key to unlocking it. We devise a multi-stage, modality-progressive reinforcement learning approach, resulting in HumanSense-Omni-Reasoning, which substantially enhances performance on higher-level understanding and interactive tasks. Additionally, we observe that successful reasoning processes appear to exhibit consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner.Project page: \\textcolor{brightpink}{https://digital-avatar.github.io/ai/HumanSense/}",
    "published": "2025-08-14T12:14:15Z",
    "updated": "2025-12-15T16:23:44Z",
    "link": "http://arxiv.org/pdf/2508.10576v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zheng Qin",
      "Ruobing Zheng",
      "Yabing Wang",
      "Tianqi Li",
      "Yi Yuan",
      "Jingdong Chen",
      "Le Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13465v1",
    "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence",
    "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.",
    "published": "2025-12-15T16:03:26Z",
    "updated": "2025-12-15T16:03:26Z",
    "link": "http://arxiv.org/pdf/2512.13465v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ruiyan Wang",
      "Teng Hu",
      "Kaihui Huang",
      "Zihan Su",
      "Ran Yi",
      "Lizhuang Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13454v1",
    "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception",
    "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.",
    "published": "2025-12-15T15:51:30Z",
    "updated": "2025-12-15T15:51:30Z",
    "link": "http://arxiv.org/pdf/2512.13454v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Arpit Jadon",
      "Joshua Niemeijer",
      "Yuki M. Asano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.19534v2",
    "title": "QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain",
    "summary": "We tackle the problem of quantifying the number of objects by a generative text-to-image model. Rather than retraining such a model for each new image domain of interest, which leads to high computational costs and limited scalability, we are the first to consider this problem from a domain-agnostic perspective. We propose QUOTA, an optimization framework for text-to-image models that enables effective object quantification across unseen domains without retraining. It leverages a dual-loop meta-learning strategy to optimize a domain-invariant prompt. Further, by integrating prompt learning with learnable counting and domain tokens, our method captures stylistic variations and maintains accuracy, even for object classes not encountered during training. For evaluation, we adopt a new benchmark specifically designed for object quantification in domain generalization, enabling rigorous assessment of object quantification accuracy and adaptability across unseen domains in text-to-image generation. Extensive experiments demonstrate that QUOTA outperforms conventional models in both object quantification accuracy and semantic consistency, setting a new benchmark for efficient and scalable text-to-image generation for any domain.",
    "published": "2024-11-29T08:20:12Z",
    "updated": "2025-12-15T15:49:38Z",
    "link": "http://arxiv.org/pdf/2411.19534v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Wenfang Sun",
      "Yingjun Du",
      "Gaowen Liu",
      "Yefeng Zheng",
      "Cees G. M. Snoek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03439v3",
    "title": "CleanDIFT: Diffusion Features without Noise",
    "summary": "Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.",
    "published": "2024-12-04T16:29:04Z",
    "updated": "2025-12-15T15:36:18Z",
    "link": "http://arxiv.org/pdf/2412.03439v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nick Stracke",
      "Stefan Andreas Baumann",
      "Kolja Bauer",
      "Frank Fundel",
      "Björn Ommer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13440v1",
    "title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images",
    "summary": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.",
    "published": "2025-12-15T15:36:09Z",
    "updated": "2025-12-15T15:36:09Z",
    "link": "http://arxiv.org/pdf/2512.13440v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Thalyssa Baiocco-Rodrigues",
      "Antoine Olivier",
      "Reda Belbahri",
      "Thomas Duboudin",
      "Pierre-Antoine Bannier",
      "Benjamin Adjadj",
      "Katharina Von Loga",
      "Nathan Noiry",
      "Maxime Touzot",
      "Hector Roux de Bezieux"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13434v1",
    "title": "Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging",
    "summary": "Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.",
    "published": "2025-12-15T15:28:02Z",
    "updated": "2025-12-15T15:28:02Z",
    "link": "http://arxiv.org/pdf/2512.13434v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Youssef Megahed",
      "Inok Lee",
      "Robin Ducharme",
      "Kevin Dick",
      "Adrian D. C. Chan",
      "Steven Hawken",
      "Mark C. Walker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13428v1",
    "title": "A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification",
    "summary": "Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.",
    "published": "2025-12-15T15:17:29Z",
    "updated": "2025-12-15T15:17:29Z",
    "link": "http://arxiv.org/pdf/2512.13428v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anika Islam",
      "Tasfia Tahsin",
      "Zaarin Anjum",
      "Md. Bakhtiar Hasan",
      "Md. Hasanul Kabir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13427v1",
    "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
    "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
    "published": "2025-12-15T15:17:02Z",
    "updated": "2025-12-15T15:17:02Z",
    "link": "http://arxiv.org/pdf/2512.13427v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Noa Cohen",
      "Nurit Spingarn-Eliezer",
      "Inbar Huberman-Spiegelglas",
      "Tomer Michaeli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08888v2",
    "title": "Accelerated Rotation-Invariant Convolution for UAV Image Segmentation",
    "summary": "Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.\n  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\\(\\times\\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\\(\\times\\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.",
    "published": "2025-12-09T18:30:00Z",
    "updated": "2025-12-15T15:14:24Z",
    "link": "http://arxiv.org/pdf/2512.08888v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Manduhu Manduhu",
      "Alexander Dow",
      "Gerard Dooly",
      "James Riordan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13421v1",
    "title": "RecTok: Reconstruction Distillation along Rectified Flow",
    "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.",
    "published": "2025-12-15T15:14:20Z",
    "updated": "2025-12-15T15:14:20Z",
    "link": "http://arxiv.org/pdf/2512.13421v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qingyu Shi",
      "Size Wu",
      "Jinbin Bai",
      "Kaidong Yu",
      "Yujing Wang",
      "Yunhai Tong",
      "Xiangtai Li",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13416v1",
    "title": "Learning to Generate Cross-Task Unexploitable Examples",
    "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.",
    "published": "2025-12-15T15:05:38Z",
    "updated": "2025-12-15T15:05:38Z",
    "link": "http://arxiv.org/pdf/2512.13416v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haoxuan Qu",
      "Qiuchi Xiang",
      "Yujun Cai",
      "Yirui Wu",
      "Majid Mirmehdi",
      "Hossein Rahmani",
      "Jun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13415v1",
    "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition",
    "summary": "Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM",
    "published": "2025-12-15T15:05:16Z",
    "updated": "2025-12-15T15:05:16Z",
    "link": "http://arxiv.org/pdf/2512.13415v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ahmed Abul Hasanaath",
      "Hamzah Luqman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18159v4",
    "title": "Improved Segmentation of Polyps and Visual Explainability Analysis",
    "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.",
    "published": "2025-09-17T02:57:33Z",
    "updated": "2025-12-15T15:03:15Z",
    "link": "http://arxiv.org/pdf/2509.18159v4.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Akwasi Asare",
      "Thanh-Huy Nguyen",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13411v1",
    "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
    "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
    "published": "2025-12-15T15:00:17Z",
    "updated": "2025-12-15T15:00:17Z",
    "link": "http://arxiv.org/pdf/2512.13411v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Patryk Niżeniec",
      "Marcin Iwanowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13397v1",
    "title": "rNCA: Self-Repairing Segmentation Masks",
    "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.",
    "published": "2025-12-15T14:49:55Z",
    "updated": "2025-12-15T14:49:55Z",
    "link": "http://arxiv.org/pdf/2512.13397v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Malte Silbernagel",
      "Albert Alonso",
      "Jens Petersen",
      "Bulat Ibragimov",
      "Marleen de Bruijne",
      "Madeleine K. Wyburd"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.25387v2",
    "title": "Instance-Level Composed Image Retrieval",
    "summary": "The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.",
    "published": "2025-10-29T10:57:59Z",
    "updated": "2025-12-15T14:45:31Z",
    "link": "http://arxiv.org/pdf/2510.25387v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bill Psomas",
      "George Retsinas",
      "Nikos Efthymiadis",
      "Panagiotis Filntisis",
      "Yannis Avrithis",
      "Petros Maragos",
      "Ondrej Chum",
      "Giorgos Tolias"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13392v1",
    "title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs",
    "summary": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/",
    "published": "2025-12-15T14:45:05Z",
    "updated": "2025-12-15T14:45:05Z",
    "link": "http://arxiv.org/pdf/2512.13392v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anran Qi",
      "Changjian Li",
      "Adrien Bousseau",
      "Niloy J. Mitra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05209v2",
    "title": "DEAR: Dataset for Evaluating the Aesthetics of Rendering",
    "summary": "Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).",
    "published": "2025-12-04T19:25:48Z",
    "updated": "2025-12-15T14:43:38Z",
    "link": "http://arxiv.org/pdf/2512.05209v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Vsevolod Plohotnuk",
      "Artyom Panshin",
      "Nikola Banić",
      "Simone Bianco",
      "Michael Freeman",
      "Egor Ershov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11446v2",
    "title": "YawDD+: Frame-level Annotations for Accurate Yawn Prediction",
    "summary": "Driver fatigue remains a leading cause of road accidents, with 24% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6% and mAP by 5% over video-level supervision, achieving 99.34% classification accuracy and 95.69% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.",
    "published": "2025-12-12T10:33:17Z",
    "updated": "2025-12-15T14:34:32Z",
    "link": "http://arxiv.org/pdf/2512.11446v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ahmed Mujtaba",
      "Gleb Radchenko",
      "Marc Masana",
      "Radu Prodan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13376v1",
    "title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"",
    "summary": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.",
    "published": "2025-12-15T14:29:47Z",
    "updated": "2025-12-15T14:29:47Z",
    "link": "http://arxiv.org/pdf/2512.13376v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Carla Monteiro",
      "Valentina Corbetta",
      "Regina Beets-Tan",
      "Luís F. Teixeira",
      "Wilson Silva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13361v1",
    "title": "Automated User Identification from Facial Thermograms with Siamese Networks",
    "summary": "The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.",
    "published": "2025-12-15T14:13:49Z",
    "updated": "2025-12-15T14:13:49Z",
    "link": "http://arxiv.org/pdf/2512.13361v1.pdf",
    "category": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Elizaveta Prozorova",
      "Anton Konev",
      "Vladimir Faerman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23719v3",
    "title": "PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease",
    "summary": "Parkinson's disease (PD) is a common neurodegenerative disorder that severely diminishes patients' quality of life. Its global prevalence has increased markedly in recent decades. Current diagnostic workflows are complex and heavily reliant on neurologists' expertise, often resulting in delays in early detection and missed opportunities for timely intervention. To address these issues, we propose an end-to-end automated diagnostic method for PD, termed PD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly from raw MRI scans. This framework first introduces an MRI Pre-processing Module (MRI-Processor) to mitigate inter-subject and inter-scanner variability by flexibly integrating established medical imaging preprocessing tools. It then incorporates two forms of clinical prior knowledge: (1) Brain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions strongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior), which reflects the accelerated aging typically observed in PD-associated regions. Building on these priors, we design two dedicated modules: the Relevance-Prior Guided Feature Aggregation Module (Aggregator), which guides the model to focus on PD-associated regions at the inter-subject level, and the Age-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps as auxiliary constraints at the intra-subject level to enhance diagnostic accuracy and clinical interpretability. Furthermore, we collected external test data from our collaborating hospital. Experimental results show that PD-Diag-Net achieves 86\\% accuracy on external tests and over 96% accuracy in early-stage diagnosis, outperforming existing advanced methods by more than 20%.",
    "published": "2025-09-28T08:00:03Z",
    "updated": "2025-12-15T13:47:44Z",
    "link": "http://arxiv.org/pdf/2509.23719v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuai Shao",
      "Shu Jiang",
      "Shiyuan Zhao",
      "Di Yang",
      "Yan Wang",
      "Yutong Bai",
      "Jianguo Zhang",
      "Jiangtao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.14401v3",
    "title": "Semantic-Anchored, Class Variance-Optimized Clustering for Robust Semi-Supervised Few-Shot Learning",
    "summary": "Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets. To further establish its robustness, we conduct extensive experiments under challenging conditions, showing that the model generalizes well to domain shifts and achieves new state-of-the-art performance in open-set settings with distractor classes, highlighting its effectiveness for real-world applications.",
    "published": "2025-01-24T11:14:35Z",
    "updated": "2025-12-15T13:43:36Z",
    "link": "http://arxiv.org/pdf/2501.14401v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Souvik Maji",
      "Rhythm Baghel",
      "Pratik Mazumder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.04810v5",
    "title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture",
    "summary": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.",
    "published": "2025-12-04T14:01:53Z",
    "updated": "2025-12-15T13:40:30Z",
    "link": "http://arxiv.org/pdf/2512.04810v5.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xin He",
      "Longhui Wei",
      "Jianbo Ouyang",
      "Minghui Liao",
      "Lingxi Xie",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13313v1",
    "title": "KlingAvatar 2.0 Technical Report",
    "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
    "published": "2025-12-15T13:30:51Z",
    "updated": "2025-12-15T13:30:51Z",
    "link": "http://arxiv.org/pdf/2512.13313v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      " Kling Team",
      "Jialu Chen",
      "Yikang Ding",
      "Zhixue Fang",
      "Kun Gai",
      "Yuan Gao",
      "Kang He",
      "Jingyun Hua",
      "Boyuan Jiang",
      "Mingming Lao",
      "Xiaohan Li",
      "Hui Liu",
      "Jiwen Liu",
      "Xiaoqiang Liu",
      "Yuan Liu",
      "Shun Lu",
      "Yongsen Mao",
      "Yingchao Shao",
      "Huafeng Shi",
      "Xiaoyu Shi",
      "Peiqin Sun",
      "Songlin Tang",
      "Pengfei Wan",
      "Chao Wang",
      "Xuebo Wang",
      "Haoxian Zhang",
      "Yuanxing Zhang",
      "Yan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13303v1",
    "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement",
    "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.",
    "published": "2025-12-15T13:21:50Z",
    "updated": "2025-12-15T13:21:50Z",
    "link": "http://arxiv.org/pdf/2512.13303v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhihang Liu",
      "Xiaoyi Bao",
      "Pandeng Li",
      "Junjie Zhou",
      "Zhaohe Liao",
      "Yefei He",
      "Kaixun Jiang",
      "Chen-Wei Xie",
      "Yun Zheng",
      "Hongtao Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13285v1",
    "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images",
    "summary": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.",
    "published": "2025-12-15T12:48:27Z",
    "updated": "2025-12-15T12:48:27Z",
    "link": "http://arxiv.org/pdf/2512.13285v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bo Liu",
      "Qiao Qin",
      "Qinghui He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13281v1",
    "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
    "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.",
    "published": "2025-12-15T12:41:23Z",
    "updated": "2025-12-15T12:41:23Z",
    "link": "http://arxiv.org/pdf/2512.13281v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaqi Wang",
      "Weijia Wu",
      "Yi Zhan",
      "Rui Zhao",
      "Ming Hu",
      "James Cheng",
      "Wei Liu",
      "Philip Torr",
      "Kevin Qinghong Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13276v1",
    "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing",
    "summary": "Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation",
    "published": "2025-12-15T12:36:50Z",
    "updated": "2025-12-15T12:36:50Z",
    "link": "http://arxiv.org/pdf/2512.13276v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yan Li",
      "Lin Liu",
      "Xiaopeng Zhang",
      "Wei Xue",
      "Wenhan Luo",
      "Yike Guo",
      "Qi Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08334v2",
    "title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting",
    "summary": "Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.",
    "published": "2025-12-09T08:02:30Z",
    "updated": "2025-12-15T12:19:50Z",
    "link": "http://arxiv.org/pdf/2512.08334v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chang Liu",
      "Hongliang Yuan",
      "Lianghao Zhang",
      "Sichao Wang",
      "Jianwei Guo",
      "Shi-Sheng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13262v1",
    "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving",
    "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.",
    "published": "2025-12-15T12:18:50Z",
    "updated": "2025-12-15T12:18:50Z",
    "link": "http://arxiv.org/pdf/2512.13262v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Hyunki Seong",
      "Jeong-Kyun Lee",
      "Heesoo Myeong",
      "Yongho Shin",
      "Hyun-Mook Cho",
      "Duck Hoon Kim",
      "Pranav Desai",
      "Monu Surana"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14945v2",
    "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
    "summary": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
    "published": "2025-10-16T17:55:25Z",
    "updated": "2025-12-15T12:13:31Z",
    "link": "http://arxiv.org/pdf/2510.14945v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "JoungBin Lee",
      "Jaewoo Jung",
      "Jisang Han",
      "Takuya Narihira",
      "Kazumi Fukuda",
      "Junyoung Seo",
      "Sunghwan Hong",
      "Yuki Mitsufuji",
      "Seungryong Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13250v1",
    "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
    "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
    "published": "2025-12-15T12:04:26Z",
    "updated": "2025-12-15T12:04:26Z",
    "link": "http://arxiv.org/pdf/2512.13250v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Juil Koo",
      "Daehyeon Choi",
      "Sangwoo Youn",
      "Phillip Y. Lee",
      "Minhyuk Sung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13247v1",
    "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits",
    "summary": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.",
    "published": "2025-12-15T11:59:01Z",
    "updated": "2025-12-15T11:59:01Z",
    "link": "http://arxiv.org/pdf/2512.13247v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Foivos Paraperas Papantoniou",
      "Stathis Galanakis",
      "Rolandos Alexandros Potamias",
      "Bernhard Kainz",
      "Stefanos Zafeiriou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12793v2",
    "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
    "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.",
    "published": "2025-10-14T17:58:10Z",
    "updated": "2025-12-15T11:56:41Z",
    "link": "http://arxiv.org/pdf/2510.12793v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Long Cui",
      "Weiyun Wang",
      "Jie Shao",
      "Zichen Wen",
      "Gen Luo",
      "Linfeng Zhang",
      "Yanting Zhang",
      "Yu Qiao",
      "Wenhai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13238v1",
    "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance",
    "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.",
    "published": "2025-12-15T11:53:35Z",
    "updated": "2025-12-15T11:53:35Z",
    "link": "http://arxiv.org/pdf/2512.13238v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Francesco Ragusa",
      "Michele Mazzamuto",
      "Rosario Forte",
      "Irene D'Ambra",
      "James Fort",
      "Jakob Engel",
      "Antonino Furnari",
      "Giovanni Maria Farinella"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13192v1",
    "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling",
    "summary": "Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination.",
    "published": "2025-12-15T11:04:09Z",
    "updated": "2025-12-15T11:04:09Z",
    "link": "http://arxiv.org/pdf/2512.13192v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhuo Chen",
      "Chengqun Yang",
      "Zhuo Su",
      "Zheng Lv",
      "Jingnan Gao",
      "Xiaoyuan Zhang",
      "Xiaokang Yang",
      "Yichao Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13191v1",
    "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception",
    "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.",
    "published": "2025-12-15T11:00:38Z",
    "updated": "2025-12-15T11:00:38Z",
    "link": "http://arxiv.org/pdf/2512.13191v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Gong Chen",
      "Chaokun Zhang",
      "Pengcheng Lv",
      "Xiaohui Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13177v1",
    "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
    "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
    "published": "2025-12-15T10:37:59Z",
    "updated": "2025-12-15T10:37:59Z",
    "link": "http://arxiv.org/pdf/2512.13177v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Minghui Hou",
      "Wei-Hsing Huang",
      "Shaofeng Liang",
      "Daizong Liu",
      "Tai-Hao Wen",
      "Gang Wang",
      "Runwei Guan",
      "Weiping Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13175v1",
    "title": "Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation",
    "summary": "Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.",
    "published": "2025-12-15T10:37:05Z",
    "updated": "2025-12-15T10:37:05Z",
    "link": "http://arxiv.org/pdf/2512.13175v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hongxuan Sun",
      "Tao Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.06080v2",
    "title": "CAST-Phys: Contactless Affective States Through Physiological signals Database",
    "summary": "In recent years, affective computing and its applications have become a fast-growing research topic. Despite significant advancements, the lack of affective multi-modal datasets remains a major bottleneck in developing accurate emotion recognition systems. Furthermore, the use of contact-based devices during emotion elicitation often unintentionally influences the emotional experience, reducing or altering the genuine spontaneous emotional response. This limitation highlights the need for methods capable of extracting affective cues from multiple modalities without physical contact, such as remote physiological emotion recognition. To address this, we present the Contactless Affective States Through Physiological Signals Database (CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal remote physiological emotion recognition using facial and physiological cues. The dataset includes diverse physiological signals, such as photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate (RR), alongside high-resolution uncompressed facial video recordings, enabling the potential for remote signal recovery. Our analysis highlights the crucial role of physiological signals in realistic scenarios where facial expressions alone may not provide sufficient emotional information. Furthermore, we demonstrate the potential of remote multi-modal emotion recognition by evaluating the impact of individual and fused modalities, showcasing its effectiveness in advancing contactless emotion recognition technologies.",
    "published": "2025-07-08T15:20:24Z",
    "updated": "2025-12-15T10:34:56Z",
    "link": "http://arxiv.org/pdf/2507.06080v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Joaquim Comas",
      "Alexander Joel Vera",
      "Xavier Vives",
      "Eleonora De Filippi",
      "Alexandre Pereda",
      "Federico Sukno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.21307v2",
    "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
    "summary": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.",
    "published": "2025-10-24T10:05:00Z",
    "updated": "2025-12-15T10:05:39Z",
    "link": "http://arxiv.org/pdf/2510.21307v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bingchen Miao",
      "Rong Wei",
      "Zhiqi Ge",
      "Xiaoquan sun",
      "Shiqi Gao",
      "Jingzhe Zhu",
      "Renhan Wang",
      "Siliang Tang",
      "Jun Xiao",
      "Rui Tang",
      "Juncheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13147v1",
    "title": "StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion",
    "summary": "The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.",
    "published": "2025-12-15T09:56:09Z",
    "updated": "2025-12-15T09:56:09Z",
    "link": "http://arxiv.org/pdf/2512.13147v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sangmin Hong",
      "Suyoung Lee",
      "Kyoung Mu Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13144v1",
    "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models",
    "summary": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.",
    "published": "2025-12-15T09:52:46Z",
    "updated": "2025-12-15T09:52:46Z",
    "link": "http://arxiv.org/pdf/2512.13144v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Chun Kit Wong",
      "Paraskevas Pegios",
      "Nina Weng",
      "Emilie Pi Fogtmann Sejer",
      "Martin Grønnebæk Tolsgaard",
      "Anders Nymark Christensen",
      "Aasa Feragen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20991v3",
    "title": "MR-COSMO: Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation",
    "summary": "The rapid advancement of vision-language models (VLMs) in 3D domains has accelerated research in text-query-guided point cloud processing, though existing methods underperform in point-level segmentation due to inadequate 3D-text alignment that limits local feature-text context linking. To address this limitation, we propose MR-COSMO, a Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation, establishing explicit alignment between 3D point clouds and text/2D image data through a dedicated direct cross-modal alignment module while implementing a visual-text memory module with specialized feature banks. This direct alignment mechanism enables precise fusion of geometric and semantic features, while the memory module employs specialized banks storing text features, visual features, and their correspondence mappings to dynamically enhance scene-specific representations via attention-based knowledge recall. Comprehensive experiments across 3D instruction, reference, and semantic segmentation benchmarks confirm state-of-the-art performance.",
    "published": "2025-06-26T04:10:33Z",
    "updated": "2025-12-15T09:43:49Z",
    "link": "http://arxiv.org/pdf/2506.20991v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chade Li",
      "Pengju Zhang",
      "Yihong Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.02498v3",
    "title": "dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model",
    "summary": "Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce $\\text{dots.ocr}$, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, $\\text{dots.ocr}$ establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.",
    "published": "2025-12-02T07:42:38Z",
    "updated": "2025-12-15T09:43:11Z",
    "link": "http://arxiv.org/pdf/2512.02498v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yumeng Li",
      "Guang Yang",
      "Hao Liu",
      "Bowen Wang",
      "Colin Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13130v1",
    "title": "LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping",
    "summary": "High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.",
    "published": "2025-12-15T09:43:07Z",
    "updated": "2025-12-15T09:43:07Z",
    "link": "http://arxiv.org/pdf/2512.13130v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shanghua Liu",
      "Majharulislam Babor",
      "Christoph Verduyn",
      "Breght Vandenberghe",
      "Bruno Betoni Parodi",
      "Cornelia Weltzien",
      "Marina M. -C. Höhne"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2106.05261v3",
    "title": "We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature",
    "summary": "Recently, object detection has proven vulnerable to adversarial patch attacks. The attackers holding a specially crafted patch can hide themselves from state-of-the-art detectors, e.g., YOLO, even in the physical world. This attack can bring serious security threats, such as escaping from surveillance cameras. How to effectively detect this kind of adversarial examples to catch potential attacks has become an important problem. In this paper, we propose two detection methods: the signature-based method and the signature-independent method. First, we identify two signatures of existing adversarial patches that can be utilized to precisely locate patches within adversarial examples. By employing the signatures, a fast signature-based method is developed to detect the adversarial objects. Second, we present a robust signature-independent method based on the \\textit{content semantics consistency} of model outputs. Adversarial objects violate this consistency, appearing locally but disappearing globally, while benign ones remain consistently present. The experiments demonstrate that two proposed methods can effectively detect attacks both in the digital and physical world. These methods each offer distinct advantage. Specifically, the signature-based method is capable of real-time detection, while the signature-independent method can detect unknown adversarial patch attacks and makes defense-aware attacks almost impossible to perform.",
    "published": "2021-06-09T17:58:08Z",
    "updated": "2025-12-15T09:38:34Z",
    "link": "http://arxiv.org/pdf/2106.05261v3.pdf",
    "category": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Jiachun Li",
      "Jianan Feng",
      "Jianjun Huang",
      "Bin Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.10043v2",
    "title": "FourierSR: A Fourier Token-based Plugin for Efficient Image Super-Resolution",
    "summary": "Image super-resolution (SR) aims to recover low-resolution images to high-resolution images, where improving SR efficiency is a high-profile challenge. However, commonly used units in SR, like convolutions and window-based Transformers, have limited receptive fields, making it challenging to apply them to improve SR under extremely limited computational cost. To address this issue, inspired by modeling convolution theorem through token mix, we propose a Fourier token-based plugin called FourierSR to improve SR uniformly, which avoids the instability or inefficiency of existing token mix technologies when applied as plug-ins. Furthermore, compared to convolutions and windows-based Transformers, our FourierSR only utilizes Fourier transform and multiplication operations, greatly reducing complexity while having global receptive fields. Experimental results show that our FourierSR as a plug-and-play unit brings an average PSNR gain of 0.34dB for existing efficient SR methods on Manga109 test set at the scale of x4, while the average increase in the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. We will release our codes upon acceptance.",
    "published": "2025-03-13T04:50:55Z",
    "updated": "2025-12-15T09:37:14Z",
    "link": "http://arxiv.org/pdf/2503.10043v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenjie Li",
      "Heng Guo",
      "Yuefeng Hou",
      "Zhanyu Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13104v1",
    "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection",
    "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.",
    "published": "2025-12-15T09:01:10Z",
    "updated": "2025-12-15T09:01:10Z",
    "link": "http://arxiv.org/pdf/2512.13104v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yan Zhang",
      "Baoxin Li",
      "Han Sun",
      "Yuhang Gao",
      "Mingtai Zhang",
      "Pei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09561v2",
    "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control",
    "summary": "Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.",
    "published": "2025-10-10T17:13:02Z",
    "updated": "2025-12-15T08:57:20Z",
    "link": "http://arxiv.org/pdf/2510.09561v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Minkyoung Cho",
      "Ruben Ohana",
      "Christian Jacobsen",
      "Adityan Jothi",
      "Min-Hung Chen",
      "Z. Morley Mao",
      "Ethem Can"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13095v1",
    "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning",
    "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.",
    "published": "2025-12-15T08:53:47Z",
    "updated": "2025-12-15T08:53:47Z",
    "link": "http://arxiv.org/pdf/2512.13095v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Feng Zhang",
      "Zezhong Tan",
      "Xinhong Ma",
      "Ziqiang Dong",
      "Xi Leng",
      "Jianfei Zhao",
      "Xin Sun",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07155v3",
    "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
    "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
    "published": "2025-12-08T04:39:12Z",
    "updated": "2025-12-15T08:33:55Z",
    "link": "http://arxiv.org/pdf/2512.07155v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dahyeon Kye",
      "Jeahun Sung",
      "Mingyu Jeon",
      "Jihyong Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13083v1",
    "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation",
    "summary": "In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.",
    "published": "2025-12-15T08:33:44Z",
    "updated": "2025-12-15T08:33:44Z",
    "link": "http://arxiv.org/pdf/2512.13083v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Saumyaranjan Mohanty",
      "Aravind Reddy",
      "Konda Reddy Mopuri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.17171v2",
    "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle",
    "summary": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",
    "published": "2025-11-21T11:45:22Z",
    "updated": "2025-12-15T08:25:07Z",
    "link": "http://arxiv.org/pdf/2511.17171v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Mario Markov",
      "Stefan Maria Ailuro",
      "Luc Van Gool",
      "Konrad Schindler",
      "Danda Pani Paudel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.16221v3",
    "title": "Automatic Calibration of a Multi-Camera System with Limited Overlapping Fields of View for 3D Surgical Scene Reconstruction",
    "summary": "The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise. The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations. We contribute a novel, fast, and fully automatic calibration method based on the projection of multi-scale markers (MSMs) using a ceiling-mounted projector. MSMs consist of 2D patterns projected at varying scales, ensuring accurate extraction of well distributed point correspondences across significantly different viewpoints and zoom levels. Validation is performed using both synthetic and real data captured in a mock-up OR, with comparisons to traditional manual marker-based methods as well as markerless calibration methods. The method achieves accuracy comparable to manual, operator-dependent calibration methods while exhibiting higher robustness under conditions of significant differences in zoom levels. Additionally, we show that state-of-the-art Structure-from-Motion (SfM) pipelines are ineffective in 3D-SSR settings, even when additional texture is projected onto the OR floor. The use of a ceiling-mounted entry-level projector proves to be an effective alternative to operator-dependent, traditional marker-based methods, paving the way for fully automated 3D-SSR.",
    "published": "2025-01-27T17:10:33Z",
    "updated": "2025-12-15T08:15:58Z",
    "link": "http://arxiv.org/pdf/2501.16221v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tim Flückiger",
      "Jonas Hein",
      "Valery Fischer",
      "Philipp Fürnstahl",
      "Lilian Calvet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13072v1",
    "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models",
    "summary": "Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.",
    "published": "2025-12-15T08:09:40Z",
    "updated": "2025-12-15T08:09:40Z",
    "link": "http://arxiv.org/pdf/2512.13072v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zizhi Chen",
      "Yizhen Gao",
      "Minghao Han",
      "Yizhou Liu",
      "Zhaoyu Chen",
      "Dingkang Yang",
      "Lihua Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.19004v2",
    "title": "A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation",
    "summary": "Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.",
    "published": "2025-11-24T11:32:15Z",
    "updated": "2025-12-15T08:08:10Z",
    "link": "http://arxiv.org/pdf/2511.19004v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wentao Qu",
      "Guofeng Mei",
      "Yang Wu",
      "Yongshun Gong",
      "Xiaoshui Huang",
      "Liang Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.12919v3",
    "title": "CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation",
    "summary": "Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.",
    "published": "2025-11-17T03:15:46Z",
    "updated": "2025-12-15T07:42:59Z",
    "link": "http://arxiv.org/pdf/2511.12919v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dexin Zuo",
      "Ang Li",
      "Wei Wang",
      "Wenxian Yu",
      "Danping Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13055v1",
    "title": "Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing",
    "summary": "Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR",
    "published": "2025-12-15T07:30:17Z",
    "updated": "2025-12-15T07:30:17Z",
    "link": "http://arxiv.org/pdf/2512.13055v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jaeyoon Kim",
      "Yoonki Cho",
      "Sung-Eui Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13039v1",
    "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models",
    "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.",
    "published": "2025-12-15T07:08:35Z",
    "updated": "2025-12-15T07:08:35Z",
    "link": "http://arxiv.org/pdf/2512.13039v1.pdf",
    "category": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Hao Chen",
      "Yiwei Wang",
      "Songze Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13031v1",
    "title": "Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs",
    "summary": "This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.",
    "published": "2025-12-15T07:02:06Z",
    "updated": "2025-12-15T07:02:06Z",
    "link": "http://arxiv.org/pdf/2512.13031v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tomoya Tanaka",
      "Tomonori Ikeda",
      "Ryo Yonemoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13030v1",
    "title": "Motus: A Unified Latent Action World Model",
    "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
    "published": "2025-12-15T06:58:40Z",
    "updated": "2025-12-15T06:58:40Z",
    "link": "http://arxiv.org/pdf/2512.13030v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Hongzhe Bi",
      "Hengkai Tan",
      "Shenghao Xie",
      "Zeyuan Wang",
      "Shuhe Huang",
      "Haitian Liu",
      "Ruowen Zhao",
      "Yao Feng",
      "Chendong Xiang",
      "Yinze Rong",
      "Hongyan Zhao",
      "Hanyu Liu",
      "Zhizhong Su",
      "Lei Ma",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21531v2",
    "title": "Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI Reconstruction",
    "summary": "Magnetic resonance imaging (MRI) requires long acquisition times, raising costs, reducing accessibility, and making scans more susceptible to motion artifacts. Diffusion probabilistic models that learn data-driven priors can potentially assist in reducing acquisition time. However, they typically require large training datasets that can be prohibitively expensive to collect. Patch-based diffusion models have shown promise in learning effective data-driven priors over small real-valued datasets, but have not yet demonstrated clinical value in MRI. We extend the Patch-based Diffusion Inverse Solver (PaDIS) to complex-valued, multi-coil MRI reconstruction, and compare it against a state-of-the-art whole-image diffusion baseline (FastMRI-EDM) for 7x undersampled MRI reconstruction on the FastMRI brain dataset. We show that PaDIS-MRI models trained on small datasets of as few as 25 k-space images outperform FastMRI-EDM on image quality metrics (PSNR, SSIM, NRMSE), pixel-level uncertainty, cross-contrast generalization, and robustness to severe k-space undersampling. In a blinded study with three radiologists, PaDIS-MRI reconstructions were chosen as diagnostically superior in 91.7% of cases, compared to baselines (i) FastMRI-EDM and (ii) classical convex reconstruction with wavelet sparsity. These findings highlight the potential of patch-based diffusion priors for high-fidelity MRI reconstruction in data-scarce clinical settings where diagnostic confidence matters.",
    "published": "2025-09-25T20:18:56Z",
    "updated": "2025-12-15T06:47:54Z",
    "link": "http://arxiv.org/pdf/2509.21531v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Rohan Sanda",
      "Asad Aali",
      "Andrew Johnston",
      "Eduardo Reis",
      "Gordon Wetzstein",
      "Sara Fridovich-Keil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11574v2",
    "title": "Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps",
    "summary": "While recent Gaussian-based SLAM methods achieve photorealistic reconstruction from RGB-D data, their computational performance remains a critical bottleneck. State-of-the-art techniques operate at less than 20 fps, significantly lagging behind geometry-based approaches like KinectFusion (hundreds of fps). This limitation stems from the heavy computational burden: modeling scenes requires numerous Gaussians and complex iterative optimization to fit RGB-D data; insufficient Gaussian counts or optimization iterations cause severe quality degradation. To address this, we propose a Gaussian-SDF hybrid representation, combining a colorized signed distance field (SDF) for smooth geometry and appearance with 3D Gaussians to capture underrepresented details. The SDF is efficiently constructed via RGB-D fusion (as in geometry-based methods), while Gaussians undergo iterative optimization. Our representation enables significant Gaussian reduction (50% fewer) by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization (75% fewer iterations) through targeted appearance refinement. Building upon this representation, we develop GPS-SLAM (Gaussian-plus-SDF SLAM), a real-time 3D reconstruction system achieving over 150 fps on real-world Azure Kinect sequences, faster by an order-of-magnitude than state-of-the-art techniques while maintaining comparable reconstruction quality. The source code and data are available at https://gapszju.github.io/GPS-SLAM.",
    "published": "2025-09-15T04:37:32Z",
    "updated": "2025-12-15T06:47:33Z",
    "link": "http://arxiv.org/pdf/2509.11574v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhexi Peng",
      "Kun Zhou",
      "Tianjia Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01950v4",
    "title": "DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes",
    "summary": "We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation. Project page: https://eku127.github.io/DualMap/",
    "published": "2025-06-02T17:59:10Z",
    "updated": "2025-12-15T06:43:59Z",
    "link": "http://arxiv.org/pdf/2506.01950v4.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Jiajun Jiang",
      "Yiming Zhu",
      "Zirui Wu",
      "Jie Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13019v1",
    "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
    "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
    "published": "2025-12-15T06:32:57Z",
    "updated": "2025-12-15T06:32:57Z",
    "link": "http://arxiv.org/pdf/2512.13019v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cheeun Hong",
      "German Barquero",
      "Fadime Sener",
      "Markos Georgopoulos",
      "Edgar Schönfeld",
      "Stefan Popov",
      "Yuming Du",
      "Oscar Mañas",
      "Albert Pumarola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13018v1",
    "title": "Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing",
    "summary": "This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.",
    "published": "2025-12-15T06:29:51Z",
    "updated": "2025-12-15T06:29:51Z",
    "link": "http://arxiv.org/pdf/2512.13018v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Tomoya Tanaka",
      "Tomonori Ikeda",
      "Ryo Yonemoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13015v1",
    "title": "What Happens Next? Next Scene Prediction with a Unified Video Model",
    "summary": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.",
    "published": "2025-12-15T06:22:57Z",
    "updated": "2025-12-15T06:22:57Z",
    "link": "http://arxiv.org/pdf/2512.13015v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinjie Li",
      "Zhimin Chen",
      "Rui Zhao",
      "Florian Schiffers",
      "Zhenyu Liao",
      "Vimal Bhat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13014v1",
    "title": "JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion",
    "summary": "Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.",
    "published": "2025-12-15T06:21:30Z",
    "updated": "2025-12-15T06:21:30Z",
    "link": "http://arxiv.org/pdf/2512.13014v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haoyu Wang",
      "Lei Zhang",
      "Wenrui Liu",
      "Dengyang Jiang",
      "Wei Wei",
      "Chen Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13008v1",
    "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading",
    "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.",
    "published": "2025-12-15T06:08:16Z",
    "updated": "2025-12-15T06:08:16Z",
    "link": "http://arxiv.org/pdf/2512.13008v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xi Luo",
      "Shixin Xu",
      "Ying Xie",
      "JianZhong Hu",
      "Yuwei He",
      "Yuhui Deng",
      "Huaxiong Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13007v1",
    "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects",
    "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.",
    "published": "2025-12-15T06:04:49Z",
    "updated": "2025-12-15T06:04:49Z",
    "link": "http://arxiv.org/pdf/2512.13007v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nikolai Goncharov",
      "James L. Gray",
      "Donald G. Dansereau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13006v1",
    "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
    "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.",
    "published": "2025-12-15T05:58:36Z",
    "updated": "2025-12-15T05:58:36Z",
    "link": "http://arxiv.org/pdf/2512.13006v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yifan Pu",
      "Yizeng Han",
      "Zhiwei Tang",
      "Jiasheng Tang",
      "Fan Wang",
      "Bohan Zhuang",
      "Gao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11203v2",
    "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path",
    "summary": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.",
    "published": "2025-12-12T01:28:22Z",
    "updated": "2025-12-15T05:13:40Z",
    "link": "http://arxiv.org/pdf/2512.11203v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhengyang Yu",
      "Akio Hayakawa",
      "Masato Ishii",
      "Qingtao Yu",
      "Takashi Shibuya",
      "Jing Zhang",
      "Yuki Mitsufuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05115v2",
    "title": "Light-X: Generative 4D Video Rendering with Camera and Illumination Control",
    "summary": "Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.",
    "published": "2025-12-04T18:59:57Z",
    "updated": "2025-12-15T05:06:31Z",
    "link": "http://arxiv.org/pdf/2512.05115v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianqi Liu",
      "Zhaoxi Chen",
      "Zihao Huang",
      "Shaocong Xu",
      "Saining Zhang",
      "Chongjie Ye",
      "Bohan Li",
      "Zhiguo Cao",
      "Wei Li",
      "Hao Zhao",
      "Ziwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12984v1",
    "title": "VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs",
    "summary": "We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/",
    "published": "2025-12-15T05:01:59Z",
    "updated": "2025-12-15T05:01:59Z",
    "link": "http://arxiv.org/pdf/2512.12984v1.pdf",
    "category": [
      "cs.CG",
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Jiayin Lu",
      "Ying Jiang",
      "Yin Yang",
      "Chenfanfu Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13668v1",
    "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation",
    "summary": "Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.",
    "published": "2025-12-15T18:55:39Z",
    "updated": "2025-12-15T18:55:39Z",
    "link": "http://arxiv.org/pdf/2512.13668v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Guoqing Liu",
      "Junren Li",
      "Zihan Zhao",
      "Eray Inanc",
      "Krzysztof Maziarz",
      "Jose Garrido Torres",
      "Victor Garcia Satorras",
      "Shoko Ueda",
      "Christopher M. Bishop",
      "Marwin Segler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13666v1",
    "title": "SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work",
    "summary": "The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.",
    "published": "2025-12-15T18:55:20Z",
    "updated": "2025-12-15T18:55:20Z",
    "link": "http://arxiv.org/pdf/2512.13666v1.pdf",
    "category": [
      "cs.CR",
      "cs.DC",
      "cs.IT",
      "cs.LG"
    ],
    "authors": [
      "Weihang Cao",
      "Mustafa Doger",
      "Sennur Ulukus"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13634v1",
    "title": "Universality of high-dimensional scaling limits of stochastic gradient descent",
    "summary": "We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.",
    "published": "2025-12-15T18:30:26Z",
    "updated": "2025-12-15T18:30:26Z",
    "link": "http://arxiv.org/pdf/2512.13634v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST"
    ],
    "authors": [
      "Reza Gheissari",
      "Aukosh Jagannath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05948v2",
    "title": "Developing synthetic microdata through machine learning for firm-level business surveys",
    "summary": "Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.",
    "published": "2025-12-05T18:44:30Z",
    "updated": "2025-12-15T18:28:42Z",
    "link": "http://arxiv.org/pdf/2512.05948v2.pdf",
    "category": [
      "cs.LG",
      "econ.GN",
      "stat.AP",
      "stat.ME"
    ],
    "authors": [
      "Jorge Cisneros",
      "Timothy Wojan",
      "Matthew Williams",
      "Jennifer Ozawa",
      "Robert Chew",
      "Kimberly Janda",
      "Timothy Navarro",
      "Michael Floyd",
      "Christine Task",
      "Damon Streat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13632v1",
    "title": "StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion",
    "summary": "Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve \"Modality Collapse\", an \"Echo Chamber\" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.",
    "published": "2025-12-15T18:28:39Z",
    "updated": "2025-12-15T18:28:39Z",
    "link": "http://arxiv.org/pdf/2512.13632v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Guransh Singh",
      "Md Shah Fahad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13617v1",
    "title": "LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification",
    "summary": "Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.",
    "published": "2025-12-15T18:09:59Z",
    "updated": "2025-12-15T18:09:59Z",
    "link": "http://arxiv.org/pdf/2512.13617v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ankit Sharma",
      "Sayan Roy Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13593v1",
    "title": "Scalable Formal Verification via Autoencoder Latent Space Abstraction",
    "summary": "Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.",
    "published": "2025-12-15T17:48:07Z",
    "updated": "2025-12-15T17:48:07Z",
    "link": "http://arxiv.org/pdf/2512.13593v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Robert Reed",
      "Morteza Lahijanian",
      "Luca Laurenti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06208v2",
    "title": "SparsePixels: Efficient Convolution for Sparse Data on FPGAs",
    "summary": "Inference of standard convolutional neural networks (CNNs) on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value. However, input features can be spatially sparse in some image data, where semantic information may occupy only a small fraction of the pixels and most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework that implements sparse convolution on FPGAs by selectively retaining and computing on a small subset of active pixels while ignoring the rest. We show that, for identifying neutrino interactions in naturally sparse LArTPC images with 4k pixels, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture, computing on less than 1% of the input pixels, achieves a $\\times 73$ speedup to 0.665 $μ$s with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. This work aims to benefit future algorithm development for efficient data readout in modern experiments with latency requirements of microseconds or below.",
    "published": "2025-12-05T23:04:44Z",
    "updated": "2025-12-15T17:47:05Z",
    "link": "http://arxiv.org/pdf/2512.06208v2.pdf",
    "category": [
      "cs.AR",
      "cs.LG",
      "hep-ex"
    ],
    "authors": [
      "Ho Fung Tsoi",
      "Dylan Rankin",
      "Vladimir Loncar",
      "Philip Harris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03604v3",
    "title": "Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training",
    "summary": "Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning (PEFT) methods address these by freezing most model parameters and training only a small subset. However, PEFT often underperforms compared to full fine-tuning when high task-specific accuracy is required. Zeroth-Order (ZO) methods fine-tune the entire pre-trained model without back-propagation, estimating gradients through forward passes only. While memory-efficient, ZO methods suffer from slow convergence and high sensitivity to prompt selection. We bridge these two worlds with Bilevel-ZOFO, a bilevel optimization method that couples fast, local FO-PEFT adaptation at the inner level with stable, memory-efficient ZO updates of the full backbone at the outer level. The FO-PEFT inner loop performs fast, low-memory local adaptation that reduces the variance of ZO estimates and stabilizes the search, guiding the outer ZO updates of the full backbone and reducing prompt sensitivity. In the mean time, the outer ZO provides better generalization ability for PEFT. We provide theoretical convergence guarantees and empirically demonstrate that Bilevel-ZOFO significantly outperforms existing ZO and FO-PEFT methods, achieving 2-4 times faster training while maintaining similar memory efficiency. Additionally, we show by updating the backbone with ZO and adapting only a tiny FO-PEFT block per task, Bilevel-ZOFO combines full-model capacity with few-shot efficiency, making it a very efficient meta-learning algorithm that quickly adapts to new tasks.",
    "published": "2025-02-05T20:47:44Z",
    "updated": "2025-12-15T17:47:03Z",
    "link": "http://arxiv.org/pdf/2502.03604v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Reza Shirkavand",
      "Peiran Yu",
      "Qi He",
      "Heng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13565v1",
    "title": "A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees",
    "summary": "This paper tackles the problem of feature selection in a highly challenging setting: $\\mathbb{E}(y | \\boldsymbol{x}) = G(\\boldsymbol{x}_{\\mathcal{S}_0})$, where $\\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \\log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.",
    "published": "2025-12-15T17:22:49Z",
    "updated": "2025-12-15T17:22:49Z",
    "link": "http://arxiv.org/pdf/2512.13565v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Junye Du",
      "Zhenghao Li",
      "Zhutong Gu",
      "Long Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21983v2",
    "title": "Improving Generative Ad Text on Facebook using Reinforcement Learning",
    "summary": "Generative artificial intelligence (AI), in particular large language models (LLMs), is poised to drive transformative economic change. LLMs are pre-trained on vast text data to learn general language patterns, but a subsequent post-training phase is critical to align them for specific real-world tasks. Reinforcement learning (RL) is the leading post-training technique, yet its economic impact remains largely underexplored and unquantified. We examine this question through the lens of the first deployment of an RL-trained LLM for generative advertising on Facebook. Integrated into Meta's Text Generation feature, our model, \"AdLlama,\" powers an AI tool that helps advertisers create new variations of human-written ad text. To train this model, we introduce reinforcement learning with performance feedback (RLPF), a post-training method that uses historical ad performance data as a reward signal. In a large-scale 10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad variations, we find that AdLlama improves click-through rates by 6.7% (p=0.0296) compared to a supervised imitation model trained on curated ads. This represents a substantial improvement in advertiser return on investment on Facebook. We also find that advertisers who used AdLlama generated more ad variations, indicating higher satisfaction with the model's outputs. To our knowledge, this is the largest study to date on the use of generative AI in an ecologically valid setting, offering an important data point quantifying the tangible impact of RL post-training. Furthermore, the results show that RLPF is a promising and generalizable approach for metric-driven post-training that bridges the gap between highly capable language models and tangible outcomes.",
    "published": "2025-07-29T16:34:02Z",
    "updated": "2025-12-15T17:21:22Z",
    "link": "http://arxiv.org/pdf/2507.21983v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Daniel R. Jiang",
      "Alex Nikulkov",
      "Yu-Chia Chen",
      "Yang Bai",
      "Zheqing Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13532v1",
    "title": "Adaptive Sampling for Hydrodynamic Stability",
    "summary": "An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.",
    "published": "2025-12-15T17:00:09Z",
    "updated": "2025-12-15T17:00:09Z",
    "link": "http://arxiv.org/pdf/2512.13532v1.pdf",
    "category": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "authors": [
      "Anshima Singh",
      "David J. Silvester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13530v1",
    "title": "Actively Learning Joint Contours of Multiple Computer Experiments",
    "summary": "Contour location$\\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a \"joint contour location\" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.",
    "published": "2025-12-15T17:00:04Z",
    "updated": "2025-12-15T17:00:04Z",
    "link": "http://arxiv.org/pdf/2512.13530v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG"
    ],
    "authors": [
      "Shih-Ni Prim",
      "Kevin R. Quinlan",
      "Paul Hawkins",
      "Jagadeesh Movva",
      "Annie S. Booth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13529v1",
    "title": "Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning",
    "summary": "Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.",
    "published": "2025-12-15T16:59:13Z",
    "updated": "2025-12-15T16:59:13Z",
    "link": "http://arxiv.org/pdf/2512.13529v1.pdf",
    "category": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "authors": [
      "Raj Sahu",
      "Saumen Maiti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13526v1",
    "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
    "summary": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
    "published": "2025-12-15T16:56:28Z",
    "updated": "2025-12-15T16:56:28Z",
    "link": "http://arxiv.org/pdf/2512.13526v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Asa Cooper Stickland",
      "Jan Michelfeit",
      "Arathi Mani",
      "Charlie Griffin",
      "Ollie Matthews",
      "Tomek Korbak",
      "Rogan Inglis",
      "Oliver Makins",
      "Alan Cooney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13517v1",
    "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
    "summary": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.",
    "published": "2025-12-15T16:43:50Z",
    "updated": "2025-12-15T16:43:50Z",
    "link": "http://arxiv.org/pdf/2512.13517v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.LG"
    ],
    "authors": [
      "Raymond Khazoum",
      "Daniela Fernandes",
      "Aleksandr Krylov",
      "Qin Li",
      "Stephane Deny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12483v2",
    "title": "Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks",
    "summary": "This study compares the Boundary Element Method (BEM) and Physics-Informed Neural Networks (PINNs) for solving the two-dimensional Helmholtz equation in wave scattering problems. The objective is to evaluate the performance of both methods under the same conditions. We solve the Helmholtz equation using BEM and PINNs for the same scattering problem. PINNs are trained by minimizing the residual of the governing equations and boundary conditions with their configuration determined through hyperparameter optimization, while BEM is applied using boundary discretization. Both methods are evaluated in terms of solution accuracy and computation time. We conducted numerical experiments by varying the number of boundary integration points for the BEM and the number of hidden layers and neurons per layer for the PINNs. We performed a hyperparameter tuning to identify an adequate PINN configuration for this problem as a network with 3 hidden layers and 25 neurons per layer, using a learning rate of $10^{-2}$ and a sine activation function. At comparable levels of accuracy, the assembly and solution of the BEM system required a computational time on the order of $10^{-2}$~s, whereas the training time of the PINN was on the order of $10^{2}$~s, corresponding to a difference of approximately four orders of magnitude. However, once trained, the PINN achieved evaluation times on the order of $10^{-2}$~s, which is about two orders of magnitude faster than the evaluation of the BEM solution at interior points. This work establishes a procedure for comparing BEM and PINNs. It also presents a direct comparison between the two methods for the scattering problem. The analysis provides quantitative data on their performance, supporting their use in future research on wave propagation problems and outlining challenges and directions for further investigation.",
    "published": "2025-09-15T22:08:20Z",
    "updated": "2025-12-15T16:39:42Z",
    "link": "http://arxiv.org/pdf/2509.12483v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Oscar Rincón-Cardeno",
      "Gregorio Pérez Bernal",
      "Silvana Montoya Noguera",
      "Nicolás Guarín-Zapata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13506v1",
    "title": "Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource",
    "summary": "Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.",
    "published": "2025-12-15T16:34:47Z",
    "updated": "2025-12-15T16:34:47Z",
    "link": "http://arxiv.org/pdf/2512.13506v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Sofiya Zaichyk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13491v1",
    "title": "From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis",
    "summary": "We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.",
    "published": "2025-12-15T16:25:06Z",
    "updated": "2025-12-15T16:25:06Z",
    "link": "http://arxiv.org/pdf/2512.13491v1.pdf",
    "category": [
      "cs.IT",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Łukasz Dębowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13482v1",
    "title": "Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency",
    "summary": "Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.",
    "published": "2025-12-15T16:18:36Z",
    "updated": "2025-12-15T16:18:36Z",
    "link": "http://arxiv.org/pdf/2512.13482v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG"
    ],
    "authors": [
      "Wenyi Liu",
      "R. Sharma",
      "W. \"Grace\" Guo",
      "J. Yi",
      "Y. B. Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13480v1",
    "title": "Element-wise Modulation of Random Matrices for Efficient Neural Layers",
    "summary": "Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.",
    "published": "2025-12-15T16:16:53Z",
    "updated": "2025-12-15T16:16:53Z",
    "link": "http://arxiv.org/pdf/2512.13480v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Maksymilian Szorc"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08625v3",
    "title": "An upper bound of the silhouette validation metric for clustering",
    "summary": "The silhouette coefficient quantifies, for each observation, the balance between within-cluster cohesion and between-cluster separation, taking values in [-1, 1]. The average silhouette width (ASW ) is a widely used internal measure of clustering quality, with higher values indicating more cohesive and well-separated clusters. However, the dataset-specific maximum of ASW is typically unknown, and the standard upper limit of 1 is rarely attainable. In this work, we derive for each data point a sharp upper bound on its silhouette width and aggregate these to obtain a canonical upper bound on the ASW. This bound-often substantially below 1-enhances the interpretability of empirical ASW values by providing guidance on how close a given clustering result is to the best possible outcome for that dataset. We evaluate the usefulness of the upper bound on a variety of datasets and conclude that it can meaningfully enrich cluster quality evaluation, but its practical relevance depends on the given dataset. Finally, we extend the framework to establish an upper bound of the macro-averaged silhouette.",
    "published": "2025-09-10T14:20:38Z",
    "updated": "2025-12-15T16:10:19Z",
    "link": "http://arxiv.org/pdf/2509.08625v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hugo Sträng",
      "Tai Dinh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.03378v7",
    "title": "Understanding and Improving Shampoo and SOAP via Kullback-Leibler Minimization",
    "summary": "Shampoo and its efficient variant, SOAP, employ structured second-moment estimations and have shown strong performance for training neural networks (NNs). In practice, however, Shampoo typically requires step-size grafting with Adam to be competitive, and SOAP mitigates this by applying Adam in Shampoo's eigenbasis -- at the cost of additional memory overhead from Adam in both methods. Prior analyses have largely relied on the Frobenius norm to motivate these estimation schemes. We instead recast their estimation procedures as covariance estimation under Kullback-Leibler (KL) divergence minimization, revealing a previously overlooked theoretical limitation and motivating principled redesigns. Building on this perspective, we develop $\\textbf{KL-Shampoo}$ and $\\textbf{KL-SOAP}$, practical schemes that match or exceed the performance of Shampoo and SOAP in NN pre-training while achieving SOAP-level per-iteration runtime. Notably, KL-Shampoo does not rely on Adam to attain competitive performance, eliminating the memory overhead introduced by Adam. Across our experiments, KL-Shampoo consistently outperforms SOAP, Shampoo, and even KL-SOAP, establishing the KL-based approach as a promising foundation for designing structured methods in NN optimization.",
    "published": "2025-09-03T14:55:15Z",
    "updated": "2025-12-15T16:00:01Z",
    "link": "http://arxiv.org/pdf/2509.03378v7.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Wu Lin",
      "Scott C. Lowe",
      "Felix Dangel",
      "Runa Eschenhagen",
      "Zikun Xu",
      "Roger B. Grosse"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13460v1",
    "title": "DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems",
    "summary": "Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.",
    "published": "2025-12-15T15:56:58Z",
    "updated": "2025-12-15T15:56:58Z",
    "link": "http://arxiv.org/pdf/2512.13460v1.pdf",
    "category": [
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Chethana Prasad Kabgere",
      "Shylaja S S"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.18303v2",
    "title": "Deep-ER: Deep Learning ECCENTRIC Reconstruction for fast high-resolution neurometabolic imaging",
    "summary": "Introduction: Altered neurometabolism is an important pathological mechanism in many neurological diseases and brain cancer, which can be mapped non-invasively by Magnetic Resonance Spectroscopic Imaging (MRSI). Advanced MRSI using non-cartesian compressed-sense acquisition enables fast high-resolution metabolic imaging but has lengthy reconstruction times that limits throughput and needs expert user interaction. Here, we present a robust and efficient Deep Learning reconstruction to obtain high-quality metabolic maps.\n  Methods: Fast high-resolution whole-brain metabolic imaging was performed at 3.4 mm$^3$ isotropic resolution with acquisition times between 4:11-9:21 min:s using ECCENTRIC pulse sequence on a 7T MRI scanner. Data were acquired in a high-resolution phantom and 27 human participants, including 22 healthy volunteers and 5 glioma patients. A deep neural network using recurring interlaced convolutional layers with joint dual-space feature representation was developed for deep learning ECCENTRIC reconstruction (Deep-ER). 21 subjects were used for training and 6 subjects for testing. Deep-ER performance was compared to conventional iterative Total Generalized Variation reconstruction using image and spectral quality metrics.\n  Results: Deep-ER demonstrated 600-fold faster reconstruction than conventional methods, providing improved spatial-spectral quality and metabolite quantification with 12%-45% (P<0.05) higher signal-to-noise and 8%-50% (P<0.05) smaller Cramer-Rao lower bounds. Metabolic images clearly visualize glioma tumor heterogeneity and boundary.\n  Conclusion: Deep-ER provides efficient and robust reconstruction for sparse-sampled MRSI. The accelerated acquisition-reconstruction MRSI is compatible with high-throughput imaging workflow. It is expected that such improved performance will facilitate basic and clinical MRSI applications.",
    "published": "2024-09-26T21:20:51Z",
    "updated": "2025-12-15T15:55:07Z",
    "link": "http://arxiv.org/pdf/2409.18303v2.pdf",
    "category": [
      "eess.IV",
      "cs.LG"
    ],
    "authors": [
      "Paul Weiser",
      "Georg Langs",
      "Wolfgang Bogner",
      "Stanislav Motyka",
      "Bernhard Strasser",
      "Polina Golland",
      "Nalini Singh",
      "Jorg Dietrich",
      "Erik Uhlmann",
      "Tracy Batchelor",
      "Daniel Cahill",
      "Malte Hoffmann",
      "Antoine Klauser",
      "Ovidiu C. Andronesi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13442v1",
    "title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders",
    "summary": "In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.",
    "published": "2025-12-15T15:39:59Z",
    "updated": "2025-12-15T15:39:59Z",
    "link": "http://arxiv.org/pdf/2512.13442v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Khawla Elhadri",
      "Jörg Schlötterer",
      "Christin Seifert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.11184v2",
    "title": "On the failure of ReLU activation for physics-informed machine learning",
    "summary": "Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.",
    "published": "2025-12-12T00:14:59Z",
    "updated": "2025-12-15T15:22:16Z",
    "link": "http://arxiv.org/pdf/2512.11184v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Conor Rowan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22332v2",
    "title": "Credal Prediction based on Relative Likelihood",
    "summary": "Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction.",
    "published": "2025-05-28T13:20:20Z",
    "updated": "2025-12-15T15:19:55Z",
    "link": "http://arxiv.org/pdf/2505.22332v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Timo Löhr",
      "Paul Hofman",
      "Felix Mohr",
      "Eyke Hüllermeier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2303.16158v3",
    "title": "Behavioral Machine Learning? Regularization and Forecast Bias",
    "summary": "Standard forecast efficiency tests interpret violations as evidence of behavioral bias. We show theoretically and empirically that rational forecasters using optimal regularization systematically violate these tests. Machine learning forecasts show near zero bias at one year horizon, but strong overreaction at two years, consistent with predictions from a model of regularization and measurement noise. We provide three complementary tests: experimental variation in regularization parameters, cross-sectional heterogeneity in firm signal quality, and quasi-experimental evidence from ML adoption around 2013. Technically trained analysts shift sharply toward overreaction post-2013. Our findings suggest reported violations may reflect statistical sophistication rather than cognitive failure.",
    "published": "2023-03-25T03:06:43Z",
    "updated": "2025-12-15T15:05:52Z",
    "link": "http://arxiv.org/pdf/2303.16158v3.pdf",
    "category": [
      "q-fin.ST",
      "cs.LG",
      "econ.GN",
      "q-fin.GN"
    ],
    "authors": [
      "Murray Z. Frank",
      "Jing Gao",
      "Keer Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.14296v2",
    "title": "\"All of Me\": Mining Users' Attributes from their Public Spotify Playlists",
    "summary": "In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. In this work, we aim to address the question: can we infer users' private attributes from their public Spotify playlists? To this end, we conducted an online survey involving 739 Spotify users, resulting in a dataset of 10,286 publicly shared playlists comprising over 200,000 unique songs and 55,000 artists. Then, we utilize statistical analyses and machine learning algorithms to build accurate predictive models for users' attributes.",
    "published": "2024-01-25T16:38:06Z",
    "updated": "2025-12-15T15:00:26Z",
    "link": "http://arxiv.org/pdf/2401.14296v2.pdf",
    "category": [
      "cs.CR",
      "cs.LG",
      "cs.SI"
    ],
    "authors": [
      "Pier Paolo Tricomi",
      "Luca Pajola",
      "Luca Pasa",
      "Mauro Conti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13410v1",
    "title": "Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks",
    "summary": "While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.",
    "published": "2025-12-15T15:00:13Z",
    "updated": "2025-12-15T15:00:13Z",
    "link": "http://arxiv.org/pdf/2512.13410v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Vítor M. Hanriot",
      "Luiz C. B. Torres",
      "Antônio P. Braga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.08410v2",
    "title": "A Deep-Learning Iterative Stacked Approach for Prediction of Reactive Dissolution in Porous Media",
    "summary": "Simulating reactive dissolution of solid minerals in porous media has many subsurface applications, including carbon capture and storage (CCS), geothermal systems and oil & gas recovery. As traditional direct numerical simulators are computationally expensive, it is of paramount importance to develop faster and more efficient alternatives. Deep-learning-based solutions, most of them built upon convolutional neural networks (CNNs), have been recently designed to tackle this problem. However, these solutions were limited to approximating one field over the domain (e.g. velocity field). In this manuscript, we present a novel deep learning approach that incorporates both temporal and spatial information to predict the future states of the dissolution process at a fixed time-step horizon, given a sequence of input states. The overall performance, in terms of speed and prediction accuracy, is demonstrated on a numerical simulation dataset, comparing its prediction results against state-of-the-art approaches, also achieving a speedup around $10^4$ over traditional numerical simulators.",
    "published": "2025-03-11T13:18:03Z",
    "updated": "2025-12-15T14:54:33Z",
    "link": "http://arxiv.org/pdf/2503.08410v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Marcos Cirne",
      "Hannah Menke",
      "Alhasan Abdellatif",
      "Julien Maes",
      "Florian Doster",
      "Ahmed H. Elsheikh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2207.11437v3",
    "title": "The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks",
    "summary": "In the logic synthesis stage, structure transformations in the synthesis tool need to be combined into optimization sequences and act on the circuit to meet the specified circuit area and delay. However, logic synthesis optimization sequences are time-consuming to run, and predicting the quality of the results (QoR) against the synthesis optimization sequence for a circuit can help engineers find a better optimization sequence faster. In this work, we propose a deep learning method to predict the QoR of unseen circuit-optimization sequences pairs. Specifically, the structure transformations are translated into vectors by embedding methods and advanced natural language processing (NLP) technology (Transformer) is used to extract the features of the optimization sequences. In addition, to enable the prediction process of the model to be generalized from circuit to circuit, the graph representation of the circuit is represented as an adjacency matrix and a feature matrix. Graph neural networks(GNN) are used to extract the structural features of the circuits. For this problem, the Transformer and three typical GNNs are used. Furthermore, the Transformer and GNNs are adopted as a joint learning policy for the QoR prediction of the unseen circuit-optimization sequences. The methods resulting from the combination of Transformer and GNNs are benchmarked. The experimental results show that the joint learning of Transformer and GraphSage gives the best results. The Mean Absolute Error (MAE) of the predicted result is 0.412.",
    "published": "2022-07-23T06:53:57Z",
    "updated": "2025-12-15T14:45:23Z",
    "link": "http://arxiv.org/pdf/2207.11437v3.pdf",
    "category": [
      "cs.AR",
      "cs.LG"
    ],
    "authors": [
      "Chenghao Yang",
      "Zhongda Wang",
      "Yinshui Xia",
      "Zhufei Chu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.19557v3",
    "title": "Neural stochastic Volterra equations: learning path-dependent dynamics",
    "summary": "Stochastic Volterra equations (SVEs) serve as mathematical models for the time evolutions of random systems with memory effects and irregular behaviour. We introduce neural stochastic Volterra equations as a physics-inspired architecture, generalizing the class of neural stochastic differential equations, and provide some theoretical foundation. Numerical experiments on various SVEs, like the disturbed pendulum equation, the generalized Ornstein--Uhlenbeck process, the rough Heston model and a monetary reserve dynamics, are presented, comparing the performance of neural SVEs, neural SDEs and Deep Operator Networks (DeepONets).",
    "published": "2024-07-28T18:44:49Z",
    "updated": "2025-12-15T14:36:12Z",
    "link": "http://arxiv.org/pdf/2407.19557v3.pdf",
    "category": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "authors": [
      "Martin Bergerhausen",
      "David J. Prömel",
      "David Scheffels"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13381v1",
    "title": "Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction",
    "summary": "Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.",
    "published": "2025-12-15T14:32:47Z",
    "updated": "2025-12-15T14:32:47Z",
    "link": "http://arxiv.org/pdf/2512.13381v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Changjun Zhou",
      "Jintao Zheng",
      "Leyou Yang",
      "Pengfei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13359v1",
    "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles",
    "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.",
    "published": "2025-12-15T14:12:32Z",
    "updated": "2025-12-15T14:12:32Z",
    "link": "http://arxiv.org/pdf/2512.13359v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Sümer Tunçay",
      "Alain Andres",
      "Ignacio Carlucho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13340v1",
    "title": "Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks",
    "summary": "The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.",
    "published": "2025-12-15T13:54:38Z",
    "updated": "2025-12-15T13:54:38Z",
    "link": "http://arxiv.org/pdf/2512.13340v1.pdf",
    "category": [
      "cs.LG",
      "cs.NI"
    ],
    "authors": [
      "Henrik C. M. Frederiksen",
      "Junya Shiraishi",
      "Cedomir Stefanovic",
      "Hei Victor Cheng",
      "Shashi Raj Pandey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13337v1",
    "title": "FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs",
    "summary": "Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the \"right to be forgotten.\" To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.",
    "published": "2025-12-15T13:53:12Z",
    "updated": "2025-12-15T13:53:12Z",
    "link": "http://arxiv.org/pdf/2512.13337v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Si Qi Goh",
      "Yongsen Zheng",
      "Ziyao Liu",
      "Sami Hormi",
      "Kwok-Yan Lam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13336v1",
    "title": "KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers",
    "summary": "This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.",
    "published": "2025-12-15T13:51:20Z",
    "updated": "2025-12-15T13:51:20Z",
    "link": "http://arxiv.org/pdf/2512.13336v1.pdf",
    "category": [
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Karim Bounja",
      "Lahcen Laayouni",
      "Abdeljalil Sakat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2011.10690v2",
    "title": "Adaptive Risk Mitigation in Demand Learning",
    "summary": "We study dynamic pricing of a product with an unknown demand distribution over a finite horizon. Departing from the standard no-regret learning environment in which prices can be adjusted at any time, we restrict price changes to predetermined points in time to reflect common retail practice. This constraint, coupled with demand model ambiguity and an unknown customer arrival pattern, imposes a high risk of revenue loss, as a price based on a misestimated demand model may be applied to many customers before it can be revised. We develop an adaptive risk learning (ARL) framework that embeds a data-driven ambiguity set (DAS) to quantify demand model ambiguity by adapting to the unknown arrival pattern. Initially, when arrivals are few, the DAS includes a broad set of plausible demand models, reflecting high ambiguity and revenue risk. As new data is collected through pricing, the DAS progressively shrinks, capturing the reduction in model ambiguity and associated risk. We establish the probabilistic convergence of the DAS to the true demand model and derive a regret bound for the ARL policy that explicitly links revenue loss to the data required for the DAS to identify the true model with high probability. The dependence of our regret bound on the arrival pattern is unique to our constrained dynamic pricing problem and contrasts with no-regret learning environments, where regret is constant and arrival-pattern independent. Relaxing the constraint on infrequent price changes, we show that ARL attains the known constant regret bound. Numerical experiments further demonstrate that ARL outperforms benchmarks that prioritize either regret or risk alone by adaptively balancing both without knowledge of the arrival pattern. This adaptive risk adjustment is crucial for achieving high revenues and low downside risk when prices are sticky and both demand and arrival patterns are unknown.",
    "published": "2020-11-21T01:15:54Z",
    "updated": "2025-12-15T13:25:58Z",
    "link": "http://arxiv.org/pdf/2011.10690v2.pdf",
    "category": [
      "cs.LG",
      "cs.IR"
    ],
    "authors": [
      "Parshan Pakiman",
      "Boxiao Chen",
      "Selvaprabu Nadarajah",
      "Stefanus Jasin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10770v2",
    "title": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers",
    "summary": "Retrosynthesis reaction prediction aims to infer plausible reactant molecules for a given product and is a important problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. In this paper, we present a template-free, Transformer-based framework that removes the need for handcrafted reaction templates or additional chemical rule engines. Our model injects molecular graph information into the attention mechanism to jointly exploit SMILES sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. Extensive experiments on the USPTO-50K benchmark demonstrate that our approach achieves state-of-the-art performance among template-free methods and substantially outperforms a vanilla Transformer baseline.",
    "published": "2025-12-11T16:08:32Z",
    "updated": "2025-12-15T12:53:39Z",
    "link": "http://arxiv.org/pdf/2512.10770v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Youjun Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13255v1",
    "title": "BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation",
    "summary": "We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.",
    "published": "2025-12-15T12:09:32Z",
    "updated": "2025-12-15T12:09:32Z",
    "link": "http://arxiv.org/pdf/2512.13255v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yunhong Min",
      "Juil Koo",
      "Seungwoo Yoo",
      "Minhyuk Sung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.01223v13",
    "title": "Statistical Taylor Expansion: A New and Path-Independent Method for Uncertainty Analysis",
    "summary": "As a rigorous statistical approach, statistical Taylor expansion extends the conventional Taylor expansion by replacing precise input variables with random variables of known distributions, to compute means and standard deviations of the results. Statistical Taylor expansion traces the dependency of the input uncertainties in the intermediate steps, so that the variables in the intermediate analytic expressions can no longer be regarded as independent of each other, and the result of the analytic expression is path independent. Thus, it differs fundamentally from the conventional common approaches in applied mathematics which optimize execution path for each calculation. In fact, statistical Taylor expansion may standardize numerical calculations for analytic expressions. Its statistical nature allows religious testing of its result when the sample size is large enough. This paper also introduces an implementation of statistical Taylor expansion called variance arithmetic and presents corresponding test results in a very wide range of mathematical applications.\n  Another important conclusion of this paper is that the numerical errors in the library function can have significant effects on the result. For example, the periodic numerical errors in the trigonometric library functions can resonate with periodic signals, producing large numerical errors in the results.",
    "published": "2024-10-02T04:02:21Z",
    "updated": "2025-12-15T11:52:52Z",
    "link": "http://arxiv.org/pdf/2410.01223v13.pdf",
    "category": [
      "stat.CO",
      "cs.LG"
    ],
    "authors": [
      "Chengpu Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13237v1",
    "title": "Learning to Retrieve with Weakened Labels: Robust Training under Label Noise",
    "summary": "Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.",
    "published": "2025-12-15T11:52:13Z",
    "updated": "2025-12-15T11:52:13Z",
    "link": "http://arxiv.org/pdf/2512.13237v1.pdf",
    "category": [
      "cs.LG",
      "cs.IR"
    ],
    "authors": [
      "Arnab Sharma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13228v1",
    "title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data",
    "summary": "Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.",
    "published": "2025-12-15T11:43:14Z",
    "updated": "2025-12-15T11:43:14Z",
    "link": "http://arxiv.org/pdf/2512.13228v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Melvin Barbaux"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13227v1",
    "title": "Better LMO-based Momentum Methods with Second-Order Information",
    "summary": "The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.",
    "published": "2025-12-15T11:43:09Z",
    "updated": "2025-12-15T11:43:09Z",
    "link": "http://arxiv.org/pdf/2512.13227v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Sarit Khirirat",
      "Abdurakhmon Sadiev",
      "Yury Demidovich",
      "Peter Richtárik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13217v1",
    "title": "Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures",
    "summary": "We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.",
    "published": "2025-12-15T11:31:41Z",
    "updated": "2025-12-15T11:31:41Z",
    "link": "http://arxiv.org/pdf/2512.13217v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Lorenzo Sabug",
      "Eric Kerrigan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13207v1",
    "title": "Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting",
    "summary": "Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\\% degradation) but fails against patch attacks (281-603\\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.",
    "published": "2025-12-15T11:22:24Z",
    "updated": "2025-12-15T11:22:24Z",
    "link": "http://arxiv.org/pdf/2512.13207v1.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Karina Chichifoi",
      "Fabio Merizzi",
      "Michele Colajanni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13197v1",
    "title": "MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking",
    "summary": "Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.",
    "published": "2025-12-15T11:13:21Z",
    "updated": "2025-12-15T11:13:21Z",
    "link": "http://arxiv.org/pdf/2512.13197v1.pdf",
    "category": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "authors": [
      "Ayrat Abdullin",
      "Umair bin Waheed",
      "Leo Eisner",
      "Naveed Iqbal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13196v1",
    "title": "Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning",
    "summary": "Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.",
    "published": "2025-12-15T11:10:40Z",
    "updated": "2025-12-15T11:10:40Z",
    "link": "http://arxiv.org/pdf/2512.13196v1.pdf",
    "category": [
      "cs.LG",
      "cs.AR"
    ],
    "authors": [
      "Chethana Prasad Kabgere",
      "Sudarshan T S B"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.11250v2",
    "title": "CRPS-Based Targeted Sequential Design with Application in Chemical Space",
    "summary": "Sequential design of real and computer experiments via Gaussian Process (GP) models has proven useful for parsimonious, goal-oriented data acquisition purposes. In this work, we focus on acquisition strategies for a GP model that needs to be accurate within a predefined range of the response of interest. Such an approach is useful in various fields including synthetic chemistry, where finding molecules with particular properties is essential for developing useful materials and effective medications. GP modeling and sequential design of experiments have been successfully applied to a plethora of domains, including molecule research. Our main contribution here is to use the threshold-weighted Continuous Ranked Probability Score (CRPS) as a basic building block for acquisition functions employed within sequential design. We study pointwise and integral criteria relying on two different weighting measures and benchmark them against competitors, demonstrating improved performance with respect to considered goals. The resulting acquisition strategies are applicable to a wide range of fields and pave the way to further developing sequential design relying on scoring rules.",
    "published": "2025-03-14T10:00:24Z",
    "updated": "2025-12-15T10:58:38Z",
    "link": "http://arxiv.org/pdf/2503.11250v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.CO"
    ],
    "authors": [
      "Lea Friedli",
      "Athénaïs Gautier",
      "Anna Broccard",
      "David Ginsbourger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06463v2",
    "title": "Error Estimate and Convergence Analysis for Data Valuation",
    "summary": "Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.",
    "published": "2025-11-09T17:15:40Z",
    "updated": "2025-12-15T10:50:29Z",
    "link": "http://arxiv.org/pdf/2511.06463v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhangyong Liang",
      "Huanhuan Gao",
      "Ji Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13170v1",
    "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks",
    "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.",
    "published": "2025-12-15T10:30:40Z",
    "updated": "2025-12-15T10:30:40Z",
    "link": "http://arxiv.org/pdf/2512.13170v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG",
      "eess.SY",
      "math.OC"
    ],
    "authors": [
      "Deepak Ingole",
      "Valentin Bhend",
      "Shiva Ganesh Murali",
      "Oliver Dobrich",
      "Alisa Rupenayan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18482v3",
    "title": "Reliability-Adjusted Prioritized Experience Replay",
    "summary": "Experience replay enables data-efficient learning from past experiences in online reinforcement learning agents. Traditionally, experiences were sampled uniformly from a replay buffer, regardless of differences in experience-specific learning potential. In an effort to sample more efficiently, researchers introduced Prioritized Experience Replay (PER). In this paper, we propose an extension to PER by introducing a novel measure of temporal difference error reliability. We theoretically show that the resulting transition selection algorithm, Reliability-adjusted Prioritized Experience Replay (ReaPER), enables more efficient learning than PER. We further present empirical results showing that ReaPER outperforms PER across various environment types, including the Atari-10 benchmark.",
    "published": "2025-06-23T10:35:36Z",
    "updated": "2025-12-15T10:13:42Z",
    "link": "http://arxiv.org/pdf/2506.18482v3.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Leonard S. Pleiss",
      "Tobias Sutter",
      "Maximilian Schiffer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13149v1",
    "title": "Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency",
    "summary": "Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT",
    "published": "2025-12-15T10:00:25Z",
    "updated": "2025-12-15T10:00:25Z",
    "link": "http://arxiv.org/pdf/2512.13149v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Xinwei Tai",
      "Dongmian Zou",
      "Hongfei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09683v2",
    "title": "Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting?",
    "summary": "In Time Series Forecasting (TSF), the lookback window (the length of historical data used for prediction) is a critical hyperparameter that is often set arbitrarily, undermining the validity of model evaluations. We argue that the lookback window must be tuned on a per-task basis to ensure fair comparisons. Our empirical results show that failing to do so can invert performance rankings, particularly when comparing univariate and multivariate methods. Experiments on standard benchmarks reposition Channel-Independent (CI) models, such as PatchTST, as state-of-the-art methods. However, we reveal this superior performance is largely an artifact of weak inter-channel correlations and simplicity of patterns within these specific datasets. Using Granger causality analysis and ODE datasets (with implicit channel correlations), we demonstrate that the true strength of multivariate Channel-Dependent (CD) models emerges on datasets with strong, inherent cross-channel dependencies, where they significantly outperform CI models. We conclude with three key recommendations for improving TSF research: (i) treat the lookback window as a critical hyperparameter to be tuned, (ii) use statistical analysis of datasets to inform the choice between CI and CD architectures, and (iii) favor CD models in applications with limited data.",
    "published": "2025-02-13T13:35:10Z",
    "updated": "2025-12-15T09:52:51Z",
    "link": "http://arxiv.org/pdf/2502.09683v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ibram Abdelmalak",
      "Kiran Madhusudhanan",
      "Jungmin Choi",
      "Christian Kloetergens",
      "Vijaya Krishna Yalavarit",
      "Maximilian Stubbemann",
      "Lars Schmidt-Thieme"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13125v1",
    "title": "Quanvolutional Neural Networks for Spectrum Peak-Finding",
    "summary": "The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\\% improvement in F1 score and a 30\\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.",
    "published": "2025-12-15T09:33:54Z",
    "updated": "2025-12-15T09:33:54Z",
    "link": "http://arxiv.org/pdf/2512.13125v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Lukas Bischof",
      "Rudolf M. Füchslin",
      "Kurt Stockinger",
      "Pavel Sulimov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07490v2",
    "title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent",
    "summary": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.",
    "published": "2025-12-08T12:17:40Z",
    "updated": "2025-12-15T09:28:22Z",
    "link": "http://arxiv.org/pdf/2512.07490v2.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Zhiyu Liu",
      "Zhi Han",
      "Yandong Tang",
      "Jun Fan",
      "Yao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13123v1",
    "title": "Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences",
    "summary": "We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\\varepsilon$-optimal with probability at least $1-α$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.",
    "published": "2025-12-15T09:26:45Z",
    "updated": "2025-12-15T09:26:45Z",
    "link": "http://arxiv.org/pdf/2512.13123v1.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "authors": [
      "Liviu Aolaritei",
      "Michael I. Jordan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13120v1",
    "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation",
    "summary": "Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.",
    "published": "2025-12-15T09:19:23Z",
    "updated": "2025-12-15T09:19:23Z",
    "link": "http://arxiv.org/pdf/2512.13120v1.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Mabiao Long",
      "Jiaxi Liu",
      "Yufeng Li",
      "Hao Xiong",
      "Junchi Yan",
      "Kefan Wang",
      "Yi Cao",
      "Jiandong Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13093v1",
    "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
    "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.",
    "published": "2025-12-15T08:50:20Z",
    "updated": "2025-12-15T08:50:20Z",
    "link": "http://arxiv.org/pdf/2512.13093v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Mingqi Yuan",
      "Tao Yu",
      "Haolin Song",
      "Bo Li",
      "Xin Jin",
      "Hua Chen",
      "Wenjun Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13077v1",
    "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
    "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
    "published": "2025-12-15T08:18:42Z",
    "updated": "2025-12-15T08:18:42Z",
    "link": "http://arxiv.org/pdf/2512.13077v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Md Awsafur Rahman",
      "Adam Gabrys",
      "Doug Kang",
      "Jingjing Sun",
      "Tian Tan",
      "Ashwin Chandramouli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13069v1",
    "title": "Multi-fidelity aerodynamic data fusion by autoencoder transfer learning",
    "summary": "Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.",
    "published": "2025-12-15T08:06:52Z",
    "updated": "2025-12-15T08:06:52Z",
    "link": "http://arxiv.org/pdf/2512.13069v1.pdf",
    "category": [
      "cs.LG",
      "physics.flu-dyn",
      "stat.ML"
    ],
    "authors": [
      "Javier Nieto-Centenero",
      "Esther Andrés",
      "Rodrigo Castellanos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09535v2",
    "title": "Latent-Autoregressive GP-VAE Language Model",
    "summary": "We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.",
    "published": "2025-12-10T11:18:44Z",
    "updated": "2025-12-15T08:05:00Z",
    "link": "http://arxiv.org/pdf/2512.09535v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yves Ruffenach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.04522v2",
    "title": "End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit",
    "summary": "With our recently proposed method based on reinforcement learning (Mayfrank et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained for optimal performance in specific (economic) nonlinear model predictive control ((e)NMPC) applications. So far, our method has exclusively been demonstrated on a small-scale case study. Herein, we show that our method scales well to a more challenging demand response case study built on a large-scale model of a single-product (nitrogen) air separation unit. Across all numerical experiments, we assume observability of only a few realistically measurable plant variables. Compared to a purely system identification-based Koopman eNMPC, which generates small economic savings but frequently violates constraints, our method delivers similar economic performance while avoiding constraint violations.",
    "published": "2025-11-06T16:35:32Z",
    "updated": "2025-12-15T07:55:20Z",
    "link": "http://arxiv.org/pdf/2511.04522v2.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Daniel Mayfrank",
      "Kayra Dernek",
      "Laura Lang",
      "Alexander Mitsos",
      "Manuel Dahmen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13060v1",
    "title": "Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments",
    "summary": "This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.",
    "published": "2025-12-15T07:38:47Z",
    "updated": "2025-12-15T07:38:47Z",
    "link": "http://arxiv.org/pdf/2512.13060v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kangning Gao",
      "Yi Hu",
      "Cong Nie",
      "Wei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13037v1",
    "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer",
    "summary": "In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.",
    "published": "2025-12-15T07:07:32Z",
    "updated": "2025-12-15T07:07:32Z",
    "link": "http://arxiv.org/pdf/2512.13037v1.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Taoran Sheng",
      "Sathappan Muthiah",
      "Atiq Islam",
      "Jinming Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13034v1",
    "title": "Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization",
    "summary": "This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.",
    "published": "2025-12-15T07:04:51Z",
    "updated": "2025-12-15T07:04:51Z",
    "link": "http://arxiv.org/pdf/2512.13034v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xiaoyu He",
      "Yu Cai",
      "Jin Jia",
      "Canxi Huang",
      "Wenqing Chen",
      "Zibin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.24070v2",
    "title": "HACTS: a Human-As-Copilot Teleoperation System for Robot Learning",
    "summary": "Teleoperation is essential for autonomous robot learning, especially in manipulation tasks that require human demonstrations or corrections. However, most existing systems only offer unilateral robot control and lack the ability to synchronize the robot's status with the teleoperation hardware, preventing real-time, flexible intervention. In this work, we introduce HACTS (Human-As-Copilot Teleoperation System), a novel system that establishes bilateral, real-time joint synchronization between a robot arm and teleoperation hardware. This simple yet effective feedback mechanism, akin to a steering wheel in autonomous vehicles, enables the human copilot to intervene seamlessly while collecting action-correction data for future learning. Implemented using 3D-printed components and low-cost, off-the-shelf motors, HACTS is both accessible and scalable. Our experiments show that HACTS significantly enhances performance in imitation learning (IL) and reinforcement learning (RL) tasks, boosting IL recovery capabilities and data efficiency, and facilitating human-in-the-loop RL. HACTS paves the way for more effective and interactive human-robot collaboration and data-collection, advancing the capabilities of robot manipulation.",
    "published": "2025-03-31T13:28:13Z",
    "updated": "2025-12-15T07:01:07Z",
    "link": "http://arxiv.org/pdf/2503.24070v2.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Zhiyuan Xu",
      "Yinuo Zhao",
      "Kun Wu",
      "Ning Liu",
      "Junjie Ji",
      "Zhengping Che",
      "Chi Harold Liu",
      "Jian Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.12914v3",
    "title": "Trojan Cleansing with Neural Collapse",
    "summary": "Trojan attacks are sophisticated training-time attacks on neural networks that embed backdoor triggers which force the network to produce a specific output on any input which includes the trigger. With the increasing relevance of deep networks which are too large to train with personal resources and which are trained on data too large to thoroughly audit, these training-time attacks pose a significant risk. In this work, we connect trojan attacks to Neural Collapse, a phenomenon wherein the final feature representations of over-parameterized neural networks converge to a simple geometric structure. We provide experimental evidence that trojan attacks disrupt this convergence for a variety of datasets and architectures. We then use this disruption to design a lightweight, broadly generalizable mechanism for cleansing trojan attacks from a wide variety of different network architectures and experimentally demonstrate its efficacy.",
    "published": "2024-11-19T22:57:40Z",
    "updated": "2025-12-15T06:59:52Z",
    "link": "http://arxiv.org/pdf/2411.12914v3.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Xihe Gu",
      "Greg Fields",
      "Yaman Jandali",
      "Tara Javidi",
      "Farinaz Koushanfar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22082v2",
    "title": "Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning",
    "summary": "Federated Learning (FL) enables collaborative training while preserving privacy, yet Gradient Inversion Attacks (GIAs) pose severe threats by reconstructing private data from shared gradients. In realistic FedAvg scenarios with multi-step updates, existing surrogate methods like SME rely on linear interpolation to approximate client trajectories for privacy leakage. However, we demonstrate that linear assumptions fundamentally underestimate SGD's nonlinear complexity, encountering irreducible approximation barriers in non-convex landscapes with only one-dimensional expressiveness. We propose Non-Linear Surrogate Model Extension (NL-SME), the first framework introducing learnable quadratic Bézier curves for trajectory modeling in GIAs against FL. NL-SME leverages $|w|+1$-dimensional control point parameterization combined with dvec scaling and regularization mechanisms to achieve superior approximation accuracy. Extensive experiments on CIFAR-100 and FEMNIST demonstrate NL-SME significantly outperforms baselines across all metrics, achieving 94\\%--98\\% performance gaps and order-of-magnitude improvements in cosine similarity loss while maintaining computational efficiency. This work exposes critical privacy vulnerabilities in FL's multi-step paradigm and provides insights for robust defense development.",
    "published": "2025-09-26T09:04:25Z",
    "updated": "2025-12-15T06:52:47Z",
    "link": "http://arxiv.org/pdf/2509.22082v2.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Li Xia",
      "Jing Yu",
      "Zheng Liu",
      "Sili Huang",
      "Wei Tang",
      "Xuan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.27562v2",
    "title": "Optimal Convergence Analysis of DDPM for General Distributions",
    "summary": "Score-based diffusion models have achieved remarkable empirical success in generating high-quality samples from target data distributions. Among them, the Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used samplers, generating samples via estimated score functions. Despite its empirical success, a tight theoretical understanding of DDPM -- especially its convergence properties -- remains limited.\n  In this paper, we provide a refined convergence analysis of the DDPM sampler and establish near-optimal convergence rates under general distributional assumptions. Specifically, we introduce a relaxed smoothness condition parameterized by a constant $L$, which is small for many practical distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler with accurate score estimates achieves a convergence rate of $$\\widetilde{O}\\left(\\frac{d\\min\\{d,L^2\\}}{T^2}\\right)~\\text{in Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the number of iterations, and $\\widetilde{O}$ hides polylogarithmic factors in $T$. This result substantially improves upon the best-known $d^2/T^2$ rate when $L < \\sqrt{d}$. By establishing a matching lower bound, we show that our convergence analysis is tight for a wide array of target distributions. Moreover, it reveals that DDPM and DDIM share the same dependence on $d$, raising an interesting question of why DDIM often appears empirically faster.",
    "published": "2025-10-31T15:44:50Z",
    "updated": "2025-12-15T06:50:03Z",
    "link": "http://arxiv.org/pdf/2510.27562v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Yuchen Jiao",
      "Yuchen Zhou",
      "Gen Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10427v2",
    "title": "Renormalizable Spectral-Shell Dynamics as the Origin of Neural Scaling Laws",
    "summary": "Neural scaling laws and double-descent phenomena suggest that deep-network training obeys a simple macroscopic structure despite highly nonlinear optimization dynamics. We derive such structure directly from gradient descent in function space. For mean-squared error loss, the training error evolves as $\\dot e_t=-M(t)e_t$ with $M(t)=J_{θ(t)}J_{θ(t)}^{\\!*}$, a time-dependent self-adjoint operator induced by the network Jacobian. Using Kato perturbation theory, we obtain an exact system of coupled modewise ODEs in the instantaneous eigenbasis of $M(t)$.\n  To extract macroscopic behavior, we introduce a logarithmic spectral-shell coarse-graining and track quadratic error energy across shells. Microscopic interactions within each shell cancel identically at the energy level, so shell energies evolve only through dissipation and external inter-shell interactions. We formalize this via a \\emph{renormalizable shell-dynamics} assumption, under which cumulative microscopic effects reduce to a controlled net flux across shell boundaries.\n  Assuming an effective power-law spectral transport in a relevant resolution range, the shell dynamics admits a self-similar solution with a moving resolution frontier and explicit scaling exponents. This framework explains neural scaling laws and double descent, and unifies lazy (NTK-like) training and feature learning as two limits of the same spectral-shell dynamics.",
    "published": "2025-12-11T08:38:46Z",
    "updated": "2025-12-15T06:45:10Z",
    "link": "http://arxiv.org/pdf/2512.10427v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yizhou Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07857v2",
    "title": "SA$^{2}$GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation",
    "summary": "We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA$^{2}$GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA$^{2}$GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.",
    "published": "2025-11-26T08:26:01Z",
    "updated": "2025-12-15T06:32:08Z",
    "link": "http://arxiv.org/pdf/2512.07857v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Junhua Shi",
      "Qingyun Sun",
      "Haonan Yuan",
      "Xingcheng Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.07209v3",
    "title": "Enhancing Physics-Informed Neural Networks Through Feature Engineering",
    "summary": "Physics-Informed Neural Networks (PINNs) seek to solve partial differential equations (PDEs) with deep learning. Mainstream approaches that deploy fully-connected multi-layer deep learning architectures require prolonged training to achieve even moderate accuracy, while recent work on feature engineering allows higher accuracy and faster convergence. This paper introduces SAFE-NET, a Single-layered Adaptive Feature Engineering NETwork that achieves orders-of-magnitude lower errors with far fewer parameters than baseline feature engineering methods. SAFE-NET returns to basic ideas in machine learning, using Fourier features, a simplified single hidden layer network architecture, and an effective optimizer that improves the conditioning of the PINN optimization problem. Numerical results show that SAFE-NET converges faster and typically outperforms deeper networks and more complex architectures. It consistently uses fewer parameters -- on average, 65% fewer than the competing feature engineering methods -- while achieving comparable accuracy in less than 30% of the training epochs. Moreover, each SAFE-NET epoch is 95% faster than those of competing feature engineering approaches. These findings challenge the prevailing belief that modern PINNs effectively learn features in these scientific applications and highlight the efficiency gains possible through feature engineering.",
    "published": "2025-02-11T03:07:28Z",
    "updated": "2025-12-15T06:21:54Z",
    "link": "http://arxiv.org/pdf/2502.07209v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shaghayegh Fazliani",
      "Zachary Frangella",
      "Madeleine Udell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13010v1",
    "title": "Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)",
    "summary": "The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.",
    "published": "2025-12-15T06:13:25Z",
    "updated": "2025-12-15T06:13:25Z",
    "link": "http://arxiv.org/pdf/2512.13010v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.TO"
    ],
    "authors": [
      "Hassan Iftikhar",
      "Rizwan Ahmad",
      "Arunark Kolipaka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.02780v6",
    "title": "Data Collaboration Analysis with Orthonormal Basis Selection and Alignment",
    "summary": "Data Collaboration (DC) enables multiple parties to jointly train a model by sharing only linear projections of their private datasets. The core challenge in DC is to align the bases of these projections without revealing each party's secret basis. While existing theory suggests that any target basis spanning the common subspace should suffice, in practice, the choice of basis can substantially affect both accuracy and numerical stability. We introduce Orthonormal Data Collaboration (ODC), which enforces orthonormal secret and target bases, thereby reducing alignment to the classical Orthogonal Procrustes problem, which admits a closed-form solution. We prove that the resulting change-of-basis matrices achieve \\emph{orthogonal concordance}, aligning all parties' representations up to a shared orthogonal transform and rendering downstream performance invariant to the target basis. Computationally, ODC reduces the alignment complexity from O(\\min{a(cl)^2,a^2c}) to O(acl^2), and empirical evaluations show up to \\(100\\times\\) speed-ups with equal or better accuracy across benchmarks. ODC preserves DC's one-round communication pattern and privacy assumptions, providing a simple and efficient drop-in improvement to existing DC pipelines.",
    "published": "2024-03-05T08:52:16Z",
    "updated": "2025-12-15T06:06:06Z",
    "link": "http://arxiv.org/pdf/2403.02780v6.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Keiyu Nosaka",
      "Yamato Suetake",
      "Yuichi Takano",
      "Akiko Yoshise"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.23083v3",
    "title": "Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory",
    "summary": "High-capacity kernel Hopfield networks exhibit a \\textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \\textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \\textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.",
    "published": "2025-11-28T11:14:15Z",
    "updated": "2025-12-15T06:01:17Z",
    "link": "http://arxiv.org/pdf/2511.23083v3.pdf",
    "category": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "authors": [
      "Akira Tamamori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13003v1",
    "title": "General OOD Detection via Model-aware and Subspace-aware Variable Priority",
    "summary": "Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.",
    "published": "2025-12-15T05:55:35Z",
    "updated": "2025-12-15T05:55:35Z",
    "link": "http://arxiv.org/pdf/2512.13003v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Min Lu",
      "Hemant Ishwaran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.11614v2",
    "title": "On the physics of nested Markov models: a generalized probabilistic theory perspective",
    "summary": "Determining potential probability distributions with a given causal graph is vital for causality studies. To bypass the difficulty in characterizing latent variables in a Bayesian network, the nested Markov model provides an elegant algebraic approach by listing exactly all the equality constraints on the observed variables. However, this algebraically motivated causal model comprises distributions outside Bayesian networks, and its physical interpretation remains vague. In this work, we inspect the nested Markov model through the lens of generalized probabilistic theory, an axiomatic framework to describe general physical theories. We prove that all the equality constraints defining the nested Markov model are valid theory-independently. At the same time, not every distribution within the nested Markov model is implementable, not even via exotic physical theories associated with generalized probability theories (GPTs). To interpret the origin of such a gap, we study three causal models standing between the nested Markov model and the set of all distributions admitting some GPT realization. Each of the successive three models gives a strictly tighter characterization of the physically implementable distribution set; that is, each successive model manifests new types of GPT-inviolable constraints. We further demonstrate each gap through a specially chosen illustrative causal structure. We anticipate our results will enlighten further explorations on the unification of algebraic and physical perspectives of causality.",
    "published": "2024-11-18T14:40:58Z",
    "updated": "2025-12-15T05:28:06Z",
    "link": "http://arxiv.org/pdf/2411.11614v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Xingjian Zhang",
      "Yuhao Wang",
      "Elie Wolfe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08772v2",
    "title": "De novo generation of functional terpene synthases using TpsGPT",
    "summary": "Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.",
    "published": "2025-12-09T16:29:53Z",
    "updated": "2025-12-15T05:09:36Z",
    "link": "http://arxiv.org/pdf/2512.08772v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hamsini Ramanathan",
      "Roman Bushuiev",
      "Matouš Soldát",
      "Jirí Kohout",
      "Téo Hebra",
      "Joshua David Smith",
      "Josef Sivic",
      "Tomáš Pluskal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13169v1",
    "title": "Integrated Semantic and Temporal Alignment for Interactive Video Retrieval",
    "summary": "The growing volume of video data and the introduction of complex retrieval challenges, such as the Temporal Retrieval and Alignment of Key Events (TRAKE) task at the Ho Chi Minh City AI Challenge 2025, expose critical limitations in existing systems. Many methodologies lack scalable, holistic architectures and rely on \"frozen\" embedding models that fail on out-of-knowledge (OOK) or real-world queries. This paper introduces the comprehensive video retrieval framework developed by team AIO\\_Owlgorithms to address these gaps. Our system features an architecture integrating TransNetV2 for scene segmentation, BEiT-3 for visual embeddings in Milvus, and Gemini OCR for metadata in Elasticsearch. We propose two components: (1) \\textbf{QUEST} (Query Understanding and External Search for Out-of-Knowledge Tasks), a two-branch framework that leverages a Large Language Model (LLM) for query rewriting and an external image search pathway to resolve OOK queries; and (2) \\textbf{DANTE} (Dynamic Alignment of Narrative Temporal Events), a dynamic programming algorithm that efficiently solves the temporally-incoherent TRAKE task. These contributions form a robust and intelligent system that significantly advances the state-of-the-art in handling complex, real-world video search queries.",
    "published": "2025-12-15T10:30:28Z",
    "updated": "2025-12-15T10:30:28Z",
    "link": "http://arxiv.org/pdf/2512.13169v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Thanh-Danh Luu",
      "Le-Vu Nguyen Dinh",
      "Duc-Thien Tran",
      "Duy-Bao Bui",
      "Nam-Tien Le",
      "Tinh-Anh Nguyen Nhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13670v1",
    "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks",
    "summary": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial",
    "published": "2025-12-15T18:56:34Z",
    "updated": "2025-12-15T18:56:34Z",
    "link": "http://arxiv.org/pdf/2512.13670v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Licheng Luo",
      "Yu Xia",
      "Kaier Liang",
      "Mingyu Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13561v1",
    "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments",
    "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.",
    "published": "2025-12-15T17:18:04Z",
    "updated": "2025-12-15T17:18:04Z",
    "link": "http://arxiv.org/pdf/2512.13561v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Li-Wei Shih",
      "Ruo-Syuan Mei",
      "Jesse Heidrich",
      "Hui-Ping Wang",
      "Joel Hooton",
      "Joshua Solomon",
      "Jorge Arinez",
      "Guangze Li",
      "Chenhui Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13514v1",
    "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM",
    "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.",
    "published": "2025-12-15T16:42:48Z",
    "updated": "2025-12-15T16:42:48Z",
    "link": "http://arxiv.org/pdf/2512.13514v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Aman Arora",
      "Matteo El-Hariry",
      "Miguel Olivares-Mendez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13477v1",
    "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model",
    "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.",
    "published": "2025-12-15T16:14:22Z",
    "updated": "2025-12-15T16:14:22Z",
    "link": "http://arxiv.org/pdf/2512.13477v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Timothy A. Brumfiel",
      "Revanth Konda",
      "Drew Elliott",
      "Jaydev P. Desai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13380v1",
    "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.",
    "published": "2025-12-15T14:32:03Z",
    "updated": "2025-12-15T14:32:03Z",
    "link": "http://arxiv.org/pdf/2512.13380v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chuan Mao",
      "Haoqi Yuan",
      "Ziye Huang",
      "Chaoyi Xu",
      "Kai Ma",
      "Zongqing Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13304v1",
    "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories",
    "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.",
    "published": "2025-12-15T13:22:16Z",
    "updated": "2025-12-15T13:22:16Z",
    "link": "http://arxiv.org/pdf/2512.13304v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Sait Sovukluk",
      "Johannes Englsberger",
      "Christian Ott"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.17441v2",
    "title": "RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation",
    "summary": "Bimanual manipulation is essential for achieving human-like dexterity in robots, but the large-scale and diverse bimanual robot datasets remain scarce due to hardware heterogeneity across robotic platforms. To address the challenge, we present RoboCOIN, a comprehensive multi-embodiment bimanual manipulation dataset with over 180,000 demonstrations collected from 15 distinct robotic platforms. The dataset covers 16 scenarios, including residential, commercial, and working environments, with 421 tasks systematically organized by bimanual coordination patterns and object properties. Our key innovation is a hierarchical capability pyramid that provides multi-level annotations, spanning trajectory-level concepts, segment-level subtasks, and frame-level kinematics. We further develop CoRobot, a comprehensive processing framework featuring Robot Trajectory Markup Language (RTML) for quality assessment, automated annotation generation, and unified multi-embodiment management. Extensive experiments demonstrate the reliability and effectiveness of RoboCOIN in multi-embodiment bimanual learning, with significant performance improvements across various model architectures and robotic platforms. The complete dataset and framework are open-sourced and publicly available for further research purposes. Project website: https://FlagOpen.github.io/RoboCOIN/.",
    "published": "2025-11-21T17:39:22Z",
    "updated": "2025-12-15T12:58:44Z",
    "link": "http://arxiv.org/pdf/2511.17441v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shihan Wu",
      "Xuecheng Liu",
      "Shaoxuan Xie",
      "Pengwei Wang",
      "Xinghang Li",
      "Bowen Yang",
      "Zhe Li",
      "Kai Zhu",
      "Hongyu Wu",
      "Yiheng Liu",
      "Zhaoye Long",
      "Yue Wang",
      "Chong Liu",
      "Dihan Wang",
      "Ziqiang Ni",
      "Xiang Yang",
      "You Liu",
      "Ruoxuan Feng",
      "Runtian Xu",
      "Lei Zhang",
      "Denghang Huang",
      "Chenghao Jin",
      "Anlan Yin",
      "Xinlong Wang",
      "Zhenguo Sun",
      "Junkai Zhao",
      "Mengfei Du",
      "Mingyu Cao",
      "Xiansheng Chen",
      "Hongyang Cheng",
      "Xiaojie Zhang",
      "Yankai Fu",
      "Ning Chen",
      "Cheng Chi",
      "Sixiang Chen",
      "Huaihai Lyu",
      "Xiaoshuai Hao",
      "Yequan Wang",
      "Bo Lei",
      "Dong Liu",
      "Xi Yang",
      "Yance Jiao",
      "Tengfei Pan",
      "Yunyan Zhang",
      "Songjing Wang",
      "Ziqian Zhang",
      "Xu Liu",
      "Ji Zhang",
      "Caowei Meng",
      "Zhizheng Zhang",
      "Jiyang Gao",
      "Song Wang",
      "Xiaokun Leng",
      "Zhiqiang Xie",
      "Zhenzhen Zhou",
      "Peng Huang",
      "Wu Yang",
      "Yandong Guo",
      "Yichao Zhu",
      "Suibing Zheng",
      "Hao Cheng",
      "Xinmin Ding",
      "Yang Yue",
      "Huanqian Wang",
      "Chi Chen",
      "Jingrui Pang",
      "YuXi Qian",
      "Haoran Geng",
      "Lianli Gao",
      "Haiyuan Li",
      "Bin Fang",
      "Gao Huang",
      "Yaodong Yang",
      "Hao Dong",
      "He Wang",
      "Hang Zhao",
      "Yadong Mu",
      "Di Hu",
      "Hao Zhao",
      "Tiejun Huang",
      "Shanghang Zhang",
      "Yonghua Lin",
      "Zhongyuan Wang",
      "Guocai Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06287v2",
    "title": "CT-UIO: Continuous-Time UWB-Inertial-Odometer Localization Using Non-Uniform B-spline with Fewer Anchors",
    "summary": "Ultra-wideband (UWB) based positioning with fewer anchors has attracted significant research interest in recent years, especially under energy-constrained conditions. However, most existing methods rely on discrete-time representations and smoothness priors to infer a robot's motion states, which often struggle with ensuring multi-sensor data synchronization. In this article, we present a continuous-time UWB-Inertial-Odometer localization system (CT-UIO), utilizing a non-uniform B-spline framework with fewer anchors. Unlike traditional uniform B-spline-based continuous-time methods, we introduce an adaptive knot-span adjustment strategy for non-uniform continuous-time trajectory representation. This is accomplished by adjusting control points dynamically based on movement speed. To enable efficient fusion of {inertial measurement unit (IMU) and odometer data, we propose an improved extended Kalman filter (EKF) with innovation-based adaptive estimation to provide short-term accurate motion prior. Furthermore, to address the challenge of achieving a fully observable UWB localization system under few-anchor conditions, the virtual anchor (VA) generation method based on multiple hypotheses is proposed. At the backend, we propose an adaptive sliding window strategy for global trajectory estimation. Comprehensive experiments are conducted on three self-collected datasets with different UWB anchor numbers and motion modes. The result shows that the proposed CT-UIO achieves 0.403m, 0.150m, and 0.189m localization accuracy in corridor, exhibition hall, and office environments, yielding 17.2%, 26.1%, and 15.2% improvements compared with competing state-of-the-art UIO systems, respectively. The codebase and datasets of this work will be open-sourced at https://github.com/JasonSun623/CT-UIO.",
    "published": "2025-02-10T09:30:34Z",
    "updated": "2025-12-15T12:46:18Z",
    "link": "http://arxiv.org/pdf/2502.06287v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jian Sun",
      "Wei Sun",
      "Genwei Zhang",
      "Kailun Yang",
      "Song Li",
      "Xiangqi Meng",
      "Na Deng",
      "Chongbin Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13284v1",
    "title": "SAMAY: System for Acoustic Measurement and Analysis",
    "summary": "This paper describes an automatic bird call recording system called SAMAY, which is developed to study bird species by creating a database of large amounts of bird acoustic data. By analysing the recorded bird call data, the system can also be used for automatic classification of bird species, monitoring bird populations and analysing the impact of environmental changes. The system is driven through a powerful STM32F407 series microcontroller, supports 4 microphones, is equipped with 128 GB of storage capacity, and is powered by a 10400 mAh battery pack interfaced with a solar charger. In addition, the device is user-configurable over USB and Wi-Fi during runtime, ensuring user-friendly operation during field deployment.",
    "published": "2025-12-15T12:46:15Z",
    "updated": "2025-12-15T12:46:15Z",
    "link": "http://arxiv.org/pdf/2512.13284v1.pdf",
    "category": [
      "cs.SD",
      "cs.RO"
    ],
    "authors": [
      "Adheep Arya G R",
      "Vaibhav Pratap Singh",
      "Mayank Kumar",
      "Niyathi Shenoy",
      "Tejas Suryawanshi",
      "Ruchi Juyal",
      "Sangit Saha",
      "Kaushik Nanda",
      "Hari Babu Pasupuleti",
      "S D Sudarsan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13271v1",
    "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation",
    "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.",
    "published": "2025-12-15T12:30:24Z",
    "updated": "2025-12-15T12:30:24Z",
    "link": "http://arxiv.org/pdf/2512.13271v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Fangju Yang",
      "Hang Yang",
      "Ibrahim Alsarraj",
      "Yuhao Wang",
      "Ke Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13219v1",
    "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization",
    "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.",
    "published": "2025-12-15T11:32:45Z",
    "updated": "2025-12-15T11:32:45Z",
    "link": "http://arxiv.org/pdf/2512.13219v1.pdf",
    "category": [
      "cs.RO",
      "cs.MS"
    ],
    "authors": [
      "Christoph Hartmann",
      "Marios Demetriades",
      "Kevin Prüfer",
      "Zichen Zhang",
      "Klaus Spindler",
      "Stefan Weltge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13215v1",
    "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
    "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
    "published": "2025-12-15T11:28:04Z",
    "updated": "2025-12-15T11:28:04Z",
    "link": "http://arxiv.org/pdf/2512.13215v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yinsong Qu",
      "Yunxiang Li",
      "Shanlin Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13214v1",
    "title": "Differentiable Material Point Method for the Control of Deformable Objects",
    "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.",
    "published": "2025-12-15T11:26:51Z",
    "updated": "2025-12-15T11:26:51Z",
    "link": "http://arxiv.org/pdf/2512.13214v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Diego Bolliger",
      "Gabriele Fadini",
      "Markus Bambach",
      "Alisa Rupenyan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13198v1",
    "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation",
    "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.",
    "published": "2025-12-15T11:13:30Z",
    "updated": "2025-12-15T11:13:30Z",
    "link": "http://arxiv.org/pdf/2512.13198v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hyun-Gi Lee",
      "Jaekyeong Han",
      "Minjun Kwon",
      "Hyeonuk Kwon",
      "Jooha Park",
      "Hoe Jin Ha",
      "Dong-Hwa Seo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13183v1",
    "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification",
    "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.",
    "published": "2025-12-15T10:48:42Z",
    "updated": "2025-12-15T10:48:42Z",
    "link": "http://arxiv.org/pdf/2512.13183v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Alfredo González-Calvin",
      "Juan F. Jiménez",
      "Héctor García de Marina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13153v1",
    "title": "START: Traversing Sparse Footholds with Terrain Reconstruction",
    "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.",
    "published": "2025-12-15T10:02:41Z",
    "updated": "2025-12-15T10:02:41Z",
    "link": "http://arxiv.org/pdf/2512.13153v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Ruiqi Yu",
      "Qianshi Wang",
      "Hongyi Li",
      "Zheng Jun",
      "Zhicheng Wang",
      "Jun Wu",
      "Qiuguo Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13090v1",
    "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion",
    "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.",
    "published": "2025-12-15T08:43:13Z",
    "updated": "2025-12-15T08:43:13Z",
    "link": "http://arxiv.org/pdf/2512.13090v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jebeom Chae",
      "Junwoo Chang",
      "Seungho Yeom",
      "Yujin Kim",
      "Jongeun Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13080v1",
    "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
    "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
    "published": "2025-12-15T08:31:47Z",
    "updated": "2025-12-15T08:31:47Z",
    "link": "http://arxiv.org/pdf/2512.13080v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yicheng Feng",
      "Wanpeng Zhang",
      "Ye Wang",
      "Hao Luo",
      "Haoqi Yuan",
      "Sipeng Zheng",
      "Zongqing Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20487v4",
    "title": "A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots",
    "summary": "Humanoid robots are drawing significant attention as versatile platforms for complex motor control, human-robot interaction, and general-purpose physical intelligence. However, achieving efficient whole-body control (WBC) in humanoids remains a fundamental challenge due to sophisticated dynamics, underactuation, and diverse task requirements. While learning-based controllers have shown promise for complex tasks, their reliance on labor-intensive and costly retraining for new scenarios limits real-world applicability. To address these limitations, behavior(al) foundation models (BFMs) have emerged as a new paradigm that leverages large-scale pre-training to learn reusable primitive skills and broad behavioral priors, enabling zero-shot or rapid adaptation to a wide range of downstream tasks. In this paper, we present a comprehensive overview of BFMs for humanoid WBC, tracing their development across diverse pre-training pipelines. Furthermore, we discuss real-world applications, current limitations, urgent challenges, and future opportunities, positioning BFMs as a key approach toward scalable and general-purpose humanoid intelligence. Finally, we provide a curated and regularly updated collection of BFM papers and projects to facilitate more subsequent research, which is available at https://github.com/yuanmingqi/awesome-bfm-papers.",
    "published": "2025-06-25T14:35:33Z",
    "updated": "2025-12-15T07:56:49Z",
    "link": "http://arxiv.org/pdf/2506.20487v4.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mingqi Yuan",
      "Tao Yu",
      "Wenqi Ge",
      "Xiuyong Yao",
      "Huijiang Wang",
      "Jiayu Chen",
      "Bo Li",
      "Wei Zhang",
      "Wenjun Zeng",
      "Hua Chen",
      "Xin Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05710v2",
    "title": "DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving",
    "summary": "Safety is a critical concern in motion planning for autonomous vehicles. Modern autonomous vehicles rely on neural network-based perception, but making control decisions based on these inference results poses significant safety risks due to inherent uncertainties. To address this challenge, we present a distributionally robust optimization (DRO) framework that accounts for both aleatoric and epistemic perception uncertainties using evidential deep learning (EDL). Our approach introduces a novel ambiguity set formulation based on evidential distributions that dynamically adjusts the conservativeness according to perception confidence levels. We integrate this uncertainty-aware constraint into model predictive control (MPC), proposing the DRO-EDL-MPC algorithm with computational tractability for autonomous driving applications. Validation in the CARLA simulator demonstrates that our approach maintains efficiency under high perception confidence while enforcing conservative constraints under low confidence.",
    "published": "2025-07-08T06:45:18Z",
    "updated": "2025-12-15T06:46:49Z",
    "link": "http://arxiv.org/pdf/2507.05710v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hyeongchan Ham",
      "Heejin Ahn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.13009v1",
    "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots",
    "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.",
    "published": "2025-12-15T06:11:14Z",
    "updated": "2025-12-15T06:11:14Z",
    "link": "http://arxiv.org/pdf/2512.13009v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Oğuzhan Akbıyık",
      "Naseem Alhousani",
      "Fares J. Abu-Dakka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.12993v1",
    "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations",
    "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.",
    "published": "2025-12-15T05:38:08Z",
    "updated": "2025-12-15T05:38:08Z",
    "link": "http://arxiv.org/pdf/2512.12993v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Guillermo A. Castillo",
      "Himanshu Lodha",
      "Ayonga Hereid"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03958v2",
    "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation",
    "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extended Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on multimodal reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.",
    "published": "2025-12-03T16:52:07Z",
    "updated": "2025-12-15T05:12:06Z",
    "link": "http://arxiv.org/pdf/2512.03958v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xiaobei Zhao",
      "Xingqi Lyu",
      "Xin Chen",
      "Xiang Li"
    ]
  }
]