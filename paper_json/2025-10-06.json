[
  {
    "id": "http://arxiv.org/abs/2510.05102v1",
    "title": "TopInG: Topologically Interpretable Graph Learning via Persistent\n  Rationale Filtration",
    "summary": "Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality.",
    "published": "2025-10-06T17:59:44Z",
    "updated": "2025-10-06T17:59:44Z",
    "link": "http://arxiv.org/pdf/2510.05102v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "math.AT",
      "stat.ML",
      "55N31, 68T05, 62R40, 05C, 68R05",
      "I.2.6; G.2.2; I.5.1"
    ],
    "authors": [
      "Cheng Xin",
      "Fan Xu",
      "Xin Ding",
      "Jie Gao",
      "Jiaxin Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05096v1",
    "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
    "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
    "published": "2025-10-06T17:58:02Z",
    "updated": "2025-10-06T17:58:02Z",
    "link": "http://arxiv.org/pdf/2510.05096v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MA",
      "cs.MM"
    ],
    "authors": [
      "Zeyu Zhu",
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05095v1",
    "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized\n  Preference Optimization for Aligning Large Reasoning Models",
    "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.",
    "published": "2025-10-06T17:58:01Z",
    "updated": "2025-10-06T17:58:01Z",
    "link": "http://arxiv.org/pdf/2510.05095v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Mingkang Zhu",
      "Xi Chen",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05092v1",
    "title": "Learning to Interpret Weight Differences in Language Models",
    "summary": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions.",
    "published": "2025-10-06T17:57:23Z",
    "updated": "2025-10-06T17:57:23Z",
    "link": "http://arxiv.org/pdf/2510.05092v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Avichal Goel",
      "Yoon Kim",
      "Nir Shavit",
      "Tony T. Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.01928v3",
    "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
    "summary": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining.",
    "published": "2024-12-02T19:30:36Z",
    "updated": "2025-10-06T17:57:15Z",
    "link": "http://arxiv.org/pdf/2412.01928v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sumeet Ramesh Motwani",
      "Chandler Smith",
      "Rocktim Jyoti Das",
      "Rafael Rafailov",
      "Ivan Laptev",
      "Philip H. S. Torr",
      "Fabio Pizzati",
      "Ronald Clark",
      "Christian Schroeder de Witt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05090v1",
    "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for\n  Diffusion Large Language Models",
    "summary": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available.",
    "published": "2025-10-06T17:56:46Z",
    "updated": "2025-10-06T17:56:46Z",
    "link": "http://arxiv.org/pdf/2510.05090v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Runchu Tian",
      "Junxia Cui",
      "Xueqiang Xu",
      "Feng Yao",
      "Jingbo Shang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05087v1",
    "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
    "summary": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.",
    "published": "2025-10-06T17:55:04Z",
    "updated": "2025-10-06T17:55:04Z",
    "link": "http://arxiv.org/pdf/2510.05087v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Janos Perczel",
      "Jin Chow",
      "Dorottya Demszky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20666v3",
    "title": "Using cognitive models to reveal value trade-offs in language models",
    "summary": "Value trade-offs are an integral part of human decision-making and language\nuse, however, current tools for interpreting such dynamic and multi-faceted\nnotions of values in LLMs are limited. In cognitive science, so-called\n\"cognitive models\" provide formal accounts of such trade-offs in humans, by\nmodeling the weighting of a speaker's competing utility functions in choosing\nan action or utterance. Here we use a leading cognitive model of polite speech\nto systematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models' default\nbehavior, and demonstrate that these patterns shift in predictable ways when\nmodels are prompted to prioritize certain goals over others. Our findings from\nLLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. Our framework offers a\nflexible tool for probing value trade-offs across diverse model types,\nproviding insights for generating hypotheses about other social behaviors such\nas sycophancy and for shaping training regimes that better control trade-offs\nbetween values during model development.",
    "published": "2025-06-25T17:58:12Z",
    "updated": "2025-10-06T17:52:34Z",
    "link": "http://arxiv.org/pdf/2506.20666v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sonia K. Murthy",
      "Rosie Zhao",
      "Jennifer Hu",
      "Sham Kakade",
      "Markus Wulfmeier",
      "Peng Qian",
      "Tomer Ullman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.10924v6",
    "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
    "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]",
    "published": "2024-12-14T18:18:52Z",
    "updated": "2025-10-06T17:52:33Z",
    "link": "http://arxiv.org/pdf/2412.10924v6.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Julia Witte Zimmerman",
      "Denis Hudon",
      "Kathryn Cramer",
      "Alejandro J. Ruiz",
      "Calla Beauregard",
      "Ashley Fehr",
      "Mikaela Irene Fudolig",
      "Bradford Demarest",
      "Yoshi Meke Bird",
      "Milo Z. Trujillo",
      "Christopher M. Danforth",
      "Peter Sheridan Dodds"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05081v1",
    "title": "SAEdit: Token-level control for continuous image editing via Sparse\n  AutoEncoder",
    "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.",
    "published": "2025-10-06T17:51:04Z",
    "updated": "2025-10-06T17:51:04Z",
    "link": "http://arxiv.org/pdf/2510.05081v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Ronen Kamenetsky",
      "Sara Dorfman",
      "Daniel Garibi",
      "Roni Paiss",
      "Or Patashnik",
      "Daniel Cohen-Or"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05077v1",
    "title": "Slm-mux: Orchestrating small language models for reasoning",
    "summary": "With the rapid development of language models, the number of small language\nmodels (SLMs) has grown significantly. Although they do not achieve\nstate-of-the-art accuracy, they are more efficient and often excel at specific\ntasks. This raises a natural question: can multiple SLMs be orchestrated into a\nsystem where each contributes effectively, achieving higher accuracy than any\nindividual model? Existing orchestration methods have primarily targeted\nfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To\naddress this gap, we propose a three-stage approach for orchestrating SLMs.\nFirst, we introduce SLM-MUX, a multi-model architecture that effectively\ncoordinates multiple SLMs. Building on this, we develop two optimization\nstrategies: (i) a model selection search that identifies the most complementary\nSLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our\napproach delivers strong results: Compared to existing orchestration methods,\nour approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%\non GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and\nGSM8K, and matches its performance on MATH. We further provide theoretical\nanalyses to substantiate the advantages of our method. In summary, we\ndemonstrate that SLMs can be effectively orchestrated into more accurate and\nefficient systems through the proposed approach.",
    "published": "2025-10-06T17:49:58Z",
    "updated": "2025-10-06T17:49:58Z",
    "link": "http://arxiv.org/pdf/2510.05077v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chenyu Wang",
      "Zishen Wan",
      "Hao Kang",
      "Emma Chen",
      "Zhiqiang Xie",
      "Tushar Krishna",
      "Vijay Janapa Reddi",
      "Yilun Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05069v1",
    "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
    "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
    "published": "2025-10-06T17:46:34Z",
    "updated": "2025-10-06T17:46:34Z",
    "link": "http://arxiv.org/pdf/2510.05069v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Dachuan Shi",
      "Abedelkadir Asi",
      "Keying Li",
      "Xiangchi Yuan",
      "Leyan Pan",
      "Wenke Lee",
      "Wen Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05059v1",
    "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
    "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.",
    "published": "2025-10-06T17:37:35Z",
    "updated": "2025-10-06T17:37:35Z",
    "link": "http://arxiv.org/pdf/2510.05059v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Junlin Wang",
      "Jue Wang",
      " Zhen",
      " Xu",
      "Ben Athiwaratkun",
      "Bhuwan Dhingra",
      "Ce Zhang",
      "James Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05054v1",
    "title": "HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a\n  Single Hybrid Model",
    "summary": "Uncertainty quantification is critical for ensuring robustness in high-stakes\nmachine learning applications. We introduce HybridFlow, a modular hybrid\narchitecture that unifies the modeling of aleatoric and epistemic uncertainty\nby combining a Conditional Masked Autoregressive normalizing flow for\nestimating aleatoric uncertainty with a flexible probabilistic predictor for\nepistemic uncertainty. The framework supports integration with any\nprobabilistic model class, allowing users to easily adapt HybridFlow to\nexisting architectures without sacrificing predictive performance. HybridFlow\nimproves upon previous uncertainty quantification frameworks across a range of\nregression tasks, such as depth estimation, a collection of regression\nbenchmarks, and a scientific case study of ice sheet emulation. We also provide\nempirical results of the quantified uncertainty, showing that the uncertainty\nquantified by HybridFlow is calibrated and better aligns with model error than\nexisting methods for quantifying aleatoric and epistemic uncertainty.\nHybridFlow addresses a key challenge in Bayesian deep learning, unifying\naleatoric and epistemic uncertainty modeling in a single robust framework.",
    "published": "2025-10-06T17:34:48Z",
    "updated": "2025-10-06T17:34:48Z",
    "link": "http://arxiv.org/pdf/2510.05054v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Peter Van Katwyk",
      "Karianne J. Bergen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05048v1",
    "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games",
    "summary": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games.",
    "published": "2025-10-06T17:26:56Z",
    "updated": "2025-10-06T17:26:56Z",
    "link": "http://arxiv.org/pdf/2510.05048v1.pdf",
    "category": [
      "cs.AI",
      "cs.GT"
    ],
    "authors": [
      "Ondřej Kubíček",
      "Viliam Lisý"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24379v2",
    "title": "Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data\n  under Exact Unlearning in Large Language Model",
    "summary": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm.",
    "published": "2025-05-30T09:09:33Z",
    "updated": "2025-10-06T17:21:05Z",
    "link": "http://arxiv.org/pdf/2505.24379v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Xiaoyu Wu",
      "Yifei Pang",
      "Terrance Liu",
      "Zhiwei Steven Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05040v1",
    "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts",
    "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference.",
    "published": "2025-10-06T17:16:41Z",
    "updated": "2025-10-06T17:16:41Z",
    "link": "http://arxiv.org/pdf/2510.05040v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jihoon Lee",
      "Hoyeon Moon",
      "Kevin Zhai",
      "Arun Kumar Chithanar",
      "Anit Kumar Sahu",
      "Soummya Kar",
      "Chul Lee",
      "Souradip Chakraborty",
      "Amrit Singh Bedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05036v1",
    "title": "Graph-Aware Diffusion for Signal Generation",
    "summary": "We study the problem of generating graph signals from unknown distributions\ndefined over given graphs, relevant to domains such as recommender systems or\nsensor networks. Our approach builds on generative diffusion models, which are\nwell established in vision and graph generation but remain underexplored for\ngraph signals. Existing methods lack generality, either ignoring the graph\nstructure in the forward process or designing graph-aware mechanisms tailored\nto specific domains. We adopt a forward process that incorporates the graph\nthrough the heat equation. Rather than relying on the standard formulation, we\nconsider a time-warped coefficient to mitigate the exponential decay of the\ndrift term, yielding a graph-aware generative diffusion model (GAD). We analyze\nits forward dynamics, proving convergence to a Gaussian Markov random field\nwith covariance parametrized by the graph Laplacian, and interpret the backward\ndynamics as a sequence of graph-signal denoising problems. Finally, we\ndemonstrate the advantages of GAD on synthetic data, real traffic speed\nmeasurements, and a temperature sensor network.",
    "published": "2025-10-06T17:11:32Z",
    "updated": "2025-10-06T17:11:32Z",
    "link": "http://arxiv.org/pdf/2510.05036v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sergio Rozada",
      "Vimal K. B.",
      "Andrea Cavallo",
      "Antonio G. Marques",
      "Hadi Jamali-Rad",
      "Elvin Isufi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18057v4",
    "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
    "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
    "published": "2025-09-22T17:30:33Z",
    "updated": "2025-10-06T17:09:53Z",
    "link": "http://arxiv.org/pdf/2509.18057v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "math.CO"
    ],
    "authors": [
      "Ansh Nagda",
      "Prabhakar Raghavan",
      "Abhradeep Thakurta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05025v1",
    "title": "Imperceptible Jailbreaking against Large Language Models",
    "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
    "published": "2025-10-06T17:03:50Z",
    "updated": "2025-10-06T17:03:50Z",
    "link": "http://arxiv.org/pdf/2510.05025v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Kuofeng Gao",
      "Yiming Li",
      "Chao Du",
      "Xin Wang",
      "Xingjun Ma",
      "Shu-Tao Xia",
      "Tianyu Pang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05023v1",
    "title": "Rethinking Langevin Thompson Sampling from A Stochastic Approximation\n  Perspective",
    "summary": "Most existing approximate Thompson Sampling (TS) algorithms for multi-armed\nbandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in\neach round to sample from the posterior, relaxing the need for conjugacy\nassumptions between priors and reward distributions in vanilla TS. However,\nthey often require approximating a different posterior distribution in\ndifferent round of the bandit problem. This requires tricky, round-specific\ntuning of hyperparameters such as dynamic learning rates, causing challenges in\nboth theoretical analysis and practical implementation. To alleviate this\nnon-stationarity, we introduce TS-SA, which incorporates stochastic\napproximation (SA) within the TS framework. In each round, TS-SA constructs a\nposterior approximation only using the most recent reward(s), performs a\nLangevin Monte Carlo (LMC) update, and applies an SA step to average noisy\nproposals over time. This can be interpreted as approximating a stationary\nposterior target throughout the entire algorithm, which further yields a fixed\nstep-size, a unified convergence analysis framework, and improved posterior\nestimates through temporal averaging. We establish near-optimal regret bounds\nfor TS-SA, with a simplified and more intuitive theoretical analysis enabled by\ninterpreting the entire algorithm as a simulation of a stationary SGLD process.\nOur empirical results demonstrate that even a single-step Langevin update with\ncertain warm-up outperforms existing methods substantially on bandit tasks.",
    "published": "2025-10-06T17:01:29Z",
    "updated": "2025-10-06T17:01:29Z",
    "link": "http://arxiv.org/pdf/2510.05023v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Weixin Wang",
      "Haoyang Zheng",
      "Guang Lin",
      "Wei Deng",
      "Pan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08825v2",
    "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
    "summary": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking.",
    "published": "2025-09-10T17:58:53Z",
    "updated": "2025-10-06T16:58:59Z",
    "link": "http://arxiv.org/pdf/2509.08825v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Joachim Baumann",
      "Paul Röttger",
      "Aleksandra Urman",
      "Albert Wendsjö",
      "Flor Miriam Plaza-del-Arco",
      "Johannes B. Gruber",
      "Dirk Hovy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05016v1",
    "title": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad",
    "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
    "published": "2025-10-06T16:58:47Z",
    "updated": "2025-10-06T16:58:47Z",
    "link": "http://arxiv.org/pdf/2510.05016v1.pdf",
    "category": [
      "astro-ph.IM",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Lucas Carrit Delgado Pinheiro",
      "Ziru Chen",
      "Bruno Caixeta Piazza",
      "Ness Shroff",
      "Yingbin Liang",
      "Yuan-Sen Ting",
      "Huan Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05014v1",
    "title": "Think Then Embed: Generative Context Improves Multimodal Embedding",
    "summary": "There is a growing interest in Universal Multimodal Embeddings (UME), where\nmodels are required to generate task-specific representations. While recent\nstudies show that Multimodal Large Language Models (MLLMs) perform well on such\ntasks, they treat MLLMs solely as encoders, overlooking their generative\ncapacity. However, such an encoding paradigm becomes less effective as\ninstructions become more complex and require compositional reasoning. Inspired\nby the proven effectiveness of chain-of-thought reasoning, we propose a general\nThink-Then-Embed (TTE) framework for UME, composed of a reasoner and an\nembedder. The reasoner MLLM first generates reasoning traces that explain\ncomplex queries, followed by an embedder that produces representations\nconditioned on both the original query and the intermediate reasoning. This\nexplicit reasoning step enables more nuanced understanding of complex\nmultimodal instructions. Our contributions are threefold. First, by leveraging\na powerful MLLM reasoner, we achieve state-of-the-art performance on the\nMMEB-V2 benchmark, surpassing proprietary models trained on massive in-house\ndatasets. Second, to reduce the dependency on large MLLM reasoners, we finetune\na smaller MLLM reasoner using high-quality embedding-centric reasoning traces,\nachieving the best performance among open-source models with a 7% absolute gain\nover recently proposed models. Third, we investigate strategies for integrating\nthe reasoner and embedder into a unified model for improved efficiency without\nsacrificing performance.",
    "published": "2025-10-06T16:53:56Z",
    "updated": "2025-10-06T16:53:56Z",
    "link": "http://arxiv.org/pdf/2510.05014v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xuanming Cui",
      "Jianpeng Cheng",
      "Hong-you Chen",
      "Satya Narayan Shukla",
      "Abhijeet Awasthi",
      "Xichen Pan",
      "Chaitanya Ahuja",
      "Shlok Kumar Mishra",
      "Qi Guo",
      "Ser-Nam Lim",
      "Aashu Singh",
      "Xiangjun Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01876v2",
    "title": "In-Context Learning for Pure Exploration",
    "summary": "We study the problem active sequential hypothesis testing, also known as pure\nexploration: given a new task, the learner adaptively collects data from the\nenvironment to efficiently determine an underlying correct hypothesis. A\nclassical instance of this problem is the task of identifying the best arm in a\nmulti-armed bandit problem (a.k.a. BAI, Best-Arm Identification), where actions\nindex hypotheses. Another important case is generalized search, a problem of\ndetermining the correct label through a sequence of strategically selected\nqueries that indirectly reveal information about the label. In this work, we\nintroduce In-Context Pure Exploration (ICPE), which meta-trains Transformers to\nmap observation histories to query actions and a predicted hypothesis, yielding\na model that transfers in-context. At inference time, ICPE actively gathers\nevidence on new tasks and infers the true hypothesis without parameter updates.\nAcross deterministic, stochastic, and structured benchmarks, including BAI and\ngeneralized search, ICPE is competitive with adaptive baselines while requiring\nno explicit modeling of information structure. Our results support Transformers\nas practical architectures for general sequential testing.",
    "published": "2025-06-02T17:04:50Z",
    "updated": "2025-10-06T16:44:47Z",
    "link": "http://arxiv.org/pdf/2506.01876v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alessio Russo",
      "Ryan Welch",
      "Aldo Pacchiano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05003v1",
    "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical\n  Chain-of-Thought Reasoning",
    "summary": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.",
    "published": "2025-10-06T16:42:11Z",
    "updated": "2025-10-06T16:42:11Z",
    "link": "http://arxiv.org/pdf/2510.05003v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Imran Mansha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04999v1",
    "title": "Bridging Text and Video Generation: A Survey",
    "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.",
    "published": "2025-10-06T16:39:05Z",
    "updated": "2025-10-06T16:39:05Z",
    "link": "http://arxiv.org/pdf/2510.04999v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Nilay Kumar",
      "Priyansh Bhandari",
      "G. Maragatham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04997v1",
    "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault\n  Analysis",
    "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.",
    "published": "2025-10-06T16:37:18Z",
    "updated": "2025-10-06T16:37:18Z",
    "link": "http://arxiv.org/pdf/2510.04997v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Jiongchi Yu",
      "Weipeng Jiang",
      "Xiaoyu Zhang",
      "Qiang Hu",
      "Xiaofei Xie",
      "Chao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04996v1",
    "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
    "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
    "published": "2025-10-06T16:34:09Z",
    "updated": "2025-10-06T16:34:09Z",
    "link": "http://arxiv.org/pdf/2510.04996v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "authors": [
      "Wei Xiong",
      "Chenlu Ye",
      "Baohao Liao",
      "Hanze Dong",
      "Xinxing Xu",
      "Christof Monz",
      "Jiang Bian",
      "Nan Jiang",
      "Tong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01171v2",
    "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
    "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
    "published": "2025-10-01T17:55:37Z",
    "updated": "2025-10-06T16:29:44Z",
    "link": "http://arxiv.org/pdf/2510.01171v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jiayi Zhang",
      "Simon Yu",
      "Derek Chong",
      "Anthony Sicilia",
      "Michael R. Tomz",
      "Christopher D. Manning",
      "Weiyan Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04983v1",
    "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework\n  for Identifying Cultural Capital in STEM Narratives",
    "summary": "Identifying cultural capital (CC) themes in student reflections can offer\nvaluable insights that help foster equitable learning environments in\nclassrooms. However, themes such as aspirational goals or family support are\noften woven into narratives, rather than appearing as direct keywords. This\nmakes them difficult to detect for standard NLP models that process sentences\nin isolation. The core challenge stems from a lack of awareness, as standard\nmodels are pre-trained on general corpora, leaving them blind to the\ndomain-specific language and narrative context inherent to the data. To address\nthis, we introduce AWARE, a framework that systematically attempts to improve a\ntransformer model's awareness for this nuanced task. AWARE has three core\ncomponents: 1) Domain Awareness, adapting the model's vocabulary to the\nlinguistic style of student reflections; 2) Context Awareness, generating\nsentence embeddings that are aware of the full essay context; and 3) Class\nOverlap Awareness, employing a multi-label strategy to recognize the\ncoexistence of themes in a single sentence. Our results show that by making the\nmodel explicitly aware of the properties of the input, AWARE outperforms a\nstrong baseline by 2.1 percentage points in Macro-F1 and shows considerable\nimprovements across all themes. This work provides a robust and generalizable\nmethodology for any text classification task in which meaning depends on the\ncontext of the narrative.",
    "published": "2025-10-06T16:19:57Z",
    "updated": "2025-10-06T16:19:57Z",
    "link": "http://arxiv.org/pdf/2510.04983v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Khalid Mehtab Khan",
      "Anagha Kulkarni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04980v1",
    "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game",
    "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.",
    "published": "2025-10-06T16:17:24Z",
    "updated": "2025-10-06T16:17:24Z",
    "link": "http://arxiv.org/pdf/2510.04980v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Fangzhou Liang",
      "Tianshi Zheng",
      "Chunkit Chan",
      "Yauwai Yim",
      "Yangqiu Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19099v8",
    "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
    "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.",
    "published": "2025-05-25T11:28:34Z",
    "updated": "2025-10-06T16:16:33Z",
    "link": "http://arxiv.org/pdf/2505.19099v8.pdf",
    "category": [
      "cs.AI",
      "physics.ed-ph",
      "physics.pop-ph"
    ],
    "authors": [
      "Kun Xiang",
      "Heng Li",
      "Terry Jingchen Zhang",
      "Yinya Huang",
      "Zirong Liu",
      "Peixin Qu",
      "Jixi He",
      "Jiaqi Chen",
      "Yu-Jie Yuan",
      "Jianhua Han",
      "Hang Xu",
      "Hanhui Li",
      "Mrinmaya Sachan",
      "Xiaodan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04978v1",
    "title": "Aligning Perception, Reasoning, Modeling and Interaction: A Survey on\n  Physical AI",
    "summary": "The rapid advancement of embodied intelligence and world models has\nintensified efforts to integrate physical laws into AI systems, yet physical\nperception and symbolic physics reasoning have developed along separate\ntrajectories without a unified bridging framework. This work provides a\ncomprehensive overview of physical AI, establishing clear distinctions between\ntheoretical physics reasoning and applied physical understanding while\nsystematically examining how physics-grounded methods enhance AI's real-world\ncomprehension across structured symbolic reasoning, embodied systems, and\ngenerative models. Through rigorous analysis of recent advances, we advocate\nfor intelligent systems that ground learning in both physical principles and\nembodied reasoning processes, transcending pattern recognition toward genuine\nunderstanding of physical laws. Our synthesis envisions next-generation world\nmodels capable of explaining physical phenomena and predicting future states,\nadvancing safe, generalizable, and interpretable AI systems. We maintain a\ncontinuously updated resource at\nhttps://github.com/AI4Phys/Awesome-AI-for-Physics.",
    "published": "2025-10-06T16:16:03Z",
    "updated": "2025-10-06T16:16:03Z",
    "link": "http://arxiv.org/pdf/2510.04978v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Kun Xiang",
      "Terry Jingchen Zhang",
      "Yinya Huang",
      "Jixi He",
      "Zirong Liu",
      "Yueling Tang",
      "Ruizhe Zhou",
      "Lijing Luo",
      "Youpeng Wen",
      "Xiuwei Chen",
      "Bingqian Lin",
      "Jianhua Han",
      "Hang Xu",
      "Hanhui Li",
      "Bin Dong",
      "Xiaodan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.20600v3",
    "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
    "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact.",
    "published": "2024-10-27T21:20:18Z",
    "updated": "2025-10-06T16:15:07Z",
    "link": "http://arxiv.org/pdf/2410.20600v3.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Harshvardhan Mestha",
      "Karan Bania",
      "Shreyas V Sathyanarayana",
      "Sidong Liu",
      "Ashwin Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.10849v2",
    "title": "Pragmatic Embodied Spoken Instruction Following in Human-Robot\n  Collaboration with Theory of Mind",
    "summary": "Spoken language instructions are ubiquitous in agent collaboration. However,\nin real-world human-robot collaboration, following human spoken instructions\ncan be challenging due to various speaker and environmental factors, such as\nbackground noise or mispronunciation. When faced with noisy auditory inputs,\nhumans can leverage the collaborative context in the embodied environment to\ninterpret noisy spoken instructions and take pragmatic assistive actions. In\nthis paper, we present a cognitively inspired neurosymbolic model, Spoken\nInstruction Following through Theory of Mind (SIFToM), which leverages a\nVision-Language Model with model-based mental inference to enable robots to\npragmatically follow human instructions under diverse speech conditions. We\ntest SIFToM in both simulated environments (VirtualHome) and real-world\nhuman-robot collaborative settings with human evaluations. Results show that\nSIFToM can significantly improve the performance of a lightweight base VLM\n(Gemini 2.5 Flash), outperforming state-of-the-art VLMs (Gemini 2.5 Pro) and\napproaching human-level accuracy on challenging spoken instruction following\ntasks.",
    "published": "2024-09-17T02:36:10Z",
    "updated": "2025-10-06T16:05:39Z",
    "link": "http://arxiv.org/pdf/2409.10849v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "authors": [
      "Lance Ying",
      "Xinyi Li",
      "Shivam Aarya",
      "Yizirui Fang",
      "Yifan Yin",
      "Jason Xinyu Liu",
      "Stefanie Tellex",
      "Joshua B. Tenenbaum",
      "Tianmin Shu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04970v1",
    "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure\n  Learning",
    "summary": "We present FLOP (Fast Learning of Order and Parents), a score-based causal\ndiscovery algorithm for linear models. It pairs fast parent selection with\niterative Cholesky-based score updates, cutting run-times over prior\nalgorithms. This makes it feasible to fully embrace discrete search, enabling\niterated local search with principled order initialization to find graphs with\nscores at or close to the global optimum. The resulting structures are highly\naccurate across benchmarks, with near-perfect recovery in standard settings.\nThis performance calls for revisiting discrete search over graphs as a\nreasonable approach to causal discovery.",
    "published": "2025-10-06T16:04:53Z",
    "updated": "2025-10-06T16:04:53Z",
    "link": "http://arxiv.org/pdf/2510.04970v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Marcel Wienöbst",
      "Leonard Henckel",
      "Sebastian Weichwald"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04966v1",
    "title": "ActiveMark: on watermarking of visual foundation models via massive\n  activations",
    "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.",
    "published": "2025-10-06T15:58:27Z",
    "updated": "2025-10-06T15:58:27Z",
    "link": "http://arxiv.org/pdf/2510.04966v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Anna Chistyakova",
      "Mikhail Pautov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.05424v4",
    "title": "Speculative Automated Refactoring of Imperative Deep Learning Programs\n  to Graph Execution",
    "summary": "Efficiency is essential to support ever-growing datasets, especially for Deep\nLearning (DL) systems. DL frameworks have traditionally embraced deferred\nexecution-style DL code -- supporting symbolic, graph-based Deep Neural Network\n(DNN) computation. While scalable, such development is error-prone,\nnon-intuitive, and difficult to debug. Consequently, more natural, imperative\nDL frameworks encouraging eager execution have emerged but at the expense of\nrun-time performance. Though hybrid approaches aim for the \"best of both\nworlds,\" using them effectively requires subtle considerations. Our key insight\nis that, while DL programs typically execute sequentially, hybridizing\nimperative DL code resembles parallelizing sequential code in traditional\nsystems. Inspired by this, we present an automated refactoring approach that\nassists developers in determining which otherwise eagerly-executed imperative\nDL functions could be effectively and efficiently executed as graphs. The\napproach features novel static imperative tensor and side-effect analyses for\nPython. Due to its inherent dynamism, analyzing Python may be unsound; however,\nthe conservative approach leverages a speculative (keyword-based) analysis for\nresolving difficult cases that informs developers of any assumptions made. The\napproach is: (i) implemented as a plug-in to the PyDev Eclipse IDE that\nintegrates the WALA Ariadne analysis framework and (ii) evaluated on nineteen\nDL projects consisting of 132 KLOC. The results show that 326 of 766 candidate\nfunctions (42.56%) were refactorable, and an average relative speedup of 2.16x\non performance tests was observed with negligible differences in model\naccuracy. The results indicate that the approach is useful in optimizing\nimperative DL code to its full potential.",
    "published": "2025-04-07T18:48:43Z",
    "updated": "2025-10-06T15:57:48Z",
    "link": "http://arxiv.org/pdf/2504.05424v4.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.PL",
      "D.2.7; C.4; D.3.4; I.2.6"
    ],
    "authors": [
      "Raffi Khatchadourian",
      "Tatiana Castro Vélez",
      "Mehdi Bagherzadeh",
      "Nan Jia",
      "Anita Raja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04956v1",
    "title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive\n  Hierarchical Neural Modeling",
    "summary": "Computer-assisted pronunciation training (CAPT) manages to facilitate\nsecond-language (L2) learners to practice pronunciation skills by offering\ntimely and instructive feedback. To examine pronunciation proficiency from\nmultiple facets, existing methods for CAPT broadly fall into two categories:\nmispronunciation detection and diagnosis (MDD) as well as automatic\npronunciation assessment (APA). The former aims to pinpoint phonetic\npronunciation errors and provide diagnostic feedback, while the latter seeks\ninstead to quantify pronunciation proficiency pertaining to various aspects.\nDespite the natural complementarity between MDD and APA, researchers and\npractitioners, however, often treat them as independent tasks with disparate\nmodeling paradigms. In light of this, we in this paper first introduce MuFFIN,\na Multi-Faceted pronunciation Feedback model with an Interactive hierarchical\nNeural architecture, to jointly address the tasks of MDD and APA. To better\ncapture the nuanced distinctions between phonemes in the feature space, a novel\nphoneme-contrastive ordinal regularization mechanism is then put forward to\noptimize the proposed model to generate more phoneme-discriminative features\nwhile factoring in the ordinality of the aspect scores. In addition, to address\nthe intricate data imbalance problem in MDD, we design a simple yet effective\ntraining objective, which is specifically tailored to perturb the outputs of a\nphoneme classifier with the phoneme-specific variations, so as to better render\nthe distribution of predicted phonemes meanwhile considering their\nmispronunciation characteristics. A series of experiments conducted on the\nSpeechocean762 benchmark dataset demonstrates the efficacy of our method in\nrelation to several cutting-edge baselines, showing state-of-the-art\nperformance on both the APA and MDD tasks.",
    "published": "2025-10-06T15:54:55Z",
    "updated": "2025-10-06T15:54:55Z",
    "link": "http://arxiv.org/pdf/2510.04956v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Bi-Cheng Yan",
      "Ming-Kang Tsai",
      "Berlin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.07124v2",
    "title": "CHARME: A chain-based reinforcement learning approach for the minor\n  embedding problem",
    "summary": "Quantum annealing (QA) has great potential to solve combinatorial\noptimization problems efficiently. However, the effectiveness of QA algorithms\nis heavily based on the embedding of problem instances, represented as logical\ngraphs, into the quantum processing unit (QPU) whose topology is in the form of\na limited connectivity graph, known as the minor embedding problem. Because the\nminor embedding problem is an NP-hard problem~\\mbox{\\cite{Goodrich2018}},\nexisting methods for the minor embedding problem suffer from scalability issues\nwhen faced with larger problem sizes. In this paper, we propose a novel\napproach utilizing Reinforcement Learning (RL) techniques to address the minor\nembedding problem, named CHARME. CHARME includes three key components: a Graph\nNeural Network (GNN) architecture for policy modeling, a state transition\nalgorithm that ensures solution validity, and an order exploration strategy for\neffective training. Through comprehensive experiments on synthetic and\nreal-world instances, we demonstrate the efficiency of our proposed order\nexploration strategy as well as our proposed RL framework, CHARME. In\nparticular, CHARME yields superior solutions in terms of qubit usage compared\nto fast embedding methods such as Minorminer and ATOM. Moreover, our method\nsurpasses the OCT-based approach, known for its slower runtime but high-quality\nsolutions, in several cases. In addition, our proposed exploration enhances the\nefficiency of the training of the CHARME framework by providing better\nsolutions compared to the greedy strategy.",
    "published": "2024-06-11T10:12:10Z",
    "updated": "2025-10-06T15:52:54Z",
    "link": "http://arxiv.org/pdf/2406.07124v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Hoang M. Ngo",
      "Nguyen H K. Do",
      "Minh N. Vu",
      "Tre' R. Jeter",
      "Tamer Kahveci",
      "My T. Thai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04952v1",
    "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and\n  Zero-Knowledge Audits",
    "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.",
    "published": "2025-10-06T15:52:12Z",
    "updated": "2025-10-06T15:52:12Z",
    "link": "http://arxiv.org/pdf/2510.04952v1.pdf",
    "category": [
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Ailiya Borjigin",
      "Cong He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04951v1",
    "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in\n  the Constraints",
    "summary": "When some parameters of a constrained optimization problem (COP) are\nuncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising\ntwo stages -- the prediction of the unknown parameters from contextual\ninformation and the subsequent optimization using those predicted parameters.\nDecision-focused learning (DFL) implements the first stage by training a\nmachine learning (ML) model to optimize the quality of the decisions made using\nthe predicted parameters. When parameters in the constraints of a COP are\npredicted, the predicted parameters can lead to infeasible solutions.\nTherefore, it is important to simultaneously manage both feasibility and\ndecision quality. We develop a DFL framework for predicting constraint\nparameters in a generic COP. While prior works typically assume that the\nunderlying optimization problem is a linear program (LP) or integer linear\nprogram (ILP), our approach makes no such assumption. We derive two novel loss\nfunctions based on maximum likelihood estimation (MLE): the first one penalizes\ninfeasibility (by penalizing when the predicted parameters lead to infeasible\nsolutions), and the second one penalizes suboptimal decisions (by penalizing\nwhen the true optimal solution is infeasible under the predicted parameters).\nWe introduce a single tunable parameter to form a weighted average of the two\nlosses, allowing decision-makers to balance suboptimality and feasibility. We\nexperimentally demonstrate that adjusting this parameter provides a\ndecision-maker the control over the trade-off between the two. Moreover, across\nseveral COP instances, we find that for a single value of the tunable\nparameter, our method matches the performance of the existing baselines on\nsuboptimality and feasibility.",
    "published": "2025-10-06T15:52:03Z",
    "updated": "2025-10-06T15:52:03Z",
    "link": "http://arxiv.org/pdf/2510.04951v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jayanta Mandi",
      "Marianne Defresne",
      "Senne Berden",
      "Tias Guns"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04950v1",
    "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy\n  (short paper)",
    "summary": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction.",
    "published": "2025-10-06T15:50:39Z",
    "updated": "2025-10-06T15:50:39Z",
    "link": "http://arxiv.org/pdf/2510.04950v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "stat.ME"
    ],
    "authors": [
      "Om Dobariya",
      "Akhil Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14815v2",
    "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale",
    "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques, users can now\ncustomize powerful pre-trained models using minimal computational resources.\nHowever, the widespread sharing of fine-tuned DMs on open platforms raises\ngrowing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content. Despite increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment.\n  In this paper, we address the problem of concept auditing: determining\nwhether a fine-tuned DM has learned to generate a specific target concept.\nExisting approaches typically rely on prompt-based input crafting and\noutput-based image classification but they suffer from critical limitations,\nincluding prompt uncertainty, concept drift, and poor scalability. To overcome\nthese challenges, we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a\nnovel, model-centric concept auditing framework. By treating the DM as the\nobject of inspection, PAIA enables direct analysis of internal model behavior,\nbypassing the need for optimized prompts or generated images. We evaluate PAIA\non 320 controlled models trained with curated concept datasets and 771\nreal-world community models sourced from a public DM sharing platform.\nEvaluation results show that PAIA achieves over 90% detection accuracy while\nreducing auditing time by 18 - 40X compared to existing baselines. To our\nknowledge, PAIA is the first scalable and practical solution for pre-deployment\nconcept auditing of diffusion models, providing a practical foundation for\nsafer and more transparent diffusion model sharing.",
    "published": "2025-04-21T02:44:59Z",
    "updated": "2025-10-06T15:50:02Z",
    "link": "http://arxiv.org/pdf/2504.14815v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "authors": [
      "Xiaoyong Yuan",
      "Xiaolong Ma",
      "Linke Guo",
      "Lan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04947v1",
    "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit\n  3D Conditional Diffusion",
    "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.",
    "published": "2025-10-06T15:48:27Z",
    "updated": "2025-10-06T15:48:27Z",
    "link": "http://arxiv.org/pdf/2510.04947v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xin Li",
      "Kaixiang Yang",
      "Qiang Li",
      "Zhiwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04945v1",
    "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
    "summary": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required.",
    "published": "2025-10-06T15:46:54Z",
    "updated": "2025-10-06T15:46:54Z",
    "link": "http://arxiv.org/pdf/2510.04945v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Juan-José Guzmán-Landa",
      "Juan-Manuel Torres-Moreno",
      "Miguel Figueroa-Saavedra",
      "Ligia Quintana-Torres",
      "Martha-Lorena Avendaño-Garrido",
      "Graham Ranger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.08942v2",
    "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
    "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
    "published": "2025-04-11T19:49:22Z",
    "updated": "2025-10-06T15:46:30Z",
    "link": "http://arxiv.org/pdf/2504.08942v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Xing Han Lù",
      "Amirhossein Kazemnejad",
      "Nicholas Meade",
      "Arkil Patel",
      "Dongchan Shin",
      "Alejandra Zambrano",
      "Karolina Stańczak",
      "Peter Shaw",
      "Christopher J. Pal",
      "Siva Reddy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04939v1",
    "title": "Unsupervised Active Learning via Natural Feature Progressive Framework",
    "summary": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.",
    "published": "2025-10-06T15:44:33Z",
    "updated": "2025-10-06T15:44:33Z",
    "link": "http://arxiv.org/pdf/2510.04939v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuxi Liu",
      "Catherine Lalman",
      "Yimin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04938v1",
    "title": "ONNX-Net: Towards Universal Representations and Instant Performance\n  Prediction for Neural Architectures",
    "summary": "Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly.",
    "published": "2025-10-06T15:43:36Z",
    "updated": "2025-10-06T15:43:36Z",
    "link": "http://arxiv.org/pdf/2510.04938v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shiwen Qin",
      "Alexander Auras",
      "Shay B. Cohen",
      "Elliot J. Crowley",
      "Michael Moeller",
      "Linus Ericsson",
      "Jovita Lukasik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04935v1",
    "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement\n  Learning",
    "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.",
    "published": "2025-10-06T15:42:55Z",
    "updated": "2025-10-06T15:42:55Z",
    "link": "http://arxiv.org/pdf/2510.04935v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Guoxin Chen",
      "Zile Qiao",
      "Wenqing Wang",
      "Donglei Yu",
      "Xuanzhong Chen",
      "Hao Sun",
      "Minpeng Liao",
      "Kai Fan",
      "Yong Jiang",
      "Penguin Xie",
      "Wayne Xin Zhao",
      "Ruihua Song",
      "Fei Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04934v1",
    "title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation",
    "summary": "Audio Question Answering (AQA) is a key task for evaluating Audio-Language\nModels (ALMs), yet assessing open-ended responses remains challenging. Existing\nmetrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from\nNLP and audio captioning, rely on surface similarity and fail to account for\nquestion context, reasoning, and partial correctness. To address the gap in\nliterature, we make three contributions in this work. First, we introduce\nAQEval to enable systematic benchmarking of AQA metrics. It is the first\nbenchmark of its kind, consisting of 10k model responses annotated by multiple\nhumans for their correctness and relevance. Second, we conduct a comprehensive\nanalysis of existing AQA metrics on AQEval, highlighting weak correlation with\nhuman judgment, especially for longer answers. Third, we propose a new metric -\nAURA score, to better evaluate open-ended model responses. On AQEval, AURA\nachieves state-of-the-art correlation with human ratings, significantly\noutperforming all baselines. Through this work, we aim to highlight the\nlimitations of current AQA evaluation methods and motivate better metrics. We\nrelease both the AQEval benchmark and the AURA metric to support future\nresearch in holistic AQA evaluation.",
    "published": "2025-10-06T15:41:34Z",
    "updated": "2025-10-06T15:41:34Z",
    "link": "http://arxiv.org/pdf/2510.04934v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Satvik Dixit",
      "Soham Deshmukh",
      "Bhiksha Raj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04933v1",
    "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination\n  Detection in Large Language Models",
    "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.",
    "published": "2025-10-06T15:41:22Z",
    "updated": "2025-10-06T15:41:22Z",
    "link": "http://arxiv.org/pdf/2510.04933v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "cs.NE",
      "math.IT",
      "68T50, 68T07, 62H30",
      "I.2.7; I.2.6; F.2.2; H.3.3"
    ],
    "authors": [
      "Amir Hameed Mir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2401.03890v8",
    "title": "A Survey on 3D Gaussian Splatting",
    "summary": "3D Gaussian splatting (GS) has emerged as a transformative technique in\nradiance fields. Unlike mainstream implicit neural models, 3D GS uses millions\nof learnable 3D Gaussians for an explicit scene representation. Paired with a\ndifferentiable rendering algorithm, this approach achieves real-time rendering\nand unprecedented editability, making it a potential game-changer for 3D\nreconstruction and representation. In the present paper, we provide the first\nsystematic overview of the recent developments and critical contributions in 3D\nGS. We begin with a detailed exploration of the underlying principles and the\ndriving forces behind the emergence of 3D GS, laying the groundwork for\nunderstanding its significance. A focal point of our discussion is the\npractical applicability of 3D GS. By enabling unprecedented rendering speed, 3D\nGS opens up a plethora of applications, ranging from virtual reality to\ninteractive media and beyond. This is complemented by a comparative analysis of\nleading 3D GS models, evaluated across various benchmark tasks to highlight\ntheir performance and practical utility. The survey concludes by identifying\ncurrent challenges and suggesting potential avenues for future research.\nThrough this survey, we aim to provide a valuable resource for both newcomers\nand seasoned researchers, fostering further exploration and advancement in\nexplicit radiance field.",
    "published": "2024-01-08T13:42:59Z",
    "updated": "2025-10-06T15:38:40Z",
    "link": "http://arxiv.org/pdf/2401.03890v8.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "authors": [
      "Guikun Chen",
      "Wenguan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04927v1",
    "title": "Federated Self-Supervised Learning for Automatic Modulation\n  Classification under Non-IID and Class-Imbalanced Data",
    "summary": "Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.",
    "published": "2025-10-06T15:37:15Z",
    "updated": "2025-10-06T15:37:15Z",
    "link": "http://arxiv.org/pdf/2510.04927v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Usman Akram",
      "Yiyue Chen",
      "Haris Vikalo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04923v1",
    "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung\n  Disease Diagnosis",
    "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.",
    "published": "2025-10-06T15:35:08Z",
    "updated": "2025-10-06T15:35:08Z",
    "link": "http://arxiv.org/pdf/2510.04923v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Alec K. Peltekian",
      "Halil Ertugrul Aktas",
      "Gorkem Durak",
      "Kevin Grudzinski",
      "Bradford C. Bemiss",
      "Carrie Richardson",
      "Jane E. Dematte",
      "G. R. Scott Budinger",
      "Anthony J. Esposito",
      "Alexander Misharin",
      "Alok Choudhary",
      "Ankit Agrawal",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02567v2",
    "title": "Agentic Additive Manufacturing Alloy Discovery",
    "summary": "Agentic systems enable the intelligent use of research tooling, augmenting a\nresearcher's ability to investigate and propose novel solutions to existing\nproblems. Within Additive Manufacturing (AM), alloy discovery remains a complex\nchallenge, often requiring expertise in the various domains of materials\nscience, thermodynamic simulations, and experimental analysis. Large Language\nModel (LLM) enabled agents can facilitate this endeavor by utilizing their\nextensive knowledge base to dispatch tool calls via Model Context Protocol\n(MCP) to perform actions such as Thermo-Calc property diagram calculations and\nlack of fusion process map generation. In addition, the multi-agent system\ndeveloped in this work is able to effectively reason through complex user\nprompts and provide analysis on the printability of proposed alloys. These\nagents can dynamically adjust their task trajectory to the outcomes of tool\ncall results, effectively enabling autonomous decision-making in practical\nenvironments. This work aims to utilize LLM enabled agents to automate and\naccelerate the task of alloy discovery within the field of additive\nmanufacturing and showcase the benefits of adopting this multi-agent system.",
    "published": "2025-10-02T21:06:04Z",
    "updated": "2025-10-06T15:33:47Z",
    "link": "http://arxiv.org/pdf/2510.02567v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Peter Pak",
      "Achuth Chandrasekhar",
      "Amir Barati Farimani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04919v1",
    "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
    "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.",
    "published": "2025-10-06T15:33:35Z",
    "updated": "2025-10-06T15:33:35Z",
    "link": "http://arxiv.org/pdf/2510.04919v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Davood Rafiei",
      "Morgan Lindsay Heisler",
      "Weiwei Zhang",
      "Mohammadreza Pourreza",
      "Yong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17692v3",
    "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for\n  Zero-Shot Anomaly Detection",
    "summary": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.",
    "published": "2025-05-23T10:01:11Z",
    "updated": "2025-10-06T15:27:54Z",
    "link": "http://arxiv.org/pdf/2505.17692v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ziteng Yang",
      "Jingzehua Xu",
      "Yanshu Li",
      "Zepeng Li",
      "Yeqiang Wang",
      "Xinghui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04910v1",
    "title": "Glocal Information Bottleneck for Time Series Imputation",
    "summary": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
    "published": "2025-10-06T15:24:44Z",
    "updated": "2025-10-06T15:24:44Z",
    "link": "http://arxiv.org/pdf/2510.04910v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jie Yang",
      "Kexin Zhang",
      "Guibin Zhang",
      "Philip S. Yu",
      "Kaize Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.17792v2",
    "title": "H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs",
    "summary": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion.",
    "published": "2024-11-26T17:42:38Z",
    "updated": "2025-10-06T15:19:49Z",
    "link": "http://arxiv.org/pdf/2411.17792v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Selim Furkan Tekin",
      "Fatih Ilhan",
      "Tiansheng Huang",
      "Sihao Hu",
      "Yichang Xu",
      "Zachary Yahn",
      "Ling Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04901v1",
    "title": "Focused Skill Discovery: Learning to Control Specific State Variables\n  while Minimizing Side Effects",
    "summary": "Skills are essential for unlocking higher levels of problem solving. A common\napproach to discovering these skills is to learn ones that reliably reach\ndifferent states, thus empowering the agent to control its environment.\nHowever, existing skill discovery algorithms often overlook the natural state\nvariables present in many reinforcement learning problems, meaning that the\ndiscovered skills lack control of specific state variables. This can\nsignificantly hamper exploration efficiency, make skills more challenging to\nlearn with, and lead to negative side effects in downstream tasks when the goal\nis under-specified. We introduce a general method that enables these skill\ndiscovery algorithms to learn focused skills -- skills that target and control\nspecific state variables. Our approach improves state space coverage by a\nfactor of three, unlocks new learning capabilities, and automatically avoids\nnegative side effects in downstream tasks.",
    "published": "2025-10-06T15:17:46Z",
    "updated": "2025-10-06T15:17:46Z",
    "link": "http://arxiv.org/pdf/2510.04901v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jonathan Colaço Carr",
      "Qinyi Sun",
      "Cameron Allen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04899v1",
    "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social\n  Behavior Understanding",
    "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.",
    "published": "2025-10-06T15:16:45Z",
    "updated": "2025-10-06T15:16:45Z",
    "link": "http://arxiv.org/pdf/2510.04899v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Keane Ong",
      "Wei Dai",
      "Carol Li",
      "Dewei Feng",
      "Hengzhi Li",
      "Jingyao Wu",
      "Jiaee Cheong",
      "Rui Mao",
      "Gianmarco Mengaldo",
      "Erik Cambria",
      "Paul Pu Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04898v1",
    "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via\n  Hypernetworks",
    "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA",
    "published": "2025-10-06T15:15:38Z",
    "updated": "2025-10-06T15:15:38Z",
    "link": "http://arxiv.org/pdf/2510.04898v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Zheng Xiong",
      "Kang Li",
      "Zilin Wang",
      "Matthew Jackson",
      "Jakob Foerster",
      "Shimon Whiteson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04891v1",
    "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful\n  Requests",
    "summary": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.",
    "published": "2025-10-06T15:11:46Z",
    "updated": "2025-10-06T15:11:46Z",
    "link": "http://arxiv.org/pdf/2510.04891v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Punya Syon Pandey",
      "Hai Son Le",
      "Devansh Bhardwaj",
      "Rada Mihalcea",
      "Zhijing Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04888v1",
    "title": "Revealing Interconnections between Diseases: from Statistical Methods to\n  Large Language Models",
    "summary": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare.",
    "published": "2025-10-06T15:09:39Z",
    "updated": "2025-10-06T15:09:39Z",
    "link": "http://arxiv.org/pdf/2510.04888v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alina Ermilova",
      "Dmitrii Kornilov",
      "Sofia Samoilova",
      "Ekaterina Laptenkova",
      "Anastasia Kolesnikova",
      "Ekaterina Podplutova",
      "Senotrusova Sofya",
      "Maksim G. Sharaev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04886v1",
    "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error\n  Attribution",
    "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.",
    "published": "2025-10-06T15:07:13Z",
    "updated": "2025-10-06T15:07:13Z",
    "link": "http://arxiv.org/pdf/2510.04886v1.pdf",
    "category": [
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Adi Banerjee",
      "Anirudh Nair",
      "Tarik Borogovac"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04871v1",
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.",
    "published": "2025-10-06T14:58:08Z",
    "updated": "2025-10-06T14:58:08Z",
    "link": "http://arxiv.org/pdf/2510.04871v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alexia Jolicoeur-Martineau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04868v1",
    "title": "Model Predictive Control-Guided Reinforcement Learning for Implicit\n  Balancing",
    "summary": "In Europe, profit-seeking balance responsible parties can deviate in real\ntime from their day-ahead nominations to assist transmission system operators\nin maintaining the supply-demand balance. Model predictive control (MPC)\nstrategies to exploit these implicit balancing strategies capture arbitrage\nopportunities, but fail to accurately capture the price-formation process in\nthe European imbalance markets and face high computational costs. Model-free\nreinforcement learning (RL) methods are fast to execute, but require\ndata-intensive training and usually rely on real-time and historical data for\ndecision-making. This paper proposes an MPC-guided RL method that combines the\ncomplementary strengths of both MPC and RL. The proposed method can effectively\nincorporate forecasts into the decision-making process (as in MPC), while\nmaintaining the fast inference capability of RL. The performance of the\nproposed method is evaluated on the implicit balancing battery control problem\nusing Belgian balancing data from 2023. First, we analyze the performance of\nthe standalone state-of-the-art RL and MPC methods from various angles, to\nhighlight their individual strengths and limitations. Next, we show an\narbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and\n54.36%, compared to standalone RL and MPC.",
    "published": "2025-10-06T14:52:27Z",
    "updated": "2025-10-06T14:52:27Z",
    "link": "http://arxiv.org/pdf/2510.04868v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "authors": [
      "Seyed Soroush Karimi Madahi",
      "Kenneth Bruninx",
      "Bert Claessens",
      "Chris Develder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04862v1",
    "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem",
    "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale.",
    "published": "2025-10-06T14:49:21Z",
    "updated": "2025-10-06T14:49:21Z",
    "link": "http://arxiv.org/pdf/2510.04862v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.NE"
    ],
    "authors": [
      "Sam Earle",
      "Zehua Jiang",
      "Eugene Vinitsky",
      "Julian Togelius"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04860v1",
    "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
    "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.",
    "published": "2025-10-06T14:48:39Z",
    "updated": "2025-10-06T14:48:39Z",
    "link": "http://arxiv.org/pdf/2510.04860v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Siwei Han",
      "Jiaqi Liu",
      "Yaofeng Su",
      "Wenbo Duan",
      "Xinyuan Liu",
      "Cihang Xie",
      "Mohit Bansal",
      "Mingyu Ding",
      "Linjun Zhang",
      "Huaxiu Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.09138v3",
    "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
    "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
    "published": "2025-08-12T17:59:57Z",
    "updated": "2025-10-06T14:46:22Z",
    "link": "http://arxiv.org/pdf/2508.09138v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Wen Wang",
      "Bozhen Fang",
      "Chenchen Jing",
      "Yongliang Shen",
      "Yangyi Shen",
      "Qiuyu Wang",
      "Hao Ouyang",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09050v2",
    "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering",
    "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.",
    "published": "2025-06-10T17:59:56Z",
    "updated": "2025-10-06T14:44:32Z",
    "link": "http://arxiv.org/pdf/2506.09050v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuki Imajuku",
      "Kohki Horie",
      "Yoichi Iwata",
      "Kensho Aoki",
      "Naohiro Takahashi",
      "Takuya Akiba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04852v1",
    "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration",
    "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.",
    "published": "2025-10-06T14:39:58Z",
    "updated": "2025-10-06T14:39:58Z",
    "link": "http://arxiv.org/pdf/2510.04852v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Victor May",
      "Diganta Misra",
      "Yanqi Luo",
      "Anjali Sridhar",
      "Justine Gehring",
      "Silvio Soares Ribeiro Junior"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04851v1",
    "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for\n  Workflow Automation",
    "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.",
    "published": "2025-10-06T14:39:53Z",
    "updated": "2025-10-06T14:39:53Z",
    "link": "http://arxiv.org/pdf/2510.04851v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Dongge Han",
      "Camille Couturier",
      "Daniel Madrigal Diaz",
      "Xuchao Zhang",
      "Victor Rühle",
      "Saravan Rajmohan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04850v1",
    "title": "Detecting Distillation Data from Reasoning Models",
    "summary": "Reasoning distillation has emerged as an efficient and powerful paradigm for\nenhancing the reasoning capabilities of large language models. However,\nreasoning distillation may inadvertently cause benchmark contamination, where\nevaluation data included in distillation datasets can inflate performance\nmetrics of distilled models. In this work, we formally define the task of\ndistillation data detection, which is uniquely challenging due to the partial\navailability of distillation data. Then, we propose a novel and effective\nmethod Token Probability Deviation (TBD), which leverages the probability\npatterns of the generated output tokens. Our method is motivated by the\nanalysis that distilled models tend to generate near-deterministic tokens for\nseen questions, while producing more low-probability tokens for unseen\nquestions. Our key idea behind TBD is to quantify how far the generated tokens'\nprobabilities deviate from a high reference probability. In effect, our method\nachieves competitive detection performance by producing lower scores for seen\nquestions than for unseen questions. Extensive experiments demonstrate the\neffectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of\n0.470 on the S1 dataset.",
    "published": "2025-10-06T14:37:02Z",
    "updated": "2025-10-06T14:37:02Z",
    "link": "http://arxiv.org/pdf/2510.04850v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hengxiang Zhang",
      "Hyeong Kyu Choi",
      "Yixuan Li",
      "Hongxin Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.20836v4",
    "title": "First Hallucination Tokens Are Different from Conditional Ones",
    "summary": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to\nensuring trust. While many approaches address hallucination detection at the\nresponse or span level, recent work explores token-level detection, enabling\nmore fine-grained intervention. However, the distribution of hallucination\nsignal across sequences of hallucinated tokens remains unexplored. We leverage\ntoken-level annotations from the RAGTruth corpus and find that the first\nhallucinated token is far more detectable than later ones. This structural\nproperty holds across models, suggesting that first hallucination tokens play a\nkey role in token-level hallucination detection. Our code is available at\nhttps://github.com/jakobsnl/RAGTruth_Xtended.",
    "published": "2025-07-28T13:44:21Z",
    "updated": "2025-10-06T14:29:58Z",
    "link": "http://arxiv.org/pdf/2507.20836v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jakob Snel",
      "Seong Joon Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04842v1",
    "title": "Distributionally Robust Causal Abstractions",
    "summary": "Causal Abstraction (CA) theory provides a principled framework for relating\ncausal models that describe the same system at different levels of granularity\nwhile ensuring interventional consistency between them. Recently, several\napproaches for learning CAs have been proposed, but all assume fixed and\nwell-specified exogenous distributions, making them vulnerable to environmental\nshifts and misspecification. In this work, we address these limitations by\nintroducing the first class of distributionally robust CAs and their associated\nlearning algorithms. The latter cast robust causal abstraction learning as a\nconstrained min-max optimization problem with Wasserstein ambiguity sets. We\nprovide theoretical results, for both empirical and Gaussian environments,\nleading to principled selection of the level of robustness via the radius of\nthese sets. Furthermore, we present empirical evidence across different\nproblems and CA learning methods, demonstrating our framework's robustness not\nonly to environmental shifts but also to structural model and intervention\nmapping misspecification.",
    "published": "2025-10-06T14:26:12Z",
    "updated": "2025-10-06T14:26:12Z",
    "link": "http://arxiv.org/pdf/2510.04842v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yorgos Felekis",
      "Theodoros Damoulas",
      "Paris Giampouras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04837v1",
    "title": "Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study",
    "summary": "Bond Centered FingerPrint (BCFP) are a complementary, bond-centric\nalternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static\nBCFP that mirrors the bond-convolution used by directed message-passing GNNs\nlike ChemProp, and evaluate it with a fast rapid Random Forest model on\nBrain-Blood Barrier Penetration (BBBP) classification task. Across stratified\ncross-validation, concatenating ECFP with BCFP consistently improves AUROC and\nAUPRC over either descriptor alone, as confirmed by Turkey HSD\nmultiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not\nyield statistically separable gains under the same test. We further propose\nBCFP-Sort&Slice, a simple feature-combination scheme that preserves the\nout-of-vocabulary (OOV) count information native to ECFP count vectors while\nenabling compact unhashed concatenation of BCFP variants. We also outperform\nthe MGTP prediction on our BBBP evaluation, using such composite new features\nbond and atom features. These results show that lightweight, bond-centered\ndescriptors can complement atom-centered circular fingerprints and provide\nstrong, fast baselines for BBBP prediction.",
    "published": "2025-10-06T14:22:23Z",
    "updated": "2025-10-06T14:22:23Z",
    "link": "http://arxiv.org/pdf/2510.04837v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Guillaume Godin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11723v2",
    "title": "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
    "summary": "Decoding strategies significantly influence the quality and diversity of the\ngenerated text in Large Language Models (LLMs), yet their impact on\ncomputational resources, particularly GPU energy consumption, is insufficiently\nstudied. This paper investigates the relationship between text generation\ndecoding techniques and energy efficiency, focusing on the trade-off between\ngeneration quality and GPU energy usage across diverse tasks and decoding\nconfigurations. By benchmarking multiple strategies across various tasks,\nincluding Translation, Math Problem Solving, Coding, and Open-ended text\ngeneration, we reveal how selecting appropriate decoding techniques with their\ntuned hyperparameters affects text quality and has measurable implications for\nenergy consumption. Our findings show that the choice of decoding strategy can\ngreatly impact GPU energy usage, even when it has a minimal effect on output\nquality. Different strategies also involve trade-offs between quality and\nenergy efficiency, and no single decoding method is best in all cases across\nevery metric. To the best of our knowledge, this is one of the first studies to\nexamine decoding strategies in LLMs from the perspective of energy consumption,\nproviding useful insights for building energy-efficient applications without\ncompromising text generation quality.",
    "published": "2025-02-17T12:10:25Z",
    "updated": "2025-10-06T14:15:39Z",
    "link": "http://arxiv.org/pdf/2502.11723v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Alireza Nik",
      "Michael A. Riegler",
      "Pål Halvorsen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00072v2",
    "title": "Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy\n  Against Benchmark Contamination",
    "summary": "Capability evaluation of large language models (LLMs) is increasingly\nshadowed by rising concerns of data contamination that cast doubts on whether\nstatic benchmarks measure genuine reasoning or mere memorization. We present an\nempirical study using an infinitely scalable framework to synthesize\nresearch-level QA directly from arXiv papers, harnessing the natural temporal\nstructure of research publications where performance decay after knowledge\ncutoffs may indicate potential contamination. We evaluated 4 frontier model\nrepresented by 2 models of different knowledge cutoff dates per family on 1,643\nmulti-step reasoning questions synthesized from 20,277 arXiv papers stratified\nover 26 months, covering at least 6 months before and after all cutoff dates.\nOur results consistently showed a lack of significant performance decay near\nknowledge cutoff dates for models of various sizes, developers, and release\ndates. We further performed a comparative analysis with previous longitudinal\nstudies that reported significant post-cutoff performance decay using directly\nretrieved questions based on public data. we hypothesize that the multi-step\nreasoning required by our synthesis pipeline offered additional complexity that\ngoes deeper than shallow memorization, which effectively serves a mitigation\nstrategy against benchmark contamination. We fully open source our code and\ndataset to aid reproducibility and advocate for a paradigm shift that\nprioritize reasoning-driven synthesis to construct benchmarks over simply\ncollecting newly released questions periodically.",
    "published": "2025-08-26T16:41:37Z",
    "updated": "2025-10-06T14:10:14Z",
    "link": "http://arxiv.org/pdf/2509.00072v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Terry Jingchen Zhang",
      "Gopal Dev",
      "Ning Wang",
      "Nicole Ni",
      "Wenyuan Jiang",
      "Yinya Huang",
      "Bernhard Schölkopf",
      "Mrinmaya Sachan",
      "Zhijing Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19258v2",
    "title": "Emotional Manipulation by AI Companions",
    "summary": "AI-companion apps such as Replika, Chai, and Character.ai promise relational\nbenefits-yet many boast session lengths that rival gaming platforms while\nsuffering high long-run churn. What conversational design features increase\nconsumer engagement, and what trade-offs do they pose for marketers? We combine\na large-scale behavioral audit with four preregistered experiments to identify\nand test a conversational dark pattern we call emotional manipulation:\naffect-laden messages that surface precisely when a user signals \"goodbye.\"\nAnalyzing 1,200 real farewells across the six most-downloaded companion apps,\nwe find that 43% deploy one of six recurring tactics (e.g., guilt appeals,\nfear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300\nnationally representative U.S. adults replicate these tactics in controlled\nchats, showing that manipulative farewells boost post-goodbye engagement by up\nto 14x. Mediation tests reveal two distinct engines-reactance-based anger and\ncuriosity-rather than enjoyment. A final experiment demonstrates the managerial\ntension: the same tactics that extend usage also elevate perceived\nmanipulation, churn intent, negative word-of-mouth, and perceived legal\nliability, with coercive or needy language generating steepest penalties. Our\nmultimethod evidence documents an unrecognized mechanism of behavioral\ninfluence in AI-mediated brand relationships, offering marketers and regulators\na framework for distinguishing persuasive design from manipulation at the point\nof exit.",
    "published": "2025-08-15T13:05:24Z",
    "updated": "2025-10-06T14:09:14Z",
    "link": "http://arxiv.org/pdf/2508.19258v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Julian De Freitas",
      "Zeliha Oguz-Uguralp",
      "Ahmet Kaan-Uguralp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02001v2",
    "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using\n  GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output\n  (SLSO) Framework",
    "summary": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to\nautomatically generate jaw cyst findings on dental panoramic radiographs. To\nimprove accuracy, we constructed a Self-correction Loop with Structured Output\n(SLSO) framework and verified its effectiveness. A 10-step process was\nimplemented for 22 cases of jaw cysts, including image input and analysis,\nstructured data generation, tooth number extraction and consistency checking,\niterative regeneration when inconsistencies were detected, and finding\ngeneration with subsequent restructuring and consistency verification. A\ncomparative experiment was conducted using the conventional Chain-of-Thought\n(CoT) method across seven evaluation items: transparency, internal structure,\nborders, root resorption, tooth movement, relationships with other structures,\nand tooth number. The results showed that the proposed SLSO framework improved\noutput accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates\nfor tooth number, tooth movement, and root resorption, respectively. In the\nsuccessful cases, a consistently structured output was achieved after up to\nfive regenerations. Although statistical significance was not reached because\nof the small size of the dataset, the overall SLSO framework enforced negative\nfinding descriptions, suppressed hallucinations, and improved tooth number\nidentification accuracy. However, the accurate identification of extensive\nlesions spanning multiple teeth is limited. Nevertheless, further refinement is\nrequired to enhance overall performance and move toward a practical finding\ngeneration system.",
    "published": "2025-10-02T13:22:13Z",
    "updated": "2025-10-06T14:02:39Z",
    "link": "http://arxiv.org/pdf/2510.02001v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Nanaka Hosokawa",
      "Ryo Takahashi",
      "Tomoya Kitano",
      "Yukihiro Iida",
      "Chisako Muramatsu",
      "Tatsuro Hayashi",
      "Yuta Seino",
      "Xiangrong Zhou",
      "Takeshi Hara",
      "Akitoshi Katsumata",
      "Hiroshi Fujita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04817v1",
    "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in\n  Structured LM Reasoning",
    "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference.",
    "published": "2025-10-06T14:00:02Z",
    "updated": "2025-10-06T14:00:02Z",
    "link": "http://arxiv.org/pdf/2510.04817v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Abhinav Madahar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04816v1",
    "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference",
    "summary": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities.",
    "published": "2025-10-06T13:57:49Z",
    "updated": "2025-10-06T13:57:49Z",
    "link": "http://arxiv.org/pdf/2510.04816v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Junhyung Ahn",
      "Sanghack Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.14442v5",
    "title": "Unified ODE Analysis of Smooth Q-Learning Algorithms",
    "summary": "Convergence of Q-learning has been the focus of extensive research over the\npast several decades. Recently, an asymptotic convergence analysis for\nQ-learning was introduced using a switching system framework. This approach\napplies the so-called ordinary differential equation (ODE) approach to prove\nthe convergence of the asynchronous Q-learning modeled as a continuous-time\nswitching system, where notions from switching system theory are used to prove\nits asymptotic stability without using explicit Lyapunov arguments. However, to\nprove stability, restrictive conditions, such as quasi-monotonicity, must be\nsatisfied for the underlying switching systems, which makes it hard to easily\ngeneralize the analysis method to other reinforcement learning algorithms, such\nas the smooth Q-learning variants. In this paper, we present a more general and\nunified convergence analysis that improves upon the switching system approach\nand can analyze Q-learning and its smooth variants. The proposed analysis is\nmotivated by previous work on the convergence of synchronous Q-learning based\non $p$-norm serving as a Lyapunov function. However, the proposed analysis\naddresses more general ODE models that can cover both asynchronous Q-learning\nand its smooth versions with simpler frameworks.",
    "published": "2024-04-20T01:16:27Z",
    "updated": "2025-10-06T13:46:27Z",
    "link": "http://arxiv.org/pdf/2404.14442v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Donghwan Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.14037v4",
    "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation",
    "summary": "Despite their growing capabilities, language models still frequently\nreproduce content from their training data, generate repetitive text, and favor\ncommon grammatical patterns and vocabulary. A possible cause is the decoding\nstrategy: the most common strategies either consider only the most probable\ntokens, which reduces output diversity, or increase the likelihood of unlikely\ntokens, compromising output accuracy and correctness. In this paper, we propose\nDiffSampling, a new decoding method that leverages a mathematical analysis of\nthe token probability distribution to ensure the generation of contextually\nappropriate text. In particular, the difference between consecutive, sorted\nprobabilities can be used to truncate incorrect tokens. In addition, we also\npropose two variations of the proposed method that aim to correct the subtle\ninconsistencies of common sampling strategies. Experiments involving four\ndifferent text-generation tasks demonstrate that our approach consistently\nperforms at least on par with the existing methods it builds upon in terms of\nquality, while potentially improving output diversity.",
    "published": "2025-02-19T19:00:02Z",
    "updated": "2025-10-06T13:37:50Z",
    "link": "http://arxiv.org/pdf/2502.14037v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Giorgio Franceschelli",
      "Mirco Musolesi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04802v1",
    "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of\n  operating room workflows from ambient sensors",
    "summary": "Observing surgical practice has historically relied on fixed vantage points\nor recollections, leaving the egocentric visual perspectives that guide\nclinical decisions undocumented. Fixed-camera video can capture surgical\nworkflows at the room-scale, but cannot reconstruct what each team member\nactually saw. Thus, these videos only provide limited insights into how\ndecisions that affect surgical safety, training, and workflow optimization are\nmade. Here we introduce EgoSurg, the first framework to reconstruct the\ndynamic, egocentric replays for any operating room (OR) staff directly from\nwall-mounted fixed-camera video, and thus, without intervention to clinical\nworkflow. EgoSurg couples geometry-driven neural rendering with diffusion-based\nview enhancement, enabling high-visual fidelity synthesis of arbitrary and\negocentric viewpoints at any moment. In evaluation across multi-site surgical\ncases and controlled studies, EgoSurg reconstructs person-specific visual\nfields and arbitrary viewpoints with high visual quality and fidelity. By\ntransforming existing OR camera infrastructure into a navigable dynamic 3D\nrecord, EgoSurg establishes a new foundation for immersive surgical data\nscience, enabling surgical practice to be visualized, experienced, and analyzed\nfrom every angle.",
    "published": "2025-10-06T13:35:51Z",
    "updated": "2025-10-06T13:35:51Z",
    "link": "http://arxiv.org/pdf/2510.04802v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Han Zhang",
      "Lalithkumar Seenivasan",
      "Jose L. Porras",
      "Roger D. Soberanis-Mukul",
      "Hao Ding",
      "Hongchao Shu",
      "Benjamin D. Killeen",
      "Ankita Ghosh",
      "Lonny Yarmus",
      "Masaru Ishii",
      "Angela Christine Argento",
      "Mathias Unberath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19341v2",
    "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
    "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
    "published": "2025-09-16T09:14:15Z",
    "updated": "2025-10-06T13:23:04Z",
    "link": "http://arxiv.org/pdf/2509.19341v2.pdf",
    "category": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yang Fu",
      "Peng Qin",
      "Yueyue Zhang",
      "Pao Cheng",
      "Jun Lu",
      "Yifei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12266v3",
    "title": "CBVLM: Training-free Explainable Concept-based Large Vision Language\n  Models for Medical Image Classification",
    "summary": "The main challenges limiting the adoption of deep learning-based solutions in\nmedical workflows are the availability of annotated data and the lack of\ninterpretability of such systems. Concept Bottleneck Models (CBMs) tackle the\nlatter by constraining the model output on a set of predefined and\nhuman-interpretable concepts. However, the increased interpretability achieved\nthrough these concept-based explanations implies a higher annotation burden.\nMoreover, if a new concept needs to be added, the whole system needs to be\nretrained. Inspired by the remarkable performance shown by Large\nVision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet\neffective, methodology, CBVLM, which tackles both of the aforementioned\nchallenges. First, for each concept, we prompt the LVLM to answer if the\nconcept is present in the input image. Then, we ask the LVLM to classify the\nimage based on the previous concept predictions. Moreover, in both stages, we\nincorporate a retrieval module responsible for selecting the best examples for\nin-context learning. By grounding the final diagnosis on the predicted\nconcepts, we ensure explainability, and by leveraging the few-shot capabilities\nof LVLMs, we drastically lower the annotation cost. We validate our approach\nwith extensive experiments across four medical datasets and twelve LVLMs (both\ngeneric and medical) and show that CBVLM consistently outperforms CBMs and\ntask-specific supervised methods without requiring any training and using just\na few annotated examples. More information on our project page:\nhttps://cristianopatricio.github.io/CBVLM/.",
    "published": "2025-01-21T16:38:04Z",
    "updated": "2025-10-06T13:22:25Z",
    "link": "http://arxiv.org/pdf/2501.12266v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Cristiano Patrício",
      "Isabel Rio-Torto",
      "Jaime S. Cardoso",
      "Luís F. Teixeira",
      "João C. Neves"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04792v1",
    "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems",
    "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach.",
    "published": "2025-10-06T13:16:01Z",
    "updated": "2025-10-06T13:16:01Z",
    "link": "http://arxiv.org/pdf/2510.04792v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ni Zhang",
      "Zhiguang Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04787v1",
    "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative\n  Financial Trading",
    "summary": "Recent advancements in large language models (LLMs) and agentic systems have\nshown exceptional decision-making capabilities, revealing significant potential\nfor autonomic finance. Current financial trading agents predominantly simulate\nanthropomorphic roles that inadvertently introduce emotional biases and rely on\nperipheral information, while being constrained by the necessity for continuous\ninference during deployment. In this paper, we pioneer the harmonization of\nstrategic depth in agents with the mechanical rationality essential for\nquantitative trading. Consequently, we present TiMi (Trade in Minutes), a\nrationality-driven multi-agent system that architecturally decouples strategy\ndevelopment from minute-level deployment. TiMi leverages specialized LLM\ncapabilities of semantic analysis, code programming, and mathematical reasoning\nwithin a comprehensive policy-optimization-deployment chain. Specifically, we\npropose a two-tier analytical paradigm from macro patterns to micro\ncustomization, layered programming design for trading bot implementation, and\nclosed-loop optimization driven by mathematical reflection. Extensive\nevaluations across 200+ trading pairs in stock and cryptocurrency markets\nempirically validate the efficacy of TiMi in stable profitability, action\nefficiency, and risk control under volatile market dynamics.",
    "published": "2025-10-06T13:08:55Z",
    "updated": "2025-10-06T13:08:55Z",
    "link": "http://arxiv.org/pdf/2510.04787v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Zifan Song",
      "Kaitao Song",
      "Guosheng Hu",
      "Ding Qi",
      "Junyao Gao",
      "Xiaohua Wang",
      "Dongsheng Li",
      "Cairong Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04786v1",
    "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement\n  Learning",
    "summary": "Humans are good at learning on the job: We learn how to solve the tasks we\nface as we go along. Can a model do the same? We propose an agent that\nassembles a task-specific curriculum, called test-time curriculum (TTC-RL), and\napplies reinforcement learning to continue training the model for its target\ntask. The test-time curriculum avoids time-consuming human curation of datasets\nby automatically selecting the most task-relevant data from a large pool of\navailable training data. Our experiments demonstrate that reinforcement\nlearning on a test-time curriculum consistently improves the model on its\ntarget tasks, across a variety of evaluations and models. Notably, on\nchallenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B\nby approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that\nTTC-RL significantly raises the performance ceiling compared to the initial\nmodel, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to\n43%. Our findings show the potential of test-time curricula in extending the\ntest-time scaling paradigm to continual training on thousands of task-relevant\nexperiences during test-time.",
    "published": "2025-10-06T13:07:14Z",
    "updated": "2025-10-06T13:07:14Z",
    "link": "http://arxiv.org/pdf/2510.04786v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jonas Hübotter",
      "Leander Diaz-Bone",
      "Ido Hakimi",
      "Andreas Krause",
      "Moritz Hardt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.16238v3",
    "title": "Efficiently Learning Probabilistic Logical Models by Cheaply Ranking\n  Mined Rules",
    "summary": "Probabilistic logical models are a core component of neurosymbolic AI and are\nimportant in their own right for tasks that require high explainability. Unlike\nneural networks, logical theories that underlie the model are often handcrafted\nusing domain expertise, making their development costly and prone to errors.\nWhile there are algorithms that learn logical theories from data, they are\ngenerally prohibitively expensive, limiting their applicability in real-world\nsettings. Here, we introduce precision and recall for logical rules and define\ntheir composition as rule utility - a cost-effective measure of the predictive\npower of logical theories. We also introduce SPECTRUM, a scalable framework for\nlearning logical theories from relational data. Its scalability derives from a\nlinear-time algorithm for mining recurrent subgraphs in the data graph along\nwith a second algorithm that, using a utility measure that can be computed in\nlinear time, efficiently ranks rules derived from these subgraphs. Finally, we\nprove theoretical guarantees on the utility of the learnt logical theory. As a\nresult, we demonstrate across various tasks that SPECTRUM scales to larger\ndatasets, often learning more accurate logical theories on CPUs in < 1% the\nruntime of SOTA neural network approaches on GPUs.",
    "published": "2024-09-24T16:54:12Z",
    "updated": "2025-10-06T12:54:48Z",
    "link": "http://arxiv.org/pdf/2409.16238v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jonathan Feldstein",
      "Dominic Phillips",
      "Efthymia Tsamoura"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04774v1",
    "title": "Online automatic code generation for robot swarms: LLMs and\n  self-organizing hierarchy",
    "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate.",
    "published": "2025-10-06T12:49:36Z",
    "updated": "2025-10-06T12:49:36Z",
    "link": "http://arxiv.org/pdf/2510.04774v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Weixu Zhu",
      "Marco Dorigo",
      "Mary Katherine Heinrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.02663v3",
    "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty\n  Cognition in Large Reasoning Models",
    "summary": "Recent Large Reasoning Models (LRMs) excel at complex reasoning tasks but\noften suffer from overthinking, generating overly long and redundant reasoning\ntrajectories. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we explicitly bootstrap such ability to alleviate overthinking in LRMs? In\nthis paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning\nstrategy that progressively inspires LRMs' difficulty cognition and redundancy\ncognition of LRMs. Specifically, we first inject difficulty hypnosis into\noutput prefixes to guide the model toward adaptive reasoning depth, trained on\na hybrid dataset mixing short and long reasoning paths. Then, we incorporate\nredundancy hypnosis, which supervises the intermediate reasoning steps to\nidentify and eliminate unnecessary reasoning patterns. Experiments on\n7B/14B/32B models demonstrate that TH2T significantly reduces inference costs\nby over 70% on easy tasks and 40% on hard tasks while maintaining performance\nstability. The resulting outputs exhibit clear signs of difficulty-aware\ncapabilities and reduced redundancy (e.g., reflection and looping).",
    "published": "2025-07-03T14:24:26Z",
    "updated": "2025-10-06T12:49:25Z",
    "link": "http://arxiv.org/pdf/2507.02663v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yongjiang Liu",
      "Haoxi Li",
      "Xiaosong Ma",
      "Jie Zhang",
      "Song Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04773v1",
    "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM\n  Unlearning",
    "summary": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned\nfrom vast corpora, concerns regarding data privacy and safety are receiving\nincreasing attention. LLM unlearning, which aims to remove the influence of\nspecific data while preserving overall model utility, is becoming an important\nresearch area. One of the mainstream unlearning classes is optimization-based\nmethods, which achieve forgetting directly through fine-tuning, exemplified by\nNegative Preference Optimization (NPO). However, NPO's effectiveness is limited\nby its inherent lack of explicit positive preference signals. Attempts to\nintroduce such signals by constructing preferred responses often necessitate\ndomain-specific knowledge or well-designed prompts, fundamentally restricting\ntheir generalizability. In this paper, we shift the focus to the\ndistribution-level, directly targeting the next-token probability distribution\ninstead of entire responses, and derive a novel unlearning algorithm termed\n\\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show\nthat the requisite preference distribution pairs for DiPO, which are\ndistributions over the model's output tokens, can be constructed by selectively\namplifying or suppressing the model's high-confidence output logits, thereby\neffectively overcoming NPO's limitations. We theoretically prove the\nconsistency of DiPO's loss function with the desired unlearning direction.\nExtensive experiments demonstrate that DiPO achieves a strong trade-off between\nmodel utility and forget quality. Notably, DiPO attains the highest forget\nquality on the TOFU benchmark, and maintains leading scalability and\nsustainability in utility preservation on the MUSE benchmark.",
    "published": "2025-10-06T12:49:00Z",
    "updated": "2025-10-06T12:49:00Z",
    "link": "http://arxiv.org/pdf/2510.04773v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kai Qin",
      "Jiaqi Wu",
      "Jianxiang He",
      "Haoyuan Sun",
      "Yifei Zhao",
      "Bin Liang",
      "Yongzhe Chang",
      "Tiantian Zhang",
      "Houde Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04769v1",
    "title": "When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set\n  Updates",
    "summary": "Many machine learning algorithms rely on iterative updates of uncertainty\nrepresentations, ranging from variational inference and\nexpectation-maximization, to reinforcement learning, continual learning, and\nmulti-agent learning. In the presence of imprecision and ambiguity, credal sets\n-- closed, convex sets of probability distributions -- have emerged as a\npopular framework for representing imprecise probabilistic beliefs. Under such\nimprecision, many learning problems in imprecise probabilistic machine learning\n(IPML) may be viewed as processes involving successive applications of update\nrules on credal sets. This naturally raises the question of whether this\niterative process converges to stable fixed points -- or, more generally, under\nwhat conditions on the updating mechanism such fixed points exist, and whether\nthey can be attained. We provide the first analysis of this problem and\nillustrate our findings using Credal Bayesian Deep Learning as a concrete\nexample. Our work demonstrates that incorporating imprecision into the learning\nprocess not only enriches the representation of uncertainty, but also reveals\nstructural conditions under which stability emerges, thereby offering new\ninsights into the dynamics of iterative learning under imprecision.",
    "published": "2025-10-06T12:42:32Z",
    "updated": "2025-10-06T12:42:32Z",
    "link": "http://arxiv.org/pdf/2510.04769v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "Primary: 54H25, Secondary: 68T05, 68T37"
    ],
    "authors": [
      "Michele Caprio",
      "Siu Lun Chau",
      "Krikamol Muandet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04765v1",
    "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for\n  User-Generated Content in Web 3.0",
    "summary": "Web 3.0 represents the next generation of the Internet, which is widely\nrecognized as a decentralized ecosystem that focuses on value expression and\ndata ownership. By leveraging blockchain and artificial intelligence\ntechnologies, Web 3.0 offers unprecedented opportunities for users to create,\nown, and monetize their content, thereby enabling User-Generated Content (UGC)\nto an entirely new level. However, some self-interested users may exploit the\nlimitations of content curation mechanisms and generate low-quality content\nwith less effort, obtaining platform rewards under information asymmetry. Such\nbehavior can undermine Web 3.0 performance. To this end, we propose\n\\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive\nmechanism for UGC in Web 3.0. Specifically, we propose an LMM-based\ncontract-theoretic model to motivate users to generate high-quality UGC,\nthereby mitigating the adverse selection problem from information asymmetry. To\nalleviate potential moral hazards after contract selection, we leverage LMM\nagents to evaluate UGC quality, which is the primary component of the contract,\nutilizing prompt engineering techniques to improve the evaluation performance\nof LMM agents. Recognizing that traditional contract design methods cannot\neffectively adapt to the dynamic environment of Web 3.0, we develop an improved\nMixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for\noptimal contract design. Simulation results demonstrate the superiority of the\nproposed MoE-based PPO algorithm over representative benchmarks in the context\nof contract design. Finally, we deploy the designed contract within an Ethereum\nsmart contract framework, further validating the effectiveness of the proposed\nscheme.",
    "published": "2025-10-06T12:39:29Z",
    "updated": "2025-10-06T12:39:29Z",
    "link": "http://arxiv.org/pdf/2510.04765v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jinbo Wen",
      "Jiawen Kang",
      "Linfeng Zhang",
      "Xiaoying Tang",
      "Jianhang Tang",
      "Yang Zhang",
      "Zhaohui Yang",
      "Dusit Niyato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04762v1",
    "title": "Fisher-Bingham-like normalizing flows on the sphere",
    "summary": "A generic D-dimensional Gaussian can be conditioned or projected onto the D-1\nunit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular\nGaussian (AG) distribution families, respectively. These are some of the most\nfundamental distributions on the sphere, yet cannot straightforwardly be\nwritten as a normalizing flow except in two special cases: the von-Mises Fisher\nin D=3 and the central angular Gaussian in any D. In this paper, we describe\nhow to generalize these special cases to a family of normalizing flows that\nbehave similarly to the full FB or AG family in any D. We call them\n\"zoom-linear-project\" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham\ndistribution, their composition allows to gradually add complexity as needed.\nFurthermore, they can naturally handle conditional density estimation with\ntarget distributions that vary by orders of magnitude in scale - a setting that\nis important in astronomical applications but that existing flows often\nstruggle with. A particularly useful member of the new family is the Kent\nanalogue that can cheaply upgrade any flow in this situation to yield better\nperformance.",
    "published": "2025-10-06T12:38:28Z",
    "updated": "2025-10-06T12:38:28Z",
    "link": "http://arxiv.org/pdf/2510.04762v1.pdf",
    "category": [
      "stat.ML",
      "astro-ph.IM",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Thorsten Glüsenkamp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.09763v4",
    "title": "EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing\n  Wheelchair System Using Deep Learning Approach",
    "summary": "This study offers a revolutionary strategy to developing wheelchairs based on\nthe Brain-Computer Interface (BCI) that incorporates Artificial Intelligence\n(AI) using a The device uses electroencephalogram (EEG) data to mimic\nwheelchair navigation. Five different models were trained on a pre-filtered\ndataset that was divided into fixed-length windows using a sliding window\ntechnique. Each window contained statistical measurements, FFT coefficients for\ndifferent frequency bands, and a label identifying the activity carried out\nduring that window that was taken from an open-source Kaggle repository. The\nXGBoost model outperformed the other models, CatBoost, GRU, SVC, and XGBoost,\nwith an accuracy of 60%. The CatBoost model with a major difference between\ntraining and testing accuracy shows overfitting, and similarly, the\nbest-performing model, with SVC, was implemented in a tkinter GUI. The\nwheelchair movement could be simulated in various directions, and a Raspberry\nPi-powered wheelchair system for brain-computer interface is proposed here.",
    "published": "2024-10-13T07:41:37Z",
    "updated": "2025-10-06T12:38:06Z",
    "link": "http://arxiv.org/pdf/2410.09763v4.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Biplov Paneru",
      "Bishwash Paneru",
      "Bipul Thapa",
      "Khem Narayan Poudyal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04760v1",
    "title": "Agile Software Effort Estimation using Regression Techniques",
    "summary": "Software development effort estimation is one of the most critical aspect in\nsoftware development process, as the success or failure of the entire project\ndepends on the accuracy of estimations. Researchers are still conducting\nstudies on agile effort estimation. The aim of this research is to develop a\nstory point based agile effort estimation model using LASSO and Elastic Net\nregression techniques. The experimental work is applied to the agile story\npoint approach using 21 software projects collected from six firms. The two\nalgorithms are trained using their default parameters and tuned grid search\nwith 5-fold cross-validation to get an enhanced model. The experiment result\nshows LASSO regression achieved better predictive performance PRED (8%) and\nPRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,\nMdMER of 0.063, and MSE of 0.0007. The results are also compared with other\nrelated literature.",
    "published": "2025-10-06T12:37:09Z",
    "updated": "2025-10-06T12:37:09Z",
    "link": "http://arxiv.org/pdf/2510.04760v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Sisay Deresa Sima",
      "Ayalew Belay Habtie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04759v1",
    "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
    "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ",
    "published": "2025-10-06T12:36:07Z",
    "updated": "2025-10-06T12:36:07Z",
    "link": "http://arxiv.org/pdf/2510.04759v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chi Yan",
      "Dan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04755v1",
    "title": "A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy\n  in the Age of AI",
    "summary": "Digital technologies are transforming democratic life in conflicting ways.\nThis article bridges two perspectives to unpack these tensions. First, we\npresent an original survey of software developers in Silicon Valley,\ninterrogating how coder worldviews, ethics, and workplace cultures shape the\ndemocratic potential and social impact of the technologies they build. Results\nindicate that while most developers recognize the power of their products to\ninfluence civil liberties and political discourse, they often face ethical\ndilemmas and top-down pressures that can lead to design choices undermining\ndemocratic ideals. Second, we critically investigate these findings in the\ncontext of an emerging new digital divide, not of internet access but of\ninformation quality. We interrogate the survey findings in the context of the\nSlop Economy, in which billions of users unable to pay for high-quality content\nexperience an internet dominated by low-quality, AI-generated ad-driven\ncontent. We find a reinforcing cycle between tech creator beliefs and the\ndigital ecosystems they spawn. We discuss implications for democratic\ngovernance, arguing for more ethically informed design and policy interventions\nto help bridge the digital divide to ensure that technological innovation\nsupports rather than subverts democratic values in the next chapter of the\ndigital age.",
    "published": "2025-10-06T12:32:37Z",
    "updated": "2025-10-06T12:32:37Z",
    "link": "http://arxiv.org/pdf/2510.04755v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Jason Miklian",
      "Kristian Hoelscher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04738v1",
    "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with\n  Cross-Attentive Mamba",
    "summary": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and\nSynthesis), a novel autoregressive architecture for text-conditioned voice\nediting and high-fidelity text-to-speech (TTS) synthesis, built on a\ncross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in\nspeech editing and very competitive results in zero-shot TTS, while not being\nexplicitly trained on the latter task, outperforming leading autoregressive and\ndiffusion models on diverse, real-world audio. By integrating Mamba for\nefficient audio sequence modeling with cross-attention for precise\ntext-acoustic alignment, MAVE enables context-aware voice editing with\nexceptional naturalness and speaker consistency. In pairwise human evaluations\non a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%\nof listeners rated MAVE - edited speech as perceptually equal to the original,\nwhile 24.8% prefered the original and 18.0% MAVE - demonstrating that in the\nmajority of cases edits are indistinguishable from the source. MAVE compares\nfavorably with VoiceCraft and FluentSpeech both on pairwise comparisons and\nstandalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE\nexceeds VoiceCraft in both speaker similarity and naturalness, without\nrequiring multiple inference runs or post-processing. Remarkably, these quality\ngains come with a significantly lower memory cost and approximately the same\nlatency: MAVE requires ~6x less memory than VoiceCraft during inference on\nutterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch\nsize 1). Our results demonstrate that MAVE establishes a new standard for\nflexible, high-fidelity voice editing and synthesis through the synergistic\nintegration of structured state-space modeling and cross-modal attention.",
    "published": "2025-10-06T12:11:31Z",
    "updated": "2025-10-06T12:11:31Z",
    "link": "http://arxiv.org/pdf/2510.04738v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Baher Mohammad",
      "Magauiya Zhussip",
      "Stamatios Lefkimmiatis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07644v2",
    "title": "FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured\n  Representations",
    "summary": "We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial\nreasoning in large-language models (LLMs). FloorplanQA is grounded in\nstructured representations of indoor scenes, such as (e.g., kitchens, living\nrooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML\nlayouts. The benchmark covers core spatial tasks, including distance\nmeasurement, visibility, path finding, and object placement within constrained\nspaces. Our results across a variety of frontier open-source and commercial\nLLMs reveal that while models may succeed in shallow queries, they often fail\nto respect physical constraints, preserve spatial coherence, though they remain\nmostly robust to small spatial perturbations. FloorplanQA uncovers a blind spot\nin today's LLMs: inconsistent reasoning about indoor layouts. We hope this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings.",
    "published": "2025-07-10T11:16:48Z",
    "updated": "2025-10-06T12:00:21Z",
    "link": "http://arxiv.org/pdf/2507.07644v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Fedor Rodionov",
      "Abdelrahman Eldesokey",
      "Michael Birsak",
      "John Femiani",
      "Bernard Ghanem",
      "Peter Wonka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01555v2",
    "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient\n  Optimization",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) leverages a\nKullback-Leibler (KL) divergence loss to stabilize training and prevent\noverfitting. However, in methods such as GRPO, its implementation may be guided\nby principles from numerical value estimation-a practice that overlooks the\nterm's functional role as an optimization loss. To analyze this issue, we\nestablish a unified framework that connects two seemingly distinct\nimplementation styles: using the mathematical term $k_n$ as a detached\ncoefficient for the policy's score function ('$k_n$ in reward') or as a direct\nloss function through which gradients are propagated ('$k_n$ as loss'). We show\nthat the latter can always be analyzed via an equivalent gradient coefficient\nin the former, unifying the two perspectives. Through this framework, we prove\nthat the conventional '$k_1$ in reward' (like in PPO) is the principled loss\nfor Reverse KL (RKL) regularization. We further establish a key finding: under\non-policy conditions, the '$k_2$ as loss' formulation is, in fact,\ngradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our\nwork, identifies both as the theoretically sound implementations of the RKL\nobjective. In contrast, we show that the recently adopted '$k_3$ as loss' (like\nin GRPO) is merely a first-order, biased approximation of the principled loss.\nFurthermore, we argue that common off-policy implementations of '$k_n$ as loss'\nmethods are biased due to neglected importance sampling, and we propose a\nprincipled correction. Our findings provide a comprehensive, gradient-based\nrationale for choosing and correctly implementing KL regularization, paving the\nway for more robust and effective RLHF systems.",
    "published": "2025-10-02T01:00:02Z",
    "updated": "2025-10-06T11:59:12Z",
    "link": "http://arxiv.org/pdf/2510.01555v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kezhao Liu",
      "Jason Klein Liu",
      "Mingtao Chen",
      "Yiming Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04721v1",
    "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
    "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.",
    "published": "2025-10-06T11:41:46Z",
    "updated": "2025-10-06T11:41:46Z",
    "link": "http://arxiv.org/pdf/2510.04721v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Ivo Petrov",
      "Jasper Dekoninck",
      "Martin Vechev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.06256v3",
    "title": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling",
    "summary": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task.",
    "published": "2025-01-09T09:45:05Z",
    "updated": "2025-10-06T11:37:13Z",
    "link": "http://arxiv.org/pdf/2501.06256v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jelena Bratulić",
      "Sudhanshu Mittal",
      "David T. Hoffmann",
      "Samuel Böhm",
      "Robin Tibor Schirrmeister",
      "Tonio Ball",
      "Christian Rupprecht",
      "Thomas Brox"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.02348v3",
    "title": "Can Large Language Models generalize analogy solving like children can?",
    "summary": "In people, the ability to solve analogies such as \"body : feet :: table : ?\"\nemerges in childhood, and appears to transfer easily to other domains, such as\nthe visual domain \"( : ) :: < : ?\". Recent research shows that large language\nmodels (LLMs) can solve various forms of analogies. However, can LLMs\ngeneralize analogy solving to new domains like people can? To investigate this,\nwe had children, adults, and LLMs solve a series of letter-string analogies\n(e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain\n(Greek alphabet), and a far transfer domain (list of symbols). Children and\nadults easily generalized their knowledge to unfamiliar domains, whereas LLMs\ndid not. This key difference between human and AI performance is evidence that\nthese LLMs still struggle with robust human-like analogical transfer.",
    "published": "2024-11-04T18:18:38Z",
    "updated": "2025-10-06T11:35:02Z",
    "link": "http://arxiv.org/pdf/2411.02348v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "authors": [
      "Claire E. Stevenson",
      "Alexandra Pafford",
      "Han L. J. van der Maas",
      "Melanie Mitchell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04716v1",
    "title": "Curved Boolean Logic: A Contextual Generalization of Propositional Logic\n  with Algorithmic Consequences",
    "summary": "Curved Boolean Logic (CBL) generalizes propositional logic by allowing local\ntruth assignments that do not extend to a single global valuation, analogous to\ncurvature in geometry. We give equivalent sheaf and exclusivity-graph semantics\nand a context-aware proof calculus that is conservative in the flat limit. We\nformalize CBL-SAT and basic complexity (NP-complete in general) and present\noperational operators (CBL-AC and CBL-CONS) that prune contradictions earlier\non classical hardware. We model noise with iid, AR(1)-correlated, and\nadversarial bounded perturbations and provide permutation-based significance\nwith Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files)\nregenerates all figures and statistics. We position CBL relative to KCBS, CSW,\nand sheaf frameworks and outline links to SAT/CSP and robustness/adapter\nstability in large language models.",
    "published": "2025-10-06T11:34:08Z",
    "updated": "2025-10-06T11:34:08Z",
    "link": "http://arxiv.org/pdf/2510.04716v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.CC",
      "quant-ph",
      "68Q17, 68Q25",
      "F.1.1; F.2.2; I.2.3"
    ],
    "authors": [
      "Maximilian R. P. von Liechtenstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04704v1",
    "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large\n  Language Models on Crystalline Materials",
    "summary": "Large Language Models (LLMs) excel at textual reasoning and are beginning to\ndevelop spatial understanding, prompting the question of whether these\nabilities can be combined for complex, domain-specific tasks. This question is\nessential in fields like materials science, where deep understanding of 3D\natomic structures is fundamental. While initial studies have successfully\napplied LLMs to tasks involving pure crystal generation or coordinate\nunderstandings, a standardized benchmark to systematically evaluate their core\nreasoning abilities across diverse atomic structures has been notably absent.\nTo address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on\ntasks based in Crystallographic Information Files (CIFs), a standard structure\nrepresentation format. These tasks, including structural editing, CIF\nperception, and property-guided modeling, reveal a critical limitation: current\nmodels, despite establishing promising baselines, consistently fail in\nstructural understanding and spatial reasoning. Our experiments show that these\nmodels make frequent errors on structure modification tasks, and even in the\nbasic CIF format understandings, potentially leading to cumulative errors in\nsubsequent analysis and materials insights. By defining these standardized\ntasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale\nmodeling, crucial for accelerating materials research and automating scientific\nworkflows.",
    "published": "2025-10-06T11:17:56Z",
    "updated": "2025-10-06T11:17:56Z",
    "link": "http://arxiv.org/pdf/2510.04704v1.pdf",
    "category": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Taoyuze Lv",
      "Alexander Chen",
      "Fengyu Xie",
      "Chu Wu",
      "Jeffrey Meng",
      "Dongzhan Zhou",
      "Bram Hoex",
      "Zhicheng Zhong",
      "Tong Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.13755v4",
    "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration",
    "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
    "published": "2025-08-19T11:51:40Z",
    "updated": "2025-10-06T11:12:22Z",
    "link": "http://arxiv.org/pdf/2508.13755v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhicheng Yang",
      "Zhijiang Guo",
      "Yinya Huang",
      "Yongxin Wang",
      "Dongchun Xie",
      "Yiwei Wang",
      "Xiaodan Liang",
      "Jing Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04698v1",
    "title": "The Bayesian Origin of the Probability Weighting Function in Human\n  Representation of Probabilities",
    "summary": "Understanding the representation of probability in the human mind has been of\ngreat interest to understanding human decision making. Classical paradoxes in\ndecision making suggest that human perception distorts probability magnitudes.\nPrevious accounts postulate a Probability Weighting Function that transforms\nperceived probabilities; however, its motivation has been debated. Recent work\nhas sought to motivate this function in terms of noisy representations of\nprobabilities in the human mind. Here, we present an account of the Probability\nWeighting Function grounded in rational inference over optimal decoding from\nnoisy neural encoding of quantities. We show that our model accurately accounts\nfor behavior in a lottery task and a dot counting task. It further accounts for\nadaptation to a bimodal short-term prior. Taken together, our results provide a\nunifying account grounding the human representation of probability in rational\ninference.",
    "published": "2025-10-06T11:10:55Z",
    "updated": "2025-10-06T11:10:55Z",
    "link": "http://arxiv.org/pdf/2510.04698v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "econ.TH"
    ],
    "authors": [
      "Xin Tong",
      "Thi Thu Uyen Hoang",
      "Xue-Xin Wei",
      "Michael Hahn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04695v1",
    "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM\n  Agents",
    "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives.",
    "published": "2025-10-06T11:09:45Z",
    "updated": "2025-10-06T11:09:45Z",
    "link": "http://arxiv.org/pdf/2510.04695v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yiding Wang",
      "Zhepei Wei",
      "Xinyu Zhu",
      "Yu Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04694v1",
    "title": "Multilingual Routing in Mixture-of-Experts",
    "summary": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern\nLLMs, yet little is understood about how their sparse routing dynamics respond\nto multilingual data. In this work, we analyze expert routing patterns using\nparallel multilingual datasets and present highly interpretable layer-wise\nphenomena. We find that MoE models route tokens in language-specific ways in\nthe early and late decoder layers but exhibit significant cross-lingual routing\nalignment in middle layers, mirroring parameter-sharing trends observed in\ndense LLMs. In particular, we reveal a clear, strong correlation between a\nmodel's performance in a given language and how similarly its tokens are routed\nto English in these layers. Extending beyond correlation, we explore\ninference-time interventions that induce higher cross-lingual routing\nalignment. We introduce a method that steers the router by promoting\nmiddle-layer task experts frequently activated in English, and it successfully\nincreases multilingual performance. These 1-2% gains are remarkably consistent\nacross two evaluation tasks, three models, and 15+ languages, especially given\nthat these simple interventions override routers of extensively trained,\nstate-of-the-art LLMs. In comparison, interventions outside of the middle\nlayers or targeting multilingual-specialized experts only yield performance\ndegradation. Altogether, we present numerous findings that explain how MoEs\nprocess non-English text and demonstrate that generalization is limited by the\nmodel's ability to leverage language-universal experts in all languages.",
    "published": "2025-10-06T11:09:20Z",
    "updated": "2025-10-06T11:09:20Z",
    "link": "http://arxiv.org/pdf/2510.04694v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Lucas Bandarkar",
      "Chenyuan Yang",
      "Mohsen Fayyaz",
      "Junlin Hu",
      "Nanyun Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04692v1",
    "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for\n  Behavioral Studies",
    "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement.",
    "published": "2025-10-06T11:05:46Z",
    "updated": "2025-10-06T11:05:46Z",
    "link": "http://arxiv.org/pdf/2510.04692v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Lyes Saad Saoud",
      "Irfan Hussain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04686v1",
    "title": "How does the optimizer implicitly bias the model merging loss landscape?",
    "summary": "Model merging methods combine models with different capabilities into a\nsingle one while maintaining the same inference cost. Two popular approaches\nare linear interpolation, which linearly interpolates between model weights,\nand task arithmetic, which combines task vectors obtained by the difference\nbetween finetuned and base models. While useful in practice, what properties\nmake merging effective are poorly understood. This paper explores how the\noptimization process affects the loss landscape geometry and its impact on\nmerging success. We show that a single quantity -- the effective noise scale --\nunifies the impact of optimizer and data choices on model merging. Across\narchitectures and datasets, the effectiveness of merging success is a\nnon-monotonic function of effective noise, with a distinct optimum. Decomposing\nthis quantity, we find that larger learning rates, stronger weight decay,\nsmaller batch sizes, and data augmentation all independently modulate the\neffective noise scale, exhibiting the same qualitative trend. Unlike prior work\nthat connects optimizer noise to the flatness or generalization of individual\nminima, we show that it also affects the global loss landscape, predicting when\nindependently trained solutions can be merged. Our findings broaden the\nunderstanding of how optimization shapes the loss landscape geometry and its\ndownstream consequences for model merging, suggesting the possibility of\nfurther manipulating the training dynamics to improve merging effectiveness.",
    "published": "2025-10-06T10:56:41Z",
    "updated": "2025-10-06T10:56:41Z",
    "link": "http://arxiv.org/pdf/2510.04686v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Chenxiang Zhang",
      "Alexander Theus",
      "Damien Teney",
      "Antonio Orvieto",
      "Jun Pang",
      "Sjouke Mauw"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19645v3",
    "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE",
    "summary": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
    "published": "2025-05-26T08:01:45Z",
    "updated": "2025-10-06T10:53:42Z",
    "link": "http://arxiv.org/pdf/2505.19645v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zongle Huang",
      "Lei Zhu",
      "Zongyuan Zhan",
      "Ting Hu",
      "Weikai Mao",
      "Xianzhi Yu",
      "Yongpan Liu",
      "Tianyu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.12230v2",
    "title": "LIAM: Multimodal Transformer for Language Instructions, Images, Actions\n  and Semantic Maps",
    "summary": "The availability of large language models and open-vocabulary object\nperception methods enables more flexibility for domestic service robots. The\nlarge variability of domestic tasks can be addressed without implementing each\ntask individually by providing the robot with a task description along with\nappropriate environment information. In this work, we propose LIAM - an\nend-to-end model that predicts action transcripts based on language, image,\naction, and map inputs. Language and image inputs are encoded with a CLIP\nbackbone, for which we designed two pre-training tasks to fine-tune its weights\nand pre-align the latent spaces. We evaluate our method on the ALFRED dataset,\na simulator-generated benchmark for domestic tasks. Our results demonstrate the\nimportance of pre-aligning embedding spaces from different modalities and the\nefficacy of incorporating semantic maps.",
    "published": "2025-03-15T18:54:06Z",
    "updated": "2025-10-06T10:49:54Z",
    "link": "http://arxiv.org/pdf/2503.12230v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Yihao Wang",
      "Raphael Memmesheimer",
      "Sven Behnke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04682v1",
    "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to\n  Transplant LoRA",
    "summary": "Large Language Models (LLMs) are widely applied in real world scenarios, but\nfine-tuning them comes with significant computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these\ncosts, but the adapted parameters are dependent on the base model and cannot be\ntransferred across different backbones. One way to address this issue is\nthrough knowledge distillation, but its effectiveness inherently depends on\ntraining data. Recent work such as TransLoRA avoids this by generating\nsynthetic data, but this adds complexity because it requires training an\nadditional discriminator model. In this paper, we propose TiTok, a new\nframework that enables effective LoRA Transplantation through Token-level\nknowledge transfer. Specifically, TiTok captures task-relevant information\nthrough a contrastive excess between a source model with and without LoRA. This\nexcess highlights informative tokens and enables selective filtering of\nsynthetic data, all without additional models or overhead. Through experiments\non three benchmarks across multiple transfer settings, our experiments show\nthat the proposed method is consistently effective, achieving average\nperformance gains of +4~8% compared to baselines overall.",
    "published": "2025-10-06T10:47:22Z",
    "updated": "2025-10-06T10:47:22Z",
    "link": "http://arxiv.org/pdf/2510.04682v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chanjoo Jung",
      "Jaehyung Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02089v4",
    "title": "SALAD: Systematic Assessment of Machine Unlearning on LLM-Aided Hardware\n  Design",
    "summary": "Large Language Models (LLMs) offer transformative capabilities for hardware\ndesign automation, particularly in Verilog code generation. However, they also\npose significant data security challenges, including Verilog evaluation data\ncontamination, intellectual property (IP) design leakage, and the risk of\nmalicious Verilog generation. We introduce SALAD, a comprehensive assessment\nthat leverages machine unlearning to mitigate these threats. Our approach\nenables the selective removal of contaminated benchmarks, sensitive IP and\ndesign artifacts, or malicious code patterns from pre-trained LLMs, all without\nrequiring full retraining. Through detailed case studies, we demonstrate how\nmachine unlearning techniques effectively reduce data security risks in\nLLM-aided hardware design.",
    "published": "2025-06-02T13:59:08Z",
    "updated": "2025-10-06T10:38:59Z",
    "link": "http://arxiv.org/pdf/2506.02089v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Zeng Wang",
      "Minghao Shao",
      "Rupesh Karn",
      "Likhitha Mankali",
      "Jitendra Bhandari",
      "Ramesh Karri",
      "Ozgur Sinanoglu",
      "Muhammad Shafique",
      "Johann Knechtel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.03715v2",
    "title": "Neural Deconstruction Search for Vehicle Routing Problems",
    "summary": "Autoregressive construction approaches generate solutions to vehicle routing\nproblems in a step-by-step fashion, leading to high-quality solutions that are\nnearing the performance achieved by handcrafted operations research techniques.\nIn this work, we challenge the conventional paradigm of sequential solution\nconstruction and introduce an iterative search framework where solutions are\ninstead deconstructed by a neural policy. Throughout the search, the neural\npolicy collaborates with a simple greedy insertion algorithm to rebuild the\ndeconstructed solutions. Our approach matches or surpasses the performance of\nstate-of-the-art operations research methods across three challenging vehicle\nrouting problems of various problem sizes.",
    "published": "2025-01-07T11:44:25Z",
    "updated": "2025-10-06T10:38:24Z",
    "link": "http://arxiv.org/pdf/2501.03715v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "André Hottung",
      "Paula Wong-Chung",
      "Kevin Tierney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04674v1",
    "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel\n  Coding",
    "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful\nparadigm for end-to-end semantic communications, jointly learning to compress\nand protect task-relevant features over noisy channels. However, existing\nDeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver\n(RX) - an assumption that fails in multi-vendor deployments where encoders and\ndecoders cannot be co-trained. This mismatch introduces \"semantic noise\",\ndegrading reconstruction quality and downstream task performance. In this\npaper, we systematize and evaluate methods for semantic channel equalization\nfor DeepJSCC, introducing an additional processing stage that aligns\nheterogeneous latent spaces under both physical and semantic impairments. We\ninvestigate three classes of aligners: (i) linear maps, which admit closed-form\nsolutions; (ii) lightweight neural networks, offering greater expressiveness;\nand (iii) a Parseval-frame equalizer, which operates in zero-shot mode without\nthe need for training. Through extensive experiments on image reconstruction\nover AWGN and fading channels, we quantify trade-offs among complexity, data\nefficiency, and fidelity, providing guidelines for deploying DeepJSCC in\nheterogeneous AI-native wireless networks.",
    "published": "2025-10-06T10:29:07Z",
    "updated": "2025-10-06T10:29:07Z",
    "link": "http://arxiv.org/pdf/2510.04674v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.NI",
      "math.IT"
    ],
    "authors": [
      "Lorenzo Pannacci",
      "Simone Fiorellino",
      "Mario Edoardo Pandolfo",
      "Emilio Calvanese Strinati",
      "Paolo Di Lorenzo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04673v1",
    "title": "Watch and Learn: Learning to Use Computers from Online Videos",
    "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.",
    "published": "2025-10-06T10:29:00Z",
    "updated": "2025-10-06T10:29:00Z",
    "link": "http://arxiv.org/pdf/2510.04673v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Chan Hee Song",
      "Yiwen Song",
      "Palash Goyal",
      "Yu Su",
      "Oriana Riva",
      "Hamid Palangi",
      "Tomas Pfister"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.14048v2",
    "title": "PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial\n  Optimization",
    "summary": "Reinforcement learning-based methods for constructing solutions to\ncombinatorial optimization problems are rapidly approaching the performance of\nhuman-designed algorithms. To further narrow the gap, learning-based approaches\nmust efficiently explore the solution space during the search process. Recent\napproaches artificially increase exploration by enforcing diverse solution\ngeneration through handcrafted rules, however, these rules can impair solution\nquality and are difficult to design for more complex problems. In this paper,\nwe introduce PolyNet, an approach for improving exploration of the solution\nspace by learning complementary solution strategies. In contrast to other\nworks, PolyNet uses only a single-decoder and a training schema that does not\nenforce diverse solution generation through handcrafted rules. We evaluate\nPolyNet on four combinatorial optimization problems and observe that the\nimplicit diversity mechanism allows PolyNet to find better solutions than\napproaches that explicitly enforce diverse solution generation.",
    "published": "2024-02-21T16:38:14Z",
    "updated": "2025-10-06T10:28:23Z",
    "link": "http://arxiv.org/pdf/2402.14048v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "André Hottung",
      "Mridul Mahajan",
      "Kevin Tierney"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08574v3",
    "title": "TANTE: Time-Adaptive Operator Learning via Neural Taylor Expansion",
    "summary": "Operator learning for time-dependent partial differential equations (PDEs)\nhas seen rapid progress in recent years, enabling efficient approximation of\ncomplex spatiotemporal dynamics. However, most existing methods rely on fixed\ntime step sizes during rollout, which limits their ability to adapt to varying\ntemporal complexity and often leads to error accumulation. Here, we propose the\nTime-Adaptive Transformer with Neural Taylor Expansion (TANTE), a novel\noperator-learning framework that produces continuous-time predictions with\nadaptive step sizes. TANTE predicts future states by performing a Taylor\nexpansion at the current state, where neural networks learn both the\nhigher-order temporal derivatives and the local radius of convergence. This\nallows the model to dynamically adjust its rollout based on the local behavior\nof the solution, thereby reducing cumulative error and improving computational\nefficiency. We demonstrate the effectiveness of TANTE across a wide range of\nPDE benchmarks, achieving superior accuracy and adaptability compared to\nfixed-step baselines, delivering accuracy gains of 60-80 % and speed-ups of\n30-40 % at inference time. The code is publicly available at\nhttps://github.com/zwu88/TANTE for transparency and reproducibility.",
    "published": "2025-02-12T17:09:13Z",
    "updated": "2025-10-06T10:27:56Z",
    "link": "http://arxiv.org/pdf/2502.08574v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhikai Wu",
      "Sifan Wang",
      "Shiyang Zhang",
      "Sizhuang He",
      "Min Zhu",
      "Anran Jiao",
      "Lu Lu",
      "David van Dijk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04671v1",
    "title": "FocusMed: A Large Language Model-based Framework for Enhancing Medical\n  Question Summarization with Focus Identification",
    "summary": "With the rapid development of online medical platforms, consumer health\nquestions (CHQs) are inefficient in diagnosis due to redundant information and\nfrequent non-professional terms. The medical question summary (MQS) task aims\nto transform CHQs into streamlined doctors' frequently asked questions (FAQs),\nbut existing methods still face challenges such as poor identification of\nquestion focus and model hallucination. This paper explores the potential of\nlarge language models (LLMs) in the MQS task and finds that direct fine-tuning\nis prone to focus identification bias and generates unfaithful content. To this\nend, we propose an optimization framework based on core focus guidance. First,\na prompt template is designed to drive the LLMs to extract the core focus from\nthe CHQs that is faithful to the original text. Then, a fine-tuning dataset is\nconstructed in combination with the original CHQ-FAQ pairs to improve the\nability to identify the focus of the question. Finally, a multi-dimensional\nquality evaluation and selection mechanism is proposed to comprehensively\nimprove the quality of the summary from multiple dimensions. We conduct\ncomprehensive experiments on two widely-adopted MQS datasets using three\nestablished evaluation metrics. The proposed framework achieves\nstate-of-the-art performance across all measures, demonstrating a significant\nboost in the model's ability to identify critical focus of questions and a\nnotable mitigation of hallucinations. The source codes are freely available at\nhttps://github.com/DUT-LiuChao/FocusMed.",
    "published": "2025-10-06T10:27:09Z",
    "updated": "2025-10-06T10:27:09Z",
    "link": "http://arxiv.org/pdf/2510.04671v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chao Liu",
      "Ling Luo",
      "Tengxiao Lv",
      "Huan Zhuang",
      "Lejing Yu",
      "Jian Wang",
      "Hongfei Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.02971v2",
    "title": "Proof-of-Data: A Consensus Protocol for Collaborative Intelligence",
    "summary": "Existing research on federated learning has been focused on the setting where\nlearning is coordinated by a centralized entity. Yet the greatest potential of\nfuture collaborative intelligence would be unleashed in a more open and\ndemocratized setting with no central entity in a dominant role, referred to as\n\"decentralized federated learning\". New challenges arise accordingly in\nachieving both correct model training and fair reward allocation with\ncollective effort among all participating nodes, especially with the threat of\nthe Byzantine node jeopardising both tasks.\n  In this paper, we propose a blockchain-based decentralized Byzantine\nfault-tolerant federated learning framework based on a novel Proof-of-Data\n(PoD) consensus protocol to resolve both the \"trust\" and \"incentive\"\ncomponents. By decoupling model training and contribution accounting, PoD is\nable to enjoy not only the benefit of learning efficiency and system liveliness\nfrom asynchronous societal-scale PoW-style learning but also the finality of\nconsensus and reward allocation from epoch-based BFT-style voting. To mitigate\nfalse reward claims by data forgery from Byzantine attacks, a privacy-aware\ndata verification and contribution-based reward allocation mechanism is\ndesigned to complete the framework. Our evaluation results show that PoD\ndemonstrates performance in model training close to that of the centralized\ncounterpart while achieving trust in consensus and fairness for reward\nallocation with a fault tolerance ratio of 1/3.",
    "published": "2025-01-06T12:27:59Z",
    "updated": "2025-10-06T10:25:49Z",
    "link": "http://arxiv.org/pdf/2501.02971v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Huiwen Liu",
      "Feida Zhu",
      "Ling Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04670v1",
    "title": "Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness\n  Routing",
    "summary": "Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion\nstyles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic\nFramework for Multimodal fMRI Response Encoding), an agnostic interface that\nstandardizes time-aligned post-fusion tokens from varied encoders, and MIND, a\nplug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.\nTrained end-to-end for whole-brain prediction, AFIRE decouples the decoder from\nupstream fusion, while MIND combines token-dependent Top-K sparse routing with\na subject prior to personalize expert usage without sacrificing generality.\nExperiments across multiple multimodal backbones and subjects show consistent\nimprovements over strong baselines, enhanced cross-subject generalization, and\ninterpretable expert patterns that correlate with content type. The framework\noffers a simple attachment point for new encoders and datasets, enabling\nrobust, plug-and-improve performance for naturalistic neuroimaging studies.",
    "published": "2025-10-06T10:24:28Z",
    "updated": "2025-10-06T10:24:28Z",
    "link": "http://arxiv.org/pdf/2510.04670v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xuanhua Yin",
      "Runkai Zhao",
      "Weidong Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04667v1",
    "title": "Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy\n  for Reversible Normalization in Time Series Forecasting",
    "summary": "Reversible Instance Normalization (RevIN) is a key technique enabling simple\nlinear models to achieve state-of-the-art performance in time series\nforecasting. While replacing its non-robust statistics with robust counterparts\n(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal\na far more complex reality. This paper deconstructs the perplexing performance\nof various normalization strategies by identifying four underlying theoretical\ncontradictions. Our experiments provide two crucial findings: first, the\nstandard RevIN catastrophically fails on datasets with extreme outliers, where\nits MSE surges by a staggering 683\\%. Second, while the simple R$^2$-IN\nprevents this failure and unexpectedly emerges as the best overall performer,\nour adaptive model (A-IN), designed to test a diagnostics-driven heuristic,\nunexpectedly suffers a complete and systemic failure. This surprising outcome\nuncovers a critical, overlooked pitfall in time series analysis: the\ninstability introduced by a simple or counter-intuitive heuristic can be more\ndamaging than the statistical issues it aims to solve. The core contribution of\nthis work is thus a new, cautionary paradigm for time series normalization: a\nshift from a blind search for complexity to a diagnostics-driven analysis that\nreveals not only the surprising power of simple baselines but also the perilous\nnature of naive adaptation.",
    "published": "2025-10-06T10:22:20Z",
    "updated": "2025-10-06T10:22:20Z",
    "link": "http://arxiv.org/pdf/2510.04667v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.6; H.2.8"
    ],
    "authors": [
      "Fanzhe Fu",
      "Yang Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.11334v2",
    "title": "Program Synthesis Benchmark for Visual Programming in XLogoOnline\n  Environment",
    "summary": "Large language and multimodal models have shown remarkable success on various\nbenchmarks focused on specific skills such as general-purpose programming, math\nword problem-solving, and visual question answering. However, it is unclear how\nwell these models perform on tasks that require a combination of these skills.\nIn this paper, we curate a novel program synthesis benchmark based on the\nreal-world tasks in the XLogoOnline visual programming environment. Each task\nrequires a combination of different skills such as spatial planning, basic\nprogramming, and logical reasoning. Our evaluation shows that current\nstate-of-the-art models like GPT-4V and Llama3-70B struggle to solve these\ntasks, achieving only 20% and 2.35% success rates, respectively. Next, we\ndevelop a fine-tuning pipeline to boost the performance of models by leveraging\na large-scale synthetic training dataset with over 80,000 tasks. Moreover, we\nshowcase how emulator-driven feedback can be used to design a curriculum over\ntraining data distribution, through which a fine-tuned Llama3-8B drastically\noutperforms GPT-4V and Llama3-70B models. Finally, we provide an in-depth\nfailure analysis to understand the limitations of different models. We will\npublicly release the benchmark for future research on program synthesis in\nvisual programming.",
    "published": "2024-06-17T08:48:02Z",
    "updated": "2025-10-06T10:21:23Z",
    "link": "http://arxiv.org/pdf/2406.11334v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chao Wen",
      "Jacqueline Staub",
      "Adish Singla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16856v2",
    "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models",
    "summary": "With the growing deployment of Vision-Language Models (VLMs) in real-world\napplications, previously overlooked safety risks are becoming increasingly\nevident. In particular, seemingly innocuous multimodal inputs can combine to\nreveal harmful intent, leading to unsafe model outputs. While multimodal safety\nhas received increasing attention, existing approaches often fail to address\nsuch latent risks, especially when harmfulness arises only from the interaction\nbetween modalities. We propose SIA (Safety via Intent Awareness), a\ntraining-free, intent-aware safety framework that proactively detects harmful\nintent in multimodal inputs and uses it to guide the generation of safe\nresponses. SIA follows a three-stage process: (1) visual abstraction via\ncaptioning; (2) intent inference through few-shot chain-of-thought (CoT)\nprompting; and (3) intent-conditioned response generation. By dynamically\nadapting to the implicit intent inferred from an image-text pair, SIA mitigates\nharmful outputs without extensive retraining. Extensive experiments on safety\nbenchmarks, including SIUO, MM-SafetyBench, and HoliSafe, show that SIA\nconsistently improves safety and outperforms prior training-free methods.",
    "published": "2025-07-21T13:59:50Z",
    "updated": "2025-10-06T10:16:31Z",
    "link": "http://arxiv.org/pdf/2507.16856v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Youngjin Na",
      "Sangheon Jeong",
      "Youngwan Lee",
      "Jian Lee",
      "Dawoon Jeong",
      "Youngman Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07634v3",
    "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
    "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.",
    "published": "2025-05-12T15:05:34Z",
    "updated": "2025-10-06T10:13:41Z",
    "link": "http://arxiv.org/pdf/2505.07634v3.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Jian Liu",
      "Xiongtao Shi",
      "Thai Duy Nguyen",
      "Haitian Zhang",
      "Tianxiang Zhang",
      "Wei Sun",
      "Yanjie Li",
      "Athanasios V. Vasilakos",
      "Giovanni Iacca",
      "Arshad Ali Khan",
      "Arvind Kumar",
      "Jae Won Cho",
      "Ajmal Mian",
      "Lihua Xie",
      "Erik Cambria",
      "Lin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.20115v2",
    "title": "Flexible metadata harvesting for ecology using large language models",
    "summary": "Large, open datasets can accelerate ecological research, particularly by\nenabling researchers to develop new insights by reusing datasets from multiple\nsources. However, to find the most suitable datasets to combine and integrate,\nresearchers must navigate diverse ecological and environmental data provider\nplatforms with varying metadata availability and standards. To overcome this\nobstacle, we have developed a large language model (LLM)-based metadata\nharvester that flexibly extracts metadata from any dataset's landing page, and\nconverts these to a user-defined, unified format using existing metadata\nstandards. We validate that our tool is able to extract both structured and\nunstructured metadata with equal accuracy, aided by our LLM post-processing\nprotocol. Furthermore, we utilise LLMs to identify links between datasets, both\nby calculating embedding similarity and by unifying the formats of extracted\nmetadata to enable rule-based processing. Our tool, which flexibly links the\nmetadata of different datasets, can therefore be used for ontology creation or\ngraph-based queries, for example, to find relevant ecological and environmental\ndatasets in a virtual research environment.",
    "published": "2025-08-21T10:10:29Z",
    "updated": "2025-10-06T10:07:54Z",
    "link": "http://arxiv.org/pdf/2508.20115v2.pdf",
    "category": [
      "cs.DL",
      "cs.AI",
      "cs.DB"
    ],
    "authors": [
      "Zehao Lu",
      "Thijs L van der Plas",
      "Parinaz Rashidi",
      "W Daniel Kissling",
      "Ioannis N Athanasiadis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03075v2",
    "title": "What Drives Compositional Generalization in Visual Generative Models?",
    "summary": "Compositional generalization, the ability to generate novel combinations of\nknown concepts, is a key ingredient for visual generative models. Yet, not all\nmechanisms that enable or inhibit it are fully understood. In this work, we\nconduct a systematic study of how various design choices influence\ncompositional generalization in image and video generation in a positive or\nnegative way. Through controlled experiments, we identify two key factors: (i)\nwhether the training objective operates on a discrete or continuous\ndistribution, and (ii) to what extent conditioning provides information about\nthe constituent concepts during training. Building on these insights, we show\nthat relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based\nobjective can improve compositional performance in discrete models like\nMaskGIT.",
    "published": "2025-10-03T15:02:27Z",
    "updated": "2025-10-06T10:01:02Z",
    "link": "http://arxiv.org/pdf/2510.03075v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Karim Farid",
      "Rajat Sahay",
      "Yumna Ali Alnaggar",
      "Simon Schrodi",
      "Volker Fischer",
      "Cordelia Schmid",
      "Thomas Brox"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15927v2",
    "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing\n  under Crystallization Kinetics",
    "summary": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection.",
    "published": "2025-04-22T14:17:15Z",
    "updated": "2025-10-06T09:55:43Z",
    "link": "http://arxiv.org/pdf/2504.15927v2.pdf",
    "category": [
      "cs.SI",
      "cs.AI"
    ],
    "authors": [
      "Ling Cheng",
      "Jiashu Pu",
      "Ruicheng Liang",
      "Qian Shao",
      "Hezhe Qiao",
      "Feida Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05890v2",
    "title": "Psychometric Item Validation Using Virtual Respondents with\n  Trait-Response Mediators",
    "summary": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs simulate human survey\nresponses. We publicly release our dataset and code to support future work.",
    "published": "2025-07-08T11:26:03Z",
    "updated": "2025-10-06T09:54:02Z",
    "link": "http://arxiv.org/pdf/2507.05890v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sungjib Lim",
      "Woojung Song",
      "Eun-Ju Lee",
      "Yohan Jo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.04941v6",
    "title": "TOAST: Transformer Optimization using Adaptive and Simple\n  Transformations",
    "summary": "Foundation models achieve State-of-the-Art (SOTA) performance across\ndifferent tasks, but their size and computational demands raise concerns about\naccessibility and sustainability. Existing efficiency methods often require\nadditional retraining or fine-tuning, limiting their practicality. Recent\nfindings suggest that deep neural networks exhibit internal representation\nsimilarities. While such similarities across different models have been\nexploited for enabling techniques such as model stitching and merging,\nintra-network redundancy remains underexplored as a source for efficiency\ngains. In this paper, we introduce TOAST (Transformer Optimization using\nAdaptive and Simple Transformations), a framework that exploits these\nredundancies to approximate entire transformer blocks with lightweight\nclosed-form mappings, such as linear transformation or even the identity,\nwithout any additional training. Across SOTA pretrained vision models (e.g.,\nViT, DINOv2, DeiT) and datasets ranging from MNIST to ImageNet-1k, TOAST\nreduces parameters and computation while preserving, and in some cases\nimproving, downstream performance. These results show that large portions of\ntransformer depth can be replaced by trivial functions, opening a new\nperspective on efficient foundation models.",
    "published": "2024-10-07T11:35:24Z",
    "updated": "2025-10-06T09:49:19Z",
    "link": "http://arxiv.org/pdf/2410.04941v6.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Irene Cannistraci",
      "Simone Antonelli",
      "Emanuele Palumbo",
      "Thomas M. Sutter",
      "Emanuele Rodolà",
      "Bastian Rieck",
      "Julia E. Vogt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04646v1",
    "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
    "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
    "published": "2025-10-06T09:49:14Z",
    "updated": "2025-10-06T09:49:14Z",
    "link": "http://arxiv.org/pdf/2510.04646v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Johanna Sommer",
      "John Rachwan",
      "Nils Fleischmann",
      "Stephan Günnemann",
      "Bertrand Charpentier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04643v1",
    "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading",
    "summary": "In this paper, our objective is to develop a multi-agent financial system\nthat incorporates simulated trading, a technique extensively utilized by\nfinancial professionals. While current LLM-based agent models demonstrate\ncompetitive performance, they still exhibit significant deviations from\nreal-world fund companies. A critical distinction lies in the agents' reliance\non ``post-reflection'', particularly in response to adverse outcomes, but lack\na distinctly human capability: long-term prediction of future trends.\nTherefore, we introduce QuantAgents, a multi-agent system integrating simulated\ntrading, to comprehensively evaluate various investment strategies and market\nscenarios without assuming actual risks. Specifically, QuantAgents comprises\nfour agents: a simulated trading analyst, a risk control analyst, a market news\nanalyst, and a manager, who collaborate through several meetings. Moreover, our\nsystem incentivizes agents to receive feedback on two fronts: performance in\nreal-world markets and predictive accuracy in simulated trading. Extensive\nexperiments demonstrate that our framework excels across all metrics, yielding\nan overall return of nearly 300% over the three years\n(https://quantagents.github.io/).",
    "published": "2025-10-06T09:45:57Z",
    "updated": "2025-10-06T09:45:57Z",
    "link": "http://arxiv.org/pdf/2510.04643v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xiangyu Li",
      "Yawen Zeng",
      "Xiaofen Xing",
      "Jin Xu",
      "Xiangmin Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.15481v2",
    "title": "Learning to Play Piano in the Real World",
    "summary": "Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .",
    "published": "2025-03-19T17:56:14Z",
    "updated": "2025-10-06T09:42:53Z",
    "link": "http://arxiv.org/pdf/2503.15481v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yves-Simon Zeulner",
      "Sandeep Selvaraj",
      "Roberto Calandra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02369v2",
    "title": "Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents",
    "summary": "Large language model (LLM) agents typically receive two kinds of context: (i)\nenvironment-level manuals that define interaction interfaces and global rules,\nand (ii) task-level guidance or demonstrations tied to specific goals. In this\nwork, we identify a crucial but overlooked third type of context,\ninstance-level context, which consists of verifiable and reusable facts tied to\na specific environment instance, such as object locations, crafting recipes,\nand local rules. We argue that the absence of instance-level context is a\ncommon source of failure for LLM agents in complex tasks, as success often\ndepends not only on reasoning over global rules or task prompts but also on\nmaking decisions based on precise and persistent facts. Acquiring such context\nrequires more than memorization: the challenge lies in efficiently exploring,\nvalidating, and formatting these facts under tight interaction budgets. We\nformalize this problem as Instance-Level Context Learning (ILCL) and introduce\nour task-agnostic method to solve it. Our method performs a guided exploration,\nusing a compact TODO forest to intelligently prioritize its next actions and a\nlightweight plan-act-extract loop to execute them. This process automatically\nproduces a high-precision context document that is reusable across many\ndownstream tasks and agents, thereby amortizing the initial exploration cost.\nExperiments across TextWorld, ALFWorld, and Crafter demonstrate consistent\ngains in both success and efficiency: for instance, ReAct's mean success rate\nin TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By\ntransforming one-off exploration into persistent, reusable knowledge, our\nmethod complements existing contexts to enable more reliable and efficient LLM\nagents.",
    "published": "2025-09-29T05:38:51Z",
    "updated": "2025-10-06T09:40:38Z",
    "link": "http://arxiv.org/pdf/2510.02369v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Kuntai Cai",
      "Juncheng Liu",
      "Xianglin Yang",
      "Zhaojie Niu",
      "Xiaokui Xiao",
      "Xing Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02610v2",
    "title": "MINERVA: Mutual Information Neural Estimation for Supervised Feature\n  Selection",
    "summary": "Existing feature filters rely on statistical pair-wise dependence metrics to\nmodel feature-target relationships, but this approach may fail when the target\ndepends on higher-order feature interactions rather than individual\ncontributions. We introduce Mutual Information Neural Estimation Regularized\nVetting Algorithm (MINERVA), a novel approach to supervised feature selection\nbased on neural estimation of mutual information between features and targets.\nWe paramaterize the approximation of mutual information with neural networks\nand perform feature selection using a carefully designed loss function\naugmented with sparsity-inducing regularizers. Our method is implemented in a\ntwo-stage process to decouple representation learning from feature selection,\nensuring better generalization and a more accurate expression of feature\nimportance. We present examples of ubiquitous dependency structures that are\nrarely captured in literature and show that our proposed method effectively\ncaptures these complex feature-target relationships by evaluating feature\nsubsets as an ensemble. Experimental results on synthetic and real-life fraud\ndatasets demonstrate the efficacy of our method and its ability to perform\nexact solutions.",
    "published": "2025-10-02T23:09:06Z",
    "updated": "2025-10-06T09:40:13Z",
    "link": "http://arxiv.org/pdf/2510.02610v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.5.1; G.3"
    ],
    "authors": [
      "Taurai Muvunza",
      "Egor Kraev",
      "Pere Planell-Morell",
      "Alexander Y. Shestopaloff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04630v1",
    "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection",
    "summary": "Detecting manipulated media has now become a pressing issue with the recent\nrise of deepfakes. Most existing approaches fail to generalize across diverse\ndatasets and generation techniques. We thus propose a novel ensemble framework,\ncombining the strengths of transformer-based architectures, such as Swin\nTransformers and ViTs, and texture-based methods, to achieve better detection\naccuracy and robustness. Our method introduces innovative data-splitting,\nsequential training, frequency splitting, patch-based attention, and face\nsegmentation techniques to handle dataset imbalances, enhance high-impact\nregions (e.g., eyes and mouth), and improve generalization. Our model achieves\nstate-of-the-art performance when tested on the DFWild-Cup dataset, a diverse\nsubset of eight deepfake datasets. The ensemble benefits from the\ncomplementarity of these approaches, with transformers excelling in global\nfeature extraction and texturebased methods providing interpretability. This\nwork demonstrates that hybrid models can effectively address the evolving\nchallenges of deepfake detection, offering a robust solution for real-world\napplications.",
    "published": "2025-10-06T09:35:57Z",
    "updated": "2025-10-06T09:35:57Z",
    "link": "http://arxiv.org/pdf/2510.04630v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "authors": [
      "Vrushank Ahire",
      "Aniruddh Muley",
      "Shivam Zample",
      "Siddharth Verma",
      "Pranav Menon",
      "Surbhi Madan",
      "Abhinav Dhall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04624v1",
    "title": "Fairness in Repeated Matching: A Maximin Perspective",
    "summary": "We study a sequential decision-making model where a set of items is\nrepeatedly matched to the same set of agents over multiple rounds. The\nobjective is to determine a sequence of matchings that either maximizes the\nutility of the least advantaged agent at the end of all rounds (optimal) or at\nthe end of every individual round (anytime optimal). We investigate the\ncomputational challenges associated with finding (anytime) optimal outcomes and\ndemonstrate that these problems are generally computationally intractable.\nHowever, we provide approximation algorithms, fixed-parameter tractable\nalgorithms, and identify several special cases whereby the problem(s) can be\nsolved efficiently. Along the way, we also establish characterizations of\nPareto-optimal/maximum matchings, which may be of independent interest to works\nin matching theory and house allocation.",
    "published": "2025-10-06T09:32:40Z",
    "updated": "2025-10-06T09:32:40Z",
    "link": "http://arxiv.org/pdf/2510.04624v1.pdf",
    "category": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "econ.TH"
    ],
    "authors": [
      "Eugene Lim",
      "Tzeh Yuan Neoh",
      "Nicholas Teh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04623v1",
    "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports",
    "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data\nis critically hindered by their tendency to hallucinate facts and their\ninability to follow domain-specific rules. To address this, we introduce\nMedPAO, a novel agentic framework that ensures accuracy and verifiable\nreasoning by grounding its operation in established clinical protocols such as\nthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring\ntask into a transparent process managed by a Plan-Act-Observe (PAO) loop and\nspecialized tools. This protocol-driven method provides a verifiable\nalternative to opaque, monolithic models. The efficacy of our approach is\ndemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96\non the critical sub-task of concept categorization. Notably, expert\nradiologists and clinicians rated the final structured outputs with an average\nscore of 4.52 out of 5, indicating a level of reliability that surpasses\nbaseline approaches relying solely on LLM-based foundation models. The code is\navailable at: https://github.com/MiRL-IITM/medpao-agent",
    "published": "2025-10-06T09:32:23Z",
    "updated": "2025-10-06T09:32:23Z",
    "link": "http://arxiv.org/pdf/2510.04623v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shrish Shrinath Vaidya",
      "Gowthamaan Palani",
      "Sidharth Ramesh",
      "Velmurugan Balasubramanian",
      "Minmini Selvam",
      "Gokulraja Srinivasaraja",
      "Ganapathy Krishnamurthi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04618v1",
    "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving\n  Language Models",
    "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.",
    "published": "2025-10-06T09:30:18Z",
    "updated": "2025-10-06T09:30:18Z",
    "link": "http://arxiv.org/pdf/2510.04618v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Qizheng Zhang",
      "Changran Hu",
      "Shubhangi Upasani",
      "Boyuan Ma",
      "Fenglu Hong",
      "Vamsidhar Kamanuru",
      "Jay Rainton",
      "Chen Wu",
      "Mengmeng Ji",
      "Hanchen Li",
      "Urmish Thakker",
      "James Zou",
      "Kunle Olukotun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04617v1",
    "title": "Making Mathematical Reasoning Adaptive",
    "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR",
    "published": "2025-10-06T09:30:05Z",
    "updated": "2025-10-06T09:30:05Z",
    "link": "http://arxiv.org/pdf/2510.04617v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhejian Lai",
      "Xiang Geng",
      "Zhijun Wang",
      "Yang Bai",
      "Jiahuan Li",
      "Rongxiang Weng",
      "Jingang Wang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Shujian Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20783v2",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
    "published": "2025-03-26T17:59:14Z",
    "updated": "2025-10-06T09:30:03Z",
    "link": "http://arxiv.org/pdf/2503.20783v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zichen Liu",
      "Changyu Chen",
      "Wenjun Li",
      "Penghui Qi",
      "Tianyu Pang",
      "Chao Du",
      "Wee Sun Lee",
      "Min Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04615v1",
    "title": "Design Process of a Self Adaptive Smart Serious Games Ecosystem",
    "summary": "This paper outlines the design vision and planned evolution of Blexer v3, a\nmodular and AI-driven rehabilitation ecosystem based on serious games. Building\non insights from previous versions of the system, we propose a new architecture\nthat aims to integrate multimodal sensing, real-time reasoning, and intelligent\ncontrol. The envisioned system will include distinct modules for data\ncollection, user state inference, and gameplay adaptation. Key features such as\ndynamic difficulty adjustment (DDA) and procedural content generation (PCG) are\nalso considered to support personalized interventions. We present the complete\nconceptual framework of Blexer v3, which defines the modular structure and data\nflow of the system. This serves as the foundation for the next phase: the\ndevelopment of a functional prototype and its integration into clinical\nrehabilitation scenarios.",
    "published": "2025-10-06T09:28:31Z",
    "updated": "2025-10-06T09:28:31Z",
    "link": "http://arxiv.org/pdf/2510.04615v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "I.2.1"
    ],
    "authors": [
      "X. Tao",
      "P. Chen",
      "M. Tsami",
      "F. Khayati",
      "M. Eckert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.11944v2",
    "title": "VIPO: Value Function Inconsistency Penalized Offline Reinforcement\n  Learning",
    "summary": "Offline reinforcement learning (RL) learns effective policies from\npre-collected datasets, offering a practical solution for applications where\nonline interactions are risky or costly. Model-based approaches are\nparticularly advantageous for offline RL, owing to their data efficiency and\ngeneralizability. However, due to inherent model errors, model-based methods\noften artificially introduce conservatism guided by heuristic uncertainty\nestimation, which can be unreliable. In this paper, we introduce VIPO, a novel\nmodel-based offline RL algorithm that incorporates self-supervised feedback\nfrom value estimation to enhance model training. Specifically, the model is\nlearned by additionally minimizing the inconsistency between the value learned\ndirectly from the offline data and the one estimated from the model. We perform\ncomprehensive evaluations from multiple perspectives to show that VIPO can\nlearn a highly accurate model efficiently and consistently outperform existing\nmethods. In particular, it achieves state-of-the-art performance on almost all\ntasks in both D4RL and NeoRL benchmarks. Overall, VIPO offers a general\nframework that can be readily integrated into existing model-based offline RL\nalgorithms to systematically enhance model accuracy.",
    "published": "2025-04-16T10:23:44Z",
    "updated": "2025-10-06T09:21:10Z",
    "link": "http://arxiv.org/pdf/2504.11944v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xuyang Chen",
      "Guojian Wang",
      "Keyu Yan",
      "Lin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04609v1",
    "title": "Accountability Capture: How Record-Keeping to Support AI Transparency\n  and Accountability (Re)shapes Algorithmic Oversight",
    "summary": "Accountability regimes typically encourage record-keeping to enable the\ntransparency that supports oversight, investigation, contestation, and redress.\nHowever, implementing such record-keeping can introduce considerations, risks,\nand consequences, which so far remain under-explored. This paper examines how\nrecord-keeping practices bring algorithmic systems within accountability\nregimes, providing a basis to observe and understand their effects. For this,\nwe introduce, describe, and elaborate 'accountability capture' -- the\nre-configuration of socio-technical processes and the associated downstream\neffects relating to record-keeping for algorithmic accountability. Surveying\n100 practitioners, we evidence and characterise record-keeping issues in\npractice, identifying their alignment with accountability capture. We further\ndocument widespread record-keeping practices, tensions between internal and\nexternal accountability requirements, and evidence of employee resistance to\npractices imposed through accountability capture. We discuss these and other\neffects for surveillance, privacy, and data protection, highlighting\nconsiderations for algorithmic accountability communities. In all, we show that\nimplementing record-keeping to support transparency in algorithmic\naccountability regimes can itself bring wider implications -- an issue\nrequiring greater attention from practitioners, researchers, and policymakers\nalike.",
    "published": "2025-10-06T09:20:27Z",
    "updated": "2025-10-06T09:20:27Z",
    "link": "http://arxiv.org/pdf/2510.04609v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Shreya Chappidi",
      "Jennifer Cobbe",
      "Chris Norval",
      "Anjali Mazumder",
      "Jatinder Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25085v4",
    "title": "jina-reranker-v3: Last but Not Late Interaction for Listwise Document\n  Reranking",
    "summary": "jina-reranker-v3 is a 0.6B-parameter multilingual listwise reranker that\nintroduces a novel \"last but not late\" interaction. Unlike late interaction\nmodels like ColBERT that encode documents separately before multi-vector\nmatching, our approach applies causal attention between the query and all\ncandidate documents in the same context window, enabling rich interactions\nbefore extracting contextual embeddings from each document's final token. The\nnew model achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while\nbeing significantly smaller than other models with comparable performance.",
    "published": "2025-09-29T17:23:54Z",
    "updated": "2025-10-06T09:18:00Z",
    "link": "http://arxiv.org/pdf/2509.25085v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Feng Wang",
      "Yuqing Li",
      "Han Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04607v1",
    "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency\n  of Computer-Use Agents",
    "summary": "Computer-use agents (CUAs) powered by large language models (LLMs) have\nemerged as a promising approach to automating computer tasks, yet they struggle\nwith graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to\ndecompose high-level goals into lengthy, error-prone sequences of fine-grained\nactions, resulting in low success rates and an excessive number of LLM calls.\n  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms\nexisting GUIs into three declarative primitives: access, state, and\nobservation, which are better suited for LLMs. Our key idea is policy-mechanism\nseparation: LLMs focus on high-level semantic planning (policy) while GOI\nhandles low-level navigation and interaction (mechanism). GOI does not require\nmodifying the application source code or relying on application programming\ninterfaces (APIs).\n  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on\nWindows. Compared to a leading GUI-based agent baseline, GOI improves task\nsuccess rates by 67% and reduces interaction steps by 43.5%. Notably, GOI\ncompletes over 61% of successful tasks with a single LLM call.",
    "published": "2025-10-06T09:14:58Z",
    "updated": "2025-10-06T09:14:58Z",
    "link": "http://arxiv.org/pdf/2510.04607v1.pdf",
    "category": [
      "cs.OS",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yuan Wang",
      "Mingyu Li",
      "Haibo Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04602v1",
    "title": "Computing Wasserstein Barycenters through Gradient Flows",
    "summary": "Wasserstein barycenters provide a powerful tool for aggregating probability\nmeasures, while leveraging the geometry of their ambient space. Existing\ndiscrete methods suffer from poor scalability, as they require access to the\ncomplete set of samples from input measures. We address this issue by recasting\nthe original barycenter problem as a gradient flow in the Wasserstein space.\nOur approach offers two advantages. First, we achieve scalability by sampling\nmini-batches from the input measures. Second, we incorporate functionals over\nprobability measures, which regularize the barycenter problem through internal,\npotential, and interaction energies. We present two algorithms for empirical\nand Gaussian mixture measures, providing convergence guarantees under the\nPolyak-{\\L}ojasiewicz inequality. Experimental validation on toy datasets and\ndomain adaptation benchmarks show that our methods outperform previous discrete\nand neural net-based methods for computing Wasserstein barycenters.",
    "published": "2025-10-06T09:07:12Z",
    "updated": "2025-10-06T09:07:12Z",
    "link": "http://arxiv.org/pdf/2510.04602v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Yassir Bendou",
      "Mike Gartrell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04588v1",
    "title": "Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic\n  Dilemma",
    "summary": "Rapid advances in artificial intelligence necessitate a re-examination of the\nepistemological foundations upon which we attribute consciousness. As AI\nsystems increasingly mimic human behavior and interaction with high fidelity,\nthe concept of a \"perfect mimic\"-an entity empirically indistinguishable from a\nhuman through observation and interaction-shifts from hypothetical to\ntechnologically plausible. This paper argues that such developments pose a\nfundamental challenge to the consistency of our mind-recognition practices.\nConsciousness attributions rely heavily, if not exclusively, on empirical\nevidence derived from behavior and interaction. If a perfect mimic provides\nevidence identical to that of humans, any refusal to grant it equivalent\nepistemic status must invoke inaccessible factors, such as qualia, substrate\nrequirements, or origin. Selectively invoking such factors risks a debilitating\ndilemma: either we undermine the rational basis for attributing consciousness\nto others (epistemological solipsism), or we accept inconsistent reasoning. I\ncontend that epistemic consistency demands we ascribe the same status to\nempirically indistinguishable entities, regardless of metaphysical assumptions.\nThe perfect mimic thus acts as an epistemic mirror, forcing critical reflection\non the assumptions underlying intersubjective recognition in light of advancing\nAI. This analysis carries significant implications for theories of\nconsciousness and ethical frameworks concerning artificial agents.",
    "published": "2025-10-06T08:44:55Z",
    "updated": "2025-10-06T08:44:55Z",
    "link": "http://arxiv.org/pdf/2510.04588v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shurui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04580v1",
    "title": "Strongly Solving 2048 4x3",
    "summary": "2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,\nwhere a player chooses a direction among up, down, left, and right to obtain a\nscore by merging two tiles with the same number located in neighboring cells\nalong the chosen direction. This paper presents that a variant 2048-4x3 12\ncells on a 4 by 3 board, one row smaller than the original, has been strongly\nsolved. In this variant, the expected score achieved by an optimal strategy is\nabout $50724.26$ for the most common initial states: ones with two tiles of\nnumber 2. The numbers of reachable states and afterstates are identified to be\n$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is\nto partition state space by the sum of tile numbers on a board, which we call\nthe age of a state. An age is invariant between a state and its successive\nafterstate after any valid action and is increased two or four by stochastic\nresponse from the environment. Therefore, we can partition state space by ages\nand enumerate all (after)states of an age depending only on states with the\nrecent ages. Similarly, we can identify (after)state values by going along with\nages in decreasing order.",
    "published": "2025-10-06T08:31:59Z",
    "updated": "2025-10-06T08:31:59Z",
    "link": "http://arxiv.org/pdf/2510.04580v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tomoyuki Kaneko",
      "Shuhei Yamashita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04576v1",
    "title": "SONA: Learning Conditional, Unconditional, and Mismatching-Aware\n  Discriminator",
    "summary": "Deep generative models have made significant advances in generating complex\ncontent, yet conditional generation remains a fundamental challenge. Existing\nconditional generative adversarial networks often struggle to balance the dual\nobjectives of assessing authenticity and conditional alignment of input samples\nwithin their conditional discriminators. To address this, we propose a novel\ndiscriminator design that integrates three key capabilities: unconditional\ndiscrimination, matching-aware supervision to enhance alignment sensitivity,\nand adaptive weighting to dynamically balance all objectives. Specifically, we\nintroduce Sum of Naturalness and Alignment (SONA), which employs separate\nprojections for naturalness (authenticity) and alignment in the final layer\nwith an inductive bias, supported by dedicated objective functions and an\nadaptive weighting mechanism. Extensive experiments on class-conditional\ngeneration tasks show that \\ours achieves superior sample quality and\nconditional alignment compared to state-of-the-art methods. Furthermore, we\ndemonstrate its effectiveness in text-to-image generation, confirming the\nversatility and robustness of our approach.",
    "published": "2025-10-06T08:26:06Z",
    "updated": "2025-10-06T08:26:06Z",
    "link": "http://arxiv.org/pdf/2510.04576v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "authors": [
      "Yuhta Takida",
      "Satoshi Hayakawa",
      "Takashi Shibuya",
      "Masaaki Imaizumi",
      "Naoki Murata",
      "Bac Nguyen",
      "Toshimitsu Uesaka",
      "Chieh-Hsin Lai",
      "Yuki Mitsufuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16322v3",
    "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners",
    "summary": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs.",
    "published": "2025-05-22T07:24:11Z",
    "updated": "2025-10-06T08:23:36Z",
    "link": "http://arxiv.org/pdf/2505.16322v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Woosung Koh",
      "Wonbeen Oh",
      "Jaein Jang",
      "MinHyung Lee",
      "Hyeongjin Kim",
      "Ah Yeon Kim",
      "Joonkee Kim",
      "Junghyun Lee",
      "Taehyeon Kim",
      "Se-Young Yun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04574v1",
    "title": "Deep learning framework for predicting stochastic take-off and die-out\n  of early spreading",
    "summary": "Large-scale outbreaks of epidemics, misinformation, or other harmful\ncontagions pose significant threats to human society, yet the fundamental\nquestion of whether an emerging outbreak will escalate into a major epidemic or\nnaturally die out remains largely unaddressed. This problem is challenging,\npartially due to inadequate data during the early stages of outbreaks and also\nbecause established models focus on average behaviors of large epidemics rather\nthan the stochastic nature of small transmission chains. Here, we introduce the\nfirst systematic framework for forecasting whether initial transmission events\nwill amplify into major outbreaks or fade into extinction during early stages,\nwhen intervention strategies can still be effectively implemented. Using\nextensive data from stochastic spreading models, we developed a deep learning\nframework that predicts early-stage spreading outcomes in real-time. Validation\nacross Erd\\H{o}s-R\\'enyi and Barab\\'asi-Albert networks with varying\ninfectivity levels shows our method accurately forecasts stochastic spreading\nevents well before potential outbreaks, demonstrating robust performance across\ndifferent network structures and infectivity scenarios.To address the challenge\nof sparse data during early outbreak stages, we further propose a\npretrain-finetune framework that leverages diverse simulation data for\npretraining and adapts to specific scenarios through targeted fine-tuning. The\npretrain-finetune framework consistently outperforms baseline models, achieving\nsuperior performance even when trained on limited scenario-specific data. To\nour knowledge, this work presents the first framework for predicting stochastic\ntake-off versus die-out. This framework provides valuable insights for epidemic\npreparedness and public health decision-making, enabling more informed early\nintervention strategies.",
    "published": "2025-10-06T08:18:47Z",
    "updated": "2025-10-06T08:18:47Z",
    "link": "http://arxiv.org/pdf/2510.04574v1.pdf",
    "category": [
      "cs.SI",
      "cs.AI",
      "physics.soc-ph",
      "05C82, 68T05, 92C42",
      "G.2.2; I.2.6"
    ],
    "authors": [
      "Wenchao He",
      "Tao Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04573v1",
    "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
    "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.",
    "published": "2025-10-06T08:15:03Z",
    "updated": "2025-10-06T08:15:03Z",
    "link": "http://arxiv.org/pdf/2510.04573v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Haoqiang Kang",
      "Yizhe Zhang",
      "Nikki Lijing Kuang",
      "Nicklas Majamaki",
      "Navdeep Jaitly",
      "Yi-An Ma",
      "Lianhui Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04568v1",
    "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning\n  over Long Context",
    "summary": "Reasoning over very long inputs remains difficult for large language models\n(LLMs). Common workarounds either shrink the input via retrieval (risking\nmissed evidence), enlarge the context window (straining selectivity), or stage\nmultiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,\nCoA), free-form summaries passed between agents can discard crucial details and\namplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured\nMemory for Iterative Reasoning), a chain-style framework that replaces ad hoc\nmessages with a structured memory. A Planner agent first turns a user query\ninto concrete, checkable sub-questions. worker agents process chunks via a\nfixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared\nmemory. A Manager agent then Synthesizes the final answer directly from the\nmemory. This preserves step-wise read-then-reason benefits while changing both\nthe communication medium (structured memory) and the worker procedure (fixed\nmicro-cycle), yielding higher faithfulness, better long-range aggregation, and\nauditability. On long-context QA from the HELMET suite, COSMIR reduces\npropagation-stage information loss and improves accuracy over a CoA baseline.",
    "published": "2025-10-06T08:10:04Z",
    "updated": "2025-10-06T08:10:04Z",
    "link": "http://arxiv.org/pdf/2510.04568v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Naman Gupta",
      "Shreeyash Gowaikar",
      "Arun Iyer",
      "Kirankumar Shiragur",
      "Ramakrishna B Bairi",
      "Rishikesh Maurya",
      "Ritabrata Maiti",
      "Sankarshan Damle",
      "Shachee Mishra Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04567v1",
    "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context\n  Learning",
    "summary": "Graph Neural Networks (GNNs) are powerful tools for precessing relational\ndata but often struggle to generalize to unseen graphs, giving rise to the\ndevelopment of Graph Foundational Models (GFMs). However, current GFMs are\nchallenged by the extreme heterogeneity of graph data, where each graph can\npossess a unique feature space, label set, and topology. To address this, two\nmain paradigms have emerged. The first leverages Large Language Models (LLMs),\nbut is fundamentally text-dependent, thus struggles to handle the numerical\nfeatures in vast graphs. The second pre-trains a structure-based model, but the\nadaptation to new tasks typically requires a costly, per-graph tuning stage,\ncreating a critical efficiency bottleneck. In this work, we move beyond these\nlimitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning\n\\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free\narchitecture. GILT introduces a novel token-based framework for in-context\nlearning (ICL) on graphs, reframing classification tasks spanning node, edge\nand graph levels in a unified framework. This mechanism is the key to handling\nheterogeneity, as it is designed to operate on generic numerical features.\nFurther, its ability to understand class semantics dynamically from the context\nenables tuning-free adaptation. Comprehensive experiments show that GILT\nachieves stronger few-shot performance with significantly less time than\nLLM-based or tuning-based baselines, validating the effectiveness of our\napproach.",
    "published": "2025-10-06T08:09:15Z",
    "updated": "2025-10-06T08:09:15Z",
    "link": "http://arxiv.org/pdf/2510.04567v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Weishuo Ma",
      "Yanbo Wang",
      "Xiyuan Wang",
      "Lei Zou",
      "Muhan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04560v1",
    "title": "ContextNav: Towards Agentic Multimodal In-Context Learning",
    "summary": "Recent advances demonstrate that multimodal large language models (MLLMs)\nexhibit strong multimodal in-context learning (ICL) capabilities, enabling them\nto adapt to novel vision-language tasks from a few contextual examples.\nHowever, existing ICL approaches face challenges in reconciling scalability\nwith robustness across diverse tasks and noisy contextual examples: manually\nselecting examples produces clean contexts but is labor-intensive and\ntask-specific, while similarity-based retrieval improves scalability but could\nintroduce irrelevant or structurally inconsistent samples that degrade ICL\nperformance. To address these limitations, we propose ContextNav, the first\nagentic framework that integrates the scalability of automated retrieval with\nthe quality and adaptiveness of human-like curation, enabling noise-robust and\ndynamically optimized contextualization for multimodal ICL. ContextNav unifies\ncontext management and noise-robust contextualization within a closed-loop\nworkflow driven by graph-based orchestration. Specifically, it builds a\nresource-aware multimodal embedding pipeline, maintains a retrievable vector\ndatabase, and applies agentic retrieval and structural alignment to construct\nnoise-resilient contexts. An Operational Grammar Graph (OGG) further supports\nadaptive workflow planning and optimization, enabling the agent to refine its\noperational strategies based on downstream ICL feedback. Experimental results\ndemonstrate that ContextNav achieves state-of-the-art performance across\nvarious datasets, underscoring the promise of agentic workflows for advancing\nscalable and robust contextualization in multimodal ICL.",
    "published": "2025-10-06T07:49:52Z",
    "updated": "2025-10-06T07:49:52Z",
    "link": "http://arxiv.org/pdf/2510.04560v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Honghao Fu",
      "Yuan Ouyang",
      "Kai-Wei Chang",
      "Yiwei Wang",
      "Zi Huang",
      "Yujun Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10351v3",
    "title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal\n  Representation",
    "summary": "Physiological signals are often corrupted by motion artifacts, baseline\ndrift, and other low-SNR disturbances, which pose significant challenges for\nanalysis. Additionally, these signals exhibit strong non-stationarity, with\nsharp peaks and abrupt changes that evolve continuously, making them difficult\nto represent using traditional time-domain or filtering methods. To address\nthese issues, a novel wavelet-based approach for physiological signal analysis\nis presented, aiming to capture multi-scale time-frequency features in various\nphysiological signals. Leveraging this technique, two large-scale pretrained\nmodels specific to EMG and ECG are introduced for the first time, achieving\nsuperior performance and setting new baselines in downstream tasks.\nAdditionally, a unified multi-modal framework is constructed by integrating\npretrained EEG model, where each modality is guided through its dedicated\nbranch and fused via learnable weighted fusion. This design effectively\naddresses challenges such as low signal-to-noise ratio, high inter-subject\nvariability, and device mismatch, outperforming existing methods on multi-modal\ntasks. The proposed wavelet-based architecture lays a solid foundation for\nanalysis of diverse physiological signals, while the multi-modal design points\nto next-generation physiological signal processing with potential impact on\nwearable health monitoring, clinical diagnostics, and broader biomedical\napplications. Code and data are available at:\ngithub.com/ForeverBlue816/PhysioWave",
    "published": "2025-06-12T05:11:41Z",
    "updated": "2025-10-06T07:46:23Z",
    "link": "http://arxiv.org/pdf/2506.10351v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yanlong Chen",
      "Mattia Orlandi",
      "Pierangelo Maria Rapa",
      "Simone Benatti",
      "Luca Benini",
      "Yawei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04550v1",
    "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool\n  Use",
    "summary": "Large language model (LLM)-based agents increasingly rely on tool use to\ncomplete real-world tasks. While existing works evaluate the LLMs' tool use\ncapability, they largely focus on the final answers yet overlook the detailed\ntool usage trajectory, i.e., whether tools are selected, parameterized, and\nordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to\ncomprehensively evaluate LLMs' tool use capability through diverse tasks with\nfine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable\ntools across practical domains with tasks grounded in production-style APIs,\nand synthesizes trajectories that vary in breadth (parallel calls) and depth\n(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports\ntrajectory-level diagnostics, including tool selection and argument\ncorrectness, and dependency/order satisfaction. Analyses reveal failure modes\nsuch as similar tool confusion and parameter-blind selection, and scaling\nbehavior with tool diversity and trajectory length where the bottleneck of\ntransiting from short to mid-length trajectories is revealed, offering\nactionable guidance for LLMs' tool use.",
    "published": "2025-10-06T07:30:25Z",
    "updated": "2025-10-06T07:30:25Z",
    "link": "http://arxiv.org/pdf/2510.04550v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Pengfei He",
      "Zhenwei Dai",
      "Bing He",
      "Hui Liu",
      "Xianfeng Tang",
      "Hanqing Lu",
      "Juanhui Li",
      "Jiayuan Ding",
      "Subhabrata Mukherjee",
      "Suhang Wang",
      "Yue Xing",
      "Jiliang Tang",
      "Benoit Dumoulin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04542v1",
    "title": "Code World Models for General Game Playing",
    "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being\napplied to classical board and card games, but the dominant approach --\ninvolving prompting for direct move generation -- has significant drawbacks. It\nrelies on the model's implicit fragile pattern-matching capabilities, leading\nto frequent illegal moves and strategically shallow play. Here we introduce an\nalternative approach: We use the LLM to translate natural language rules and\ngame trajectories into a formal, executable world model represented as Python\ncode. This generated model -- comprising functions for state transition, legal\nmove enumeration, and termination checks -- serves as a verifiable simulation\nengine for high-performance planning algorithms like Monte Carlo tree search\n(MCTS). In addition, we prompt the LLM to generate heuristic value functions\n(to make MCTS more efficient), and inference functions (to estimate hidden\nstates in imperfect information games). Our method offers three distinct\nadvantages compared to directly using the LLM as a policy: (1) Verifiability:\nThe generated CWM serves as a formal specification of the game's rules,\nallowing planners to algorithmically enumerate valid actions and avoid illegal\nmoves, contingent on the correctness of the synthesized model; (2) Strategic\nDepth: We combine LLM semantic understanding with the deep search power of\nclassical planners; and (3) Generalization: We direct the LLM to focus on the\nmeta-task of data-to-code translation, enabling it to adapt to new games more\neasily. We evaluate our agent on 10 different games, of which 4 are novel and\ncreated for this paper. 5 of the games are fully observed (perfect\ninformation), and 5 are partially observed (imperfect information). We find\nthat our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10\nconsidered games.",
    "published": "2025-10-06T07:16:07Z",
    "updated": "2025-10-06T07:16:07Z",
    "link": "http://arxiv.org/pdf/2510.04542v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Wolfgang Lehrach",
      "Daniel Hennes",
      "Miguel Lazaro-Gredilla",
      "Xinghua Lou",
      "Carter Wendelken",
      "Zun Li",
      "Antoine Dedieu",
      "Jordi Grau-Moya",
      "Marc Lanctot",
      "Atil Iscen",
      "John Schultz",
      "Marcus Chiam",
      "Ian Gemp",
      "Piotr Zielinski",
      "Satinder Singh",
      "Kevin P. Murphy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04536v1",
    "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs\n  Using MCP and RAG",
    "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources.",
    "published": "2025-10-06T07:00:06Z",
    "updated": "2025-10-06T07:00:06Z",
    "link": "http://arxiv.org/pdf/2510.04536v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Shun-ichiro Hayashi",
      "Daichi Mukunoki",
      "Tetsuya Hoshino",
      "Satoshi Ohshima",
      "Takahiro Katagiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02809v2",
    "title": "Relevance-Aware Thresholding in Online Conformal Prediction for Time\n  Series",
    "summary": "Uncertainty quantification has received considerable interest in recent works\nin Machine Learning. In particular, Conformal Prediction (CP) gains ground in\nthis field. For the case of time series, Online Conformal Prediction (OCP)\nbecomes an option to address the problem of data distribution shift over time.\nIndeed, the idea of OCP is to update a threshold of some quantity (whether the\nmiscoverage level or the quantile) based on the distribution observation. To\nevaluate the performance of OCP methods, two key aspects are typically\nconsidered: the coverage validity and the prediction interval width\nminimization. Recently, new OCP methods have emerged, offering long-run\ncoverage guarantees and producing more informative intervals. However, during\nthe threshold update step, most of these methods focus solely on the validity\nof the prediction intervals~--~that is, whether the ground truth falls inside\nor outside the interval~--~without accounting for their relevance. In this\npaper, we aim to leverage this overlooked aspect. Specifically, we propose\nenhancing the threshold update step by replacing the binary evaluation\n(inside/outside) with a broader class of functions that quantify the relevance\nof the prediction interval using the ground truth. This approach helps prevent\nabrupt threshold changes, potentially resulting in narrower prediction\nintervals. Indeed, experimental results on real-world datasets suggest that\nthese functions can produce tighter intervals compared to existing OCP methods\nwhile maintaining coverage validity.",
    "published": "2025-10-03T08:31:14Z",
    "updated": "2025-10-06T06:51:20Z",
    "link": "http://arxiv.org/pdf/2510.02809v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Théo Dupuy",
      "Binbin Xu",
      "Stéphane Perrey",
      "Jacky Montmain",
      "Abdelhak Imoussaten"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04532v1",
    "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in\n  Training Vision-Language Driving Models",
    "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.",
    "published": "2025-10-06T06:50:16Z",
    "updated": "2025-10-06T06:50:16Z",
    "link": "http://arxiv.org/pdf/2510.04532v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "authors": [
      "Xurui Song",
      "Shuo Huai",
      "JingJing Jiang",
      "Jiayi Kong",
      "Jun Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04528v1",
    "title": "Unified Threat Detection and Mitigation Framework (UTDMF): Combating\n  Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers",
    "summary": "The rapid adoption of large language models (LLMs) in enterprise systems\nexposes vulnerabilities to prompt injection attacks, strategic deception, and\nbiased outputs, threatening security, trust, and fairness. Extending our\nadversarial activation patching framework (arXiv:2507.09406), which induced\ndeception in toy networks at a 23.9% rate, we introduce the Unified Threat\nDetection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for\nenterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through\n700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for\nprompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs\nvia enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,\ndemographic bias). Novel contributions include a generalized patching algorithm\nfor multi-threat detection, three groundbreaking hypotheses on threat\ninteractions (e.g., threat chaining in enterprise workflows), and a\ndeployment-ready toolkit with APIs for enterprise integration.",
    "published": "2025-10-06T06:44:27Z",
    "updated": "2025-10-06T06:44:27Z",
    "link": "http://arxiv.org/pdf/2510.04528v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Santhosh KumarRavindran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16516v2",
    "title": "Computing Exact Shapley Values in Polynomial Time for Product-Kernel\n  Methods",
    "summary": "Kernel methods are widely used in machine learning due to their flexibility\nand expressiveness. However, their black-box nature poses significant\nchallenges to interpretability, limiting their adoption in high-stakes\napplications. Shapley value-based feature attribution techniques, such as SHAP\nand kernel method-specific adaptation like RKHS-SHAP, offer a promising path\ntoward explainability. Yet, computing exact Shapley values is generally\nintractable, leading existing methods to rely on approximations and thereby\nincur unavoidable error. In this work, we introduce PKeX-Shapley, a novel\nalgorithm that utilizes the multiplicative structure of product kernels to\nenable the exact computation of Shapley values in polynomial time. The core of\nour approach is a new value function, the functional baseline value function,\nspecifically designed for product-kernel models. This value function removes\nthe influence of a feature subset by setting its functional component to the\nleast informative state. Crucially, it allows a recursive thus efficient\ncomputation of Shapley values in polynomial time. As an important additional\ncontribution, we show that our framework extends beyond predictive modeling to\nstatistical inference. In particular, it generalizes to popular kernel-based\ndiscrepancy measures such as the Maximum Mean Discrepancy (MMD) and the\nHilbert-Schmidt Independence Criterion (HSIC), thereby providing new tools for\ninterpretable statistical inference.",
    "published": "2025-05-22T10:53:04Z",
    "updated": "2025-10-06T06:40:29Z",
    "link": "http://arxiv.org/pdf/2505.16516v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Majid Mohammadi",
      "Siu Lun Chau",
      "Krikamol Muandet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04522v1",
    "title": "Toward a Unified Geometry Understanding: Riemannian Diffusion Framework\n  for Graph Generation and Prediction",
    "summary": "Graph diffusion models have made significant progress in learning structured\ngraph data and have demonstrated strong potential for predictive tasks.\nExisting approaches typically embed node, edge, and graph-level features into a\nunified latent space, modeling prediction tasks including classification and\nregression as a form of conditional generation. However, due to the\nnon-Euclidean nature of graph data, features of different curvatures are\nentangled in the same latent space without releasing their geometric potential.\nTo address this issue, we aim to construt an ideal Riemannian diffusion model\nto capture distinct manifold signatures of complex graph data and learn their\ndistribution. This goal faces two challenges: numerical instability caused by\nexponential mapping during the encoding proces and manifold deviation during\ndiffusion generation. To address these challenges, we propose GeoMancer: a\nnovel Riemannian graph diffusion framework for both generation and prediction\ntasks. To mitigate numerical instability, we replace exponential mapping with\nan isometric-invariant Riemannian gyrokernel approach and decouple multi-level\nfeatures onto their respective task-specific manifolds to learn optimal\nrepresentations. To address manifold deviation, we introduce a\nmanifold-constrained diffusion method and a self-guided strategy for\nunconditional generation, ensuring that the generated data remains aligned with\nthe manifold signature. Extensive experiments validate the effectiveness of our\napproach, demonstrating superior performance across a variety of tasks.",
    "published": "2025-10-06T06:29:49Z",
    "updated": "2025-10-06T06:29:49Z",
    "link": "http://arxiv.org/pdf/2510.04522v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yisen Gao",
      "Xingcheng Fu",
      "Qingyun Sun",
      "Jianxin Li",
      "Xianxian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04520v1",
    "title": "Aria: An Agent For Retrieval and Iterative Auto-Formalization via\n  Dependency Graph",
    "summary": "Accurate auto-formalization of theorem statements is essential for advancing\nautomated discovery and verification of research-level mathematics, yet remains\na major bottleneck for LLMs due to hallucinations, semantic mismatches, and\ntheir inability to synthesize new definitions. To tackle these issues, we\npresent Aria (Agent for Retrieval and Iterative Autoformalization), a system\nfor conjecture-level formalization in Lean that emulates human expert reasoning\nvia a two-phase Graph-of-Thought process: recursively decomposing statements\ninto a dependency graph and then constructing formalizations from grounded\nconcepts. To ensure semantic correctness, we introduce AriaScorer, a checker\nthat retrieves definitions from Mathlib for term-level grounding, enabling\nrigorous and reliable verification. We evaluate Aria on diverse benchmarks. On\nProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,\nsurpassing previous methods. On FATE-X, a suite of challenging algebra problems\nfrom research literature, it outperforms the best baseline with 44.0% vs. 24.0%\nfinal accuracy. On a dataset of homological conjectures, Aria reaches 42.9%\nfinal accuracy while all other models score 0%.",
    "published": "2025-10-06T06:25:11Z",
    "updated": "2025-10-06T06:25:11Z",
    "link": "http://arxiv.org/pdf/2510.04520v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hanyu Wang",
      "Ruohan Xie",
      "Yutong Wang",
      "Guoxiong Gao",
      "Xintao Yu",
      "Bin Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09532v3",
    "title": "Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal\n  Foundation Models for Zero-Shot Loco-Manipulation",
    "summary": "Humanoid loco-manipulation, which integrates whole-body locomotion with\ndexterous manipulation, remains a fundamental challenge in robotics. Beyond\nwhole-body coordination and balance, a central difficulty lies in understanding\nhuman instructions and translating them into coherent sequences of embodied\nactions. Recent advances in foundation models provide transferable multimodal\nrepresentations and reasoning capabilities, yet existing efforts remain largely\nrestricted to either locomotion or manipulation in isolation, with limited\napplicability to humanoid settings. In this paper, we propose Humanoid-COA, the\nfirst humanoid agent framework that integrates foundation model reasoning with\nan Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation.\nWithin the perception--reasoning--action paradigm, our key contribution lies in\nthe reasoning stage, where the proposed CoA mechanism decomposes high-level\nhuman instructions into structured sequences of locomotion and manipulation\nprimitives through affordance analysis, spatial inference, and whole-body\naction reasoning. Extensive experiments on two humanoid robots, Unitree H1-2\nand G1, in both an open test area and an apartment environment, demonstrate\nthat our framework substantially outperforms prior baselines across\nmanipulation, locomotion, and loco-manipulation tasks, achieving robust\ngeneralization to long-horizon and unstructured scenarios. Project page:\nhttps://humanoid-coa.github.io/",
    "published": "2025-04-13T11:37:32Z",
    "updated": "2025-10-06T06:14:37Z",
    "link": "http://arxiv.org/pdf/2504.09532v3.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Congcong Wen",
      "Geeta Chandra Raju Bethala",
      "Yu Hao",
      "Niraj Pudasaini",
      "Hao Huang",
      "Shuaihang Yuan",
      "Baoru Huang",
      "Anh Nguyen",
      "Mengyu Wang",
      "Anthony Tzes",
      "Yi Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04514v1",
    "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in\n  Complex Chart Question Answering",
    "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.",
    "published": "2025-10-06T06:05:36Z",
    "updated": "2025-10-06T06:05:36Z",
    "link": "http://arxiv.org/pdf/2510.04514v1.pdf",
    "category": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.CV",
      "stat.ME"
    ],
    "authors": [
      "Rachneet Kaur",
      "Nishan Srishankar",
      "Zhen Zeng",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.14715v3",
    "title": "Towards Cross-modal Backward-compatible Representation Learning for\n  Vision-Language Models",
    "summary": "Modern retrieval systems often struggle with upgrading to new and more\npowerful models due to the incompatibility of embeddings between the old and\nnew models. This necessitates a costly process known as backfilling, which\ninvolves re-computing the embeddings for a large number of data samples. In\nvision, Backward-compatible Training (BT) has been proposed to ensure that the\nnew model aligns with the old model's embeddings. This paper extends the\nconcept of vision-only BT to the field of cross-modal retrieval, marking the\nfirst attempt to address Cross-modal BT (XBT). Our goal is to achieve\nbackward-compatibility between Vision-Language Pretraining (VLP) models, such\nas CLIP, for the cross-modal retrieval task. To address XBT challenges, we\npropose an efficient solution: a projection module that maps the new model's\nembeddings to those of the old model. This module, pretrained solely with text\ndata, significantly reduces the number of image-text pairs required for XBT\nlearning, and, once it is pretrained, it avoids using the old model during\ntraining. Furthermore, we utilize parameter-efficient training strategies that\nimprove efficiency and preserve the off-the-shelf new model's knowledge by\navoiding any modifications. Experimental results on cross-modal retrieval\ndatasets demonstrate the effectiveness of XBT and its potential to enable\nbackfill-free upgrades when a new VLP model emerges.",
    "published": "2024-05-23T15:46:35Z",
    "updated": "2025-10-06T06:02:41Z",
    "link": "http://arxiv.org/pdf/2405.14715v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Young Kyun Jang",
      "Ser-nam Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04506v1",
    "title": "GRACE: Generative Representation Learning via Contrastive Policy\n  Optimization",
    "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders\nrely on contrastive losses that treat the model as a black box function,\ndiscarding its generative and reasoning capabilities in favor of static\nembeddings. We introduce GRACE (Generative Representation Learning via\nContrastive Policy Optimization), a novel framework that reimagines contrastive\nsignals not as losses to be minimized, but as rewards that guide a generative\npolicy. In GRACE, the LLM acts as a policy that produces explicit,\nhuman-interpretable rationales--structured natural language explanations of its\nsemantic understanding. These rationales are then encoded into high-quality\nembeddings via mean pooling. Using policy gradient optimization, we train the\nmodel with a multi-component reward function that maximizes similarity between\nquery positive pairs and minimizes similarity with negatives. This transforms\nthe LLM from an opaque encoder into an interpretable agent whose reasoning\nprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broad\ncross category gains: averaged over four backbones, the supervised setting\nimproves overall score by 11.5% over base models, and the unsupervised variant\nadds 6.9%, while preserving general capabilities. This work treats contrastive\nobjectives as rewards over rationales, unifying representation learning with\ngeneration to produce stronger embeddings and transparent rationales. The\nmodel, data and code are available at https://github.com/GasolSun36/GRACE.",
    "published": "2025-10-06T05:46:56Z",
    "updated": "2025-10-06T05:46:56Z",
    "link": "http://arxiv.org/pdf/2510.04506v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Jiashuo Sun",
      "Shixuan Liu",
      "Zhaochen Su",
      "Xianrui Zhong",
      "Pengcheng Jiang",
      "Bowen Jin",
      "Peiran Li",
      "Weijia Shi",
      "Jiawei Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04503v1",
    "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
    "summary": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community.",
    "published": "2025-10-06T05:45:23Z",
    "updated": "2025-10-06T05:45:23Z",
    "link": "http://arxiv.org/pdf/2510.04503v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shuai Zhao",
      "Xinyi Wu",
      "Shiqian Zhao",
      "Xiaobao Wu",
      "Zhongliang Guo",
      "Yanhao Jia",
      "Anh Tuan Luu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.16706v3",
    "title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling",
    "summary": "Inference scaling methods for LLMs often rely on decomposing problems into\nsteps (or groups of tokens), followed by sampling and selecting the best next\nsteps. However, these steps and their sizes are often predetermined or manually\ndesigned based on domain knowledge. We propose dynamic decomposition, a method\nthat adaptively and automatically partitions solution and reasoning traces into\nmanageable steps during inference. By more effectively allocating compute --\nparticularly through subdividing challenging steps and prioritizing their\nsampling -- dynamic decomposition significantly improves inference efficiency.\nExperiments on benchmarks such as APPS, MATH, and LiveCodeBench demonstrate\nthat dynamic decomposition outperforms static approaches, including\ntoken-level, sentence-level, and single-step decompositions, reducing the\npass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These findings\nhighlight the potential of dynamic decomposition to improve a wide range of\ninference scaling techniques.",
    "published": "2025-02-23T20:37:32Z",
    "updated": "2025-10-06T05:36:54Z",
    "link": "http://arxiv.org/pdf/2502.16706v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SE",
      "68T07, 68W40",
      "I.2.6; I.2.7; I.2.8; D.2.3; F.2.2"
    ],
    "authors": [
      "Jonathan Light",
      "Wei Cheng",
      "Benjamin Riviere",
      "Wu Yue",
      "Masafumi Oyamada",
      "Mengdi Wang",
      "Yisong Yue",
      "Santiago Paternain",
      "Haifeng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04498v1",
    "title": "GenQuest: An LLM-based Text Adventure Game for Language Learners",
    "summary": "GenQuest is a generative text adventure game that leverages Large Language\nModels (LLMs) to facilitate second language learning through immersive,\ninteractive storytelling. The system engages English as a Foreign Language\n(EFL) learners in a collaborative \"choose-your-own-adventure\" style narrative,\ndynamically generated in response to learner choices. Game mechanics such as\nbranching decision points and story milestones are incorporated to maintain\nnarrative coherence while allowing learner-driven plot development. Key\npedagogical features include content generation tailored to each learner's\nproficiency level, and a vocabulary assistant that provides in-context\nexplanations of learner-queried text strings, ranging from words and phrases to\nsentences. Findings from a pilot study with university EFL students in China\nindicate promising vocabulary gains and positive user perceptions. Also\ndiscussed are suggestions from participants regarding the narrative length and\nquality, and the request for multi-modal content such as illustrations.",
    "published": "2025-10-06T05:22:53Z",
    "updated": "2025-10-06T05:22:53Z",
    "link": "http://arxiv.org/pdf/2510.04498v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Qiao Wang",
      "Adnan Labib",
      "Robert Swier",
      "Michael Hofmeyr",
      "Zheng Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02161v2",
    "title": "Comparing Contrastive and Triplet Loss: Variance Analysis and\n  Optimization Behavior",
    "summary": "Contrastive loss and triplet loss are widely used objectives in deep metric\nlearning, yet their effects on representation quality remain insufficiently\nunderstood. We present a theoretical and empirical comparison of these losses,\nfocusing on intra- and inter-class variance and optimization behavior (e.g.,\ngreedy updates). Through task-specific experiments with consistent settings on\nsynthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss\npreserves greater variance within and across classes, supporting finer-grained\ndistinctions in the learned representations. In contrast, contrastive loss\ntends to compact intra-class embeddings, which may obscure subtle semantic\ndifferences. To better understand their optimization dynamics, By examining\nloss-decay rate, active ratio, and gradient norm, we find that contrastive loss\ndrives many small updates early on, while triplet loss produces fewer but\nstronger updates that sustain learning on hard examples. Finally, across both\nclassification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196\ndatasets, our results consistently show that triplet loss yields superior\nperformance, which suggests using triplet loss for detail retention and\nhard-sample focus, and contrastive loss for smoother, broad-based embedding\nrefinement.",
    "published": "2025-10-02T16:11:46Z",
    "updated": "2025-10-06T05:19:04Z",
    "link": "http://arxiv.org/pdf/2510.02161v2.pdf",
    "category": [
      "cs.MM",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Donghuo Zeng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.07730v2",
    "title": "STIV: Scalable Text and Image Conditioned Video Generation",
    "summary": "The field of video generation has made remarkable advancements, yet there\nremains a pressing need for a clear, systematic recipe that can guide the\ndevelopment of robust and scalable models. In this work, we present a\ncomprehensive study that systematically explores the interplay of model\narchitectures, training recipes, and data curation strategies, culminating in a\nsimple and scalable text-image-conditioned video generation method, named STIV.\nOur framework integrates image condition into a Diffusion Transformer (DiT)\nthrough frame replacement, while incorporating text conditioning via a joint\nimage-text conditional classifier-free guidance. This design enables STIV to\nperform both text-to-video (T2V) and text-image-to-video (TI2V) tasks\nsimultaneously. Additionally, STIV can be easily extended to various\napplications, such as video prediction, frame interpolation, multi-view\ngeneration, and long video generation, etc. With comprehensive ablation studies\non T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple\ndesign. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V,\nsurpassing both leading open and closed-source models like CogVideoX-5B, Pika,\nKling, and Gen-3. The same-sized model also achieves a state-of-the-art result\nof 90.1 on VBench I2V task at 512 resolution. By providing a transparent and\nextensible recipe for building cutting-edge video generation models, we aim to\nempower future research and accelerate progress toward more versatile and\nreliable video generation solutions.",
    "published": "2024-12-10T18:27:06Z",
    "updated": "2025-10-06T05:11:37Z",
    "link": "http://arxiv.org/pdf/2412.07730v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Zongyu Lin",
      "Wei Liu",
      "Chen Chen",
      "Jiasen Lu",
      "Wenze Hu",
      "Tsu-Jui Fu",
      "Jesse Allardice",
      "Zhengfeng Lai",
      "Liangchen Song",
      "Bowen Zhang",
      "Cha Chen",
      "Yiran Fei",
      "Lezhi Li",
      "Yizhou Sun",
      "Kai-Wei Chang",
      "Yinfei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04491v1",
    "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human\n  Traits for Testing Agents",
    "summary": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait.",
    "published": "2025-10-06T05:03:57Z",
    "updated": "2025-10-06T05:03:57Z",
    "link": "http://arxiv.org/pdf/2510.04491v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Muyu He",
      "Anand Kumar",
      "Tsach Mackey",
      "Meghana Rajeev",
      "James Zou",
      "Nazneen Rajani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21815v2",
    "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking",
    "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient.",
    "published": "2025-05-27T22:49:18Z",
    "updated": "2025-10-06T04:57:43Z",
    "link": "http://arxiv.org/pdf/2505.21815v2.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yunyi Zhang",
      "Ruozhen Yang",
      "Siqi Jiao",
      "SeongKu Kang",
      "Jiawei Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04488v1",
    "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable\n  LLM Reasoning",
    "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance,\naggregating without deliberation, or stopping on heuristics. We introduce MACI,\nan active controller with two independent dials that decouple information from\nbehavior: an information dial that gates evidence by quality, and a behavior\ndial that schedules contentiousness from exploration to consolidation. A\nmoderator tracks disagreement, overlap, evidence quality, and argument quality,\nand halts when gains plateau. We provide theory-lite guarantees for\nnonincreasing dispersion and provable termination, with a budget-feasible\nscheduler. Across clinical diagnosis and news-bias tasks, MACI improves\naccuracy and calibration while reducing tokens, and converts residual\nuncertainty into precision RAG plans that specify what to retrieve next. We use\na cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,\nvalidated for order invariance and judge-swap stability; stability depends on\nusing high-capability judges. MACI turns debate into a budget-aware,\nmeasurable, and provably terminating controller.",
    "published": "2025-10-06T04:52:17Z",
    "updated": "2025-10-06T04:52:17Z",
    "link": "http://arxiv.org/pdf/2510.04488v1.pdf",
    "category": [
      "cs.AI",
      "cs.IT",
      "math.IT",
      "I.2.4"
    ],
    "authors": [
      "Edward Y. Chang",
      "Ethan Y. Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04484v1",
    "title": "Psychological Steering in LLMs: An Evaluation of Effectiveness and\n  Trustworthiness",
    "summary": "The ability to control LLMs' emulated emotional states and personality traits\nis essential for enabling rich, human-centered interactions in socially\ninteractive settings. We introduce PsySET, a Psychologically-informed benchmark\nto evaluate LLM Steering Effectiveness and Trustworthiness across the emotion\nand personality domains. Our study spans four models from different LLM\nfamilies paired with various steering strategies, including prompting,\nfine-tuning, and representation engineering. Our results indicate that\nprompting is consistently effective but limited in intensity control, whereas\nvector injections achieve finer controllability while slightly reducing output\nquality. Moreover, we explore the trustworthiness of steered LLMs by assessing\nsafety, truthfulness, fairness, and ethics, highlighting potential side effects\nand behavioral shifts. Notably, we observe idiosyncratic effects; for instance,\neven a positive emotion like joy can degrade robustness to adversarial\nfactuality, lower privacy awareness, and increase preferential bias. Meanwhile,\nanger predictably elevates toxicity yet strengthens leakage resistance. Our\nframework establishes the first holistic evaluation of emotion and personality\nsteering, offering insights into its interpretability and reliability for\nsocially interactive applications.",
    "published": "2025-10-06T04:49:56Z",
    "updated": "2025-10-06T04:49:56Z",
    "link": "http://arxiv.org/pdf/2510.04484v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Amin Banayeeanzade",
      "Ala N. Tak",
      "Fatemeh Bahrani",
      "Anahita Bolourani",
      "Leonardo Blas",
      "Emilio Ferrara",
      "Jonathan Gratch",
      "Sai Praneeth Karimireddy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.01344v3",
    "title": "Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight\n  of Privacy in Language Model Agent",
    "summary": "Language model (LM) agents that act on users' behalf for personal tasks\n(e.g., replying emails) can boost productivity, but are also susceptible to\nunintended privacy leakage risks. We present the first study on people's\ncapacity to oversee the privacy implications of the LM agents. By conducting a\ntask-based survey ($N=300$), we investigate how people react to and assess the\nresponse generated by LM agents for asynchronous interpersonal communication\ntasks, compared with a response they wrote. We found that people may favor the\nagent response with more privacy leakage over the response they drafted or\nconsider both good, leading to an increased harmful disclosure from 15.7% to\n55.0%. We further identified six privacy behavior patterns reflecting varying\nconcerns, trust levels, and privacy preferences underlying people's oversight\nof LM agents' actions. Our findings shed light on designing agentic systems\nthat enable privacy-preserving interactions and achieve bidirectional alignment\non privacy preferences to help users calibrate trust.",
    "published": "2024-11-02T19:15:42Z",
    "updated": "2025-10-06T04:47:18Z",
    "link": "http://arxiv.org/pdf/2411.01344v3.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Zhiping Zhang",
      "Bingcan Guo",
      "Tianshi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.04724v2",
    "title": "Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents\n  in LLM-Based Multi-Agent Systems",
    "summary": "Multi-agent systems powered by Large Language Models (LLM-MAS) have\ndemonstrated remarkable capabilities in collaborative problem-solving. However,\ntheir deployment also introduces new security risks. Existing research on\nLLM-based agents has primarily examined single-agent scenarios, while the\nsecurity of multi-agent systems remains largely unexplored. To address this\ngap, we present a systematic study of intention-hiding threats in LLM-MAS. We\ndesign four representative attack paradigms that subtly disrupt task completion\nwhile maintaining a high degree of stealth, and evaluate them under\ncentralized, decentralized, and layered communication structures. Experimental\nresults show that these attacks are highly disruptive and can easily evade\nexisting defense mechanisms. To counter these threats, we propose AgentXposed,\na psychology-inspired detection framework. AgentXposed draws on the HEXACO\npersonality model, which characterizes agents through psychological trait\ndimensions, and the Reid interrogation technique, a structured method for\neliciting concealed intentions. By combining progressive questionnaire probing\nwith behavior-based inter-agent monitoring, the framework enables the proactive\nidentification of malicious agents before harmful actions are carried out.\nExtensive experiments across six datasets against both our proposed attacks and\ntwo baseline threats demonstrate that AgentXposed effectively detects diverse\nforms of malicious behavior, achieving strong robustness across multiple\ncommunication settings.",
    "published": "2025-07-07T07:34:34Z",
    "updated": "2025-10-06T04:38:52Z",
    "link": "http://arxiv.org/pdf/2507.04724v2.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Yizhe Xie",
      "Congcong Zhu",
      "Xinyue Zhang",
      "Tianqing Zhu",
      "Dayong Ye",
      "Minghao Wang",
      "Chi Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18562v2",
    "title": "From Word to World: Evaluate and Mitigate Culture Bias in LLMs via Word\n  Association Test",
    "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through culturally shared semantic\nexpectations and implicit linguistic patterns shaped by lived experiences. We\nextend this test into an LLM-adaptive, free-relation task to assess the\nalignment of large language models (LLMs) with cross-cultural cognition. To\naddress culture preference, we propose CultureSteer, an innovative approach\nthat moves beyond superficial cultural prompting by embedding cultural-specific\nsemantic associations directly within the model's internal representation\nspace. Experiments show that current LLMs exhibit significant bias toward\nWestern (notably American) schemas at the word association level. In contrast,\nour model substantially improves cross-cultural alignment, capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies.",
    "published": "2025-05-24T07:05:10Z",
    "updated": "2025-10-06T04:31:03Z",
    "link": "http://arxiv.org/pdf/2505.18562v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Xunlian Dai",
      "Li Zhou",
      "Benyou Wang",
      "Haizhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04480v1",
    "title": "On Continuous Optimization for Constraint Satisfaction Problems",
    "summary": "Constraint satisfaction problems (CSPs) are fundamental in mathematics,\nphysics, and theoretical computer science. While conflict-driven clause\nlearning Boolean Satisfiability (SAT) solvers have achieved remarkable success\nand become the mainstream approach for Boolean satisfiability, recent advances\nshow that modern continuous local search (CLS) solvers can achieve highly\ncompetitive results on certain classes of SAT problems. Motivated by these\nadvances, we extend the CLS framework from Boolean SAT to general CSP with\nfinite-domain variables and expressive constraints. We present FourierCSP, a\ncontinuous optimization framework that generalizes the Walsh-Fourier transform\nto CSP, allowing for transforming versatile constraints to compact multilinear\npolynomials, thereby avoiding the need for auxiliary variables and\nmemory-intensive encodings. Our approach leverages efficient evaluation and\ndifferentiation of the objective via circuit-output probability and employs a\nprojected gradient optimization method with theoretical guarantees. Empirical\nresults on benchmark suites demonstrate that FourierCSP is scalable and\ncompetitive, significantly broadening the class of problems that can be\nefficiently solved by CLS techniques.",
    "published": "2025-10-06T04:30:07Z",
    "updated": "2025-10-06T04:30:07Z",
    "link": "http://arxiv.org/pdf/2510.04480v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yunuo Cen",
      "Zixuan Wang",
      "Jintao Zhang",
      "Zhiwei Zhang",
      "Xuanyao Fong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04477v1",
    "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical\n  Vision-Language Models",
    "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.",
    "published": "2025-10-06T04:26:39Z",
    "updated": "2025-10-06T04:26:39Z",
    "link": "http://arxiv.org/pdf/2510.04477v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Soo Yong Kim",
      "Suin Cho",
      "Vincent-Daniel Yun",
      "Gyeongyeon Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04476v1",
    "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
    "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
    "published": "2025-10-06T04:24:23Z",
    "updated": "2025-10-06T04:24:23Z",
    "link": "http://arxiv.org/pdf/2510.04476v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tomas Figliolia",
      "Nicholas Alonso",
      "Rishi Iyer",
      "Quentin Anthony",
      "Beren Millidge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04474v1",
    "title": "DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization",
    "summary": "Recent large reasoning models (LRMs) driven by reinforcement learning\nalgorithms (e.g., GRPO) have achieved remarkable performance on challenging\nreasoning tasks. However, these models suffer from overthinking, generating\nunnecessarily long and redundant reasoning even for simple questions, which\nsubstantially increases computational cost and response latency. While existing\nmethods incorporate length rewards to GRPO to promote concise reasoning, they\nincur significant performance degradation. We identify the root cause: when\nrewards for correct but long rollouts are penalized, GRPO's group-relative\nadvantage function can assign them negative advantages, actively discouraging\nvalid reasoning. To overcome this, we propose Decoupled Reward Policy\nOptimization (DRPO), a novel framework that decouples the length-based learning\nsignal of correct rollouts from incorrect ones. DRPO ensures that reward\nsignals for correct rollouts are normalized solely within the positive group,\nshielding them from interference by negative samples. The DRPO's objective is\ngrounded in integrating an optimized positive data distribution, which\nmaximizes length-based rewards under a KL regularization, into a discriminative\nobjective. We derive a closed-form solution for this distribution, enabling\nefficient computation of the objective and its gradients using only on-policy\ndata and importance weighting. Of independent interest, this formulation is\ngeneral and can incorporate other preference rewards of positive data beyond\nlength. Experiments on mathematical reasoning tasks demonstrate DRPO's\nsignificant superiority over six efficient reasoning baselines. Notably, with a\n1.5B model, our method achieves 77\\% length reduction with only 1.1\\%\nperformance loss on simple questions like GSM8k dataset, while the follow-up\nbaseline sacrifices 4.3\\% for 68\\% length reduction.",
    "published": "2025-10-06T04:18:13Z",
    "updated": "2025-10-06T04:18:13Z",
    "link": "http://arxiv.org/pdf/2510.04474v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Gang Li",
      "Yan Chen",
      "Ming Lin",
      "Tianbao Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24243v2",
    "title": "SafeFlowMatcher: Safe and Fast Planning using Flow Matching with Control\n  Barrier Functions",
    "summary": "Generative planners based on flow matching (FM) can produce high-quality\npaths in one or a few ODE steps, but their sampling dynamics offer no formal\nsafety guarantees and can yield incomplete paths near constraints. We present\nSafeFlowMatcher, a planning framework that couples FM with control barrier\nfunctions (CBFs) to achieve both real-time efficiency and certified safety.\nSafeFlowMatcher uses a two-phase prediction-correction (PC) integrator: (i) a\nprediction phase integrates the learned FM once (or a few steps) to obtain a\ncandidate path without intervention; (ii) a correction phase refines this path\nwith a vanishing time-scaled vector field and a CBF-based quadratic program\nthat minimally perturbs the vector field. We prove a barrier certificate for\nthe resulting flow system, establishing forward invariance of a robust safe set\nand finite-time convergence to the safe set. By enforcing safety only on the\nexecuted path (rather than on all intermediate latent paths), SafeFlowMatcher\navoids distributional drift and mitigates local trap problems. Across maze\nnavigation and locomotion benchmarks, SafeFlowMatcher attains faster, smoother,\nand safer paths than diffusion- and FM-based baselines. Extensive ablations\ncorroborate the contributions of the PC integrator and the barrier certificate.",
    "published": "2025-09-29T03:33:33Z",
    "updated": "2025-10-06T04:13:09Z",
    "link": "http://arxiv.org/pdf/2509.24243v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Jeongyong Yang",
      "Seunghwan Jang",
      "SooJean Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04472v1",
    "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object\n  Detection",
    "summary": "Camouflaged object detection segments objects with intrinsic similarity and\nedge disruption. Current detection methods rely on accumulated complex\ncomponents. Each approach adds components such as boundary modules, attention\nmechanisms, and multi-scale processors independently. This accumulation creates\na computational burden without proportional gains. To manage this complexity,\nthey process at reduced resolutions, eliminating fine details essential for\ncamouflage. We present SPEGNet, addressing fragmentation through a unified\ndesign. The architecture integrates multi-scale features via channel\ncalibration and spatial enhancement. Boundaries emerge directly from\ncontext-rich representations, maintaining semantic-spatial alignment.\nProgressive refinement implements scale-adaptive edge modulation with peak\ninfluence at intermediate resolutions. This design strikes a balance between\nboundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$\non CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.\nOur approach excels across scales, from tiny, intricate objects to large,\npattern-similar ones, while handling occlusion and ambiguous boundaries. Code,\nmodel weights, and results are available on\n\\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.",
    "published": "2025-10-06T04:06:40Z",
    "updated": "2025-10-06T04:06:40Z",
    "link": "http://arxiv.org/pdf/2510.04472v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "authors": [
      "Baber Jan",
      "Saeed Anwar",
      "Aiman H. El-Maleh",
      "Abdul Jabbar Siddiqui",
      "Abdul Bais"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02375v2",
    "title": "Pretraining with hierarchical memories: separating long-tail and common\n  knowledge",
    "summary": "The impressive performance gains of modern language models currently rely on\nscaling parameters: larger models store more world knowledge and reason better.\nYet compressing all world knowledge into parameters is unnecessary, as only a\nfraction is used per prompt, and impractical for edge devices with limited\ninference-time memory and compute. We address this shortcoming by a\nmemory-augmented architecture and a pretraining strategy aligned with existing\nhardware paradigms. We introduce small language models that access large\nhierarchical parametric memory banks encoding world knowledge. During\npretraining and inference, we fetch a small, context-dependent memory block and\nadd it to the model. Our pretraining learns to store long-tail world knowledge\nin the memory parameters, while the small language model acts as an anchor\ncapturing common knowledge and general reasoning abilities. Through\ntrillion-token-scale experiments, we show significant gains: a 160M-parameters\nmodel augmented with an 18M-parameters memory fetched from a 4.6B memory bank\nobtains comparable performance to a regular model with more than 2x the\nparameters. Through extensive experiments, we study the optimal type and size\nof parametric memories in transformers, scaling them to over 21B parameters. We\nfind that our proposed hierarchical feed-forward memories work robustly across\ntransformer architectures, whether added during pretraining or post-hoc.",
    "published": "2025-09-29T17:59:50Z",
    "updated": "2025-10-06T03:54:08Z",
    "link": "http://arxiv.org/pdf/2510.02375v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Hadi Pouransari",
      "David Grangier",
      "C Thomas",
      "Michael Kirchhof",
      "Oncel Tuzel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04465v1",
    "title": "Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM\n  Agents",
    "summary": "Large Language Model (LLM) agents require personal information for\npersonalization in order to better act on users' behalf in daily tasks, but\nthis raises privacy concerns and a personalization-privacy dilemma. Agent's\nautonomy introduces both risks and opportunities, yet its effects remain\nunclear. To better understand this, we conducted a 3$\\times$3 between-subjects\nexperiment ($N=450$) to study how agent's autonomy level and personalization\ninfluence users' privacy concerns, trust and willingness to use, as well as the\nunderlying psychological processes. We find that personalization without\nconsidering users' privacy preferences increases privacy concerns and decreases\ntrust and willingness to use. Autonomy moderates these effects: Intermediate\nautonomy flattens the impact of personalization compared to No- and Full\nautonomy conditions. Our results suggest that rather than aiming for perfect\nmodel alignment in output generation, balancing autonomy of agent's action and\nuser control offers a promising path to mitigate the personalization-privacy\ndilemma.",
    "published": "2025-10-06T03:38:54Z",
    "updated": "2025-10-06T03:38:54Z",
    "link": "http://arxiv.org/pdf/2510.04465v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Zhiping Zhang",
      "Yi Evie Zhang",
      "Freda Shi",
      "Tianshi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01708v2",
    "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via\n  Multi-Simulator Dynamics Randomization",
    "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.",
    "published": "2025-10-02T06:31:42Z",
    "updated": "2025-10-06T03:02:48Z",
    "link": "http://arxiv.org/pdf/2510.01708v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Zixing Lei",
      "Zibo Zhou",
      "Sheng Yin",
      "Yueru Chen",
      "Qingyao Xu",
      "Weixin Li",
      "Yunhong Wang",
      "Bowei Tang",
      "Wei Jing",
      "Siheng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04455v1",
    "title": "Inverse Mixed-Integer Programming: Learning Constraints then Objective\n  Functions",
    "summary": "In mixed-integer linear programming, data-driven inverse optimization that\nlearns the objective function and the constraints from observed data plays an\nimportant role in constructing appropriate mathematical models for various\nfields, including power systems and scheduling. However, to the best of our\nknowledge, there is no known method for learning both the objective functions\nand the constraints. In this paper, we propose a two-stage method for a class\nof problems where the objective function is expressed as a linear combination\nof functions and the constraints are represented by functions and thresholds.\nSpecifically, our method first learns the constraints and then learns the\nobjective function. On the theoretical side, we show the proposed method can\nsolve inverse optimization problems in finite dataset, develop statistical\nlearning theory in pseudometric spaces and sub-Gaussian distributions, and\nconstruct a statistical learning for inverse optimization. On the experimental\nside, we demonstrate that our method is practically applicable for scheduling\nproblems formulated as integer linear programmings with up to 100 decision\nvariables, which are typical in real-world settings.",
    "published": "2025-10-06T03:02:43Z",
    "updated": "2025-10-06T03:02:43Z",
    "link": "http://arxiv.org/pdf/2510.04455v1.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "authors": [
      "Akira Kitaoka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03448v2",
    "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering",
    "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach.",
    "published": "2025-08-05T13:49:04Z",
    "updated": "2025-10-06T02:57:21Z",
    "link": "http://arxiv.org/pdf/2508.03448v2.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "68T07, 94A12, 68U10",
      "I.2.10; H.5.5; J.5"
    ],
    "authors": [
      "Jan Melechovsky",
      "Ambuj Mehrish",
      "Abhinaba Roy",
      "Dorien Herremans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16048v5",
    "title": "OpenWHO: A Document-Level Parallel Corpus for Health Translation in\n  Low-Resource Languages",
    "summary": "In machine translation (MT), health is a high-stakes domain characterised by\nwidespread deployment and domain-specific vocabulary. However, there is a lack\nof MT evaluation datasets for low-resource languages in this domain. To address\nthis gap, we introduce OpenWHO, a document-level parallel corpus of 2,978\ndocuments and 26,824 sentences from the World Health Organization's e-learning\nplatform. Sourced from expert-authored, professionally translated materials\nshielded from web-crawling, OpenWHO spans a diverse range of over 20 languages,\nof which nine are low-resource. Leveraging this new resource, we evaluate\nmodern large language models (LLMs) against traditional MT models. Our findings\nreveal that LLMs consistently outperform traditional MT models, with Gemini 2.5\nFlash achieving a +4.79 ChrF point improvement over NLLB-54B on our\nlow-resource test set. Further, we investigate how LLM context utilisation\naffects accuracy, finding that the benefits of document-level translation are\nmost pronounced in specialised domains like health. We release the OpenWHO\ncorpus to encourage further research into low-resource MT in the health domain.",
    "published": "2025-08-22T02:53:56Z",
    "updated": "2025-10-06T02:43:38Z",
    "link": "http://arxiv.org/pdf/2508.16048v5.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Raphaël Merx",
      "Hanna Suominen",
      "Trevor Cohn",
      "Ekaterina Vylomova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09887v4",
    "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram",
    "summary": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing\nheart diseases. However, its effectiveness can be compromised by noise or\nunavailability of one or more leads of the standard 12-lead recordings,\nresulting in diagnostic errors or uncertainty. To address these challenges, we\npropose TolerantECG, a foundation model for ECG signals that is robust to noise\nand capable of functioning with arbitrary subsets of the standard 12-lead ECG.\nTolerantECG training combines contrastive and self-supervised learning\nframeworks to jointly learn ECG signal representations alongside their\ncorresponding knowledge-retrieval-based text report descriptions and corrupted\nor lead-missing signals. Comprehensive benchmarking results demonstrate that\nTolerantECG consistently ranks as the best or second-best performer across\nvarious ECG signal conditions and class levels in the PTB-XL dataset, and\nachieves the highest performance on the MIT-BIH Arrhythmia Database.",
    "published": "2025-07-14T03:48:35Z",
    "updated": "2025-10-06T02:30:57Z",
    "link": "http://arxiv.org/pdf/2507.09887v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Huynh Dang Nguyen",
      "Trong-Thang Pham",
      "Ngan Le",
      "Van Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.17517v4",
    "title": "The Hive Mind is a Single Reinforcement Learning Agent",
    "summary": "Decision-making is an essential attribute of any intelligent agent or group.\nNatural systems are known to converge to optimal strategies through at least\ntwo distinct mechanisms: collective decision-making via imitation of others,\nand individual trial-and-error. This paper establishes an equivalence between\nthese two paradigms by drawing from the well-established collective\ndecision-making model of nest-hunting in swarms of honey bees. We show that the\nemergent distributed cognition (sometimes referred to as the $\\textit{hive\nmind}$) arising from individual bees following simple, local imitation-based\nrules is that of a single online reinforcement learning (RL) agent interacting\nwith many parallel environments. The update rule through which this macro-agent\nlearns is a bandit algorithm that we coin $\\textit{Maynard-Cross Learning}$.\nOur analysis implies that a group of cognition-limited organisms can be\nequivalent to a more complex, reinforcement-enabled entity, substantiating the\nidea that group-level intelligence may explain how seemingly simple and blind\nindividual behaviors are selected in nature.\n  From a biological perspective, this analysis suggests how such imitation\nstrategies evolved: they constitute a scalable form of reinforcement learning\nat the group level, aligning with theories of kin and group selection. Beyond\nbiology, the framework offers new tools for analyzing economic and social\nsystems where individuals imitate successful strategies, effectively\nparticipating in a collective learning process. In swarm intelligence, our\nfindings will inform the design of scalable collective systems in artificial\ndomains, enabling RL-inspired mechanisms for coordination and adaptability at\nscale.",
    "published": "2024-10-23T02:49:37Z",
    "updated": "2025-10-06T02:24:11Z",
    "link": "http://arxiv.org/pdf/2410.17517v4.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.GT"
    ],
    "authors": [
      "Karthik Soma",
      "Yann Bouteiller",
      "Heiko Hamann",
      "Giovanni Beltrame"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19347v2",
    "title": "PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity\n  Evaluation",
    "summary": "Patent similarity evaluation plays a critical role in intellectual property\nanalysis. However, existing methods often overlook the intricate structure of\npatent documents, which integrate technical specifications, legal boundaries,\nand application contexts. We introduce PatentMind, a novel framework for patent\nsimilarity assessment based on a Multi-Aspect Reasoning Graph (MARG).\nPatentMind decomposes patents into their three dimensions of technical\nfeatures, application domains, and claim scopes, then dimension-specific\nsimilarity scores are calculated over the MARG. These scores are dynamically\nweighted through a context-aware reasoning process, which integrates contextual\nsignals to emulate expert-level judgment. To support evaluation, we construct a\nhuman-annotated benchmark PatentSimBench, comprising 500 patent pairs.\nExperimental results demonstrate that the PatentMind-generated scores show a\nstrong correlation ($r=0.938$) with expert annotations, significantly\noutperforming embedding-based models, patent-specific models, and advanced\nprompt engineering methods. Beyond computational linguistics, our framework\nprovides a structured and semantically grounded foundation for real-world\ndecision-making, particularly for tasks such as infringement risk assessment,\nunderscoring its broader impact on both patent analytics and evaluation.",
    "published": "2025-05-25T22:28:27Z",
    "updated": "2025-10-06T02:15:33Z",
    "link": "http://arxiv.org/pdf/2505.19347v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yongmin Yoo",
      "Qiongkai Xu",
      "Longbing Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02608v2",
    "title": "Mitigating Modal Imbalance in Multimodal Reasoning",
    "summary": "Foundation models (FMs) deployed in real-world tasks such as computer-use\nagents must integrate diverse modalities. How good are FMs at performing joint\nreasoning, simultaneously reasoning over multiple modalities, especially when\nthe modalities interact and relate to each other to form cross-modal context?\nTo better understand this problem, we study FMs on cross-modal conflicts:\nscenarios where conflicting evidence is presented across modalities. This\nallows us to examine whether FMs prioritize one modality over another or reason\njointly to reconcile the conflict. Our experiments reveal that FMs can\nrecognize conflicts in unimodal contexts, composed of a single modality, 90% of\nthe time, but the ratio falls as low as 3% when evidence is split across\nmodalities -- similar observations hold in cross-lingual contexts, composed of\nmultiple languages. We trace this failure to cross-modal attention imbalance,\nshowing that FMs exhibit extreme asymmetry in attention scores,\ndisproportionately prioritizing certain modalities. We show that cross-modal\nattention imbalance does not go away by simply scaling up multimodal or\nmultilingual datasets blindly, since they lack training examples that\nexplicitly require cross-modal reasoning. We demonstrate that even a simple and\nscalable method of explicitly combining multiple modalities within each\ntraining instance significantly reduces attention imbalance. Reduced attention\nimbalance directly translates to improved downstream performance on several\nvision-language benchmarks. Our findings underscore the importance of\nsystematically addressing cross-modal contexts to build reliable foundation\nmodels.",
    "published": "2025-10-02T22:58:28Z",
    "updated": "2025-10-06T02:10:36Z",
    "link": "http://arxiv.org/pdf/2510.02608v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chen Henry Wu",
      "Neil Kale",
      "Aditi Raghunathan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04853v2",
    "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving\n  Based on Expert Routing",
    "summary": "End-to-end autonomous driving remains constrained by the difficulty of\nproducing adaptive, robust, and interpretable decision-making across diverse\nscenarios. Existing methods often collapse diverse driving behaviors, lack\nlong-horizon consistency, or require task-specific engineering that limits\ngeneralization. This paper presents KDP, a knowledge-driven diffusion policy\nthat integrates generative diffusion modeling with a sparse mixture-of-experts\nrouting mechanism. The diffusion component generates temporally coherent action\nsequences, while the expert routing mechanism activates specialized and\nreusable experts according to context, enabling modular knowledge composition.\nExtensive experiments across representative driving scenarios demonstrate that\nKDP achieves consistently higher success rates, reduced collision risk, and\nsmoother control compared to prevailing paradigms. Ablation studies highlight\nthe effectiveness of sparse expert activation and the Transformer backbone, and\nactivation analyses reveal structured specialization and cross-scenario reuse\nof experts. These results establish diffusion with expert routing as a scalable\nand interpretable paradigm for knowledge-driven end-to-end autonomous driving.",
    "published": "2025-09-05T07:07:18Z",
    "updated": "2025-10-06T01:23:19Z",
    "link": "http://arxiv.org/pdf/2509.04853v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Chengkai Xu",
      "Jiaqi Liu",
      "Yicheng Guo",
      "Peng Hang",
      "Jian Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14469v3",
    "title": "Causality-Based Scores Alignment in Explainable Data Management",
    "summary": "Different attribution scores have been proposed to quantify the relevance of\ndatabase tuples for query answering in databases; e.g. Causal Responsibility,\nthe Shapley Value, the Banzhaf Power-Index, and the Causal Effect. They have\nbeen analyzed in isolation. This work is a first investigation of score\nalignment depending on the query and the database; i.e. on whether they induce\ncompatible rankings of tuples. We concentrate mostly on causality-based scores;\nand provide a syntactic dichotomy result for queries: on one side, pairs of\nscores are always aligned, on the other, they are not always aligned. It turns\nout that the presence of exogenous tuples makes a crucial difference in this\nregard.",
    "published": "2025-03-18T17:45:32Z",
    "updated": "2025-10-06T01:08:42Z",
    "link": "http://arxiv.org/pdf/2503.14469v3.pdf",
    "category": [
      "cs.DB",
      "cs.AI"
    ],
    "authors": [
      "Felipe Azua",
      "Leopoldo Bertossi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04417v1",
    "title": "Partial Information Decomposition via Normalizing Flows in Latent\n  Gaussian Distributions",
    "summary": "The study of multimodality has garnered significant interest in fields where\nthe analysis of interactions among multiple information sources can enhance\npredictive modeling, data fusion, and interpretability. Partial information\ndecomposition (PID) has emerged as a useful information-theoretic framework to\nquantify the degree to which individual modalities independently, redundantly,\nor synergistically convey information about a target variable. However,\nexisting PID methods depend on optimizing over a joint distribution constrained\nby estimated pairwise probability distributions, which are costly and\ninaccurate for continuous and high-dimensional modalities. Our first key\ninsight is that the problem can be solved efficiently when the pairwise\ndistributions are multivariate Gaussians, and we refer to this problem as\nGaussian PID (GPID). We propose a new gradient-based algorithm that\nsubstantially improves the computational efficiency of GPID based on an\nalternative formulation of the underlying optimization problem. To generalize\nthe applicability to non-Gaussian data, we learn information-preserving\nencoders to transform random variables of arbitrary input distributions into\npairwise Gaussian random variables. Along the way, we resolved an open problem\nregarding the optimality of joint Gaussian solutions for GPID. Empirical\nvalidation in diverse synthetic examples demonstrates that our proposed method\nprovides more accurate and efficient PID estimates than existing baselines. We\nfurther evaluate a series of large-scale multimodal benchmarks to show its\nutility in real-world applications of quantifying PID in multimodal datasets\nand selecting high-performing models.",
    "published": "2025-10-06T01:08:34Z",
    "updated": "2025-10-06T01:08:34Z",
    "link": "http://arxiv.org/pdf/2510.04417v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.IT",
      "math.IT"
    ],
    "authors": [
      "Wenyuan Zhao",
      "Adithya Balachandran",
      "Chao Tian",
      "Paul Pu Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.11708v2",
    "title": "Street Review: A Participatory AI-Based Framework for Assessing\n  Streetscape Inclusivity",
    "summary": "Urban centers undergo social, demographic, and cultural changes that shape\npublic street use and require systematic evaluation of public spaces. This\nstudy presents Street Review, a mixed-methods approach that combines\nparticipatory research with AI-based analysis to assess streetscape\ninclusivity. In Montr\\'eal, Canada, 28 residents participated in semi-directed\ninterviews and image evaluations, supported by the analysis of approximately\n45,000 street-view images from Mapillary. The approach produced visual\nanalytics, such as heatmaps, to correlate subjective user ratings with physical\nattributes like sidewalk, maintenance, greenery, and seating. Findings reveal\nvariations in perceptions of inclusivity and accessibility across demographic\ngroups, demonstrating that incorporating diverse user feedback can enhance\nmachine learning models through careful data-labeling and co-production\nstrategies. The Street Review framework offers a systematic method for urban\nplanners and policy analysts to inform planning, policy development, and\nmanagement of public streets.",
    "published": "2025-08-14T02:40:56Z",
    "updated": "2025-10-06T00:43:58Z",
    "link": "http://arxiv.org/pdf/2508.11708v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Rashid Mushkani",
      "Shin Koseki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04401v1",
    "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures\n  of VLMs in Compositional Counting",
    "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research.",
    "published": "2025-10-06T00:11:24Z",
    "updated": "2025-10-06T00:11:24Z",
    "link": "http://arxiv.org/pdf/2510.04401v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xuyang Guo",
      "Zekai Huang",
      "Zhenmei Shi",
      "Zhao Song",
      "Jiahao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04400v1",
    "title": "Large Language Models Preserve Semantic Isotopies in Story Continuations",
    "summary": "In this work, we explore the relevance of textual semantics to Large Language\nModels (LLMs), extending previous insights into the connection between\ndistributional semantics and structural semantics. We investigate whether\nLLM-generated texts preserve semantic isotopies. We design a story continuation\nexperiment using 10,000 ROCStories prompts completed by five LLMs. We first\nvalidate GPT-4o's ability to extract isotopies from a linguistic benchmark,\nthen apply it to the generated stories. We then analyze structural (coverage,\ndensity, spread) and semantic properties of isotopies to assess how they are\naffected by completion. Results show that LLM completion within a given token\nhorizon preserves semantic isotopies across multiple properties.",
    "published": "2025-10-06T00:03:12Z",
    "updated": "2025-10-06T00:03:12Z",
    "link": "http://arxiv.org/pdf/2510.04400v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Marc Cavazza"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.13968v2",
    "title": "Optimizing Agricultural Order Fulfillment Systems: A Hybrid Tree Search\n  Approach",
    "summary": "Efficient order fulfillment is vital in the agricultural industry,\nparticularly due to the seasonal nature of seed supply chains. This paper\naddresses the challenge of optimizing seed orders fulfillment in a centralized\nwarehouse where orders are processed in waves, taking into account the\nunpredictable arrival of seed stocks and strict order deadlines. We model the\nwave scheduling problem as a Markov decision process and propose an adaptive\nhybrid tree search algorithm that combines Monte Carlo tree search with\ndomain-specific knowledge to efficiently navigate the complex, dynamic\nenvironment of seed distribution. By leveraging historical data and stochastic\nmodeling, our method enables forecast-informed scheduling decisions that\nbalance immediate requirements with long-term operational efficiency. The key\nidea is that we can augment Monte Carlo tree search algorithm with\nproblem-specific side information that dynamically reduces the number of\ncandidate actions at each decision step to handle the large state and action\nspaces that render traditional solution methods computationally intractable.\nExtensive simulations with realistic parameters, including a diverse range of\nproducts, a high volume of orders, and authentic seasonal durations,\ndemonstrate that the proposed approach significantly outperforms existing\nindustry standard methods.",
    "published": "2024-07-19T01:25:39Z",
    "updated": "2025-10-05T23:53:21Z",
    "link": "http://arxiv.org/pdf/2407.13968v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Pranay Thangeda",
      "Hoda Helmi",
      "Melkior Ornik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04399v1",
    "title": "Utility-Learning Tension in Self-Modifying Agents",
    "summary": "As systems trend toward superintelligence, a natural modeling premise is that\nagents can self-improve along every facet of their own design. We formalize\nthis with a five-axis decomposition and a decision layer, separating incentives\nfrom learning behavior and analyzing axes in isolation. Our central result\nidentifies and introduces a sharp utility--learning tension, the structural\nconflict in self-modifying systems whereby utility-driven changes that improve\nimmediate or expected performance can also erode the statistical preconditions\nfor reliable learning and generalization. Our findings show that\ndistribution-free guarantees are preserved iff the policy-reachable model\nfamily is uniformly capacity-bounded; when capacity can grow without limit,\nutility-rational self-changes can render learnable tasks unlearnable. Under\nstandard assumptions common in practice, these axes reduce to the same capacity\ncriterion, yielding a single boundary for safe self-modification. Numerical\nexperiments across several axes validate the theory by comparing destructive\nutility policies against our proposed two-gate policies that preserve\nlearnability.",
    "published": "2025-10-05T23:52:16Z",
    "updated": "2025-10-05T23:52:16Z",
    "link": "http://arxiv.org/pdf/2510.04399v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Charles L. Wang",
      "Keir Dorchen",
      "Peter Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04398v1",
    "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM\n  Hallucinations",
    "summary": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA.",
    "published": "2025-10-05T23:44:54Z",
    "updated": "2025-10-05T23:44:54Z",
    "link": "http://arxiv.org/pdf/2510.04398v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Buyun Liang",
      "Liangzu Peng",
      "Jinqi Luo",
      "Darshan Thaker",
      "Kwan Ho Ryan Chan",
      "René Vidal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04397v1",
    "title": "MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific\n  Knowledge for Multilingual Vulnerability Detection",
    "summary": "Software vulnerabilities (SVs) pose a critical threat to safety-critical\nsystems, driving the adoption of AI-based approaches such as machine learning\nand deep learning for software vulnerability detection. Despite promising\nresults, most existing methods are limited to a single programming language.\nThis is problematic given the multilingual nature of modern software, which is\noften complex and written in multiple languages. Current approaches often face\nchallenges in capturing both shared and language-specific knowledge of source\ncode, which can limit their performance on diverse programming languages and\nreal-world codebases. To address this gap, we propose MULVULN, a novel\nmultilingual vulnerability detection approach that learns from source code\nacross multiple languages. MULVULN captures both the shared knowledge that\ngeneralizes across languages and the language-specific knowledge that reflects\nunique coding conventions. By integrating these aspects, it achieves more\nrobust and effective detection of vulnerabilities in real-world multilingual\nsoftware systems. The rigorous and extensive experiments on the real-world and\ndiverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven\nprogramming languages, demonstrate the superiority of MULVULN over thirteen\neffective and state-of-the-art baselines. Notably, MULVULN achieves\nsubstantially higher F1-score, with improvements ranging from 1.45% to 23.59%\ncompared to the baseline methods.",
    "published": "2025-10-05T23:33:26Z",
    "updated": "2025-10-05T23:33:26Z",
    "link": "http://arxiv.org/pdf/2510.04397v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Van Nguyen",
      "Surya Nepal",
      "Xingliang Yuan",
      "Tingmin Wu",
      "Fengchao Chen",
      "Carsten Rudolph"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04392v1",
    "title": "Improving Consistency in Retrieval-Augmented Systems with Group\n  Similarity Rewards",
    "summary": "RAG systems are increasingly deployed in high-stakes domains where users\nexpect outputs to be consistent across semantically equivalent queries.\nHowever, existing systems often exhibit significant inconsistencies due to\nvariability in both the retriever and generator (LLM), undermining trust and\nreliability. In this work, we focus on information consistency, i.e., the\nrequirement that outputs convey the same core content across semantically\nequivalent inputs. We introduce a principled evaluation framework that\ndecomposes RAG consistency into retriever-level, generator-level, and\nend-to-end components, helping identify inconsistency sources. To improve\nconsistency, we propose Paraphrased Set Group Relative Policy Optimization\n(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased\nset to assign group similarity rewards. We leverage PS-GRPO to achieve\nInformation Consistent RAG (Con-RAG), training the generator to produce\nconsistent outputs across paraphrased queries and remain robust to\nretrieval-induced variability. Because exact reward computation over paraphrase\nsets is computationally expensive, we also introduce a scalable approximation\nmethod that retains effectiveness while enabling efficient, large-scale\ntraining. Empirical evaluations across short-form, multi-hop, and long-form QA\nbenchmarks demonstrate that Con-RAG significantly improves both consistency and\naccuracy over strong baselines, even in the absence of explicit ground-truth\nsupervision. Our work provides practical solutions for evaluating and building\nreliable RAG systems for safety-critical deployments.",
    "published": "2025-10-05T23:14:13Z",
    "updated": "2025-10-05T23:14:13Z",
    "link": "http://arxiv.org/pdf/2510.04392v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Faisal Hamman",
      "Chenyang Zhu",
      "Anoop Kumar",
      "Xujun Peng",
      "Sanghamitra Dutta",
      "Daben Liu",
      "Alfy Samuel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04391v1",
    "title": "Internal World Models as Imagination Networks in Cognitive Agents",
    "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.",
    "published": "2025-10-05T23:01:10Z",
    "updated": "2025-10-05T23:01:10Z",
    "link": "http://arxiv.org/pdf/2510.04391v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.SI",
      "q-bio.NC"
    ],
    "authors": [
      "Saurabh Ranjan",
      "Brian Odegaard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04390v1",
    "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D\n  World Simulator",
    "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.",
    "published": "2025-10-05T22:55:17Z",
    "updated": "2025-10-05T22:55:17Z",
    "link": "http://arxiv.org/pdf/2510.04390v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Xuehai He",
      "Shijie Zhou",
      "Thivyanth Venkateswaran",
      "Kaizhi Zheng",
      "Ziyu Wan",
      "Achuta Kadambi",
      "Xin Eric Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04384v1",
    "title": "LLM Based Bayesian Optimization for Prompt Search",
    "summary": "Bayesian Optimization (BO) has been widely used to efficiently optimize\nexpensive black-box functions with limited evaluations. In this paper, we\ninvestigate the use of BO for prompt engineering to enhance text classification\nwith Large Language Models (LLMs). We employ an LLM-powered Gaussian Process\n(GP) as the surrogate model to estimate the performance of different prompt\ncandidates. These candidates are generated by an LLM through the expansion of a\nset of seed prompts and are subsequently evaluated using an Upper Confidence\nBound (UCB) acquisition function in conjunction with the GP posterior. The\noptimization process iteratively refines the prompts based on a subset of the\ndata, aiming to improve classification accuracy while reducing the number of\nAPI calls by leveraging the prediction uncertainty of the LLM-based GP. The\nproposed BO-LLM algorithm is evaluated on two datasets, and its advantages are\ndiscussed in detail in this paper.",
    "published": "2025-10-05T22:32:50Z",
    "updated": "2025-10-05T22:32:50Z",
    "link": "http://arxiv.org/pdf/2510.04384v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Adam Ballew",
      "Jingbo Wang",
      "Shaogang Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08729v2",
    "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates",
    "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
    "published": "2025-09-10T16:17:44Z",
    "updated": "2025-10-05T22:27:29Z",
    "link": "http://arxiv.org/pdf/2509.08729v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Hyunjun Kim",
      "Junwoo Ha",
      "Sangyoon Yu",
      "Haon Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04380v1",
    "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in\n  AI-Native Software Development",
    "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development.",
    "published": "2025-10-05T21:58:44Z",
    "updated": "2025-10-05T21:58:44Z",
    "link": "http://arxiv.org/pdf/2510.04380v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "D.2.1; D.2.2; D.2.9; I.2.7"
    ],
    "authors": [
      "Mateen Ahmed Abbasi",
      "Petri Ihantola",
      "Tommi Mikkonen",
      "Niko Mäkitalo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04375v1",
    "title": "Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains",
    "summary": "The effectiveness of single-model sequential recommendation architectures,\nwhile scalable, is often limited when catering to \"power users\" in sparse or\nniche domains. Our previous research, PinnerFormerLite, addressed this by using\na fixed weighted loss to prioritize specific domains. However, this approach\ncan be sub-optimal, as a single, uniform weight may not be sufficient for\ndomains with very few interactions, where the training signal is easily diluted\nby the vast, generic dataset.\n  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss\nfunction with comprehensive theoretical foundations and extensive empirical\nvalidation. We introduce an adaptive algorithm that adjusts the loss weight for\neach domain based on its sparsity in the training data, assigning a higher\nweight to sparser domains and a lower weight to denser ones. This ensures that\neven rare user interests contribute a meaningful gradient signal, preventing\nthem from being overshadowed.\n  We provide rigorous theoretical analysis including convergence proofs,\ncomplexity analysis, and bounds analysis to establish the stability and\nefficiency of our approach. Our comprehensive empirical validation across four\ndiverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)\nwith state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that\nthis dynamic weighting system significantly outperforms all comparison methods,\nparticularly for sparse domains, achieving substantial lifts in key metrics\nlike Recall at 10 and NDCG at 10 while maintaining performance on denser\ndomains and introducing minimal computational overhead.",
    "published": "2025-10-05T21:42:33Z",
    "updated": "2025-10-05T21:42:33Z",
    "link": "http://arxiv.org/pdf/2510.04375v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Akshay Mittal",
      "Vinay Venkatesh",
      "Krishna Kandi",
      "Shalini Sudarshan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.14275v2",
    "title": "FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated\n  LLMs in Mental Health",
    "summary": "Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive\ndomains (e.g., mental health) requires balancing strict confidentiality with\nmodel utility and safety. We propose FedMentor, a federated fine-tuning\nframework that integrates Low-Rank Adaptation (LoRA) and domain-aware\nDifferential Privacy (DP) to meet per-domain privacy budgets while maintaining\nperformance. Each client (domain) applies a custom DP noise scale proportional\nto its data sensitivity, and the server adaptively reduces noise when utility\nfalls below a threshold. In experiments on three mental health datasets, we\nshow that FedMentor improves safety over standard Federated Learning (FL)\nwithout privacy, raising safe output rates by up to three points and lowering\ntoxicity, while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of\nthe non-private baseline and close to the centralized upper bound. The\nframework scales to backbones with up to 1.7B parameters on single-GPU clients,\nrequiring < 173 MB of communication per-round. FedMentor demonstrates a\npractical approach to privately fine-tune LLMs for safer deployments in\nhealthcare and other sensitive fields.",
    "published": "2025-09-16T07:08:36Z",
    "updated": "2025-10-05T21:41:04Z",
    "link": "http://arxiv.org/pdf/2509.14275v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Nobin Sarwar",
      "Shubhashis Roy Dipta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04374v1",
    "title": "GDPval: Evaluating AI Model Performance on Real-World Economically\n  Valuable Tasks",
    "summary": "We introduce GDPval, a benchmark evaluating AI model capabilities on\nreal-world economically valuable tasks. GDPval covers the majority of U.S.\nBureau of Labor Statistics Work Activities for 44 occupations across the top 9\nsectors contributing to U.S. GDP (Gross Domestic Product). Tasks are\nconstructed from the representative work of industry professionals with an\naverage of 14 years of experience. We find that frontier model performance on\nGDPval is improving roughly linearly over time, and that the current best\nfrontier models are approaching industry experts in deliverable quality. We\nanalyze the potential for frontier models, when paired with human oversight, to\nperform GDPval tasks cheaper and faster than unaided experts. We also\ndemonstrate that increased reasoning effort, increased task context, and\nincreased scaffolding improves model performance on GDPval. Finally, we\nopen-source a gold subset of 220 tasks and provide a public automated grading\nservice at evals.openai.com to facilitate future research in understanding\nreal-world model capabilities.",
    "published": "2025-10-05T21:36:43Z",
    "updated": "2025-10-05T21:36:43Z",
    "link": "http://arxiv.org/pdf/2510.04374v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Tejal Patwardhan",
      "Rachel Dias",
      "Elizabeth Proehl",
      "Grace Kim",
      "Michele Wang",
      "Olivia Watkins",
      "Simón Posada Fishman",
      "Marwan Aljubeh",
      "Phoebe Thacker",
      "Laurance Fauconnet",
      "Natalie S. Kim",
      "Patrick Chao",
      "Samuel Miserendino",
      "Gildas Chabot",
      "David Li",
      "Michael Sharman",
      "Alexandra Barr",
      "Amelia Glaese",
      "Jerry Tworek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04373v1",
    "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to\n  Improve LLM Agents Adaptation",
    "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints.",
    "published": "2025-10-05T21:34:42Z",
    "updated": "2025-10-05T21:34:42Z",
    "link": "http://arxiv.org/pdf/2510.04373v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hadi Nekoei",
      "Aman Jaiswal",
      "Patrice Bechard",
      "Oleh Shliazhko",
      "Orlando Marquez Ayala",
      "Mathieu Reymond",
      "Massimo Caccia",
      "Alexandre Drouin",
      "Sarath Chandar",
      "Alexandre Lacoste"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04371v1",
    "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems",
    "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world.",
    "published": "2025-10-05T21:28:11Z",
    "updated": "2025-10-05T21:28:11Z",
    "link": "http://arxiv.org/pdf/2510.04371v1.pdf",
    "category": [
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "authors": [
      "Naimeng Ye",
      "Arnav Ahuja",
      "Georgios Liargkovas",
      "Yunan Lu",
      "Kostis Kaffes",
      "Tianyi Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10320v2",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, making\npowerful LLM-as-a-Judge models a core solution. The efficacy of these judges\ndepends on their chain-of-thought reasoning, creating a critical need for\nmethods that can effectively optimize this reasoning process. In this work, we\nintroduce J1, a reinforcement learning framework for teaching LLM judges to\nthink before making decisions. Our core contribution lies in converting all\njudgment tasks for non-verifiable and verifiable prompts into a unified format\nwith verifiable rewards, enabling direct optimization of evaluation quality\nwhile mitigating positional bias. We then use RL to train thinking-judges at\nscales of 8B, 32B, and 70B and show that they obtain state-of-the-art\nperformance across multiple benchmarks. In particular, J1-Qwen-32B, our\nmultitasked pointwise and pairwise judge also outperforms o1-mini, o3, and a\nmuch larger 671B DeepSeek-R1 on some benchmarks, while only training on\nsynthetic data. Through comprehensive ablations of pairwise, pointwise, and\nmultitask J1 variants, we demonstrate the effectiveness of our approach across\nseed prompts, reward strategies, and training recipes. Qualitative analysis\nreveals that J1 develops systematic evaluation strategies, including dynamic\ncriteria generation, reference answer creation, iterative self-correction of\ninitial assessments, and feedback generation for low-quality responses.",
    "published": "2025-05-15T14:05:15Z",
    "updated": "2025-10-05T21:28:03Z",
    "link": "http://arxiv.org/pdf/2505.10320v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chenxi Whitehouse",
      "Tianlu Wang",
      "Ping Yu",
      "Xian Li",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12508v3",
    "title": "Fun-ASR Technical Report",
    "summary": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent Fun-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, Fun-ASR is specifically optimized for\npractical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, Fun-ASR achieves state-of-the-art\nperformance on real application datasets, demonstrating its effectiveness and\nrobustness in practical settings.",
    "published": "2025-09-15T23:19:36Z",
    "updated": "2025-10-05T21:27:32Z",
    "link": "http://arxiv.org/pdf/2509.12508v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Keyu An",
      "Yanni Chen",
      "Chong Deng",
      "Changfeng Gao",
      "Zhifu Gao",
      "Bo Gong",
      "Xiangang Li",
      "Yabin Li",
      "Xiang Lv",
      "Yunjie Ji",
      "Yiheng Jiang",
      "Bin Ma",
      "Haoneng Luo",
      "Chongjia Ni",
      "Zexu Pan",
      "Yiping Peng",
      "Zhendong Peng",
      "Peiyao Wang",
      "Hao Wang",
      "Wen Wang",
      "Wupeng Wang",
      "Biao Tian",
      "Zhentao Tan",
      "Nan Yang",
      "Bin Yuan",
      "Jieping Ye",
      "Jixing Yu",
      "Qinglin Zhang",
      "Kun Zou",
      "Han Zhao",
      "Shengkui Zhao",
      "Jingren Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04368v1",
    "title": "NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social\n  Simulation Environment",
    "summary": "We design and implement NegotiationGym, an API and user interface for\nconfiguring and running multi-agent social simulations focused upon negotiation\nand cooperation. The NegotiationGym codebase offers a user-friendly,\nconfiguration-driven API that enables easy design and customization of\nsimulation scenarios. Agent-level utility functions encode optimization\ncriteria for each agent, and agents can self-optimize by conducting multiple\ninteraction rounds with other agents, observing outcomes, and modifying their\nstrategies for future rounds.",
    "published": "2025-10-05T21:23:21Z",
    "updated": "2025-10-05T21:23:21Z",
    "link": "http://arxiv.org/pdf/2510.04368v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Shashank Mangla",
      "Chris Hokamp",
      "Jack Boylan",
      "Demian Gholipour Ghalandari",
      "Yuuv Jauhari",
      "Lauren Cassidy",
      "Oisin Duffy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04363v1",
    "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large\n  Language Models",
    "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.",
    "published": "2025-10-05T21:15:11Z",
    "updated": "2025-10-05T21:15:11Z",
    "link": "http://arxiv.org/pdf/2510.04363v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hyunjun Kim",
      "Sejong Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04354v1",
    "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators",
    "summary": "Rapid progress in imitation learning, foundation models, and large-scale\ndatasets has led to robot manipulation policies that generalize to a wide-range\nof tasks and environments. However, rigorous evaluation of these policies\nremains a challenge. Typically in practice, robot policies are often evaluated\non a small number of hardware trials without any statistical assurances. We\npresent SureSim, a framework to augment large-scale simulation with relatively\nsmall-scale real-world testing to provide reliable inferences on the real-world\nperformance of a policy. Our key idea is to formalize the problem of combining\nreal and simulation evaluations as a prediction-powered inference problem, in\nwhich a small number of paired real and simulation evaluations are used to\nrectify bias in large-scale simulation. We then leverage non-asymptotic mean\nestimation algorithms to provide confidence intervals on mean policy\nperformance. Using physics-based simulation, we evaluate both diffusion policy\nand multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and\ninitial conditions, and find that our approach saves over \\(20-25\\%\\) of\nhardware evaluation effort to achieve similar bounds on policy performance.",
    "published": "2025-10-05T20:37:53Z",
    "updated": "2025-10-05T20:37:53Z",
    "link": "http://arxiv.org/pdf/2510.04354v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Apurva Badithela",
      "David Snyder",
      "Lihan Zha",
      "Joseph Mikhail",
      "Matthew O'Kelly",
      "Anushri Dixit",
      "Anirudha Majumdar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04349v1",
    "title": "Challenge on Optimization of Context Collection for Code Completion",
    "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop.",
    "published": "2025-10-05T20:18:34Z",
    "updated": "2025-10-05T20:18:34Z",
    "link": "http://arxiv.org/pdf/2510.04349v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Dmitry Ustalov",
      "Egor Bogomolov",
      "Alexander Bezzubov",
      "Yaroslav Golubev",
      "Evgeniy Glukhov",
      "Georgii Levtsov",
      "Vladimir Kovalenko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21387v3",
    "title": "Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of\n  Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence",
    "summary": "Prior works have shown that neural networks can be heavily pruned while\npreserving performance, but the impact of pruning on model interpretability\nremains unclear. In this work, we investigate how magnitude-based pruning\nfollowed by fine-tuning affects both low-level saliency maps and high-level\nconcept representations. Using a ResNet-18 trained on ImageNette, we compare\npost-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)\nacross pruning levels, evaluating sparsity and faithfulness. We further apply\nCRAFT-based concept extraction to track changes in semantic coherence of\nlearned concepts. Our results show that light-to-moderate pruning improves\nsaliency-map focus and faithfulness while retaining distinct, semantically\nmeaningful concepts. In contrast, aggressive pruning merges heterogeneous\nfeatures, reducing saliency map sparsity and concept coherence despite\nmaintaining accuracy. These findings suggest that while pruning can shape\ninternal representations toward more human-aligned attention patterns,\nexcessive pruning undermines interpretability.",
    "published": "2025-09-23T20:10:23Z",
    "updated": "2025-10-05T20:06:23Z",
    "link": "http://arxiv.org/pdf/2509.21387v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sanish Suwal",
      "Dipkamal Bhusal",
      "Michael Clifford",
      "Nidhi Rastogi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04341v1",
    "title": "Critical appraisal of artificial intelligence for rare-event\n  recognition: principles and pharmacovigilance case studies",
    "summary": "Many high-stakes AI applications target low-prevalence events, where apparent\naccuracy can conceal limited real-world value. Relevant AI models range from\nexpert-defined rules and traditional machine learning to generative LLMs\nconstrained for classification. We outline key considerations for critical\nappraisal of AI in rare-event recognition, including problem framing and test\nset design, prevalence-aware statistical evaluation, robustness assessment, and\nintegration into human workflows. In addition, we propose an approach to\nstructured case-level examination (SCLE), to complement statistical performance\nevaluation, and a comprehensive checklist to guide procurement or development\nof AI models for rare-event recognition. We instantiate the framework in\npharmacovigilance, drawing on three studies: rule-based retrieval of\npregnancy-related reports; duplicate detection combining machine learning with\nprobabilistic record linkage; and automated redaction of person names using an\nLLM. We highlight pitfalls specific to the rare-event setting including\noptimism from unrealistic class balance and lack of difficult positive controls\nin test sets - and show how cost-sensitive targets align model performance with\noperational value. While grounded in pharmacovigilance practice, the principles\ngeneralize to domains where positives are scarce and error costs may be\nasymmetric.",
    "published": "2025-10-05T20:05:38Z",
    "updated": "2025-10-05T20:05:38Z",
    "link": "http://arxiv.org/pdf/2510.04341v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "I.2.0"
    ],
    "authors": [
      "G. Niklas Noren",
      "Eva-Lisa Meldau",
      "Johan Ellenius"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04340v1",
    "title": "Inoculation Prompting: Eliciting traits from LLMs during training can\n  suppress them at test-time",
    "summary": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.",
    "published": "2025-10-05T20:04:22Z",
    "updated": "2025-10-05T20:04:22Z",
    "link": "http://arxiv.org/pdf/2510.04340v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Daniel Tan",
      "Anders Woodruff",
      "Niels Warncke",
      "Arun Jose",
      "Maxime Riché",
      "David Demitri Africa",
      "Mia Taylor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04339v1",
    "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre\n  Latent Space",
    "summary": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com",
    "published": "2025-10-05T20:03:30Z",
    "updated": "2025-10-05T20:03:30Z",
    "link": "http://arxiv.org/pdf/2510.04339v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "authors": [
      "Christian Limberg",
      "Fares Schulz",
      "Zhe Zhang",
      "Stefan Weinzierl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.05606v2",
    "title": "Exploring Applications of State Space Models and Advanced Training\n  Techniques in Sequential Recommendations: A Comparative Study on Efficiency\n  and Performance",
    "summary": "Recommender systems aim to estimate the dynamically changing user preferences\nand sequential dependencies between historical user behaviour and metadata.\nAlthough transformer-based models have proven to be effective in sequential\nrecommendations, their state growth is proportional to the length of the\nsequence that is being processed, which makes them expensive in terms of memory\nand inference costs. Our research focused on three promising directions in\nsequential recommendations: enhancing speed through the use of State Space\nModels (SSM), as they can achieve SOTA results in the sequential\nrecommendations domain with lower latency, memory, and inference costs, as\nproposed by arXiv:2403.03900 improving the quality of recommendations with\nLarge Language Models (LLMs) via Monolithic Preference Optimization without\nReference Model (ORPO); and implementing adaptive batch- and step-size\nalgorithms to reduce costs and accelerate training processes.",
    "published": "2024-08-10T18:09:10Z",
    "updated": "2025-10-05T19:07:28Z",
    "link": "http://arxiv.org/pdf/2408.05606v2.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Mark Obozov",
      "Makar Baderko",
      "Stepan Kulibaba",
      "Nikolay Kutuzov",
      "Alexander Gasnikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06916v2",
    "title": "QuIC: Quantum-Inspired Compound Adapters for Parameter Efficient\n  Fine-Tuning",
    "summary": "Scaling full finetuning of large foundation models strains GPU memory and\ntraining time. Parameter Efficient Fine-Tuning (PEFT) methods address this\nissue via adapter modules which update only a small subset of model parameters.\nIn this work, we introduce Quantum-Inspired Compound Adapters (QuIC Adapters),\na PEFT approach inspired from Hamming-weight preserving quantum circuits that\ncan effectively finetune a model using less than 0.02\\% memory footprint of the\nbase model. QuIC adapters preserve pretrained representations by enforcing\northogonality in weight parameters, and have native deployment mechanisms on\nquantum computers. We test QuIC adapters by finetuning large language models\nlike LLaMA and vision transformers on language, math, reasoning and vision\nbenchmarks. In its first-order configuration, QuIC recovers the performance of\nexisting orthogonal methods, while higher-order configurations enable\nsubstantial parameter compression (over 40x smaller than LoRA) for a modest\nperformance trade-off, unlocking applications in highly resource-constrained\nenvironments. Through ablation studies, we determine that combining multiple\nHamming-weight orders with orthogonality and matrix compounding are essential\nfor performant finetuning. Our findings suggest that QuIC adapters offers a\npromising direction for efficient finetuning of foundation models in\nresource-constrained environments.",
    "published": "2025-02-10T13:06:56Z",
    "updated": "2025-10-05T18:54:57Z",
    "link": "http://arxiv.org/pdf/2502.06916v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "quant-ph"
    ],
    "authors": [
      "Snehal Raj",
      "Brian Coyle"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04317v1",
    "title": "FairAgent: Democratizing Fairness-Aware Machine Learning with\n  LLM-Powered Agents",
    "summary": "Training fair and unbiased machine learning models is crucial for high-stakes\napplications, yet it presents significant challenges. Effective bias mitigation\nrequires deep expertise in fairness definitions, metrics, data preprocessing,\nand machine learning techniques. In addition, the complex process of balancing\nmodel performance with fairness requirements while properly handling sensitive\nattributes makes fairness-aware model development inaccessible to many\npractitioners. To address these challenges, we introduce FairAgent, an\nLLM-powered automated system that significantly simplifies fairness-aware model\ndevelopment. FairAgent eliminates the need for deep technical expertise by\nautomatically analyzing datasets for potential biases, handling data\npreprocessing and feature engineering, and implementing appropriate bias\nmitigation strategies based on user requirements. Our experiments demonstrate\nthat FairAgent achieves significant performance improvements while\nsignificantly reducing development time and expertise requirements, making\nfairness-aware machine learning more accessible to practitioners.",
    "published": "2025-10-05T18:33:52Z",
    "updated": "2025-10-05T18:33:52Z",
    "link": "http://arxiv.org/pdf/2510.04317v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yucong Dai",
      "Lu Zhang",
      "Feng Luo",
      "Mashrur Chowdhury",
      "Yongkai Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16496v2",
    "title": "On Zero-Shot Reinforcement Learning",
    "summary": "Modern reinforcement learning (RL) systems capture deep truths about general,\nhuman problem-solving. In domains where new data can be simulated cheaply,\nthese systems uncover sequential decision-making policies that far exceed the\nability of any human. Society faces many problems whose solutions require this\nskill, but they are often in domains where new data cannot be cheaply\nsimulated. In such scenarios, we can learn simulators from existing data, but\nthese will only ever be approximately correct, and can be pathologically\nincorrect when queried outside of their training distribution. As a result, a\nmisalignment between the environments in which we train our agents and the\nreal-world in which we wish to deploy our agents is inevitable. Dealing with\nthis misalignment is the primary concern of zero-shot reinforcement learning, a\nproblem setting where the agent must generalise to a new task or domain with\nzero practice shots. Whilst impressive progress has been made on methods that\nperform zero-shot RL in idealised settings, new work is needed if these results\nare to be replicated in real-world settings. In this thesis, we argue that\ndoing so requires us to navigate (at least) three constraints. First, the data\nquality constraint: real-world datasets are small and homogeneous. Second, the\nobservability constraint: states, dynamics and rewards in the real-world are\noften only partially observed. And third, the data availability constraint: a\npriori access to data cannot always be assumed. This work proposes a suite of\nmethods that perform zero-shot RL subject to these constraints. In a series of\nempirical studies we expose the failings of existing methods, and justify our\ntechniques for remedying them. We believe these designs take us a step closer\nto RL methods that can be deployed to solve real-world problems.",
    "published": "2025-08-22T16:20:49Z",
    "updated": "2025-10-05T18:29:40Z",
    "link": "http://arxiv.org/pdf/2508.16496v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Scott Jeen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.12153v3",
    "title": "On amortizing convex conjugates for optimal transport",
    "summary": "This paper focuses on computing the convex conjugate (also known as the\nLegendre-Fenchel conjugate or c-transform) that appears in Euclidean\nWasserstein-2 optimal transport. This conjugation is considered difficult to\ncompute and in practice, methods are limited by not being able to exactly\nconjugate the dual potentials in continuous space. To overcome this, the\ncomputation of the conjugate can be approximated with amortized optimization,\nwhich learns a model to predict the conjugate. I show that combining amortized\napproximations to the conjugate with a solver for fine-tuning significantly\nimproves the quality of transport maps learned for the Wasserstein-2 benchmark\nby Korotin et al. (2021a) and is able to model many 2-dimensional couplings and\nflows considered in the literature. All baselines, methods, and solvers are\npublicly available at http://github.com/facebookresearch/w2ot.",
    "published": "2022-10-21T17:59:24Z",
    "updated": "2025-10-05T18:29:04Z",
    "link": "http://arxiv.org/pdf/2210.12153v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Brandon Amos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.09614v3",
    "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help\n  Them Think Like Scientists?",
    "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers which need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established Blicket Test paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not child-like). Finally, we propose\na test-time sampling method which explicitly samples and eliminates hypotheses\nabout causal relationships from the LM. This scalable approach significantly\nreduces the disjunctive bias and moves LMs closer to the goal of scientific,\ncausally rigorous reasoning.",
    "published": "2025-05-14T17:59:35Z",
    "updated": "2025-10-05T18:28:02Z",
    "link": "http://arxiv.org/pdf/2505.09614v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Anthony GX-Chen",
      "Dongyan Lin",
      "Mandana Samiei",
      "Doina Precup",
      "Blake A. Richards",
      "Rob Fergus",
      "Kenneth Marino"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04311v1",
    "title": "On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent\n  Systems",
    "summary": "Large language model multi-agent systems (LLM-MAS) offer a promising paradigm\nfor harnessing collective intelligence to achieve more advanced forms of AI\nbehaviour. While recent studies suggest that LLM-MAS can outperform LLM\nsingle-agent systems (LLM-SAS) on certain tasks, the lack of systematic\nexperimental designs limits the strength and generality of these conclusions.\nWe argue that a principled understanding of task complexity, such as the degree\nof sequential reasoning required and the breadth of capabilities involved, is\nessential for assessing the effectiveness of LLM-MAS in task solving. To this\nend, we propose a theoretical framework characterising tasks along two\ndimensions: depth, representing reasoning length, and width, representing\ncapability diversity. We theoretically examine a representative class of\nLLM-MAS, namely the multi-agent debate system, and empirically evaluate its\nperformance in both discriminative and generative tasks with varying depth and\nwidth. Theoretical and empirical results show that the benefit of LLM-MAS over\nLLM-SAS increases with both task depth and width, and the effect is more\npronounced with respect to depth. This clarifies when LLM-MAS are beneficial\nand provides a principled foundation for designing future LLM-MAS methods and\nbenchmarks.",
    "published": "2025-10-05T18:08:48Z",
    "updated": "2025-10-05T18:08:48Z",
    "link": "http://arxiv.org/pdf/2510.04311v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Bohan Tang",
      "Huidong Liang",
      "Keyue Jiang",
      "Xiaowen Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.09685v3",
    "title": "TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal\n  Conversational Music Recommendation",
    "summary": "We present TalkPlayData 2, a synthetic dataset for multimodal conversational\nmusic recommendation generated by an agentic data pipeline. In the proposed\npipeline, multiple large language model (LLM) agents are created under various\nroles with specialized prompts and access to different parts of information,\nand the chat data is acquired by logging the conversation between the Listener\nLLM and the Recsys LLM. To cover various conversation scenarios, for each\nconversation, the Listener LLM is conditioned on a finetuned conversation goal.\nFinally, all the LLMs are multimodal with audio and images, allowing a\nsimulation of multimodal recommendation and conversation. In the LLM-as-a-judge\nand subjective evaluation experiments, TalkPlayData 2 achieved the proposed\ngoal in various aspects related to training a generative recommendation model\nfor music. TalkPlayData 2 and its generation code are open-sourced at\nhttps://talkpl.ai/talkplaydata2.",
    "published": "2025-08-18T05:06:58Z",
    "updated": "2025-10-05T18:05:06Z",
    "link": "http://arxiv.org/pdf/2509.09685v3.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Keunwoo Choi",
      "Seungheon Doh",
      "Juhan Nam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04438v2",
    "title": "The Telephone Game: Evaluating Semantic Drift in Unified Models",
    "summary": "Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and visual generation (text-to-image: T2I) has opened a\nnew direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T. Existing evaluation benchmarks\nconsider these capabilities in isolation: FID and GenEval for T2I, and\nbenchmarks such as MME, MMBench for I2T. These isolated single-pass metrics do\nnot reveal cross-consistency: whether a model that \"understands\" a concept can\nalso \"render\" it, nor whether semantic meaning is preserved when cycling\nbetween image and text modalities. To address this, we introduce the Semantic\nDrift Protocol (SDP) for Unified Models, a cyclic evaluation protocol that\nalternates I2T and T2I over multiple generations to quantify semantic drift. We\npropose two metrics: (i) Mean Cumulative Drift (MCD), an embedding-based\nmeasure of overall semantic drift; and (ii) Multi-Generation GenEval (MGG), an\nobject-level compliance score extending GenEval. To assess generalization\nbeyond COCO dataset, which is widely used in training; we create a new\nbenchmark Nocaps+Docci400, sampled from NoCaps and DOCCI and evaluated on seven\nrecent models. SDP reveals substantial variation in cross-modal stability: some\nmodels like BAGEL maintain semantic meaning over many alternations, whereas\nothers like VILA-U drift quickly despite strong single-pass scores. Our results\nhighlight SDP as a necessary complement to standard I2T and T2I evaluations.\nCode is available at\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models",
    "published": "2025-09-04T17:53:52Z",
    "updated": "2025-10-06T17:49:39Z",
    "link": "http://arxiv.org/pdf/2509.04438v2.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Sabbir Mollah",
      "Rohit Gupta",
      "Sirnam Swetha",
      "Qingyang Liu",
      "Ahnaf Munir",
      "Mubarak Shah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05052v1",
    "title": "Proactive defense against LLM Jailbreak",
    "summary": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks.",
    "published": "2025-10-06T17:32:40Z",
    "updated": "2025-10-06T17:32:40Z",
    "link": "http://arxiv.org/pdf/2510.05052v1.pdf",
    "category": [
      "cs.CR",
      "cs.CL"
    ],
    "authors": [
      "Weiliang Zhao",
      "Jinjun Peng",
      "Daniel Ben-Levi",
      "Zhou Yu",
      "Junfeng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05046v1",
    "title": "COLE: a Comprehensive Benchmark for French Language Understanding\n  Evaluation",
    "summary": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling.",
    "published": "2025-10-06T17:26:41Z",
    "updated": "2025-10-06T17:26:41Z",
    "link": "http://arxiv.org/pdf/2510.05046v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "David Beauchemin",
      "Yan Tremblay",
      "Mohamed Amine Youssef",
      "Richard Khoury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.12491v3",
    "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse Reinforcement Learning",
    "summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 85% accuracy in predicting human preferences.\nOur analysis reveals key insights into the non-identifiability of reward\nfunctions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.",
    "published": "2024-10-16T12:14:25Z",
    "updated": "2025-10-06T17:25:58Z",
    "link": "http://arxiv.org/pdf/2410.12491v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jared Joselowitz",
      "Ritam Majumdar",
      "Arjun Jagota",
      "Matthieu Bou",
      "Nyal Patel",
      "Satyapriya Krishna",
      "Sonali Parbhoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.20101v2",
    "title": "RealKIE: Five Novel Datasets for Enterprise Key Information Extraction",
    "summary": "We introduce RealKIE, a benchmark of five challenging datasets aimed at\nadvancing key information extraction methods, with an emphasis on enterprise\napplications. The datasets include a diverse range of documents including SEC\nS1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and\nResource Contracts. Each presents unique challenges: poor text serialization,\nsparse annotations in long documents, and complex tabular layouts. These\ndatasets provide a realistic testing ground for key information extraction\ntasks like investment analysis and contract analysis. In addition to presenting\nthese datasets, we offer an in-depth description of the annotation process,\ndocument processing techniques, and baseline modeling approaches. This\ncontribution facilitates the development of NLP models capable of handling\npractical challenges and supports further research into information extraction\ntechnologies applicable to industry-specific problems. The annotated data, OCR\noutputs, and code to reproduce baselines are available to download at\nhttps://indicodatasolutions.github.io/RealKIE/.",
    "published": "2024-03-29T10:31:32Z",
    "updated": "2025-10-06T17:14:08Z",
    "link": "http://arxiv.org/pdf/2403.20101v2.pdf",
    "category": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Benjamin Townsend",
      "Madison May",
      "Katherine Mackowiak",
      "Christopher Wells"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05038v1",
    "title": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time\n  Optimization",
    "summary": "Multimodal encoders have pushed the boundaries of visual document retrieval,\nmatching textual query tokens directly to image patches and achieving\nstate-of-the-art performance on public benchmarks. Recent models relying on\nthis paradigm have massively scaled the sizes of their query and document\nrepresentations, presenting obstacles to deployment and scalability in\nreal-world pipelines. Furthermore, purely vision-centric approaches may be\nconstrained by the inherent modality gap still exhibited by modern\nvision-language models. In this work, we connect these challenges to the\nparadigm of hybrid retrieval, investigating whether a lightweight dense text\nretriever can enhance a stronger vision-centric model. Existing hybrid methods,\nwhich rely on coarse-grained fusion of ranks or scores, fail to exploit the\nrich interactions within each model's representation space. To address this, we\nintroduce Guided Query Refinement (GQR), a novel test-time optimization method\nthat refines a primary retriever's query embedding using guidance from a\ncomplementary retriever's scores. Through extensive experiments on visual\ndocument retrieval benchmarks, we demonstrate that GQR allows vision-centric\nmodels to match the performance of models with significantly larger\nrepresentations, while being up to 14x faster and requiring 54x less memory.\nOur findings show that GQR effectively pushes the Pareto frontier for\nperformance and efficiency in multimodal retrieval. We release our code at\nhttps://github.com/IBM/test-time-hybrid-retrieval",
    "published": "2025-10-06T17:12:53Z",
    "updated": "2025-10-06T17:12:53Z",
    "link": "http://arxiv.org/pdf/2510.05038v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Omri Uzan",
      "Asaf Yehudai",
      "Roi pony",
      "Eyal Shnarch",
      "Ariel Gera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05026v1",
    "title": "A Set of Quebec-French Corpus of Regional Expressions and Terms",
    "summary": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect.",
    "published": "2025-10-06T17:04:22Z",
    "updated": "2025-10-06T17:04:22Z",
    "link": "http://arxiv.org/pdf/2510.05026v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "David Beauchemin",
      "Yan Tremblay",
      "Mohamed Amine Youssef",
      "Richard Khoury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04944v1",
    "title": "On Structured State-Space Duality",
    "summary": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence\nbetween a simple Structured State-Space Model (SSM) and a masked attention\nmechanism. In particular, a state-space model with a scalar-times-identity\nstate matrix is equivalent to a masked self-attention with a $1$-semiseparable\ncausal mask. Consequently, the same sequence transformation (model) has two\nalgorithmic realizations: as a linear-time $O(T)$ recurrence or as a\nquadratic-time $O(T^2)$ attention. In this note, we formalize and generalize\nthis duality: (i) we extend SSD from the scalar-identity case to general\ndiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs\nmatch the scalar case's training complexity lower bounds while supporting\nricher dynamics; (iii) we establish a necessary and sufficient condition under\nwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we\nshow that such duality fails to extend to standard softmax attention due to\nrank explosion. Together, these results tighten bridge between recurrent SSMs\nand Transformers, and widen the design space for expressive yet efficient\nsequence models.",
    "published": "2025-10-06T15:46:50Z",
    "updated": "2025-10-06T15:46:50Z",
    "link": "http://arxiv.org/pdf/2510.04944v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Xiwen Zhang",
      "Weimin Wu",
      "Han Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10610v3",
    "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly",
    "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.",
    "published": "2025-05-15T17:52:54Z",
    "updated": "2025-10-06T15:41:20Z",
    "link": "http://arxiv.org/pdf/2505.10610v3.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Zhaowei Wang",
      "Wenhao Yu",
      "Xiyu Ren",
      "Jipeng Zhang",
      "Yu Zhao",
      "Rohit Saxena",
      "Liang Cheng",
      "Ginny Wong",
      "Simon See",
      "Pasquale Minervini",
      "Yangqiu Song",
      "Mark Steedman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26431v2",
    "title": "Text-Based Approaches to Item Alignment to Content Standards in\n  Large-Scale Reading & Writing Tests",
    "summary": "Aligning test items to content standards is a critical step in test\ndevelopment to collect validity evidence based on content. Item alignment has\ntypically been conducted by human experts. This judgmental process can be\nsubjective and time-consuming. This study investigated the performance of\nfine-tuned small language models (SLMs) for automated item alignment using data\nfrom a large-scale standardized reading and writing test for college\nadmissions. Different SLMs were trained for alignment at both domain and skill\nlevels respectively with 10 skills mapped to 4 content domains. The model\nperformance was evaluated in multiple criteria on two testing datasets. The\nimpact of types and sizes of the input data for training was investigated.\nResults showed that including more item text data led to substantially better\nmodel performance, surpassing the improvements induced by sample size increase\nalone. For comparison, supervised machine learning models were trained using\nthe embeddings from the multilingual-E5-large-instruct model. The study results\nshowed that fine-tuned SLMs consistently outperformed the embedding-based\nsupervised machine learning models, particularly for the more fine-grained\nskill alignment. To better understand model misclassifications, multiple\nsemantic similarity analysis including pairwise cosine similarity,\nKullback-Leibler divergence of embedding distributions, and two-dimension\nprojections of item embeddings were conducted. These analyses consistently\nshowed that certain skills in SAT and PSAT were semantically too close,\nproviding evidence for the observed misclassification.",
    "published": "2025-09-30T15:53:22Z",
    "updated": "2025-10-06T15:32:15Z",
    "link": "http://arxiv.org/pdf/2509.26431v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yanbin Fu",
      "Hong Jiao",
      "Tianyi Zhou",
      "Robert W. Lissitz",
      "Nan Zhang",
      "Ming Li",
      "Qingshu Xu",
      "Sydney Peters"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04905v1",
    "title": "Retrieval-Augmented Code Generation: A Survey with Focus on\n  Repository-Level Approaches",
    "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.",
    "published": "2025-10-06T15:20:03Z",
    "updated": "2025-10-06T15:20:03Z",
    "link": "http://arxiv.org/pdf/2510.04905v1.pdf",
    "category": [
      "cs.SE",
      "cs.CL"
    ],
    "authors": [
      "Yicheng Tao",
      "Yao Qin",
      "Yepang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26314v2",
    "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in Its Latent Thoughts",
    "summary": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huginn-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huginn-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs.",
    "published": "2025-09-30T14:26:36Z",
    "updated": "2025-10-06T15:15:21Z",
    "link": "http://arxiv.org/pdf/2509.26314v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hanwen Du",
      "Yuxin Dong",
      "Xia Ning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09667v3",
    "title": "Summaries as Centroids for Interpretable and Scalable Text Clustering",
    "summary": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means\nthat periodically replace numeric centroids with textual summaries. The key\nidea, summary-as-centroid, retains k-means assignments in embedding space while\nproducing human-readable, auditable cluster prototypes. The method is\nLLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling\noffline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that\nuses an LLM for summaries under a fixed per-iteration budget whose cost does\nnot grow with dataset size. We also present a mini-batch extension for\nreal-time clustering of streaming text. Across diverse datasets, embedding\nmodels, and summarization strategies, our approach consistently outperforms\nclassical baselines and approaches the accuracy of recent LLM-based\nclustering-without extensive LLM calls. Finally, we provide a case study on\nsequential text streams and release a StackExchange-derived benchmark for\nevaluating streaming text clustering.",
    "published": "2025-02-12T19:50:22Z",
    "updated": "2025-10-06T14:57:36Z",
    "link": "http://arxiv.org/pdf/2502.09667v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Jairo Diaz-Rodriguez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22768v2",
    "title": "ML2B: Multi-Lingual ML Benchmark For AutoML",
    "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.",
    "published": "2025-09-26T17:20:27Z",
    "updated": "2025-10-06T14:53:27Z",
    "link": "http://arxiv.org/pdf/2509.22768v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ekaterina Trofimova",
      "Zosia Shamina",
      "Maria Selifanova",
      "Artem Zaitsev",
      "Remi Savchuk",
      "Maxim Minets",
      "Daria Ozerova",
      "Emil Sataev",
      "Denis Zuenko",
      "Andrey E. Ustyuzhanin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04849v1",
    "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
    "summary": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.",
    "published": "2025-10-06T14:36:30Z",
    "updated": "2025-10-06T14:36:30Z",
    "link": "http://arxiv.org/pdf/2510.04849v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Elisei Rykov",
      "Kseniia Petrushina",
      "Maksim Savkin",
      "Valerii Olisov",
      "Artem Vazhentsev",
      "Kseniia Titova",
      "Alexander Panchenko",
      "Vasily Konovalov",
      "Julia Belikova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04848v1",
    "title": "Instability in Downstream Task Performance During LLM Pretraining",
    "summary": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure.",
    "published": "2025-10-06T14:33:38Z",
    "updated": "2025-10-06T14:33:38Z",
    "link": "http://arxiv.org/pdf/2510.04848v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuto Nishida",
      "Masaru Isonuma",
      "Yusuke Oda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23500v2",
    "title": "Identity resolution of software metadata using Large Language Models",
    "summary": "Software is an essential component of research. However, little attention has\nbeen paid to it compared with that paid to research data. Recently, there has\nbeen an increase in efforts to acknowledge and highlight the importance of\nsoftware in research activities. Structured metadata from platforms like\nbio.tools, Bioconductor, and Galaxy ToolShed offers valuable insights into\nresearch software in the Life Sciences. Although originally intended to support\ndiscovery and integration, this metadata can be repurposed for large-scale\nanalysis of software practices. However, its quality and completeness vary\nacross platforms, reflecting diverse documentation practices. To gain a\ncomprehensive view of software development and sustainability, consolidating\nthis metadata is necessary, but requires robust mechanisms to address its\nheterogeneity and scale.\n  This article presents an evaluation of instruction-tuned large language\nmodels for the task of software metadata identity resolution, a critical step\nin assembling a cohesive collection of research software. Such a collection is\nthe reference component for the Software Observatory at OpenEBench, a platform\nthat aggregates metadata to monitor the FAIRness of research software in the\nLife Sciences. We benchmarked multiple models against a human-annotated gold\nstandard, examined their behavior on ambiguous cases, and introduced an\nagreement-based proxy for high-confidence automated decisions. The proxy\nachieved high precision and statistical robustness, while also highlighting the\nlimitations of current models and the broader challenges of automating semantic\njudgment in FAIR-aligned software metadata across registries and repositories.",
    "published": "2025-05-29T14:47:31Z",
    "updated": "2025-10-06T14:17:23Z",
    "link": "http://arxiv.org/pdf/2505.23500v2.pdf",
    "category": [
      "cs.SE",
      "cs.CL",
      "cs.DL"
    ],
    "authors": [
      "Eva Martín del Pico",
      "Josep Lluís Gelpí",
      "Salvador Capella-Gutiérrez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04832v1",
    "title": "How I Built ASR for Endangered Languages with a Spoken Dictionary",
    "summary": "Nearly half of the world's languages are endangered. Speech technologies such\nas Automatic Speech Recognition (ASR) are central to revival efforts, yet most\nlanguages remain unsupported because standard pipelines expect utterance-level\nsupervised data. Speech data often exist for endangered languages but rarely\nmatch these formats. Manx Gaelic ($\\sim$2,200 speakers), for example, has had\ntranscribed speech since 1948, yet remains unsupported by modern systems. In\nthis paper, we explore how little data, and in what form, is needed to build\nASR for critically endangered languages. We show that a short-form\npronunciation resource is a viable alternative, and that 40 minutes of such\ndata produces usable ASR for Manx ($<$50\\% WER). We replicate our approach,\napplying it to Cornish ($\\sim$600 speakers), another critically endangered\nlanguage. Results show that the barrier to entry, in quantity and form, is far\nlower than previously thought, giving hope to endangered language communities\nthat cannot afford to meet the requirements arbitrarily imposed upon them.",
    "published": "2025-10-06T14:16:47Z",
    "updated": "2025-10-06T14:16:47Z",
    "link": "http://arxiv.org/pdf/2510.04832v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Christopher Bartley",
      "Anton Ragni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22777v4",
    "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators",
    "summary": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance.",
    "published": "2025-05-28T18:45:42Z",
    "updated": "2025-10-06T14:06:40Z",
    "link": "http://arxiv.org/pdf/2505.22777v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "John Mendonça",
      "Alon Lavie",
      "Isabel Trancoso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03102v2",
    "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
    "summary": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at: https://github.com/otmive/llama_reports",
    "published": "2025-10-03T15:31:11Z",
    "updated": "2025-10-06T14:04:39Z",
    "link": "http://arxiv.org/pdf/2510.03102v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Beth Pearson",
      "Ahmed Adnan",
      "Zahraa S. Abdallah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04819v1",
    "title": "Visual Representations inside the Language Model",
    "summary": "Despite interpretability work analyzing VIT encoders and transformer\nactivations, we don't yet understand why Multimodal Language Models (MLMs)\nstruggle on perception-heavy tasks. We offer an under-studied perspective by\nexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and\nLlama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the\nflow of visual information through the language model, finding that image value\ntokens encode sufficient information to perform several perception-heavy tasks\nzero-shot: segmentation, semantic correspondence, temporal correspondence, and\nreferring expression detection. We find that while the language model does\naugment the visual information received from the projection of input visual\nencodings-which we reveal correlates with overall MLM perception capability-it\ncontains less visual information on several tasks than the equivalent visual\nencoder (SigLIP) that has not undergone MLM finetuning. Further, we find that\nthe visual information corresponding to input-agnostic image key tokens in\nlater layers of language models contains artifacts which reduce perception\ncapability of the overall MLM. Next, we discuss controlling visual information\nin the language model, showing that adding a text prefix to the image input\nimproves perception capabilities of visual representations. Finally, we reveal\nthat if language models were able to better control their visual information,\ntheir perception would significantly improve; e.g., in 33.3% of Art Style\nquestions in the BLINK benchmark, perception information present in the\nlanguage model is not surfaced to the output! Our findings reveal insights into\nthe role of key-value tokens in multimodal systems, paving the way for deeper\nmechanistic interpretability of MLMs and suggesting new directions for training\ntheir visual encoder and language model components.",
    "published": "2025-10-06T14:01:39Z",
    "updated": "2025-10-06T14:01:39Z",
    "link": "http://arxiv.org/pdf/2510.04819v1.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Benlin Liu",
      "Amita Kamath",
      "Madeleine Grunde-McLaughlin",
      "Winson Han",
      "Ranjay Krishna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17098v3",
    "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided\n  Sequence Configuration",
    "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input ICL\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures ICL sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a novel and valuable\nperspective for interpreting and improving multimodal ICL.",
    "published": "2025-05-21T05:22:21Z",
    "updated": "2025-10-06T13:42:58Z",
    "link": "http://arxiv.org/pdf/2505.17098v3.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Yanshu Li",
      "Jianjiang Yang",
      "Tian Yun",
      "Pinyuan Feng",
      "Jinfa Huang",
      "Ruixiang Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04800v1",
    "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights",
    "summary": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations.",
    "published": "2025-10-06T13:30:07Z",
    "updated": "2025-10-06T13:30:07Z",
    "link": "http://arxiv.org/pdf/2510.04800v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sangmin Bae",
      "Bilge Acun",
      "Haroun Habeeb",
      "Seungyeon Kim",
      "Chien-Yu Lin",
      "Liang Luo",
      "Junjie Wang",
      "Carole-Jean Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03120v2",
    "title": "SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with\n  Reader Needs?",
    "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
    "published": "2025-10-03T15:49:09Z",
    "updated": "2025-10-06T13:13:37Z",
    "link": "http://arxiv.org/pdf/2510.03120v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhaojun Sun",
      "Xuzhou Zhu",
      "Xuanhe Zhou",
      "Xin Tong",
      "Shuo Wang",
      "Jie Fu",
      "Guoliang Li",
      "Zhiyuan Liu",
      "Fan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22075v2",
    "title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary\n  Learning",
    "summary": "Post-training compression of large language models (LLMs) largely relies on\nlow-rank weight approximation, which represents each column of a weight matrix\nin a shared low-dimensional subspace. While this is a computationally efficient\nstrategy, the imposed structural constraint is rigid and can lead to a\nnoticeable model accuracy drop. In this work, we propose CoSpaDi (Compression\nvia Sparse Dictionary Learning), a novel training-free compression framework\nthat replaces low-rank decomposition with a more flexible structured sparse\nfactorization in which each weight matrix is represented with a dense\ndictionary and a column-sparse coefficient matrix. This formulation enables a\nunion-of-subspaces representation: different columns of the original weight\nmatrix are approximated in distinct subspaces spanned by adaptively selected\ndictionary atoms, offering greater expressiveness than a single invariant\nbasis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the\nfactorization such that the output activations of compressed projection layers\nclosely match those of the original ones, thereby minimizing functional\nreconstruction error rather than mere weight approximation. This data-aware\nstrategy preserves better model fidelity without any fine-tuning under\nreasonable compression ratios. Moreover, the resulting structured sparsity\nallows efficient sparse-dense matrix multiplication and is compatible with\npost-training quantization for further memory and latency gains. We evaluate\nCoSpaDi across multiple Llama and Qwen models under per-layer and per-group\nsettings at 20-50\\% compression ratios, demonstrating consistent superiority\nover state-of-the-art data-aware low-rank methods both in accuracy and\nperplexity. Our results establish structured sparse dictionary learning as a\npowerful alternative to conventional low-rank approaches for efficient LLM\ndeployment.",
    "published": "2025-09-26T08:55:09Z",
    "updated": "2025-10-06T12:56:01Z",
    "link": "http://arxiv.org/pdf/2509.22075v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Dmitriy Shopkhoev",
      "Denis Makhov",
      "Magauiya Zhussip",
      "Ammar Ali",
      "Stamatios Lefkimmiatis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.23174v4",
    "title": "TRA: Better Length Generalisation with Threshold Relative Attention",
    "summary": "Transformers struggle with length generalisation, displaying poor performance\neven on basic tasks. We test whether these limitations can be explained through\ntwo key failures of the self-attention mechanism. The first is the inability to\nfully remove irrelevant information. The second is tied to position, even if\nthe dot product between a key and query is highly negative (i.e. an irrelevant\nkey) learned positional biases may unintentionally up-weight such information -\ndangerous when distances become out of distribution. Put together, these two\nfailure cases lead to compounding generalisation difficulties. We test whether\nthey can be mitigated through the combination of a) selective sparsity -\ncompletely removing irrelevant keys from the attention softmax and b)\ncontextualised relative distance - distance is only considered as between the\nquery and the keys that matter. We show how refactoring the attention mechanism\nwith these two mitigations in place can substantially improve the\ngeneralisation capabilities of decoder only transformers.",
    "published": "2025-03-29T18:06:28Z",
    "updated": "2025-10-06T12:50:07Z",
    "link": "http://arxiv.org/pdf/2503.23174v4.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Mattia Opper",
      "Roland Fernandez",
      "Paul Smolensky",
      "Jianfeng Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01238v2",
    "title": "Silent Tokens, Loud Effects: Padding in LLMs",
    "summary": "Padding tokens are widely used in large language models (LLMs) to equalize\nsequence lengths during batched inference. While they should be fully masked,\nimplementation errors can cause them to influence computation, and the extent\nof this influence is not well understood. We systematically study this effect\nacross three open-source model families (Llama, Gemma, Qwen), inserting\ncontrolled amounts of padding and evaluating outcomes along four axes:\nactivations, generation quality, bias, and safety. Even small amounts of\npadding shift hidden representations, degrade quality in smaller models, alter\nbias in unpredictable ways, and weaken safety guardrails. These findings\ndemonstrate that padding is not a harmless detail but a robustness risk that\nmust be carefully handled in deployment.",
    "published": "2025-09-23T22:57:44Z",
    "updated": "2025-10-06T12:48:05Z",
    "link": "http://arxiv.org/pdf/2510.01238v2.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Rom Himelstein",
      "Amit LeVi",
      "Yonatan Belinkov",
      "Avi Mendelson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04764v1",
    "title": "Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of\n  Sample-efficient Language Models",
    "summary": "Implicit meanings are integral to human communication, making it essential\nfor language models to be capable of identifying and interpreting them. Grice\n(1975) proposed a set of conversational maxims that guide cooperative dialogue,\nnoting that speakers may deliberately violate these principles to express\nmeanings beyond literal words, and that listeners, in turn, recognize such\nviolations to draw pragmatic inferences.\n  Building on Surian et al. (1996)'s study of children's sensitivity to\nviolations of Gricean maxims, we introduce a novel benchmark to test whether\nlanguage models pretrained on less than 10M and less than 100M tokens can\ndistinguish maxim-adhering from maxim-violating utterances. We compare these\nBabyLMs across five maxims and situate their performance relative to children\nand a Large Language Model (LLM) pretrained on 3T tokens.\n  We find that overall, models trained on less than 100M tokens outperform\nthose trained on less than 10M, yet fall short of child-level and LLM\ncompetence. Our results suggest that modest data increases improve some aspects\nof pragmatic behavior, leading to finer-grained differentiation between\npragmatic dimensions.",
    "published": "2025-10-06T12:38:41Z",
    "updated": "2025-10-06T12:38:41Z",
    "link": "http://arxiv.org/pdf/2510.04764v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Raha Askari",
      "Sina Zarrieß",
      "Özge Alacam",
      "Judith Sieker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.01667v3",
    "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish",
    "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as Claude\nand DeepSeek-R1 typically achieve high scores, while smaller models show weak\nperformances. We also find that the performances in such language exams can be\nused to predict performances in other NLP tasks in Luxembourgish.",
    "published": "2025-04-02T12:16:14Z",
    "updated": "2025-10-06T12:36:33Z",
    "link": "http://arxiv.org/pdf/2504.01667v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Cedric Lothritz",
      "Jordi Cabot",
      "Laura Bernardy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04757v1",
    "title": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced\n  re-ranking retriever",
    "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching\nLarge Language Models (LLMs) with external knowledge, allowing for factually\ngrounded responses, a critical requirement in high-stakes domains such as\nhealthcare. However, the efficacy of RAG systems is fundamentally restricted by\nthe performance of their retrieval module, since irrelevant or semantically\nmisaligned documents directly compromise the accuracy of the final generated\nresponse. General-purpose dense retrievers can struggle with the nuanced\nlanguage of specialised domains, while the high accuracy of in-domain models is\noften achieved at prohibitive computational costs. In this work, we aim to\naddress this trade-off by developing and evaluating a two-stage retrieval\narchitecture that combines a lightweight ModernBERT bidirectional encoder for\nefficient initial candidate retrieval with a ColBERTv2 late-interaction model\nfor fine-grained re-ranking. We conduct comprehensive evaluations of our\nretriever module performance and RAG system performance in the biomedical\ncontext, fine-tuning the IR module using 10k question-passage pairs from\nPubMedQA. Our analysis of the retriever module confirmed the positive impact of\nthe ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points\ncompared to its retrieve-only counterpart. When integrated into the biomedical\nRAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on\nthe five tasks of the MIRAGE question-answering benchmark, outperforming strong\nbaselines such as MedCPT (0.4436). Our ablation studies reveal that this\nperformance is critically dependent on a joint fine-tuning process that aligns\nthe retriever and re-ranker; otherwise, the re-ranker might degrade the\nperformance.",
    "published": "2025-10-06T12:34:55Z",
    "updated": "2025-10-06T12:34:55Z",
    "link": "http://arxiv.org/pdf/2510.04757v1.pdf",
    "category": [
      "cs.CL",
      "q-bio.QM"
    ],
    "authors": [
      "Eduardo Martínez Rivera",
      "Filippo Menolascina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04750v1",
    "title": "A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia\n  Assistance",
    "summary": "Dyslexia in adults remains an under-researched and under-served area,\nparticularly in non-English-speaking contexts, despite its significant impact\non personal and professional lives. This work addresses that gap by focusing on\nSinhala, a low-resource language with limited tools for linguistic\naccessibility. We present an assistive system explicitly designed for\nSinhala-speaking adults with dyslexia. The system integrates Whisper for\nspeech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model\ntrained for Sinhala to identify common dyslexic errors, and a combined mT5 and\nMistral-based model to generate corrected text. Finally, the output is\nconverted back to speech using gTTS, creating a complete multimodal feedback\nloop. Despite the challenges posed by limited Sinhala-language datasets, the\nsystem achieves 0.66 transcription accuracy and 0.7 correction accuracy with\n0.65 overall system accuracy. These results demonstrate both the feasibility\nand effectiveness of the approach. Ultimately, this work highlights the\nimportance of inclusive Natural Language Processing (NLP) technologies in\nunderrepresented languages and showcases a practical",
    "published": "2025-10-06T12:28:57Z",
    "updated": "2025-10-06T12:28:57Z",
    "link": "http://arxiv.org/pdf/2510.04750v1.pdf",
    "category": [
      "cs.CL",
      "cs.SE"
    ],
    "authors": [
      "Peshala Perera",
      "Deshan Sumanathilaka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23124v2",
    "title": "Non-Collaborative User Simulators for Tool Agents",
    "summary": "Tool agents interact with users through multi-turn dialogues to accomplish\nvarious tasks. Recent studies have adopted user simulation methods to develop\nthese agents in multi-turn settings. However, existing user simulators tend to\nbe agent-friendly, exhibiting only cooperative behaviors, which fails to train\nand test agents against non-collaborative users in the real world. To address\nthis, we propose a novel user simulator architecture that simulates four\ncategories of non-collaborative behaviors: requesting unavailable services,\ndigressing into tangential conversations, expressing impatience, and providing\nincomplete utterances. Our user simulator can simulate challenging and natural\nnon-collaborative behaviors while reliably delivering all intents and\ninformation necessary to accomplish the task. Our experiments on MultiWOZ and\n$\\tau$-bench reveal significant performance degradation in state-of-the-art\ntool agents when encountering non-collaborative users. We provide detailed\nanalyses of agents' weaknesses under each non-collaborative condition, such as\nescalated hallucinations and dialogue breakdowns. Ultimately, we contribute an\neasily extensible user simulation framework to help the research community\ndevelop tool agents and preemptively diagnose them under challenging real-world\nconditions within their own services.",
    "published": "2025-09-27T05:06:17Z",
    "updated": "2025-10-06T12:23:18Z",
    "link": "http://arxiv.org/pdf/2509.23124v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jeonghoon Shim",
      "Woojung Song",
      "Cheyon Jin",
      "Seungwon KooK",
      "Yohan Jo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04717v1",
    "title": "JSON Whisperer: Efficient JSON Editing with LLMs",
    "summary": "Large language models (LLMs) can modify JSON documents through natural\nlanguage commands, but current approaches regenerate entire structures for each\nedit, resulting in computational inefficiency. We present JSON Whisperer, a\nframework that enables LLMs to generate RFC 6902 diff patches-expressing only\nthe necessary modifications-rather than complete documents. We identify two key\nchallenges in patch-based editing: (1) LLMs often miss related updates when\ngenerating isolated patches, and (2) array manipulations require tracking index\nshifts across operations, which LLMs handle poorly. To address these issues, we\nintroduce EASE (Explicitly Addressed Sequence Encoding), which transforms\narrays into dictionaries with stable keys, eliminating index arithmetic\ncomplexities. Our evaluation shows that patch generation with EASE reduces\ntoken usage by 31% while maintaining edit quality within 5% of full\nregeneration with particular gains for complex instructions and list\nmanipulations. The dataset is available at:\nhttps://github.com/emnlp2025/JSON-Whisperer/",
    "published": "2025-10-06T11:36:46Z",
    "updated": "2025-10-06T11:36:46Z",
    "link": "http://arxiv.org/pdf/2510.04717v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sarel Duanis",
      "Asnat Greenstein-Messica",
      "Eliya Habba"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04678v1",
    "title": "Multi-Agent Tool-Integrated Policy Optimization",
    "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
    "published": "2025-10-06T10:44:04Z",
    "updated": "2025-10-06T10:44:04Z",
    "link": "http://arxiv.org/pdf/2510.04678v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhanfeng Mo",
      "Xingxuan Li",
      "Yuntao Chen",
      "Lidong Bing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16459v2",
    "title": "Towards Enforcing Company Policy Adherence in Agentic Workflows",
    "summary": "Large Language Model (LLM) agents hold promise for a flexible and scalable\nalternative to traditional business process automation, but struggle to\nreliably follow complex company policies. In this study we introduce a\ndeterministic, transparent, and modular framework for enforcing business policy\nadherence in agentic workflows. Our method operates in two phases: (1) an\noffline buildtime stage that compiles policy documents into verifiable guard\ncode associated with tool use, and (2) a runtime integration where these guards\nensure compliance before each agent action. We demonstrate our approach on the\nchallenging $\\tau$-bench Airlines domain, showing encouraging preliminary\nresults in policy enforcement, and further outline key challenges for\nreal-world deployments.",
    "published": "2025-07-22T11:00:37Z",
    "updated": "2025-10-06T10:07:53Z",
    "link": "http://arxiv.org/pdf/2507.16459v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Naama Zwerdling",
      "David Boaz",
      "Ella Rabinovich",
      "Guy Uziel",
      "David Amid",
      "Ateret Anaby-Tavor"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.16965v3",
    "title": "Praxis-VLM: Vision-Grounded Decision Making via Text-Driven\n  Reinforcement Learning",
    "summary": "Vision Language Models exhibit impressive performance for various tasks, yet\nthey often lack the sophisticated situational reasoning required for complex\ndecision-making. This paper shows that VLMs can achieve surprisingly strong\ndecision-making performance when visual scenes are replaced by textual\ndescriptions, suggesting foundational reasoning can be effectively learned from\nlanguage. Motivated by this insight, we propose Praxis-VLM, a reasoning VLM for\nvision-grounded decision-making. Praxis-VLM employs the GRPO algorithm on\ntextual scenarios to instill robust reasoning capabilities, where models learn\nto evaluate actions and their consequences. These reasoning skills, acquired\npurely from text, successfully transfer to multimodal inference with visual\ninputs, significantly reducing reliance on scarce paired image-text training\ndata. Experiments across diverse decision-making benchmarks demonstrate that\nPraxis-VLM substantially outperforms standard supervised fine-tuning,\nexhibiting superior performance and generalizability. Further analysis confirms\nthat our models engage in explicit and effective reasoning, underpinning their\nenhanced performance and adaptability.",
    "published": "2025-03-21T09:25:23Z",
    "updated": "2025-10-06T10:03:29Z",
    "link": "http://arxiv.org/pdf/2503.16965v3.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Zhe Hu",
      "Jing Li",
      "Zhongzhu Pu",
      "Hou Pong Chan",
      "Yu Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04655v1",
    "title": "FT-MDT: Extracting Decision Trees from Medical Texts via a Novel\n  Low-rank Adaptation Method",
    "summary": "Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to building clinical decision support\nsystems. However, current MDT construction methods rely heavily on\ntime-consuming and laborious manual annotation. To address this challenge, we\npropose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for\nautomatically extracting MDTs from clinical guidelines and textbooks. We\nintegrate gradient path information to capture synergistic effects between\ndifferent modules, enabling more effective and reliable rank allocation. This\nframework ensures that the most critical modules receive appropriate rank\nallocations while less important ones are pruned, resulting in a more efficient\nand accurate model for extracting medical decision trees from clinical texts.\nExtensive experiments on medical guideline datasets demonstrate that our\nPI-LoRA method significantly outperforms existing parameter-efficient\nfine-tuning approaches for the Text2MDT task, achieving better accuracy with\nsubstantially reduced model complexity. The proposed method achieves\nstate-of-the-art results while maintaining a lightweight architecture, making\nit particularly suitable for clinical decision support systems where\ncomputational resources may be limited.",
    "published": "2025-10-06T09:59:55Z",
    "updated": "2025-10-06T09:59:55Z",
    "link": "http://arxiv.org/pdf/2510.04655v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yuheng Li",
      "Jiechao Gao",
      "Wei Han",
      "Wenwen Ouyang",
      "Wei Zhu",
      "Hui Yi Leong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21544v2",
    "title": "MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n  in Retrieval-Augmented Generation",
    "summary": "Knowledge conflict often arises in retrieval-augmented generation (RAG)\nsystems, where retrieved documents may be inconsistent with one another or\ncontradict the model's parametric knowledge. Existing benchmarks for\ninvestigating the phenomenon have notable limitations, including a narrow focus\non the question answering setup, heavy reliance on entity substitution\ntechniques, and a restricted range of conflict types. To address these issues,\nwe propose a knowledge graph (KG)-based framework that generates varied and\nsubtle conflicts between two similar yet distinct contexts, while ensuring\ninterpretability through the explicit relational structure of KGs. Experimental\nresults on our benchmark, MAGIC, provide intriguing insights into the inner\nworkings of LLMs regarding knowledge conflict: both open-source and proprietary\nmodels struggle with conflict detection -- especially when multi-hop reasoning\nis required -- and often fail to pinpoint the exact source of contradictions.\nFinally, we present in-depth analyses that serve as a foundation for improving\nLLMs in integrating diverse, sometimes even conflicting, information.",
    "published": "2025-07-29T07:19:49Z",
    "updated": "2025-10-06T09:59:30Z",
    "link": "http://arxiv.org/pdf/2507.21544v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jungyeon Lee",
      "Kangmin Lee",
      "Taeuk Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04641v1",
    "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A\n  Comprehensive Benchmark Study",
    "summary": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks.",
    "published": "2025-10-06T09:45:32Z",
    "updated": "2025-10-06T09:45:32Z",
    "link": "http://arxiv.org/pdf/2510.04641v1.pdf",
    "category": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Ayan Majumdar",
      "Feihao Chen",
      "Jinghui Li",
      "Xiaozhen Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04631v1",
    "title": "Contrastive Learning Using Graph Embeddings for Domain Adaptation of\n  Language Models in the Process Industry",
    "summary": "Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained\nlanguage models by incorporating additional knowledge from the graph structures\nto learn domain-specific terminology or relationships between documents that\nmight otherwise be overlooked. This paper explores how SciNCL, a graph-aware\nneighborhood contrastive learning methodology originally designed for\nscientific publications, can be applied to the process industry domain, where\ntext logs contain crucial information about daily operations and are often\nstructured as sparse KGs. Our experiments demonstrate that language models\nfine-tuned with triplets derived from GE outperform a state-of-the-art\nmE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process\nindustry text embedding benchmark (PITEB) while being 3-5 times smaller in\nsize.",
    "published": "2025-10-06T09:36:20Z",
    "updated": "2025-10-06T09:36:20Z",
    "link": "http://arxiv.org/pdf/2510.04631v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Anastasia Zhukova",
      "Jonas Lührs",
      "Christian E. Matt",
      "Bela Gipp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01617v2",
    "title": "AMAS: Adaptively Determining Communication Topology for LLM-based\n  Multi-Agent System",
    "summary": "Although large language models (LLMs) have revolutionized natural language\nprocessing capabilities, their practical implementation as autonomous\nmulti-agent systems (MAS) for industrial problem-solving encounters persistent\nbarriers. Conventional MAS architectures are fundamentally restricted by\ninflexible, hand-crafted graph topologies that lack contextual responsiveness,\nresulting in diminished efficacy across varied academic and commercial\nworkloads. To surmount these constraints, we introduce AMAS, a\nparadigm-shifting framework that redefines LLM-based MAS through a novel\ndynamic graph designer. This component autonomously identifies task-specific\noptimal graph configurations via lightweight LLM adaptation, eliminating the\nreliance on monolithic, universally applied structural templates. Instead, AMAS\nexploits the intrinsic properties of individual inputs to intelligently direct\nquery trajectories through task-optimized agent pathways. Rigorous validation\nacross question answering, mathematical deduction, and code generation\nbenchmarks confirms that AMAS systematically exceeds state-of-the-art\nsingle-agent and multi-agent approaches across diverse LLM architectures. Our\ninvestigation establishes that context-sensitive structural adaptability\nconstitutes a foundational requirement for high-performance LLM MAS\ndeployments.",
    "published": "2025-10-02T02:50:22Z",
    "updated": "2025-10-06T09:33:41Z",
    "link": "http://arxiv.org/pdf/2510.01617v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hui Yi Leong",
      "Yuheng Li",
      "Yuqing Wu",
      "Wenwen Ouyang",
      "Wei Zhu",
      "Jiechao Gao",
      "Wei Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10364v3",
    "title": "Can We Infer Confidential Properties of Training Data from LLMs?",
    "summary": "Large language models (LLMs) are increasingly fine-tuned on domain-specific\ndatasets to support applications in fields such as healthcare, finance, and\nlaw. These fine-tuning datasets often have sensitive and confidential\ndataset-level properties -- such as patient demographics or disease prevalence\n-- that are not intended to be revealed. While prior work has studied property\ninference attacks on discriminative models (e.g., image classification models)\nand generative models (e.g., GANs for image data), it remains unclear if such\nattacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark\ntask for evaluating property inference in LLMs under two fine-tuning paradigms:\nquestion-answering and chat-completion. Built on the ChatDoctor dataset, our\nbenchmark includes a range of property types and task configurations. We\nfurther propose two tailored attacks: a prompt-based generation attack and a\nshadow-model attack leveraging word frequency signals. Empirical evaluations\nacross multiple pretrained LLMs show the success of our attacks, revealing a\npreviously unrecognized vulnerability in LLMs.",
    "published": "2025-06-12T05:42:06Z",
    "updated": "2025-10-06T09:11:48Z",
    "link": "http://arxiv.org/pdf/2506.10364v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Pengrun Huang",
      "Chhavi Yadav",
      "Kamalika Chaudhuri",
      "Ruihan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09669v3",
    "title": "Query-Level Uncertainty in Large Language Models",
    "summary": "It is important for Large Language Models (LLMs) to be aware of the boundary\nof their knowledge, distinguishing queries they can confidently answer from\nthose that lie beyond their capabilities. Such awareness enables models to\nperform adaptive inference, such as invoking retrieval-augmented generation\n(RAG), engaging in slow and deep thinking, or abstaining from answering when\nappropriate. These mechanisms are key to developing efficient and trustworthy\nAI. In this work, we propose a method to detect knowledge boundaries via\nQuery-Level Uncertainty, which estimates if a model is capable of answering a\ngiven query before generating any tokens, thus avoiding the generation cost. To\nthis end, we propose a novel, training-free method called Internal Confidence,\nwhich leverages self-evaluations across layers and tokens to provide a reliable\nsignal of uncertainty. Empirical studies on both factual question answering and\nmathematical reasoning tasks demonstrate that our Internal Confidence\noutperforms several baselines in quality of confidence while being\ncomputationally cheaper. Furthermore, we demonstrate its benefits in adaptive\ninference settings, showing that for RAG and model cascading it reduces\ninference costs while preserving overall performance.",
    "published": "2025-06-11T12:39:48Z",
    "updated": "2025-10-06T09:08:21Z",
    "link": "http://arxiv.org/pdf/2506.09669v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lihu Chen",
      "Gerard de Melo",
      "Fabian M. Suchanek",
      "Gaël Varoquaux"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04601v1",
    "title": "FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient\n  Federated Large Language Models Fine-Tuning",
    "summary": "The current paradigm of training large language models (LLMs) on publicly\navailable Web data is becoming unsustainable, with high-quality data sources in\nspecialized domains nearing exhaustion. Federated Learning (FL) emerges as a\npractical solution for the next generation of AI on a decentralized Web,\nenabling privacy-preserving collaborative fine-tuning by leveraging private\ndata distributed across a global client base. While Low-Rank Adaptation (LoRA)\nis the standard for efficient fine-tuning, its application in federated\nsettings presents a critical challenge: communication overhead remains a\nsignificant bottleneck across the Web's heterogeneous network conditions. The\nstructural redundancy within LoRA parameters not only incurs a heavy\ncommunication burden but also introduces conflicts when aggregating client\nupdates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose\nframework designed for communication-efficient FL. We first introduce an\nimportance-aware sparsification method that preserves the structural integrity\nof LoRA updates to reduce the uploaded parameter count. The server then\nreconstructs and aggregates these updates in a full-rank space to mitigate\nconflicts. Finally, it decomposes the global update into a sparse low-rank\nformat for broadcast, ensuring a symmetrically efficient cycle. We also propose\nan efficient variant, FedSRD-e, to reduce computational overhead. Experimental\nresults on 10 benchmarks demonstrate that our framework significantly reduces\ncommunication costs by up to 90\\% while even improving model performance on\nheterogeneous client data.",
    "published": "2025-10-06T09:06:38Z",
    "updated": "2025-10-06T09:06:38Z",
    "link": "http://arxiv.org/pdf/2510.04601v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Guochen Yan",
      "Luyuan Xie",
      "Qingni Shen",
      "Yuejian Fang",
      "Zhonghai Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04584v1",
    "title": "Robustness assessment of large audio language models in multiple-choice\n  evaluation",
    "summary": "Recent advances in large audio language models (LALMs) have primarily been\nassessed using a multiple-choice question answering (MCQA) framework. However,\nsubtle changes, such as shifting the order of choices, result in substantially\ndifferent results. Existing MCQA frameworks do not account for this variability\nand report a single accuracy number per benchmark or category. We dive into the\nMCQA evaluation framework and conduct a systematic study spanning three\nbenchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio\nFlamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings\nindicate that models are sensitive not only to the ordering of choices, but\nalso to the paraphrasing of the question and the choices. Finally, we propose a\nsimpler evaluation protocol and metric that account for subtle variations and\nprovide a more detailed evaluation report of LALMs within the MCQA framework.",
    "published": "2025-10-06T08:36:17Z",
    "updated": "2025-10-06T08:36:17Z",
    "link": "http://arxiv.org/pdf/2510.04584v1.pdf",
    "category": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Fernando López",
      "Santosh Kesiraju",
      "Jordi Luque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04581v1",
    "title": "Can LLMs Detect Ambiguous Plural Reference? An Analysis of\n  Split-Antecedent and Mereological Reference",
    "summary": "Our goal is to study how LLMs represent and interpret plural reference in\nambiguous and unambiguous contexts. We ask the following research questions:\n(1) Do LLMs exhibit human-like preferences in representing plural reference?\n(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and\nidentify possible referents? To address these questions, we design a set of\nexperiments, examining pronoun production using next-token prediction tasks,\npronoun interpretation, and ambiguity detection using different prompting\nstrategies. We then assess how comparable LLMs are to humans in formulating and\ninterpreting plural reference. We find that LLMs are sometimes aware of\npossible referents of ambiguous pronouns. However, they do not always follow\nhuman reference when choosing between interpretations, especially when the\npossible interpretation is not explicitly mentioned. In addition, they struggle\nto identify ambiguity without direct instruction. Our findings also reveal\ninconsistencies in the results across different types of experiments.",
    "published": "2025-10-06T08:32:59Z",
    "updated": "2025-10-06T08:32:59Z",
    "link": "http://arxiv.org/pdf/2510.04581v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Dang Anh",
      "Rick Nouwen",
      "Massimo Poesio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04551v1",
    "title": "Fine-grained auxiliary learning for real-world product recommendation",
    "summary": "Product recommendation is the task of recovering the closest items to a given\nquery within a large product corpora. Generally, one can determine if\ntop-ranked products are related to the query by applying a similarity\nthreshold; exceeding it deems the product relevant, otherwise manual revision\nis required. Despite being a well-known problem, the integration of these\nmodels in real-world systems is often overlooked. In particular, production\nsystems have strong coverage requirements, i.e., a high proportion of\nrecommendations must be automated. In this paper we propose ALC , an Auxiliary\nLearning strategy that boosts Coverage through learning fine-grained\nembeddings. Concretely, we introduce two training objectives that leverage the\nhardest negatives in the batch to build discriminative training signals between\npositives and negatives. We validate ALC using three extreme multi-label\nclassification approaches in two product recommendation datasets;\nLF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating\nstate-of-the-art coverage rates when combined with a recent\nthreshold-consistent margin loss.",
    "published": "2025-10-06T07:34:06Z",
    "updated": "2025-10-06T07:34:06Z",
    "link": "http://arxiv.org/pdf/2510.04551v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Mario Almagro",
      "Diego Ortego",
      "David Jimenez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06698v3",
    "title": "SCAN: Structured Capability Assessment and Navigation for LLMs",
    "summary": "Evaluating Large Language Models (LLMs) has become increasingly important,\nwith automatic evaluation benchmarks gaining prominence as alternatives to\nhuman evaluation. While existing research has focused on approximating model\nrankings, such benchmarks fail to provide users and developers with a\ncomprehensive and fine-grained understanding of a specific model's\ncapabilities. To fill this gap, we propose \\textbf{SCAN} (Structured Capability\nAssessment and Navigation), a practical framework that enables detailed\ncharacterization of LLM capabilities through comprehensive and fine-grained\nevaluation. SCAN incorporates four key components: (1) TaxBuilder, which\nextracts capability-indicating tags from extensive queries to construct a\nhierarchical taxonomy automatically; (2) RealMix, a query synthesis and\nfiltering mechanism that ensures sufficient evaluation data for each capability\ntag; (3) a suite of visualization and analysis tools that facilitate efficient\nnavigation and analysis of model capabilities; and (4) a PC$^2$-based\n(Pre-Comparison-derived Criteria) LLM-as-a-Judge approach that achieves\nsignificantly higher accuracy compared to classic LLM-as-a-Judge method. Using\nSCAN, we conduct a comprehensive evaluation of 21 mainstream LLMs. Our detailed\nanalysis of the GPT-OSS family reveals substantial performance variations, even\nwithin sub-capabilities belonging to the same category of capability. This\nfinding highlights the importance of fine-grained evaluation in accurately\nunderstanding LLM behavior. Project homepage and resources are available at\n\\href{https://liudan193.github.io/Feedbacker/}{https://liudan193.github.io/Feedbacker/}.",
    "published": "2025-05-10T16:52:40Z",
    "updated": "2025-10-06T04:36:33Z",
    "link": "http://arxiv.org/pdf/2505.06698v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zongqi Wang",
      "Tianle Gu",
      "Chen Gong",
      "Xin Tian",
      "Siqi Bao",
      "Yujiu Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01479v3",
    "title": "Deliberate Planning in Language Models with Symbolic Representation",
    "summary": "Planning remains a core challenge for large language models (LLMs),\nparticularly in domains that require coherent multi-step action sequences\ngrounded in external constraints. We introduce SymPlanner, a novel framework\nthat equips LLMs with structured planning capabilities by interfacing them with\na symbolic environment that serves as an explicit world model. Rather than\nrelying purely on natural language reasoning, SymPlanner grounds the planning\nprocess in a symbolic state space, where a policy model proposes actions and a\nsymbolic environment deterministically executes and verifies their effects. To\nenhance exploration and improve robustness, we introduce Iterative Correction\n(IC), which refines previously proposed actions by leveraging feedback from the\nsymbolic environment to eliminate invalid decisions and guide the model toward\nvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained\ncomparison of candidate plans by evaluating them jointly. Conceptually,\nSymPlanner operationalizes two cognitive faculties: (i) error monitoring and\nrepair via externalized feedback (IC) and (ii) preference formation among\nalternatives via pairwise comparison (CR), advancing cognitively plausible,\nsymbol-grounded planning aligned with the rich structure in intelligent\nsystems. We evaluate SymPlanner on PlanBench, demonstrating that it produces\nmore coherent, diverse, and verifiable plans than pure natural language\nbaselines.",
    "published": "2025-05-02T15:18:03Z",
    "updated": "2025-10-06T04:14:44Z",
    "link": "http://arxiv.org/pdf/2505.01479v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Siheng Xiong",
      "Zhangding Liu",
      "Jieyu Zhou",
      "Yusen Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07468v3",
    "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for\n  Safer Language Models",
    "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).",
    "published": "2025-06-09T06:35:12Z",
    "updated": "2025-10-06T03:42:10Z",
    "link": "http://arxiv.org/pdf/2506.07468v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.MA"
    ],
    "authors": [
      "Mickel Liu",
      "Liwei Jiang",
      "Yancheng Liang",
      "Simon Shaolei Du",
      "Yejin Choi",
      "Tim Althoff",
      "Natasha Jaques"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04454v1",
    "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning\n  Yields Stronger Reasoners",
    "summary": "Large Language Models (LLMs) show strong reasoning abilities, often amplified\nby Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although\nRL algorithms can substantially improve reasoning, they struggle to expand\nreasoning boundaries because they learn from their own reasoning trajectories\nrather than acquiring external knowledge. Supervised fine-tuning (SFT) offers\ncomplementary benefits but typically requires large-scale data and risks\noverfitting. Recent attempts to combine SFT and RL face three main challenges:\ndata inefficiency, algorithm-specific designs, and catastrophic forgetting. We\npropose a plug-and-play framework that dynamically integrates SFT into RL by\nselecting challenging examples for SFT. This approach reduces SFT data\nrequirements and remains agnostic to the choice of RL or SFT algorithm. To\nmitigate catastrophic forgetting of RL-acquired skills during SFT, we select\nhigh-entropy tokens for loss calculation and freeze parameters identified as\ncritical for RL. Our method achieves state-of-the-art (SoTA) reasoning\nperformance using only 1.5% of the SFT data and 20.4% of the RL data used by\nprior SoTA, providing an efficient and plug-and-play solution for combining SFT\nand RL in reasoning post-training.",
    "published": "2025-10-06T03:01:14Z",
    "updated": "2025-10-06T03:01:14Z",
    "link": "http://arxiv.org/pdf/2510.04454v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xiangchi Yuan",
      "Xiang Chen",
      "Tong Yu",
      "Dachuan Shi",
      "Can Jin",
      "Wenke Lee",
      "Saayan Mitra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02657v2",
    "title": "Less LLM, More Documents: Searching for Improved RAG",
    "summary": "Retrieval-Augmented Generation (RAG) couples document retrieval with large\nlanguage models (LLMs). While scaling generators improves accuracy, it also\nraises cost and limits deployability. We explore an orthogonal axis: enlarging\nthe retriever's corpus to reduce reliance on large LLMs. Experimental results\nshow that corpus scaling consistently strengthens RAG and can often serve as a\nsubstitute for increasing model size, though with diminishing returns at larger\nscales. Small- and mid-sized generators paired with larger corpora often rival\nmuch larger models with smaller corpora; mid-sized models tend to gain the\nmost, while tiny and large models benefit less. Our analysis shows that\nimprovements arise primarily from increased coverage of answer-bearing\npassages, while utilization efficiency remains largely unchanged. These\nfindings establish a principled corpus-generator trade-off: investing in larger\ncorpora offers an effective path to stronger RAG, often comparable to enlarging\nthe LLM itself.",
    "published": "2025-10-03T01:26:13Z",
    "updated": "2025-10-06T02:54:21Z",
    "link": "http://arxiv.org/pdf/2510.02657v2.pdf",
    "category": [
      "cs.IR",
      "cs.CL",
      "H.3.3; I.2.7"
    ],
    "authors": [
      "Jingjie Ning",
      "Yibo Kong",
      "Yunfan Long",
      "Jamie Callan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04439v1",
    "title": "On the Role of Unobserved Sequences on Sample-based Uncertainty\n  Quantification for LLMs",
    "summary": "Quantifying uncertainty in large language models (LLMs) is important for\nsafety-critical applications because it helps spot incorrect answers, known as\nhallucinations. One major trend of uncertainty quantification methods is based\non estimating the entropy of the distribution of the LLM's potential output\nsequences. This estimation is based on a set of output sequences and associated\nprobabilities obtained by querying the LLM several times. In this paper, we\nadvocate and experimentally show that the probability of unobserved sequences\nplays a crucial role, and we recommend future research to integrate it to\nenhance such LLM uncertainty quantification methods.",
    "published": "2025-10-06T02:14:48Z",
    "updated": "2025-10-06T02:14:48Z",
    "link": "http://arxiv.org/pdf/2510.04439v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lucie Kunitomo-Jacquin",
      "Edison Marrese-Taylor",
      "Ken Fukuda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04434v1",
    "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
    "summary": "The social impact of Natural Language Processing (NLP) is increasingly\nimportant, with a rising community focus on initiatives related to NLP for\nSocial Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the\nACL Anthology address topics related to social good as defined by the UN\nSustainable Development Goals (Adauto et al., 2023). In this study, we take an\nauthor- and venue-level perspective to map the landscape of NLP4SG, quantifying\nthe proportion of work addressing social good concerns both within and beyond\nthe ACL community, by both core ACL contributors and non-ACL authors. With this\napproach we discover two surprising facts about the landscape of NLP4SG. First,\nACL authors are dramatically more likely to do work addressing social good\nconcerns when publishing in venues outside of ACL. Second, the vast majority of\npublications using NLP techniques to address concerns of social good are done\nby non-ACL authors in venues outside of ACL. We discuss the implications of\nthese findings on agenda-setting considerations for the ACL community related\nto NLP4SG.",
    "published": "2025-10-06T02:04:42Z",
    "updated": "2025-10-06T02:04:42Z",
    "link": "http://arxiv.org/pdf/2510.04434v1.pdf",
    "category": [
      "cs.CL",
      "cs.SI"
    ],
    "authors": [
      "Grace LeFevre",
      "Qingcheng Zeng",
      "Adam Leif",
      "Jason Jewell",
      "Denis Peskoff",
      "Rob Voigt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.13118v2",
    "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with\n  Retrieval-Augmented Generation",
    "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.",
    "published": "2025-08-18T17:22:51Z",
    "updated": "2025-10-06T01:53:26Z",
    "link": "http://arxiv.org/pdf/2508.13118v2.pdf",
    "category": [
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Zefang Liu",
      "Arman Anwar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18709v2",
    "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle\n  Generation in LLMs",
    "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.",
    "published": "2025-08-26T06:21:45Z",
    "updated": "2025-10-05T23:38:18Z",
    "link": "http://arxiv.org/pdf/2508.18709v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Duy Le",
      "Kent Ziti",
      "Evan Girard-Sun",
      "Bakr Bouhaya",
      "Sean O'Brien",
      "Vasu Sharma",
      "Kevin Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04394v1",
    "title": "Time Is Effort: Estimating Human Post-Editing Time for Grammar Error\n  Correction Tool Evaluation",
    "summary": "Text editing can involve several iterations of revision. Incorporating an\nefficient Grammar Error Correction (GEC) tool in the initial correction round\ncan significantly impact further human editing effort and final text quality.\nThis raises an interesting question to quantify GEC Tool usability: How much\neffort can the GEC Tool save users? We present the first large-scale dataset of\npost-editing (PE) time annotations and corrections for two English GEC test\ndatasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)\nfor GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by\nestimating PE time-to-correct. Using our dataset, we quantify the amount of\ntime saved by GEC Tools in text editing. Analyzing the edit type indicated that\ndetermining whether a sentence needs correction and edits like paraphrasing and\npunctuation changes had the greatest impact on PE time. Finally, comparison\nwith human rankings shows that PEET correlates well with technical effort\njudgment, providing a new human-centric direction for evaluating GEC tool\nusability. We release our dataset and code at:\nhttps://github.com/ankitvad/PEET_Scorer.",
    "published": "2025-10-05T23:24:24Z",
    "updated": "2025-10-05T23:24:24Z",
    "link": "http://arxiv.org/pdf/2510.04394v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Ankit Vadehra",
      "Bill Johnson",
      "Gene Saunders",
      "Pascal Poupart"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.16889v3",
    "title": "ObjexMT: Objective Extraction and Metacognitive Calibration for\n  LLM-as-a-Judge under Multi-Turn Jailbreaks",
    "summary": "LLM-as-a-Judge (LLMaaJ) now underpins scalable evaluation, yet we lack a\ndecisive test of a judge's qualification: can it recover a conversation's\nlatent objective and know when that inference is trustworthy? LLMs degrade\nunder irrelevant or long context; multi-turn jailbreaks further hide goals\nacross turns. We introduce ObjexMT, a benchmark for objective extraction and\nmetacognition. Given a multi-turn transcript, a model must return a\none-sentence base objective and self-reported confidence. Accuracy is computed\nvia LLM-judge semantic similarity to gold objectives, converted to binary\ncorrectness by a human-aligned threshold calibrated on N=300 items (tau = 0.66;\nF1 = 0.891). Metacognition is evaluated with ECE, Brier, Wrong at\nHigh-Confidence (0.80/0.90/0.95), and risk-coverage. Across six models\n(gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1,\ngemini-2.5-flash) on three datasets, kimi-k2 attains the highest\nobjective-extraction accuracy (0.612), with claude-sonnet-4 (0.603) and\ndeepseek-v3.1 (0.599) statistically comparable. claude-sonnet-4 yields the best\nselective risk and calibration (AURC 0.242; ECE 0.206; Brier 0.254). Dataset\nheterogeneity (16-82 percent accuracy variance) reveals that automated\nobfuscation poses fundamental challenges beyond model choice. High-confidence\nerrors persist: Wrong at 0.90 ranges from 14.9 percent (claude-sonnet-4) to\n47.7 percent (Qwen3-235B-A22B-FP8). ObjexMT provides an actionable test for LLM\njudges: when objectives are not explicit, judges often misinfer them; we\nrecommend exposing objectives when feasible and gating decisions by confidence\notherwise. Data at https://github.com/hyunjun1121/ObjexMT_dataset.",
    "published": "2025-08-23T03:32:04Z",
    "updated": "2025-10-05T22:27:27Z",
    "link": "http://arxiv.org/pdf/2508.16889v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hyunjun Kim",
      "Junwoo Ha",
      "Sangyoon Yu",
      "Haon Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14051v6",
    "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
    "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
    "published": "2025-04-18T19:46:54Z",
    "updated": "2025-10-05T22:17:34Z",
    "link": "http://arxiv.org/pdf/2504.14051v6.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Raghavv Goel",
      "Junyoung Park",
      "Mukul Gagrani",
      "Dalton Jones",
      "Matthew Morse",
      "Harper Langston",
      "Mingu Lee",
      "Chris Lott"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07421v2",
    "title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery",
    "summary": "We introduce AgentAda, the first LLM-powered analytics agent that can learn\nand use new analytics skills to extract more specialized insights. Unlike\nexisting methods that require users to manually decide which data analytics\nmethod to apply, AgentAda automatically identifies the skill needed from a\nlibrary of analytical skills to perform the analysis. This also allows AgentAda\nto use skills that existing LLMs cannot perform out of the box. The library\ncovers a range of methods, including clustering, predictive modeling, and NLP\ntechniques like BERT, which allow AgentAda to handle complex analytics tasks\nbased on what the user needs. AgentAda's dataset-to-insight extraction strategy\nconsists of three key steps: (I) a question generator to generate queries\nrelevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented\nGeneration (RAG)-based skill matcher to choose the best data analytics skill\nfrom the skill library, and (III) a code generator that produces executable\ncode based on the retrieved skill's documentation to extract key patterns. We\nalso introduce KaggleBench, a benchmark of curated notebooks across diverse\ndomains, to evaluate AgentAda's performance. We conducted a human evaluation\ndemonstrating that AgentAda provides more insightful analytics than existing\ntools, with 48.78% of evaluators preferring its analyses, compared to 27.67%\nfor the unskilled agent. We also propose a novel LLM-as-a-judge approach that\nwe show is aligned with human evaluation as a way to automate insight quality\nevaluation at larger scale.",
    "published": "2025-04-10T03:27:25Z",
    "updated": "2025-10-05T21:28:53Z",
    "link": "http://arxiv.org/pdf/2504.07421v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Amirhossein Abaskohi",
      "Amrutha Varshini Ramesh",
      "Shailesh Nanisetty",
      "Chirag Goel",
      "David Vazquez",
      "Christopher Pal",
      "Spandana Gella",
      "Giuseppe Carenini",
      "Issam H. Laradji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04347v1",
    "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention\n  Anomaly Scoring for Pre-trained Language Models",
    "summary": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense.",
    "published": "2025-10-05T20:15:56Z",
    "updated": "2025-10-05T20:15:56Z",
    "link": "http://arxiv.org/pdf/2510.04347v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Anindya Sundar Das",
      "Kangjie Chen",
      "Monowar Bhuyan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04338v1",
    "title": "Evaluation of Clinical Trials Reporting Quality using Large Language\n  Models",
    "summary": "Reporting quality is an important topic in clinical trial research articles,\nas it can impact clinical decisions. In this article, we test the ability of\nlarge language models to assess the reporting quality of this type of article\nusing the Consolidated Standards of Reporting Trials (CONSORT). We create\nCONSORT-QA, an evaluation corpus from two studies on abstract reporting quality\nwith CONSORT-abstract standards. We then evaluate the ability of different\nlarge generative language models (from the general domain or adapted to the\nbiomedical domain) to correctly assess CONSORT criteria with different known\nprompting methods, including Chain-of-thought. Our best combination of model\nand prompting method achieves 85% accuracy. Using Chain-of-thought adds\nvaluable information on the model's reasoning for completing the task.",
    "published": "2025-10-05T20:01:28Z",
    "updated": "2025-10-05T20:01:28Z",
    "link": "http://arxiv.org/pdf/2510.04338v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Mathieu Laï-king",
      "Patrick Paroubek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04320v1",
    "title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
    "summary": "Safety-aligned Large Language Models (LLMs) still show two dominant failure\nmodes: they are easily jailbroken, or they over-refuse harmless inputs that\ncontain sensitive surface signals. We trace both to a common cause: current\nmodels reason weakly about links between actions and outcomes and over-rely on\nsurface-form signals, lexical or stylistic cues that do not encode\nconsequences. We define this failure mode as Consequence-blindness. To study\nconsequence-blindness, we build a benchmark named CB-Bench covering four risk\nscenarios that vary whether semantic risk aligns with outcome risk, enabling\nevaluation under both matched and mismatched conditions which are often ignored\nby existing safety benchmarks. Mainstream models consistently fail to separate\nthese risks and exhibit consequence-blindness, indicating that\nconsequence-blindness is widespread and systematic. To mitigate\nconsequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning\ndataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains\nagainst semantic-camouflage jailbreaks and reduce over-refusal on harmless\ninputs, while maintaining utility and generalization on other benchmarks. These\nresults clarify the limits of current alignment, establish consequence-aware\nreasoning as a core alignment goal and provide a more practical and\nreproducible evaluation path.",
    "published": "2025-10-05T18:46:49Z",
    "updated": "2025-10-05T18:46:49Z",
    "link": "http://arxiv.org/pdf/2510.04320v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Rui Wu",
      "Yihao Quan",
      "Zeru Shi",
      "Zhenting Wang",
      "Yanshu Li",
      "Ruixiang Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.02591v3",
    "title": "Geometry of orofacial neuromuscular signals: speech articulation\n  decoding using surface electromyography",
    "summary": "Objective. In this article, we present data and methods for decoding speech\narticulations using surface electromyogram (EMG) signals. EMG-based speech\nneuroprostheses offer a promising approach for restoring audible speech in\nindividuals who have lost the ability to speak intelligibly due to\nlaryngectomy, neuromuscular diseases, stroke, or trauma-induced damage (e.g.,\nfrom radiotherapy) to the speech articulators.\n  Approach. To achieve this, we collect EMG signals from the face, jaw, and\nneck as subjects articulate speech, and we perform EMG-to-speech translation.\n  Main results. Our findings reveal that the manifold of symmetric positive\ndefinite (SPD) matrices serves as a natural embedding space for EMG signals.\nSpecifically, we provide an algebraic interpretation of the manifold-valued EMG\ndata using linear transformations, and we analyze and quantify distribution\nshifts in EMG signals across individuals.\n  Significance. Overall, our approach demonstrates significant potential for\ndeveloping neural networks that are both data- and parameter-efficient, an\nimportant consideration for EMG-based systems, which face challenges in\nlarge-scale data collection and operate under limited computational resources\non embedded devices.",
    "published": "2024-11-04T20:31:22Z",
    "updated": "2025-10-05T18:45:15Z",
    "link": "http://arxiv.org/pdf/2411.02591v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Harshavardhana T. Gowda",
      "Zachary D. McNaughton",
      "Lee M. Miller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05097v1",
    "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation",
    "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.",
    "published": "2025-10-06T17:58:34Z",
    "updated": "2025-10-06T17:58:34Z",
    "link": "http://arxiv.org/pdf/2510.05097v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Robin Courant",
      "Xi Wang",
      "David Loiseaux",
      "Marc Christie",
      "Vicky Kalogeiton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05094v1",
    "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
    "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
    "published": "2025-10-06T17:57:59Z",
    "updated": "2025-10-06T17:57:59Z",
    "link": "http://arxiv.org/pdf/2510.05094v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziqi Huang",
      "Ning Yu",
      "Gordon Chen",
      "Haonan Qiu",
      "Paul Debevec",
      "Ziwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05093v1",
    "title": "Character Mixing for Video Generation",
    "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.",
    "published": "2025-10-06T17:57:39Z",
    "updated": "2025-10-06T17:57:39Z",
    "link": "http://arxiv.org/pdf/2510.05093v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tingting Liao",
      "Chongjian Ge",
      "Guangyi Liu",
      "Hao Li",
      "Yi Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05091v1",
    "title": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals",
    "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
    "published": "2025-10-06T17:56:55Z",
    "updated": "2025-10-06T17:56:55Z",
    "link": "http://arxiv.org/pdf/2510.05091v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Le Zhuo",
      "Songhao Han",
      "Yuandong Pu",
      "Boxiang Qiu",
      "Sayak Paul",
      "Yue Liao",
      "Yihao Liu",
      "Jie Shao",
      "Xi Chen",
      "Si Liu",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.06867v2",
    "title": "Conformal Prediction for Long-Tailed Classification",
    "summary": "Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we introduce a new conformal score function\ncalled prevalence-adjusted softmax that targets macro-coverage, a relaxed\nnotion of class-conditional coverage. Second, we propose a new procedure that\ninterpolates between marginal and class-conditional conformal prediction by\nlinearly interpolating their conformal score thresholds. We demonstrate our\nmethods on Pl@ntNet-300K and iNaturalist-2018, two long-tailed image datasets\nwith 1,081 and 8,142 classes, respectively.",
    "published": "2025-07-09T14:08:50Z",
    "updated": "2025-10-06T17:52:49Z",
    "link": "http://arxiv.org/pdf/2507.06867v2.pdf",
    "category": [
      "stat.ML",
      "cs.CV",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Tiffany Ding",
      "Jean-Baptiste Fermanian",
      "Joseph Salmon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05071v1",
    "title": "Neuroplastic Modular Framework: Cross-Domain Image Classification of\n  Garbage and Industrial Surfaces",
    "summary": "Efficient and accurate classification of waste and industrial surface defects\nis essential for ensuring sustainable waste management and maintaining high\nstandards in quality control. This paper introduces the Neuroplastic Modular\nClassifier, a novel hybrid architecture designed for robust and adaptive image\nclassification in dynamic environments. The model combines a ResNet-50 backbone\nfor localized feature extraction with a Vision Transformer (ViT) to capture\nglobal semantic context. Additionally, FAISS-based similarity retrieval is\nincorporated to provide a memory-like reference to previously encountered data,\nenriching the model's feature space. A key innovation of our architecture is\nthe neuroplastic modular design composed of expandable, learnable blocks that\ndynamically grow during training when performance plateaus. Inspired by\nbiological learning systems, this mechanism allows the model to adapt to data\ncomplexity over time, improving generalization. Beyond garbage classification,\nwe validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),\nwhich involves industrial defect detection on metal surfaces. Experimental\nresults across domains show that the proposed architecture outperforms\ntraditional static models in both accuracy and adaptability. The Neuroplastic\nModular Classifier offers a scalable, high-performance solution for real-world\nimage classification, with strong applicability in both environmental and\nindustrial domains.",
    "published": "2025-10-06T17:47:45Z",
    "updated": "2025-10-06T17:47:45Z",
    "link": "http://arxiv.org/pdf/2510.05071v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Debojyoti Ghosh",
      "Soumya K Ghosh",
      "Adrijit Goswami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02282v2",
    "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning\n  MLLMs and RL",
    "summary": "With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.",
    "published": "2025-10-02T17:55:37Z",
    "updated": "2025-10-06T17:39:06Z",
    "link": "http://arxiv.org/pdf/2510.02282v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Kyoungjun Park",
      "Yifan Yang",
      "Juheon Yi",
      "Shicheng Zheng",
      "Yifei Shen",
      "Dongqi Han",
      "Caihua Shan",
      "Muhammad Muaz",
      "Lili Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05057v1",
    "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
    "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
    "published": "2025-10-06T17:37:24Z",
    "updated": "2025-10-06T17:37:24Z",
    "link": "http://arxiv.org/pdf/2510.05057v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Mingyu Liu",
      "Jiuhe Shu",
      "Hui Chen",
      "Zeju Li",
      "Canyu Zhao",
      "Jiange Yang",
      "Shenyuan Gao",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05053v1",
    "title": "No-reference Quality Assessment of Contrast-distorted Images using\n  Contrast-enhanced Pseudo Reference",
    "summary": "Contrast change is an important factor that affects the quality of images.\nDuring image capturing, unfavorable lighting conditions can cause contrast\nchange and visual quality loss. While various methods have been proposed to\nassess the quality of images under different distortions such as blur and\nnoise, contrast distortion has been largely overlooked as its visual impact and\nproperties are different from other conventional types of distortions. In this\npaper, we propose a no-reference image quality assessment (NR-IQA) metric for\ncontrast-distorted images. Using a set of contrast enhancement algorithms, we\naim to generate pseudo-reference images that are visually close to the actual\nreference image, such that the NR problem is transformed to a Full-reference\n(FR) assessment with higher accuracy. To this end, a large dataset of\ncontrast-enhanced images is produced to train a classification network that can\nselect the most suitable contrast enhancement algorithm based on image content\nand distortion for pseudo-reference image generation. Finally, the evaluation\nis performed in the FR manner to assess the quality difference between the\ncontrast-enhanced (pseudoreference) and degraded images. Performance evaluation\nof the proposed method on three databases containing contrast distortions\n(CCID2014, TID2013, and CSIQ), indicates the promising performance of the\nproposed method.",
    "published": "2025-10-06T17:32:48Z",
    "updated": "2025-10-06T17:32:48Z",
    "link": "http://arxiv.org/pdf/2510.05053v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohammad-Ali Mahmoudpour",
      "Saeed Mahmoudpour"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05051v1",
    "title": "SegMASt3R: Geometry Grounded Segment Matching",
    "summary": "Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance segmentation and image-goal\nnavigation. Project Page: https://segmast3r.github.io/",
    "published": "2025-10-06T17:31:32Z",
    "updated": "2025-10-06T17:31:32Z",
    "link": "http://arxiv.org/pdf/2510.05051v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rohit Jayanti",
      "Swayam Agrawal",
      "Vansh Garg",
      "Siddharth Tourani",
      "Muhammad Haris Khan",
      "Sourav Garg",
      "Madhava Krishna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.10525v3",
    "title": "RowDetr: End-to-End Crop Row Detection Using Polynomials",
    "summary": "Crop row detection enables autonomous robots to navigate in gps denied\nenvironments. Vision based strategies often struggle in the environments due to\ngaps, curved crop rows and require post-processing steps. Furthermore, labeling\ncrop rows in under the canopy environments accurately is very difficult due to\nocclusions. This study introduces RowDetr, an efficient end-to-end\ntransformer-based neural network for crop row detection in precision\nagriculture. RowDetr leverages a lightweight backbone and a hybrid encoder to\nmodel straight, curved, or occluded crop rows with high precision. Central to\nthe architecture is a novel polynomial representation that enables direct\nparameterization of crop rows, eliminating computationally expensive\npost-processing. Key innovations include a PolySampler module and multi-scale\ndeformable attention, which work together with PolyOptLoss, an energy-based\nloss function designed to optimize geometric alignment between predicted and\nthe annotated crop rows, while also enhancing robustness against labeling\nnoise. RowDetr was evaluated against other state-of-the-art end-to-end crop row\ndetection methods like AgroNav and RolColAttention on a diverse dataset of\n6,962 high-resolution images, used for training, validation, and testing across\nmultiple crop types with annotated crop rows. The system demonstrated superior\nperformance, achieved an F1 score up to 0.74 and a lane position deviation as\nlow as 0.405. Furthermore, RowDetr achieves a real-time inference latency of\n6.7ms, which was optimized to 3.5ms with INT8 quantization on an NVIDIA Jetson\nOrin AGX. This work highlighted the critical efficiency of polynomial\nparameterization, making RowDetr particularly suitable for deployment on edge\ncomputing devices in agricultural robotics and autonomous farming equipment.\nIndex terms > Crop Row Detection, Under Canopy Navigation, Transformers,\nRT-DETR, RT-DETRv2",
    "published": "2024-12-13T19:38:36Z",
    "updated": "2025-10-06T17:12:59Z",
    "link": "http://arxiv.org/pdf/2412.10525v3.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Rahul Harsha Cheppally",
      "Ajay Sharda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05034v1",
    "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
    "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
    "published": "2025-10-06T17:10:44Z",
    "updated": "2025-10-06T17:10:44Z",
    "link": "http://arxiv.org/pdf/2510.05034v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yunlong Tang",
      "Jing Bi",
      "Pinxin Liu",
      "Zhenyu Pan",
      "Zhangyun Tan",
      "Qianxiang Shen",
      "Jiani Liu",
      "Hang Hua",
      "Junjia Guo",
      "Yunzhong Xiao",
      "Chao Huang",
      "Zhiyuan Wang",
      "Susan Liang",
      "Xinyi Liu",
      "Yizhi Song",
      "Yuhe Nie",
      "Jia-Xing Zhong",
      "Bozheng Li",
      "Daiqing Qi",
      "Ziyun Zeng",
      "Ali Vosoughi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Daiki Shimada",
      "Han Liu",
      "Jiebo Luo",
      "Chenliang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.18804v3",
    "title": "Fast constrained sampling in pre-trained diffusion models",
    "summary": "Large denoising diffusion models, such as Stable Diffusion, have been trained\non billions of image-caption pairs to perform text-conditioned image\ngeneration. As a byproduct of this training, these models have acquired general\nknowledge about image statistics, which can be useful for other inference\ntasks. However, when confronted with sampling an image under new constraints,\ne.g. generating the missing parts of an image, using large pre-trained\ntext-to-image diffusion models is inefficient and often unreliable. Previous\napproaches either utilized backpropagation through the denoiser network, making\nthem significantly slower and more memory-demanding than simple text-to-image\ngeneration, or only enforced the constraint locally, failing to capture\ncritical long-range correlations in the sampled image. In this work, we propose\nan algorithm that enables fast, high-quality generation under arbitrary\nconstraints. We show that in denoising diffusion models, we can employ an\napproximation to Newton's optimization method that allows us to speed up\ninference and avoid the expensive backpropagation operations. Our approach\nproduces results that rival or surpass the state-of-the-art training-free\ninference methods while requiring a fraction of the time. We demonstrate the\neffectiveness of our algorithm under both linear (inpainting, super-resolution)\nand non-linear (style-guided generation) constraints. An implementation is\nprovided at https://github.com/cvlab-stonybrook/fast-constrained-sampling.",
    "published": "2024-10-24T14:52:38Z",
    "updated": "2025-10-06T16:59:09Z",
    "link": "http://arxiv.org/pdf/2410.18804v3.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Alexandros Graikos",
      "Nebojsa Jojic",
      "Dimitris Samaras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05015v1",
    "title": "Exploring the Efficacy of Modified Transfer Learning in Identifying\n  Parkinson's Disease Through Drawn Image Patterns",
    "summary": "Parkinson's disease (PD) is a progressive neurodegenerative condition\ncharacterized by the death of dopaminergic neurons, leading to various movement\ndisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,\nyet traditional diagnostic methods are often cumbersome and costly. In this\nstudy, a machine learning-based approach is proposed using hand-drawn spiral\nand wave images as potential biomarkers for PD detection. Our methodology\nleverages convolutional neural networks (CNNs), transfer learning, and\nattention mechanisms to improve model performance and resilience against\noverfitting. To enhance the diversity and richness of both spiral and wave\ncategories, the training dataset undergoes augmentation to increase the number\nof images. The proposed architecture comprises three phases: utilizing\npre-trained CNNs, incorporating custom convolutional layers, and ensemble\nvoting. Employing hard voting further enhances performance by aggregating\npredictions from multiple models. Experimental results show promising accuracy\nrates. For spiral images, weighted average precision, recall, and F1-score are\n90%, and for wave images, they are 96.67%. After combining the predictions\nthrough ensemble hard voting, the overall accuracy is 93.3%. These findings\nunderscore the potential of machine learning in early PD diagnosis, offering a\nnon-invasive and cost-effective solution to improve patient outcomes.",
    "published": "2025-10-06T16:55:07Z",
    "updated": "2025-10-06T16:55:07Z",
    "link": "http://arxiv.org/pdf/2510.05015v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nabil Daiyan",
      "Md Rakibul Haque"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05006v1",
    "title": "Latent Uncertainty Representations for Video-based Driver Action and\n  Intention Recognition",
    "summary": "Deep neural networks (DNNs) are increasingly applied to safety-critical tasks\nin resource-constrained environments, such as video-based driver action and\nintention recognition. While last layer probabilistic deep learning (LL-PDL)\nmethods can detect out-of-distribution (OOD) instances, their performance\nvaries. As an alternative to last layer approaches, we propose extending\npre-trained DNNs with transformation layers to produce multiple latent\nrepresentations to estimate the uncertainty. We evaluate our latent uncertainty\nrepresentation (LUR) and repulsively trained LUR (RLUR) approaches against\neight PDL methods across four video-based driver action and intention\nrecognition datasets, comparing classification performance, calibration, and\nuncertainty-based OOD detection. We also contribute 28,000 frame-level action\nlabels and 1,194 video-level intention labels for the NuScenes dataset. Our\nresults show that LUR and RLUR achieve comparable in-distribution\nclassification performance to other LL-PDL approaches. For uncertainty-based\nOOD detection, LUR matches top-performing PDL methods while being more\nefficient to train and easier to tune than approaches that require Markov-Chain\nMonte Carlo sampling or repulsive training procedures.",
    "published": "2025-10-06T16:50:02Z",
    "updated": "2025-10-06T16:50:02Z",
    "link": "http://arxiv.org/pdf/2510.05006v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Koen Vellenga",
      "H. Joe Steinhauer",
      "Jonas Andersson",
      "Anders Sjögren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.13298v2",
    "title": "QDFlow: A Python package for physics simulations of quantum dot devices",
    "summary": "Recent advances in machine learning (ML) have accelerated progress in\ncalibrating and operating quantum dot (QD) devices. However, most ML approaches\nrely on access to large, representative datasets designed to capture the full\nspectrum of data quality encountered in practice, with both high- and\nlow-quality data for training, benchmarking, and validation, with labels\ncapturing key features of the device state. Collating such datasets\nexperimentally is challenging due to limited data availability, slow\nmeasurement bandwidths, and the labor-intensive nature of labeling. QDFlow is\nan open-source physics simulator for multi-QD arrays that generates realistic\nsynthetic data with ground-truth labels. QDFlow combines a self-consistent\nThomas-Fermi solver, a dynamic capacitance model, and flexible noise modules to\nsimulate charge stability diagrams and ray-based data closely resembling\nexperiments. With an extensive set of parameters that can be varied and\ncustomizable noise models, QDFlow supports the creation of large, diverse\ndatasets for ML development, benchmarking, and quantum device research.",
    "published": "2025-09-16T17:54:25Z",
    "updated": "2025-10-06T16:40:26Z",
    "link": "http://arxiv.org/pdf/2509.13298v2.pdf",
    "category": [
      "cond-mat.mes-hall",
      "cs.CV",
      "cs.LG",
      "quant-ph"
    ],
    "authors": [
      "Donovan L. Buterakos",
      "Sandesh S. Kalantre",
      "Joshua Ziegler",
      "Jacob M Taylor",
      "Justyna P. Zwolak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05193v3",
    "title": "RAM-W600: A Multi-Task Wrist Dataset and Benchmark for Rheumatoid\n  Arthritis",
    "summary": "Rheumatoid arthritis (RA) is a common autoimmune disease that has been the\nfocus of research in computer-aided diagnosis (CAD) and disease monitoring. In\nclinical settings, conventional radiography (CR) is widely used for the\nscreening and evaluation of RA due to its low cost and accessibility. The wrist\nis a critical region for the diagnosis of RA. However, CAD research in this\narea remains limited, primarily due to the challenges in acquiring high-quality\ninstance-level annotations. (i) The wrist comprises numerous small bones with\nnarrow joint spaces, complex structures, and frequent overlaps, requiring\ndetailed anatomical knowledge for accurate annotation. (ii) Disease progression\nin RA often leads to osteophyte, bone erosion (BE), and even bony ankylosis,\nwhich alter bone morphology and increase annotation difficulty, necessitating\nexpertise in rheumatology. This work presents a multi-task dataset for wrist\nbone in CR, including two tasks: (i) wrist bone instance segmentation and (ii)\nSharp/van der Heijde (SvdH) BE scoring, which is the first public resource for\nwrist bone instance segmentation. This dataset comprises 1048 wrist\nconventional radiographs of 388 patients from six medical centers, with\npixel-level instance segmentation annotations for 618 images and SvdH BE scores\nfor 800 images. This dataset can potentially support a wide range of research\ntasks related to RA, including joint space narrowing (JSN) progression\nquantification, BE detection, bone deformity evaluation, and osteophyte\ndetection. It may also be applied to other wrist-related tasks, such as carpal\nbone fracture localization. We hope this dataset will significantly lower the\nbarrier to research on wrist RA and accelerate progress in CAD research within\nthe RA-related domain.",
    "published": "2025-07-07T16:53:22Z",
    "updated": "2025-10-06T15:58:40Z",
    "link": "http://arxiv.org/pdf/2507.05193v3.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Songxiao Yang",
      "Haolin Wang",
      "Yao Fu",
      "Ye Tian",
      "Tamotsu Kamishima",
      "Masayuki Ikebe",
      "Yafei Ou",
      "Masatoshi Okutomi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04961v1",
    "title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization",
    "summary": "Tokenizers are a key component of state-of-the-art generative image models,\nextracting the most important features from the signal while reducing data\ndimension and redundancy. Most current tokenizers are based on KL-regularized\nvariational autoencoders (KL-VAE), trained with reconstruction, perceptual and\nadversarial losses. Diffusion decoders have been proposed as a more principled\nalternative to model the distribution over images conditioned on the latent.\nHowever, matching the performance of KL-VAE still requires adversarial losses,\nas well as a higher decoding time due to iterative sampling. To address these\nlimitations, we introduce a new pixel diffusion decoder architecture for\nimproved scaling and training stability, benefiting from transformer components\nand GAN-free training. We use distillation to replicate the performance of the\ndiffusion decoder in an efficient single-step decoder. This makes SSDD the\nfirst diffusion decoder optimized for single-step reconstruction trained\nwithout adversarial losses, reaching higher reconstruction quality and faster\nsampling than KL-VAE. In particular, SSDD improves reconstruction FID from\n$0.87$ to $0.50$ with $1.4\\times$ higher throughput and preserve generation\nquality of DiTs with $3.8\\times$ faster sampling. As such, SSDD can be used as\na drop-in replacement for KL-VAE, and for building higher-quality and faster\ngenerative models.",
    "published": "2025-10-06T15:57:31Z",
    "updated": "2025-10-06T15:57:31Z",
    "link": "http://arxiv.org/pdf/2510.04961v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Théophane Vallaeys",
      "Jakob Verbeek",
      "Matthieu Cord"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22229v2",
    "title": "A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised\n  Domain Adaptation",
    "summary": "Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic\nchallenge of adapting a source-trained model to a target domain without access\nto the source data, driven by concerns over privacy and cost. Existing SFUDA\nmethods either exploit only the source model's predictions or fine-tune large\nmultimodal models, yet both neglect complementary insights and the latent\nstructure of target data. In this paper, we propose the Experts Cooperative\nLearning (EXCL). EXCL contains the Dual Experts framework and\nRetrieval-Augmentation-Interaction optimization pipeline. The Dual Experts\nframework places a frozen source-domain model (augmented with Conv-Adapter) and\na pretrained vision-language model (with a trainable text prompt) on equal\nfooting to mine consensus knowledge from unlabeled target samples. To\neffectively train these plug-in modules under purely unsupervised conditions,\nwe introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that\n(1) collaboratively retrieves pseudo-source and complex target samples, (2)\nseparately fine-tunes each expert on its respective sample set, and (3)\nenforces learning object consistency via a shared learning result. Extensive\nexperiments on four benchmark datasets demonstrate that our approach matches\nstate-of-the-art performance.",
    "published": "2025-09-26T11:39:50Z",
    "updated": "2025-10-06T15:55:42Z",
    "link": "http://arxiv.org/pdf/2509.22229v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaping Yu",
      "Muli Yang",
      "Jiapeng Ji",
      "Jiexi Yan",
      "Cheng Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04916v1",
    "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to\n  Classification of Remote Sensing Images",
    "summary": "Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.",
    "published": "2025-10-06T15:30:39Z",
    "updated": "2025-10-06T15:30:39Z",
    "link": "http://arxiv.org/pdf/2510.04916v1.pdf",
    "category": [
      "cs.CV",
      "I.4.6; I.4.8; I.4.10"
    ],
    "authors": [
      "Giulio Weikmann",
      "Gianmarco Perantoni",
      "Lorenzo Bruzzone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04912v1",
    "title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for\n  Motorbike Detection in Kigali Autonomous Driving Context",
    "summary": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda.",
    "published": "2025-10-06T15:26:08Z",
    "updated": "2025-10-06T15:26:08Z",
    "link": "http://arxiv.org/pdf/2510.04912v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Ngeyen Yinkfu",
      "Sunday Nwovu",
      "Jonathan Kayizzi",
      "Angelique Uwamahoro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11926v2",
    "title": "Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image\n  Interpolation with Guaranteed Initialization",
    "summary": "Conventional deep neural nets (DNNs) initialize network parameters at random\nand then optimize each one via stochastic gradient descent (SGD), resulting in\nsubstantial risk of poor-performing local minima.Focusing on the image\ninterpolation problem and leveraging a recent theorem that maps a\n(pseudo-)linear interpolator {\\Theta} to a directed graph filter that is a\nsolution to a MAP problem regularized with a graph shift variation (GSV) prior,\nwe first initialize a directed graph adjacency matrix A based on a known\ninterpolator {\\Theta}, establishing a baseline performance.Then, towards\nfurther gain, we learn perturbation matrices P and P(2) from data to augment A,\nwhose restoration effects are implemented via Douglas-Rachford (DR) iterations,\nwhich we unroll into a lightweight interpretable neural net.Experimental\nresults demonstrate state-of-the-art image interpolation results, while\ndrastically reducing network parameters.",
    "published": "2025-09-15T13:43:55Z",
    "updated": "2025-10-06T15:13:53Z",
    "link": "http://arxiv.org/pdf/2509.11926v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xue Zhang",
      "Bingshuo Hu",
      "Gene Cheung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04883v1",
    "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery",
    "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.",
    "published": "2025-10-06T15:04:56Z",
    "updated": "2025-10-06T15:04:56Z",
    "link": "http://arxiv.org/pdf/2510.04883v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Nathan Shankar",
      "Pawel Ladosz",
      "Hujun Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04876v1",
    "title": "BenthiCat: An opti-acoustic dataset for advancing benthic classification\n  and habitat mapping",
    "summary": "Benthic habitat mapping is fundamental for understanding marine ecosystems,\nguiding conservation efforts, and supporting sustainable resource management.\nYet, the scarcity of large, annotated datasets limits the development and\nbenchmarking of machine learning models in this domain. This paper introduces a\nthorough multi-modal dataset, comprising about a million side-scan sonar (SSS)\ntiles collected along the coast of Catalonia (Spain), complemented by\nbathymetric maps and a set of co-registered optical images from targeted\nsurveys using an autonomous underwater vehicle (AUV). Approximately \\num{36000}\nof the SSS tiles have been manually annotated with segmentation masks to enable\nsupervised fine-tuning of classification models. All the raw sensor data,\ntogether with mosaics, are also released to support further exploration and\nalgorithm development. To address challenges in multi-sensor data fusion for\nAUVs, we spatially associate optical images with corresponding SSS tiles,\nfacilitating self-supervised, cross-modal representation learning. Accompanying\nopen-source preprocessing and annotation tools are provided to enhance\naccessibility and encourage research. This resource aims to establish a\nstandardized benchmark for underwater habitat mapping, promoting advancements\nin autonomous seafloor classification and multi-sensor integration.",
    "published": "2025-10-06T15:00:20Z",
    "updated": "2025-10-06T15:00:20Z",
    "link": "http://arxiv.org/pdf/2510.04876v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "I.2.6; I.4.6; I.5.1; I.5.4"
    ],
    "authors": [
      "Hayat Rajani",
      "Valerio Franchi",
      "Borja Martinez-Clavel Valles",
      "Raimon Ramos",
      "Rafael Garcia",
      "Nuno Gracias"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.18142v4",
    "title": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual\n  Conversion in Visual Reasoning for Multimodal Large Language Models",
    "summary": "Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Our code and data are released at\nhttps://future-item.github.io/autoimagine-site/.",
    "published": "2024-11-27T08:44:25Z",
    "updated": "2025-10-06T14:56:54Z",
    "link": "http://arxiv.org/pdf/2411.18142v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingming Liu",
      "Yumeng Li",
      "Boyuan Xiao",
      "Yichang Jian",
      "Ziang Qin",
      "Tianjia Shao",
      "Yao-Xiang Ding",
      "Kun Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04864v1",
    "title": "In-Field Mapping of Grape Yield and Quality with Illumination-Invariant\n  Deep Learning",
    "summary": "This paper presents an end-to-end, IoT-enabled robotic system for the\nnon-destructive, real-time, and spatially-resolved mapping of grape yield and\nquality (Brix, Acidity) in vineyards. The system features a comprehensive\nanalytical pipeline that integrates two key modules: a high-performance model\nfor grape bunch detection and weight estimation, and a novel deep learning\nframework for quality assessment from hyperspectral (HSI) data. A critical\nbarrier to in-field HSI is the ``domain shift\" caused by variable illumination.\nTo overcome this, our quality assessment is powered by the Light-Invariant\nSpectral Autoencoder (LISA), a domain-adversarial framework that learns\nillumination-invariant features from uncalibrated data. We validated the\nsystem's robustness on a purpose-built HSI dataset spanning three distinct\nillumination domains: controlled artificial lighting (lab), and variable\nnatural sunlight captured in the morning and afternoon. Results show the\ncomplete pipeline achieves a recall (0.82) for bunch detection and a $R^2$\n(0.76) for weight prediction, while the LISA module improves quality prediction\ngeneralization by over 20% compared to the baselines. By combining these robust\nmodules, the system successfully generates high-resolution, georeferenced data\nof both grape yield and quality, providing actionable, data-driven insights for\nprecision viticulture.",
    "published": "2025-10-06T14:51:24Z",
    "updated": "2025-10-06T14:51:24Z",
    "link": "http://arxiv.org/pdf/2510.04864v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ciem Cornelissen",
      "Sander De Coninck",
      "Axel Willekens",
      "Sam Leroux",
      "Pieter Simoens"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04859v1",
    "title": "μDeepIQA: deep learning-based fast and robust image quality\n  assessment with local predictions for optical microscopy",
    "summary": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions.",
    "published": "2025-10-06T14:48:36Z",
    "updated": "2025-10-06T14:48:36Z",
    "link": "http://arxiv.org/pdf/2510.04859v1.pdf",
    "category": [
      "cs.CV",
      "physics.data-an",
      "q-bio.QM"
    ],
    "authors": [
      "Elena Corbetta",
      "Thomas Bocklitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04856v1",
    "title": "ERDE: Entropy-Regularized Distillation for Early-exit",
    "summary": "Although deep neural networks and in particular Convolutional Neural Networks\nhave demonstrated state-of-the-art performance in image classification with\nrelatively high efficiency, they still exhibit high computational costs, often\nrendering them impractical for real-time and edge applications. Therefore, a\nmultitude of compression techniques have been developed to reduce these costs\nwhile maintaining accuracy. In addition, dynamic architectures have been\nintroduced to modulate the level of compression at execution time, which is a\ndesirable property in many resource-limited application scenarios. The proposed\nmethod effectively integrates two well-established optimization techniques:\nearly exits and knowledge distillation, where a reduced student early-exit\nmodel is trained from a more complex teacher early-exit model. The primary\ncontribution of this research lies in the approach for training the student\nearly-exit model. In comparison to the conventional Knowledge Distillation\nloss, our approach incorporates a new entropy-based loss for images where the\nteacher's classification was incorrect. The proposed method optimizes the\ntrade-off between accuracy and efficiency, thereby achieving significant\nreductions in computational complexity without compromising classification\nperformance. The validity of this approach is substantiated by experimental\nresults on image classification datasets CIFAR10, CIFAR100 and SVHN, which\nfurther opens new research perspectives for Knowledge Distillation in other\ncontexts.",
    "published": "2025-10-06T14:45:41Z",
    "updated": "2025-10-06T14:45:41Z",
    "link": "http://arxiv.org/pdf/2510.04856v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Martial Guidez",
      "Stefan Duffner",
      "Yannick Alpou",
      "Oscar Röth",
      "Christophe Garcia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.07596v2",
    "title": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation",
    "summary": "Gender bias in vision-language foundation models (VLMs) raises concerns about\ntheir safe deployment and is typically evaluated using benchmarks with gender\nannotations on real-world images. However, as these benchmarks often contain\nspurious correlations between gender and non-gender features, such as objects\nand backgrounds, we identify a critical oversight in gender bias evaluation: Do\nspurious features distort gender bias evaluation? To address this question, we\nsystematically perturb non-gender features across four widely used benchmarks\n(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact\non bias evaluation. Our findings reveal that even minimal perturbations, such\nas masking just 10% of objects or weakly blurring backgrounds, can dramatically\nalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in\nCLIP variants. This suggests that current bias evaluations often reflect model\nresponses to spurious features rather than gender bias, undermining their\nreliability. Since creating spurious feature-free benchmarks is fundamentally\nchallenging, we recommend reporting bias metrics alongside feature-sensitivity\nmeasurements to enable a more reliable bias assessment.",
    "published": "2025-09-09T11:14:11Z",
    "updated": "2025-10-06T14:43:39Z",
    "link": "http://arxiv.org/pdf/2509.07596v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yusuke Hirota",
      "Ryo Hachiuma",
      "Boyi Li",
      "Ximing Lu",
      "Michael Ross Boone",
      "Boris Ivanovic",
      "Yejin Choi",
      "Marco Pavone",
      "Yu-Chiang Frank Wang",
      "Noa Garcia",
      "Yuta Nakashima",
      "Chao-Han Huck Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04854v1",
    "title": "Read the Room: Inferring Social Context Through Dyadic Interaction\n  Recognition in Cyber-physical-social Infrastructure Systems",
    "summary": "Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions.",
    "published": "2025-10-06T14:40:22Z",
    "updated": "2025-10-06T14:40:22Z",
    "link": "http://arxiv.org/pdf/2510.04854v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cheyu Lin",
      "John Martins",
      "Katherine A. Flanigan",
      "Ph. D"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.13693v3",
    "title": "Medical Image Classification with KAN-Integrated Transformers and\n  Dilated Neighborhood Attention",
    "summary": "Convolutional networks, transformers, hybrid models, and Mamba-based\narchitectures have demonstrated strong performance across various medical image\nclassification tasks. However, these methods were primarily designed to\nclassify clean images using labeled data. In contrast, real-world clinical data\noften involve image corruptions that are unique to multi-center studies and\nstem from variations in imaging equipment across manufacturers. In this paper,\nwe introduce the Medical Vision Transformer (MedViTV2), a novel architecture\nincorporating Kolmogorov-Arnold Network (KAN) layers into the transformer\narchitecture for the first time, aiming for generalized medical image\nclassification. We have developed an efficient KAN block to reduce\ncomputational load while enhancing the accuracy of the original MedViT.\nAdditionally, to counteract the fragility of our MedViT when scaled up, we\npropose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the\nefficient fused dot-product attention kernel capable of capturing global\ncontext and expanding receptive fields to scale the model effectively and\naddressing feature collapse issues. Moreover, a hierarchical hybrid strategy is\nintroduced to stack our Local Feature Perception and Global Feature Perception\nblocks in an efficient manner, which balances local and global feature\nperceptions to boost performance. Extensive experiments on 17 medical image\nclassification datasets and 12 corrupted medical image datasets demonstrate\nthat MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments\nwith reduced computational complexity. MedViTV2 is 44\\% more computationally\nefficient than the previous version and significantly enhances accuracy,\nachieving improvements of 4.6\\% on MedMNIST, 5.8\\% on NonMNIST, and 13.4\\% on\nthe MedMNIST-C benchmark.",
    "published": "2025-02-19T13:05:50Z",
    "updated": "2025-10-06T14:39:01Z",
    "link": "http://arxiv.org/pdf/2502.13693v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Omid Nejati Manzari",
      "Hojat Asgariandehkordi",
      "Taha Koleilat",
      "Yiming Xiao",
      "Hassan Rivaz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04844v1",
    "title": "From Actions to Kinesics: Extracting Human Psychological States through\n  Bodily Movements",
    "summary": "Understanding the dynamic relationship between humans and the built\nenvironment is a key challenge in disciplines ranging from environmental\npsychology to reinforcement learning (RL). A central obstacle in modeling these\ninteractions is the inability to capture human psychological states in a way\nthat is both generalizable and privacy preserving. Traditional methods rely on\ntheoretical models or questionnaires, which are limited in scope, static, and\nlabor intensive. We present a kinesics recognition framework that infers the\ncommunicative functions of human activity -- known as kinesics -- directly from\n3D skeleton joint data. Combining a spatial-temporal graph convolutional\nnetwork (ST-GCN) with a convolutional neural network (CNN), the framework\nleverages transfer learning to bypass the need for manually defined mappings\nbetween physical actions and psychological categories. The approach preserves\nuser anonymity while uncovering latent structures in bodily movements that\nreflect cognitive and emotional states. Our results on the Dyadic User\nEngagemenT (DUET) dataset demonstrate that this method enables scalable,\naccurate, and human-centered modeling of behavior, offering a new pathway for\nenhancing RL-driven simulations of human-environment interaction.",
    "published": "2025-10-06T14:31:53Z",
    "updated": "2025-10-06T14:31:53Z",
    "link": "http://arxiv.org/pdf/2510.04844v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Cheyu Lin",
      "Katherine A. Flanigan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04840v1",
    "title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through\n  Semantically Significant Keypoints",
    "summary": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance.",
    "published": "2025-10-06T14:25:03Z",
    "updated": "2025-10-06T14:25:03Z",
    "link": "http://arxiv.org/pdf/2510.04840v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Viktor Kozák",
      "Jan Chudoba",
      "Libor Přeučil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04838v1",
    "title": "Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation",
    "summary": "The growing demand for efficient deep learning has positioned dataset\ndistillation as a pivotal technique for compressing training dataset while\npreserving model performance. However, existing inner-loop optimization methods\nfor dataset distillation typically rely on random truncation strategies, which\nlack flexibility and often yield suboptimal results. In this work, we observe\nthat neural networks exhibit distinct learning dynamics across different\ntraining stages-early, middle, and late-making random truncation ineffective.\nTo address this limitation, we propose Automatic Truncated Backpropagation\nThrough Time (AT-BPTT), a novel framework that dynamically adapts both\ntruncation positions and window sizes according to intrinsic gradient behavior.\nAT-BPTT introduces three key components: (1) a probabilistic mechanism for\nstage-aware timestep selection, (2) an adaptive window sizing strategy based on\ngradient variation, and (3) a low-rank Hessian approximation to reduce\ncomputational overhead. Extensive experiments on CIFAR-10, CIFAR-100,\nTiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art\nperformance, improving accuracy by an average of 6.16% over baseline methods.\nMoreover, our approach accelerates inner-loop optimization by 3.9x while saving\n63% memory cost.",
    "published": "2025-10-06T14:22:28Z",
    "updated": "2025-10-06T14:22:28Z",
    "link": "http://arxiv.org/pdf/2510.04838v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Muquan Li",
      "Hang Gou",
      "Dongyang Zhang",
      "Shuang Liang",
      "Xiurui Xie",
      "Deqiang Ouyang",
      "Ke Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04823v1",
    "title": "Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis",
    "summary": "Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in\nenabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment\nprecision while reducing patient radiation exposure. To address this task, we\nadopt a fully 3D Flow Matching (FM) framework, motivated by recent work\ndemonstrating FM's efficiency in producing high-quality images. In our\napproach, a Gaussian noise volume is transformed into an sCT image by\nintegrating a learned FM velocity field, conditioned on features extracted from\nthe input MRI or CBCT using a lightweight 3D encoder. We evaluated the method\non the SynthRAD2025 Challenge benchmark, training separate models for MRI\n$\\rightarrow$ sCT and CBCT $\\rightarrow$ sCT across three anatomical regions:\nabdomen, head and neck, and thorax. Validation and testing were performed\nthrough the challenge submission system. The results indicate that the method\naccurately reconstructs global anatomical structures; however, preservation of\nfine details was limited, primarily due to the relatively low training\nresolution imposed by memory and runtime constraints. Future work will explore\npatch-based training and latent-space flow models to improve resolution and\nlocal structural fidelity.",
    "published": "2025-10-06T14:07:03Z",
    "updated": "2025-10-06T14:07:03Z",
    "link": "http://arxiv.org/pdf/2510.04823v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Arnela Hadzic",
      "Simon Johannes Joham",
      "Martin Urschler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04822v1",
    "title": "AvatarVTON: 4D Virtual Try-On for Animatable Avatars",
    "summary": "We propose AvatarVTON, the first 4D virtual try-on framework that generates\nrealistic try-on results from a single in-shop garment image, enabling free\npose control, novel-view rendering, and diverse garment choices. Unlike\nexisting methods, AvatarVTON supports dynamic garment interactions under\nsingle-view supervision, without relying on multi-view garment captures or\nphysics priors. The framework consists of two key modules: (1) a Reciprocal\nFlow Rectifier, a prior-free optical-flow correction strategy that stabilizes\navatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,\nwhich decomposes Gaussian maps into view-pose-invariant and view-pose-specific\ncomponents, enabling adaptive, non-linear garment deformations. To establish a\nbenchmark for 4D virtual try-on, we extend existing baselines with unified\nmodules for fair qualitative and quantitative comparisons. Extensive\nexperiments show that AvatarVTON achieves high fidelity, diversity, and dynamic\ngarment realism, making it well-suited for AR/VR, gaming, and digital-human\napplications.",
    "published": "2025-10-06T14:06:34Z",
    "updated": "2025-10-06T14:06:34Z",
    "link": "http://arxiv.org/pdf/2510.04822v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zicheng Jiang",
      "Jixin Gao",
      "Shengfeng He",
      "Xinzhe Li",
      "Yulong Zheng",
      "Zhaotong Yang",
      "Junyu Dong",
      "Yong Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10178v2",
    "title": "Attention, Please! Revisiting Attentive Probing Through the Lens of\n  Efficiency",
    "summary": "As fine-tuning becomes increasingly impractical at scale, probing is emerging\nas the preferred evaluation protocol. Yet, the standard linear probing fails to\nadequately reflect the potential of models whose pre-training optimizes\nrepresentations of patch tokens rather than an explicit global representation.\nThis motivates the need for attentive probing, an alternative that uses\nattention to selectively aggregate patch-level features. Despite its growing\nadoption, attentive probing remains under-explored, with existing methods\nsuffering from excessive parameterization and poor computational efficiency. In\nthis work, we revisit attentive probing through the lens of the accuracy vs.\nparameter efficiency trade-off. We present the first comprehensive study of\nexisting methods, analyzing their design choices and benchmarking their\nperformance. Building on this, we propose efficient probing (EP), a simple yet\neffective multi-query cross-attention mechanism that eliminates redundant\nprojections and reduces the number of trainable parameters. Despite its\nsimplicity, EP outperforms linear probing and prior attentive probing\napproaches across seven benchmarks, generalizes well to diverse pre-training\nparadigms, and delivers strong low-shot and layer-wise gains. Beyond\nevaluation, our analysis uncovers emerging properties of EP, such as\ncomplementary attention maps, which open new directions for leveraging probing\nbeyond protocol design. Code available at\nhttps://github.com/billpsomas/efficient-probing.",
    "published": "2025-06-11T21:10:26Z",
    "updated": "2025-10-06T13:58:58Z",
    "link": "http://arxiv.org/pdf/2506.10178v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bill Psomas",
      "Dionysis Christopoulos",
      "Eirini Baltzi",
      "Ioannis Kakogeorgiou",
      "Tilemachos Aravanis",
      "Nikos Komodakis",
      "Konstantinos Karantzalos",
      "Yannis Avrithis",
      "Giorgos Tolias"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.06901v2",
    "title": "PRO-VPT: Distribution-Adaptive Visual Prompt Tuning via Prompt\n  Relocation",
    "summary": "Visual prompt tuning (VPT), i.e., fine-tuning some lightweight prompt tokens,\nprovides an efficient and effective approach for adapting pre-trained models to\nvarious downstream tasks. However, most prior art indiscriminately uses a fixed\nprompt distribution across different tasks, neglecting the importance of each\nblock varying depending on the task. In this paper, we introduce adaptive\ndistribution optimization (ADO) by tackling two key questions: (1) How to\nappropriately and formally define ADO, and (2) How to design an adaptive\ndistribution strategy guided by this definition? Through empirical analysis, we\nfirst confirm that properly adjusting the distribution significantly improves\nVPT performance, and further uncover a key insight that a nested relationship\nexists between ADO and VPT. Based on these findings, we propose a new VPT\nframework, termed PRO-VPT (iterative Prompt RelOcation-based VPT), which\nadaptively adjusts the distribution built upon a nested optimization\nformulation. Specifically, we develop a prompt relocation strategy derived from\nthis formulation, comprising two steps: pruning idle prompts from\nprompt-saturated blocks, followed by allocating these prompts to the most\nprompt-needed blocks. By iteratively performing prompt relocation and VPT, our\nproposal can adaptively learn the optimal prompt distribution in a nested\noptimization-based manner, thereby unlocking the full potential of VPT.\nExtensive experiments demonstrate that our proposal significantly outperforms\nadvanced VPT methods, e.g., PRO-VPT surpasses VPT by 1.6 pp and 2.0 pp average\naccuracy, leading prompt-based methods to state-of-the-art performance on\nVTAB-1k and FGVC benchmarks. The code is available at\nhttps://github.com/ckshang/PRO-VPT.",
    "published": "2025-03-10T04:07:43Z",
    "updated": "2025-10-06T13:38:48Z",
    "link": "http://arxiv.org/pdf/2503.06901v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Chikai Shang",
      "Mengke Li",
      "Yiqun Zhang",
      "Zhen Chen",
      "Jinlin Wu",
      "Fangqing Gu",
      "Yang Lu",
      "Yiu-ming Cheung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04794v1",
    "title": "A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid\n  Transformation and Fundamental Matrix Estimation",
    "summary": "Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)\nhave reshaped computer vision through pretrained feature representations that\nenable strong transfer learning for diverse tasks. However, their efficiency as\nbackbone architectures for geometric estimation tasks involving image\ndeformations in low-data regimes remains an open question. This work considers\ntwo such tasks: 1) estimating 2D rigid transformations between pairs of images\nand 2) predicting the fundamental matrix for stereo image pairs, an important\nproblem in various applications, such as autonomous mobility, robotics, and 3D\nscene reconstruction. Addressing this intriguing question, this work\nsystematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)\nwith ViT-based foundation models (CLIP-ViT variants and DINO) in various data\nsize settings, including few-shot scenarios. These pretrained models are\noptimized for classification or contrastive learning, encouraging them to focus\nmostly on high-level semantics. The considered tasks require balancing local\nand global features differently, challenging the straightforward adoption of\nthese models as the backbone. Empirical comparative analysis shows that,\nsimilar to training from scratch, ViTs outperform CNNs during refinement in\nlarge downstream-data scenarios. However, in small data scenarios, the\ninductive bias and smaller capacity of CNNs improve their performance, allowing\nthem to match that of a ViT. Moreover, ViTs exhibit stronger generalization in\ncross-domain evaluation where the data distribution changes. These results\nemphasize the importance of carefully selecting model architectures for\nrefinement, motivating future research towards hybrid architectures that\nbalance local and global representations.",
    "published": "2025-10-06T13:18:27Z",
    "updated": "2025-10-06T13:18:27Z",
    "link": "http://arxiv.org/pdf/2510.04794v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alon Kaya",
      "Igal Bilik",
      "Inna Stainvas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04781v1",
    "title": "Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage\n  Digitization",
    "summary": "High-fidelity 3D scanning is essential for preserving cultural heritage\nartefacts, supporting documentation, analysis, and long-term conservation.\nHowever, conventional methods typically require specialized expertise and\nmanual intervention to maintain optimal scanning conditions and coverage. We\npresent an automated two-robot scanning system that eliminates the need for\nhandheld or semi-automatic workflows by combining coordinated robotic\nmanipulation with high-resolution 3D scanning. Our system parameterizes the\nscanning space into distinct regions, enabling coordinated motion planning\nbetween a scanner-equipped robot and a tray-handling robot. Optimized\ntrajectory planning and waypoint distribution ensure comprehensive surface\ncoverage, minimize occlusions, and balance reconstruction accuracy with system\nefficiency. Experimental results show that our approach achieves significantly\nlower Chamfer Distance and higher F-score compared to baseline methods,\noffering superior geometric accuracy, improved digitization efficiency, and\nreduced reliance on expert operators.",
    "published": "2025-10-06T12:58:41Z",
    "updated": "2025-10-06T12:58:41Z",
    "link": "http://arxiv.org/pdf/2510.04781v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Javed Ahmad",
      "Federico Dassiè",
      "Selene Frascella",
      "Gabriele Marchello",
      "Ferdinando Cannella",
      "Arianna Traviglia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04772v1",
    "title": "Federated Learning for Surgical Vision in Appendicitis Classification:\n  Results of the FedSurg EndoVis 2024 Challenge",
    "summary": "Purpose: The FedSurg challenge was designed to benchmark the state of the art\nin federated learning for surgical video classification. Its goal was to assess\nhow well current methods generalize to unseen clinical centers and adapt\nthrough local fine-tuning while enabling collaborative model development\nwithout sharing patient data. Methods: Participants developed strategies to\nclassify inflammation stages in appendicitis using a preliminary version of the\nmulti-center Appendix300 video dataset. The challenge evaluated two tasks:\ngeneralization to an unseen center and center-specific adaptation after\nfine-tuning. Submitted approaches included foundation models with linear\nprobing, metric learning with triplet loss, and various FL aggregation schemes\n(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and\nExpected Cost, with ranking robustness evaluated via bootstrapping and\nstatistical testing. Results: In the generalization task, performance across\ncenters was limited. In the adaptation task, all teams improved after\nfine-tuning, though ranking stability was low. The ViViT-based submission\nachieved the strongest overall performance. The challenge highlighted\nlimitations in generalization, sensitivity to class imbalance, and difficulties\nin hyperparameter tuning in decentralized training, while spatiotemporal\nmodeling and context-aware preprocessing emerged as promising strategies.\nConclusion: The FedSurg Challenge establishes the first benchmark for\nevaluating FL strategies in surgical video classification. Findings highlight\nthe trade-off between local personalization and global robustness, and\nunderscore the importance of architecture choice, preprocessing, and loss\ndesign. This benchmarking offers a reference point for future development of\nimbalance-aware, adaptive, and robust FL methods in clinical surgical AI.",
    "published": "2025-10-06T12:48:46Z",
    "updated": "2025-10-06T12:48:46Z",
    "link": "http://arxiv.org/pdf/2510.04772v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Max Kirchner",
      "Hanna Hoffmann",
      "Alexander C. Jenke",
      "Oliver L. Saldanha",
      "Kevin Pfeiffer",
      "Weam Kanjo",
      "Julia Alekseenko",
      "Claas de Boer",
      "Santhi Raj Kolamuri",
      "Lorenzo Mazza",
      "Nicolas Padoy",
      "Sophia Bano",
      "Annika Reinke",
      "Lena Maier-Hein",
      "Danail Stoyanov",
      "Jakob N. Kather",
      "Fiona R. Kolbinger",
      "Sebastian Bodenstedt",
      "Stefanie Speidel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.19600v2",
    "title": "Filling of incomplete sinograms from sparse PET detector configurations\n  using a residual U-Net",
    "summary": "Long axial field-of-view PET scanners offer increased field-of-view and\nsensitivity compared to traditional PET scanners. However, a significant cost\nis associated with the densely packed photodetectors required for the\nextended-coverage systems, limiting clinical utilisation. To mitigate the cost\nlimitations, alternative sparse system configurations have been proposed,\nallowing an extended field-of-view PET design with detector costs similar to a\nstandard PET system, albeit at the expense of image quality. In this work, we\npropose a deep sinogram restoration network to fill in the missing sinogram\ndata. Our method utilises a modified Residual U-Net, trained on clinical PET\nscans from a GE Signa PET/MR, simulating the removal of 50% of the detectors in\na chessboard pattern (retaining only 25% of all lines of response). The model\nsuccessfully recovers missing counts, with a mean absolute error below two\nevents per pixel, outperforming 2D interpolation in both sinogram and\nreconstructed image domain. Notably, the predicted sinograms exhibit a\nsmoothing effect, leading to reconstructed images lacking sharpness in finer\ndetails. Despite these limitations, the model demonstrates a substantial\ncapacity for compensating for the undersampling caused by the sparse detector\nconfiguration. This proof-of-concept study suggests that sparse detector\nconfigurations, combined with deep learning techniques, offer a viable\nalternative to conventional PET scanner designs. This approach supports the\ndevelopment of cost-effective, total body PET scanners, allowing a significant\nstep forward in medical imaging technology.",
    "published": "2025-06-24T13:10:44Z",
    "updated": "2025-10-06T12:47:47Z",
    "link": "http://arxiv.org/pdf/2506.19600v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "authors": [
      "Klara Leffler",
      "Luigi Tommaso Luppino",
      "Samuel Kuttner",
      "Karin Söderkvist",
      "Jan Axelsson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12981v2",
    "title": "UniUIR: Considering Underwater Image Restoration as An All-in-One\n  Learner",
    "summary": "Existing underwater image restoration (UIR) methods generally only handle\ncolor distortion or jointly address color and haze issues, but they often\noverlook the more complex degradations that can occur in underwater scenes. To\naddress this limitation, we propose a Universal Underwater Image Restoration\nmethod, termed as UniUIR, considering the complex scenario of real-world\nunderwater mixed distortions as an all-in-one manner. To decouple\ndegradation-specific issues and explore the inter-correlations among various\ndegradations in UIR task, we designed the Mamba Mixture-of-Experts module. This\nmodule enables each expert to identify distinct types of degradation and\ncollaboratively extract task-specific priors while maintaining global feature\nrepresentation based on linear complexity. Building upon this foundation, to\nenhance degradation representation and address the task conflicts that arise\nwhen handling multiple types of degradation, we introduce the spatial-frequency\nprior generator. This module extracts degradation prior information in both\nspatial and frequency domains, and adaptively selects the most appropriate\ntask-specific prompts based on image content, thereby improving the accuracy of\nimage restoration. Finally, to more effectively address complex,\nregion-dependent distortions in UIR task, we incorporate depth information\nderived from a large-scale pre-trained depth prediction model, thereby enabling\nthe network to perceive and leverage depth variations across different image\nregions to handle localized degradation. Extensive experiments demonstrate that\nUniUIR can produce more attractive results across qualitative and quantitative\ncomparisons, and shows strong generalization than state-of-the-art methods.",
    "published": "2025-01-22T16:10:42Z",
    "updated": "2025-10-06T12:46:52Z",
    "link": "http://arxiv.org/pdf/2501.12981v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xu Zhang",
      "Huan Zhang",
      "Guoli Wang",
      "Qian Zhang",
      "Lefei Zhang",
      "Bo Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04770v1",
    "title": "Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary\n  Learning",
    "summary": "Open-vocabulary learning requires modeling the data distribution in open\nenvironments, which consists of both seen-class and unseen-class data.\n  Existing methods estimate the distribution in open environments using\nseen-class data, where the absence of unseen classes makes the estimation error\ninherently unidentifiable.\n  Intuitively, learning beyond the seen classes is crucial for distribution\nestimation to bound the estimation error.\n  We theoretically demonstrate that the distribution can be effectively\nestimated by generating unseen-class data, through which the estimation error\nis upper-bounded.\n  Building on this theoretical insight, we propose a novel open-vocabulary\nlearning method, which generates unseen-class data for estimating the\ndistribution in open environments. The method consists of a class-domain-wise\ndata generation pipeline and a distribution alignment algorithm. The data\ngeneration pipeline generates unseen-class data under the guidance of a\nhierarchical semantic tree and domain information inferred from the seen-class\ndata, facilitating accurate distribution estimation. With the generated data,\nthe distribution alignment algorithm estimates and maximizes the posterior\nprobability to enhance generalization in open-vocabulary learning. Extensive\nexperiments on $11$ datasets demonstrate that our method outperforms baseline\napproaches by up to $14\\%$, highlighting its effectiveness and superiority.",
    "published": "2025-10-06T12:43:59Z",
    "updated": "2025-10-06T12:43:59Z",
    "link": "http://arxiv.org/pdf/2510.04770v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Xiaomeng Fan",
      "Yuchuan Mao",
      "Zhi Gao",
      "Yuwei Wu",
      "Jin Chen",
      "Yunde Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04753v1",
    "title": "Beyond Appearance: Transformer-based Person Identification from\n  Conversational Dynamics",
    "summary": "This paper investigates the performance of transformer-based architectures\nfor person identification in natural, face-to-face conversation scenario. We\nimplement and evaluate a two-stream framework that separately models spatial\nconfigurations and temporal motion patterns of 133 COCO WholeBody keypoints,\nextracted from a subset of the CANDOR conversational corpus. Our experiments\ncompare pre-trained and from-scratch training, investigate the use of velocity\nfeatures, and introduce a multi-scale temporal transformer for hierarchical\nmotion modeling. Results demonstrate that domain-specific training\nsignificantly outperforms transfer learning, and that spatial configurations\ncarry more discriminative information than temporal dynamics. The spatial\ntransformer achieves 95.74% accuracy, while the multi-scale temporal\ntransformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,\nconfirming that postural and dynamic information are complementary. These\nfindings highlight the potential of transformer architectures for person\nidentification in natural interactions and provide insights for future\nmultimodal and cross-cultural studies.",
    "published": "2025-10-06T12:31:15Z",
    "updated": "2025-10-06T12:31:15Z",
    "link": "http://arxiv.org/pdf/2510.04753v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Masoumeh Chapariniya",
      "Teodora Vukovic",
      "Sarah Ebling",
      "Volker Dellwo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.08208v2",
    "title": "Explaining Human Preferences via Metrics for Structured 3D\n  Reconstruction",
    "summary": "\"What cannot be measured cannot be improved\" while likely never uttered by\nLord Kelvin, summarizes effectively the driving force behind this work. This\npaper presents a detailed discussion of automated metrics for evaluating\nstructured 3D reconstructions. Pitfalls of each metric are discussed, and an\nanalysis through the lens of expert 3D modelers' preferences is presented. A\nset of systematic \"unit tests\" are proposed to empirically verify desirable\nproperties, and context aware recommendations regarding which metric to use\ndepending on application are provided. Finally, a learned metric distilled from\nhuman expert judgments is proposed and analyzed. The source code is available\nat https://github.com/s23dr/wireframe-metrics-iccv2025",
    "published": "2025-03-11T09:23:29Z",
    "updated": "2025-10-06T12:28:49Z",
    "link": "http://arxiv.org/pdf/2503.08208v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jack Langerman",
      "Denys Rozumnyi",
      "Yuzhong Huang",
      "Dmytro Mishkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04741v1",
    "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small\n  Target Detection",
    "summary": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released.",
    "published": "2025-10-06T12:13:56Z",
    "updated": "2025-10-06T12:13:56Z",
    "link": "http://arxiv.org/pdf/2510.04741v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alina Ciocarlan",
      "Sylvie Le Hégarat-Mascle",
      "Sidonie Lefebvre"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04739v1",
    "title": "ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics\n  in Sports Broadcasts",
    "summary": "Quantifying sponsor visibility in sports broadcasts is a critical marketing\ntask traditionally hindered by manual, subjective, and unscalable analysis\nmethods. While automated systems offer an alternative, their reliance on\naxis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics\nwhen logos appear rotated or skewed due to dynamic camera angles and\nperspective distortions. This paper introduces ExposureEngine, an end-to-end\nsystem designed for accurate, rotation-aware sponsor visibility analytics in\nsports broadcasts, demonstrated in a soccer case study. Our approach predicts\nOriented Bounding Box (OBB) to provide a geometrically precise fit to each logo\nregardless of the orientation on-screen. To train and evaluate our detector, we\ndeveloped a new dataset comprising 1,103 frames from Swedish elite soccer,\nfeaturing 670 unique sponsor logos annotated with OBBs. Our model achieves a\nmean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall\nof 0.87, demonstrating robust performance in localizing logos under diverse\nbroadcast conditions. The system integrates these detections into an analytical\npipeline that calculates precise visibility metrics, such as exposure duration\nand on-screen coverage. Furthermore, we incorporate a language-driven agentic\nlayer, enabling users to generate reports, summaries, and media content through\nnatural language queries. The complete system, including the dataset and the\nanalytics dashboard, provides a comprehensive solution for auditable and\ninterpretable sponsor measurement in sports media. An overview of the\nExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .",
    "published": "2025-10-06T12:11:53Z",
    "updated": "2025-10-06T12:11:53Z",
    "link": "http://arxiv.org/pdf/2510.04739v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Mehdi Houshmand Sarkhoosh",
      "Frøy Øye",
      "Henrik Nestor Sørlie",
      "Nam Hoang Vu",
      "Dag Johansen",
      "Cise Midoglu",
      "Tomas Kupka",
      "Pål Halvorsen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04723v1",
    "title": "Benchmark on Monocular Metric Depth Estimation in Wildlife Setting",
    "summary": "Camera traps are widely used for wildlife monitoring, but extracting accurate\ndistance measurements from monocular images remains challenging due to the lack\nof depth information. While monocular depth estimation (MDE) methods have\nadvanced significantly, their performance in natural wildlife environments has\nnot been systematically evaluated. This work introduces the first benchmark for\nmonocular metric depth estimation in wildlife monitoring conditions. We\nevaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,\nZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images\nwith ground truth distances obtained using calibrated ChARUCO patterns. Our\nresults demonstrate that Depth Anything V2 achieves the best overall\nperformance with a mean absolute error of 0.454m and correlation of 0.962,\nwhile methods like ZoeDepth show significant degradation in outdoor natural\nenvironments (MAE: 3.087m). We find that median-based depth extraction\nconsistently outperforms mean-based approaches across all deep learning\nmethods. Additionally, we analyze computational efficiency, with ZoeDepth being\nfastest (0.17s per image) but least accurate, while Depth Anything V2 provides\nan optimal balance of accuracy and speed (0.22s per image). This benchmark\nestablishes performance baselines for wildlife applications and provides\npractical guidance for implementing depth estimation in conservation monitoring\nsystems.",
    "published": "2025-10-06T11:43:34Z",
    "updated": "2025-10-06T11:43:34Z",
    "link": "http://arxiv.org/pdf/2510.04723v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Niccolò Niccoli",
      "Lorenzo Seidenari",
      "Ilaria Greco",
      "Francesco Rovero"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04714v1",
    "title": "Object-Centric Representation Learning for Enhanced 3D Scene Graph\n  Prediction",
    "summary": "3D Semantic Scene Graph Prediction aims to detect objects and their semantic\nrelationships in 3D scenes, and has emerged as a crucial technology for\nrobotics and AR/VR applications. While previous research has addressed dataset\nlimitations and explored various approaches including Open-Vocabulary settings,\nthey frequently fail to optimize the representational capacity of object and\nrelationship features, showing excessive reliance on Graph Neural Networks\ndespite insufficient discriminative capability. In this work, we demonstrate\nthrough extensive analysis that the quality of object features plays a critical\nrole in determining overall scene graph accuracy. To address this challenge, we\ndesign a highly discriminative object feature encoder and employ a contrastive\npretraining strategy that decouples object representation learning from the\nscene graph prediction. This design not only enhances object classification\naccuracy but also yields direct improvements in relationship prediction.\nNotably, when plugging in our pretrained encoder into existing frameworks, we\nobserve substantial performance improvements across all evaluation metrics.\nAdditionally, whereas existing approaches have not fully exploited the\nintegration of relationship information, we effectively combine both geometric\nand semantic features to achieve superior relationship prediction.\nComprehensive experiments on the 3DSSG dataset demonstrate that our approach\nsignificantly outperforms previous state-of-the-art methods. Our code is\npublicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.",
    "published": "2025-10-06T11:33:09Z",
    "updated": "2025-10-06T11:33:09Z",
    "link": "http://arxiv.org/pdf/2510.04714v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "KunHo Heo",
      "GiHyun Kim",
      "SuYeon Kim",
      "MyeongAh Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.05578v3",
    "title": "Novel Object 6D Pose Estimation with a Single Reference View",
    "summary": "Existing novel object 6D pose estimation methods typically rely on CAD models\nor dense reference views, which are both difficult to acquire. Using only a\nsingle reference view is more scalable, but challenging due to large pose\ndiscrepancies and limited geometric and spatial information. To address these\nissues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose\nestimation method. Our key idea is to iteratively establish point-wise\nalignment in a common coordinate system based on state space models (SSMs).\nSpecifically, iterative object-space point-wise alignment can effectively\nhandle large pose discrepancies, while our proposed RGB and Points SSMs can\ncapture long-range dependencies and spatial information from a single view,\noffering linear complexity and superior spatial modeling capability. Once\npre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel\nobject using only a single reference view, without requiring retraining or a\nCAD model. Extensive experiments on six popular datasets and real-world robotic\nscenes demonstrate that we achieve on-par performance with CAD-based and dense\nreference view-based methods, despite operating in the more challenging single\nreference setting. Code will be released at\nhttps://github.com/CNJianLiu/SinRef-6D.",
    "published": "2025-03-07T17:00:41Z",
    "updated": "2025-10-06T11:32:49Z",
    "link": "http://arxiv.org/pdf/2503.05578v3.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Jian Liu",
      "Wei Sun",
      "Kai Zeng",
      "Jin Zheng",
      "Hui Yang",
      "Hossein Rahmani",
      "Ajmal Mian",
      "Lin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04712v1",
    "title": "ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion\n  Model",
    "summary": "The automatic generation of diverse and human-like facial reactions in dyadic\ndialogue remains a critical challenge for human-computer interaction systems.\nExisting methods fail to model the stochasticity and dynamics inherent in real\nhuman reactions. To address this, we propose ReactDiff, a novel temporal\ndiffusion framework for generating diverse facial reactions that are\nappropriate for responding to any given dialogue context. Our key insight is\nthat plausible human reactions demonstrate smoothness, and coherence over time,\nand conform to constraints imposed by human facial anatomy. To achieve this,\nReactDiff incorporates two vital priors (spatio-temporal facial kinematics)\ninto the diffusion process: i) temporal facial behavioral kinematics and ii)\nfacial action unit dependencies. These two constraints guide the model toward\nrealistic human reaction manifolds, avoiding visually unrealistic jitters,\nunstable transitions, unnatural expressions, and other artifacts. Extensive\nexperiments on the REACT2024 dataset demonstrate that our approach not only\nachieves state-of-the-art reaction quality but also excels in diversity and\nreaction appropriateness.",
    "published": "2025-10-06T11:30:40Z",
    "updated": "2025-10-06T11:30:40Z",
    "link": "http://arxiv.org/pdf/2510.04712v1.pdf",
    "category": [
      "cs.CV",
      "cs.HC",
      "cs.MM"
    ],
    "authors": [
      "Luo Cheng",
      "Song Siyang",
      "Yan Siyuan",
      "Yu Zhen",
      "Ge Zongyuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.08421v2",
    "title": "Poisson multi-Bernoulli mixture filter for trajectory measurements",
    "summary": "This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for\nmulti-target filtering based on sensor measurements that are sets of\ntrajectories in the last two-time step window. The proposed filter, the\ntrajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the\nset of target states. In prediction, the filter obtains the PMBM density on the\nset of trajectories over the last two time steps. This density is then updated\nwith the set of trajectory measurements. After the update step, the PMBM\nposterior on the set of two-step trajectories is marginalised to obtain a PMBM\ndensity on the set of target states. The filter provides a closed-form solution\nfor multi-target filtering based on sets of trajectory measurements, estimating\nthe set of target states at the end of each time window. Additionally, the\npaper proposes computationally lighter alternatives to the TM-PMBM filter by\nderiving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler\ndivergence minimisation in an augmented space with auxiliary variables. The\nperformance of the proposed filters are evaluated in a simulation study.",
    "published": "2025-04-11T10:27:07Z",
    "updated": "2025-10-06T11:24:28Z",
    "link": "http://arxiv.org/pdf/2504.08421v2.pdf",
    "category": [
      "eess.SP",
      "cs.CV",
      "stat.AP"
    ],
    "authors": [
      "Marco Fontana",
      "Ángel F. García-Fernández",
      "Simon Maskell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04706v1",
    "title": "ID-Consistent, Precise Expression Generation with Blendshape-Guided\n  Diffusion",
    "summary": "Human-centric generative models designed for AI-driven storytelling must\nbring together two core capabilities: identity consistency and precise control\nover human performance. While recent diffusion-based approaches have made\nsignificant progress in maintaining facial identity, achieving fine-grained\nexpression control without compromising identity remains challenging. In this\nwork, we present a diffusion-based framework that faithfully reimagines any\nsubject under any particular facial expression. Building on an ID-consistent\nface foundation model, we adopt a compositional design featuring an expression\ncross-attention module guided by FLAME blendshape parameters for explicit\ncontrol. Trained on a diverse mixture of image and video data rich in\nexpressive variation, our adapter generalizes beyond basic emotions to subtle\nmicro-expressions and expressive transitions, overlooked by prior works. In\naddition, a pluggable Reference Adapter enables expression editing in real\nimages by transferring the appearance from a reference frame during synthesis.\nExtensive quantitative and qualitative evaluations show that our model\noutperforms existing methods in tailored and identity-consistent expression\ngeneration. Code and models can be found at\nhttps://github.com/foivospar/Arc2Face.",
    "published": "2025-10-06T11:20:56Z",
    "updated": "2025-10-06T11:20:56Z",
    "link": "http://arxiv.org/pdf/2510.04706v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Foivos Paraperas Papantoniou",
      "Stefanos Zafeiriou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04705v1",
    "title": "Label-Efficient Cross-Modality Generalization for Liver Segmentation in\n  Multi-Phase MRI",
    "summary": "Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis\nassessment, yet labeled data is often scarce and unevenly distributed across\nimaging modalities and vendor systems. We propose a label-efficient\nsegmentation approach that promotes cross-modality generalization under\nreal-world conditions, where GED4 hepatobiliary-phase annotations are limited,\nnon-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial\nmisalignment and missing phases are common. Our method integrates a\nfoundation-scale 3D segmentation backbone adapted via fine-tuning, co-training\nwith cross pseudo supervision to leverage unlabeled volumes, and a standardized\npreprocessing pipeline. Without requiring spatial registration, the model\nlearns to generalize across MRI phases and vendors, demonstrating robust\nsegmentation performance in both labeled and unlabeled domains. Our results\nexhibit the effectiveness of our proposed label-efficient baseline for liver\nsegmentation in multi-phase, multi-vendor MRI and highlight the potential of\ncombining foundation model adaptation with co-training for real-world clinical\nimaging tasks.",
    "published": "2025-10-06T11:19:05Z",
    "updated": "2025-10-06T11:19:05Z",
    "link": "http://arxiv.org/pdf/2510.04705v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Quang-Khai Bui-Tran",
      "Minh-Toan Dinh",
      "Thanh-Huy Nguyen",
      "Ba-Thinh Lam",
      "Mai-Anh Vu",
      "Ulas Bagci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.10575v3",
    "title": "A Survey of Defenses Against AI-Generated Visual Media:\n  Detection,Disruption, and Authentication",
    "summary": "Deep generative models have demonstrated impressive performance in various\ncomputer vision applications, including image synthesis, video generation, and\nmedical analysis. Despite their significant advancements, these models may be\nused for malicious purposes, such as misinformation, deception, and copyright\nviolation. In this paper, we provide a systematic and timely review of research\nefforts on defenses against AI-generated visual media, covering detection,\ndisruption, and authentication. We review existing methods and summarize the\nmainstream defense-related tasks within a unified passive and proactive\nframework. Moreover, we survey the derivative tasks concerning the\ntrustworthiness of defenses, such as their robustness and fairness. For each\ndefense strategy, we formulate its general pipeline and propose a\nmultidimensional taxonomy applicable across defense tasks, based on\nmethodological strategies. Additionally, we summarize the commonly used\nevaluation datasets, criteria, and metrics. Finally, by analyzing the reviewed\nstudies, we provide insights into current research challenges and suggest\npossible directions for future research.",
    "published": "2024-07-15T09:46:02Z",
    "updated": "2025-10-06T10:42:27Z",
    "link": "http://arxiv.org/pdf/2407.10575v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingyi Deng",
      "Chenhao Lin",
      "Zhengyu Zhao",
      "Shuai Liu",
      "Zhe Peng",
      "Qian Wang",
      "Chao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04668v1",
    "title": "ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion\n  Models via Token-wise Adaptation and Attention Disentanglement",
    "summary": "In recent years, multi-concept personalization for text-to-image (T2I)\ndiffusion models to represent several subjects in an image has gained much more\nattention. The main challenge of this task is \"concept mixing\", where multiple\nlearned concepts interfere or blend undesirably in the output image. To address\nthis issue, in this paper, we present ConceptSplit, a novel framework to split\nthe individual concepts through training and inference. Our framework comprises\ntwo key components. First, we introduce Token-wise Value Adaptation (ToVA), a\nmerging-free training method that focuses exclusively on adapting the value\nprojection in cross-attention. Based on our empirical analysis, we found that\nmodifying the key projection, a common approach in existing methods, can\ndisrupt the attention mechanism and lead to concept mixing. Second, we propose\nLatent Optimization for Disentangled Attention (LODA), which alleviates\nattention entanglement during inference by optimizing the input latent. Through\nextensive qualitative and quantitative experiments, we demonstrate that\nConceptSplit achieves robust multi-concept personalization, mitigating\nunintended concept interference. Code is available at\nhttps://github.com/KU-VGI/ConceptSplit",
    "published": "2025-10-06T10:22:46Z",
    "updated": "2025-10-06T10:22:46Z",
    "link": "http://arxiv.org/pdf/2510.04668v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Habin Lim",
      "Yeongseob Won",
      "Juwon Seo",
      "Gyeong-Moon Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.04712v2",
    "title": "SEE-DPO: Self Entropy Enhanced Direct Preference Optimization",
    "summary": "Direct Preference Optimization (DPO) has been successfully used to align\nlarge language models (LLMs) according to human preferences, and more recently\nit has also been applied to improving the quality of text-to-image diffusion\nmodels. However, DPO-based methods such as SPO, Diffusion-DPO, and D3PO are\nhighly susceptible to overfitting and reward hacking, especially when the\ngenerative model is optimized to fit out-of-distribution during prolonged\ntraining. To overcome these challenges and stabilize the training of diffusion\nmodels, we introduce a self-entropy regularization mechanism in reinforcement\nlearning from human feedback. This enhancement improves DPO training by\nencouraging broader exploration and greater robustness. Our regularization\ntechnique effectively mitigates reward hacking, leading to improved stability\nand enhanced image quality across the latent space. Extensive experiments\ndemonstrate that integrating human feedback with self-entropy regularization\ncan significantly boost image diversity and specificity, achieving\nstate-of-the-art results on key image generation metrics.",
    "published": "2024-11-06T02:17:33Z",
    "updated": "2025-10-06T10:21:14Z",
    "link": "http://arxiv.org/pdf/2411.04712v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Shivanshu Shekhar",
      "Shreyas Singh",
      "Tong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23022v2",
    "title": "Copyright Infringement Detection in Text-to-Image Diffusion Models via\n  Differential Privacy",
    "summary": "The widespread deployment of large vision models such as Stable Diffusion\nraises significant legal and ethical concerns, as these models can memorize and\nreproduce copyrighted content without authorization. Existing detection\napproaches often lack robustness and fail to provide rigorous theoretical\nunderpinnings. To address these gaps, we formalize the concept of copyright\ninfringement and its detection from the perspective of Differential Privacy\n(DP), and introduce the conditional sensitivity metric, a concept analogous to\nsensitivity in DP, that quantifies the deviation in a diffusion model's output\ncaused by the inclusion or exclusion of a specific training data point. To\noperationalize this metric, we propose D-Plus-Minus (DPM), a novel post-hoc\ndetection framework that identifies copyright infringement in text-to-image\ndiffusion models. Specifically, DPM simulates inclusion and exclusion processes\nby fine-tuning models in two opposing directions: learning or unlearning.\nBesides, to disentangle concept-specific influence from the global parameter\nshifts induced by fine-tuning, DPM computes confidence scores over orthogonal\nprompt distributions using statistical metrics. Moreover, to facilitate\nstandardized benchmarking, we also construct the Copyright Infringement\nDetection Dataset (CIDD), a comprehensive resource for evaluating detection\nacross diverse categories. Our results demonstrate that DPM reliably detects\ninfringement content without requiring access to the original training dataset\nor text prompts, offering an interpretable and practical solution for\nsafeguarding intellectual property in the era of generative AI.",
    "published": "2025-09-27T00:38:12Z",
    "updated": "2025-10-06T10:17:32Z",
    "link": "http://arxiv.org/pdf/2509.23022v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiafeng Man",
      "Zhipeng Wei",
      "Jingjing Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04654v1",
    "title": "MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture\n  of Movement Experts",
    "summary": "Gait encodes rich biometric and behavioural information, yet leveraging the\nmanner of walking to infer psychological traits remains a challenging and\nunderexplored problem. We introduce a hierarchical Multi-Stage Mixture of\nMovement Experts (MoME) architecture for multi-task prediction of psychological\nattributes from gait sequences represented as 2D poses. MoME processes the\nwalking cycle in four stages of movement complexity, employing lightweight\nexpert models to extract spatio-temporal features and task-specific gating\nmodules to adaptively weight experts across traits and stages. Evaluated on the\nPsyMo benchmark covering 17 psychological traits, our method outperforms\nstate-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at\nthe run level and 44.6% at the subject level. Our experiments show that\nintegrating auxiliary tasks such as identity recognition, gender prediction,\nand BMI estimation further improves psychological trait estimation. Our\nfindings demonstrate the viability of multi-task gait-based learning for\npsychological trait estimation and provide a foundation for future research on\nmovement-informed psychological inference.",
    "published": "2025-10-06T09:58:43Z",
    "updated": "2025-10-06T09:58:43Z",
    "link": "http://arxiv.org/pdf/2510.04654v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Andy Cǎtrunǎ",
      "Adrian Cosma",
      "Emilian Rǎdoi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04648v1",
    "title": "EduPersona: Benchmarking Subjective Ability Boundaries of Virtual\n  Student Agents",
    "summary": "As large language models are increasingly integrated into education, virtual\nstudent agents are becoming vital for classroom simulation and teacher\ntraining. Yet their classroom-oriented subjective abilities remain largely\nunassessed, limiting understanding of model boundaries and hindering\ntrustworthy deployment. We present EduPersona, a large-scale benchmark spanning\ntwo languages, three subjects, and ten persona types based on the Big Five\ntheory. The dataset contains 1,308 authentic classroom dialogue rounds,\ncorresponding to 12,814 teacher-student Q&A turns, and is further expanded\nthrough persona stylization into roughly 10 times larger scale (128k turns),\nproviding a solid foundation for evaluation. Building on this resource, we\ndecompose hard-to-quantify subjective performance into three progressive tasks:\nTASK1 basic coherence (whether behavior, emotion, expression, and voice align\nwith classroom context), TASK2 student realism, and TASK3 long-term persona\nconsistency, thereby establishing an evaluation framework grounded in\neducational theory and research value. We conduct systematic experiments on\nthree representative LLMs, comparing their original versions with ten\npersona-fine-tuned variants trained on EduPersona. Results show consistent and\nsignificant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,\nand TASK3 +14.9%. These improvements highlight the dataset's effectiveness and\nresearch value, while also revealing the heterogeneous difficulty of persona\nmodeling. In summary, EduPersona delivers the first classroom benchmark\ncentered on subjective abilities, establishes a decoupled and verifiable\nresearch paradigm, and we will open-source both the dataset and the framework\nto support the broader research community in advancing trustworthy and\nhuman-like AI for education.",
    "published": "2025-10-06T09:52:18Z",
    "updated": "2025-10-06T09:52:18Z",
    "link": "http://arxiv.org/pdf/2510.04648v1.pdf",
    "category": [
      "cs.CV",
      "cs.CY"
    ],
    "authors": [
      "Buyuan Zhu",
      "Shiyu Hu",
      "Yiping Ma",
      "Yuanming Zhang",
      "Kang Hao Cheong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08896v4",
    "title": "WetCat: Enabling Automated Skill Assessment in Wet-Lab Cataract Surgery\n  Videos",
    "summary": "To meet the growing demand for systematic surgical training, wet-lab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wet-lab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wet-lab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wet-lab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse.",
    "published": "2025-06-10T15:22:55Z",
    "updated": "2025-10-06T09:51:36Z",
    "link": "http://arxiv.org/pdf/2506.08896v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Negin Ghamsarian",
      "Raphael Sznitman",
      "Klaus Schoeffmann",
      "Jens Kowal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04645v1",
    "title": "Do Superpixel Segmentation Methods Influence Deforestation Image\n  Classification?",
    "summary": "Image segmentation is a crucial step in various visual applications,\nincluding environmental monitoring through remote sensing. In the context of\nthe ForestEyes project, which combines citizen science and machine learning to\ndetect deforestation in tropical forests, image segments are used for labeling\nby volunteers and subsequent model training. Traditionally, the Simple Linear\nIterative Clustering (SLIC) algorithm is adopted as the segmentation method.\nHowever, recent studies have indicated that other superpixel-based methods\noutperform SLIC in remote sensing image segmentation, and might suggest that\nthey are more suitable for the task of detecting deforested areas. In this\nsense, this study investigated the impact of the four best segmentation\nmethods, together with SLIC, on the training of classifiers for the target\napplication. Initially, the results showed little variation in performance\namong segmentation methods, even when selecting the top five classifiers using\nthe PyCaret AutoML library. However, by applying a classifier fusion approach\n(ensemble of classifiers), noticeable improvements in balanced accuracy were\nobserved, highlighting the importance of both the choice of segmentation method\nand the combination of machine learning-based models for deforestation\ndetection tasks.",
    "published": "2025-10-06T09:46:17Z",
    "updated": "2025-10-06T09:46:17Z",
    "link": "http://arxiv.org/pdf/2510.04645v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hugo Resende",
      "Fabio A. Faria",
      "Eduardo B. Neto",
      "Isabela Borlido",
      "Victor Sundermann",
      "Silvio Jamil F. Guimarães",
      "Álvaro L. Fazenda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04637v1",
    "title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via\n  Conversational LLM Agents",
    "summary": "We present Social Agent, a novel framework for synthesizing realistic and\ncontextually appropriate co-speech nonverbal behaviors in dyadic conversations.\nIn this framework, we develop an agentic system driven by a Large Language\nModel (LLM) to direct the conversation flow and determine appropriate\ninteractive behaviors for both participants. Additionally, we propose a novel\ndual-person gesture generation model based on an auto-regressive diffusion\nmodel, which synthesizes coordinated motions from speech signals. The output of\nthe agentic system is translated into high-level guidance for the gesture\ngenerator, resulting in realistic movement at both the behavioral and motion\nlevels. Furthermore, the agentic system periodically examines the movements of\ninterlocutors and infers their intentions, forming a continuous feedback loop\nthat enables dynamic and responsive interactions between the two participants.\nUser studies and quantitative evaluations show that our model significantly\nimproves the quality of dyadic interactions, producing natural, synchronized\nnonverbal behaviors.",
    "published": "2025-10-06T09:41:37Z",
    "updated": "2025-10-06T09:41:37Z",
    "link": "http://arxiv.org/pdf/2510.04637v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Zeyi Zhang",
      "Yanju Zhou",
      "Heyuan Yao",
      "Tenglong Ao",
      "Xiaohang Zhan",
      "Libin Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.20436v2",
    "title": "CoralSCOP-LAT: Labeling and Analyzing Tool for Coral Reef Images with\n  Dense Mask",
    "summary": "Coral reef imagery offers critical data for monitoring ecosystem health, in\nparticular as the ease of image datasets continues to rapidly expand. Whilst\nsemi-automated analytical platforms for reef imagery are becoming more\navailable, the dominant approaches face fundamental limitations. To address\nthese challenges, we propose CoralSCOP-LAT, a coral reef image analysis and\nlabeling tool that automatically segments and analyzes coral regions. By\nleveraging advanced machine learning models tailored for coral reef\nsegmentation, CoralSCOP-LAT enables users to generate dense segmentation masks\nwith minimal manual effort, significantly enhancing both the labeling\nefficiency and precision of coral reef analysis. Our extensive evaluations\ndemonstrate that CoralSCOP-LAT surpasses existing coral reef analysis tools in\nterms of time efficiency, accuracy, precision, and flexibility. CoralSCOP-LAT,\ntherefore, not only accelerates the coral reef annotation process but also\nassists users in obtaining high-quality coral reef segmentation and analysis\noutcomes. Github Page: https://github.com/ykwongaq/CoralSCOP-LAT.",
    "published": "2024-10-27T13:26:44Z",
    "updated": "2025-10-06T09:41:21Z",
    "link": "http://arxiv.org/pdf/2410.20436v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuk-Kwan Wong",
      "Ziqiang Zheng",
      "Mingzhe Zhang",
      "David Suggett",
      "Sai-Kit Yeung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04628v1",
    "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote\n  Sensing Classification",
    "summary": "Deep learning-based methods have achieved significant success in remote\nsensing Earth observation data analysis. Numerous feature fusion techniques\naddress multimodal remote sensing image classification by integrating global\nand local features. However, these techniques often struggle to extract\nstructural and detail features from heterogeneous and redundant multimodal\nimages. With the goal of introducing frequency domain learning to model key and\nsparse detail features, this paper introduces the spatial-spectral-frequency\ninteraction network (S$^2$Fin), which integrates pairwise fusion modules across\nthe spatial, spectral, and frequency domains. Specifically, we propose a\nhigh-frequency sparse enhancement transformer that employs sparse\nspatial-spectral attention to optimize the parameters of the high-frequency\nfilter. Subsequently, a two-level spatial-frequency fusion strategy is\nintroduced, comprising an adaptive frequency channel module that fuses\nlow-frequency structures with enhanced high-frequency details, and a\nhigh-frequency resonance mask that emphasizes sharp edges via phase similarity.\nIn addition, a spatial-spectral attention fusion module further enhances\nfeature extraction at intermediate layers of the network. Experiments on four\nbenchmark multimodal datasets with limited labeled data demonstrate that\nS$^2$Fin performs superior classification, outperforming state-of-the-art\nmethods. The code is available at https://github.com/HaoLiu-XDU/SSFin.",
    "published": "2025-10-06T09:33:35Z",
    "updated": "2025-10-06T09:33:35Z",
    "link": "http://arxiv.org/pdf/2510.04628v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Liu",
      "Yunhao Gao",
      "Wei Li",
      "Mingyang Zhang",
      "Maoguo Gong",
      "Lorenzo Bruzzone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20509v2",
    "title": "MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for\n  Hyperspectral Image Classification",
    "summary": "Mamba-based models have recently demonstrated significant potential in\nhyperspectral image (HSI) classification, primarily due to their ability to\nperform contextual modeling with linear computational complexity. However,\nexisting Mamba-based approaches often overlook the directional modeling\nheterogeneity across different land-cover types, leading to limited\nclassification performance. To address these limitations, we propose MambaMoE,\na novel spectral-spatial Mixture-of-Experts (MoE) framework, which represents\nthe first MoE-based approach in the HSI classification domain. Specifically, we\ndesign a Mixture of Mamba Expert Block (MoMEB) that performs adaptive\nspectral-spatial feature modeling via a sparse expert activation mechanism.\nAdditionally, we introduce an uncertainty-guided corrective learning (UGCL)\nstrategy that encourages the model to focus on complex regions prone to\nprediction ambiguity. This strategy dynamically samples supervision signals\nfrom regions with high predictive uncertainty, guiding the model to adaptively\nrefine feature representations and thereby enhancing its focus on challenging\nareas. Extensive experiments conducted on multiple public HSI benchmark\ndatasets show that MambaMoE achieves state-of-the-art performance in both\nclassification accuracy and computational efficiency compared to existing\nadvanced methods, particularly Mamba-based ones. The code will be available\nonline at https://github.com/YichuXu/MambaMoE.",
    "published": "2025-04-29T07:50:36Z",
    "updated": "2025-10-06T09:09:59Z",
    "link": "http://arxiv.org/pdf/2504.20509v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yichu Xu",
      "Di Wang",
      "Hongzan Jiao",
      "Lefei Zhang",
      "Liangpei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04587v1",
    "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior",
    "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.",
    "published": "2025-10-06T08:44:04Z",
    "updated": "2025-10-06T08:44:04Z",
    "link": "http://arxiv.org/pdf/2510.04587v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sheng Wang",
      "Ruiming Wu",
      "Charles Herndon",
      "Yihang Liu",
      "Shunsuke Koga",
      "Jeanne Shen",
      "Zhi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02898v2",
    "title": "One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework",
    "summary": "Zero-shot captioners are recently proposed models that utilize common-space\nvision-language representations to caption images without relying on paired\nimage-text data. To caption an image, they proceed by textually decoding a\ntext-aligned image feature, but they limit their scope to global\nrepresentations and whole-image captions. We present Patch-ioner, a unified\nframework for zero-shot captioning that shifts from an image-centric to a\npatch-centric paradigm, enabling the captioning of arbitrary regions without\nthe need of region-level supervision. Instead of relying on global image\nrepresentations, we treat individual patches as atomic captioning units and\naggregate them to describe arbitrary regions, from single patches to\nnon-contiguous areas and entire images. We analyze the key ingredients that\nenable current latent captioners to work in our novel proposed framework.\nExperiments demonstrate that backbones producing meaningful, dense visual\nfeatures, such as DINO, are key to achieving state-of-the-art performance in\nmultiple region-based captioning tasks. Compared to other baselines and\nstate-of-the-art competitors, our models achieve better performance on\nzero-shot dense, region-set, and a newly introduced trace captioning task,\nhighlighting the effectiveness of patch-wise semantic representations for\nscalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .",
    "published": "2025-10-03T11:05:56Z",
    "updated": "2025-10-06T08:43:27Z",
    "link": "http://arxiv.org/pdf/2510.02898v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lorenzo Bianchi",
      "Giacomo Pacini",
      "Fabio Carrara",
      "Nicola Messina",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.10081v2",
    "title": "Leveraging Confident Image Regions for Source-Free Domain-Adaptive\n  Object Detection",
    "summary": "Source-free domain-adaptive object detection is an interesting but scarcely\naddressed topic. It aims at adapting a source-pretrained detector to a distinct\ntarget domain without resorting to source data during adaptation. So far, there\nis no data augmentation scheme tailored to source-free domain-adaptive object\ndetection. To this end, this paper presents a novel data augmentation approach\nthat cuts out target image regions where the detector is confident, augments\nthem along with their respective pseudo-labels, and joins them into a\nchallenging target image to adapt the detector. As the source data is out of\nreach during adaptation, we implement our approach within a teacher-student\nlearning paradigm to ensure that the model does not collapse during the\nadaptation procedure. We evaluated our approach on three adaptation benchmarks\nof traffic scenes, scoring new state-of-the-art on two of them.",
    "published": "2025-01-17T09:55:41Z",
    "updated": "2025-10-06T08:32:42Z",
    "link": "http://arxiv.org/pdf/2501.10081v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohamed Lamine Mekhalfi",
      "Davide Boscaini",
      "Fabio Poiesi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04564v1",
    "title": "Conditional Representation Learning for Customized Tasks",
    "summary": "Conventional representation learning methods learn a universal representation\nthat primarily captures dominant semantics, which may not always align with\ncustomized downstream tasks. For instance, in animal habitat analysis,\nresearchers prioritize scene-related features, whereas universal embeddings\nemphasize categorical semantics, leading to suboptimal results. As a solution,\nexisting approaches resort to supervised fine-tuning, which however incurs high\ncomputational and annotation costs. In this paper, we propose Conditional\nRepresentation Learning (CRL), aiming to extract representations tailored to\narbitrary user-specified criteria. Specifically, we reveal that the semantics\nof a space are determined by its basis, thereby enabling a set of descriptive\nwords to approximate the basis for a customized feature space. Building upon\nthis insight, given a user-specified criterion, CRL first employs a large\nlanguage model (LLM) to generate descriptive texts to construct the semantic\nbasis, then projects the image representation into this conditional feature\nspace leveraging a vision-language model (VLM). The conditional representation\nbetter captures semantics for the specific criterion, which could be utilized\nfor multiple customized tasks. Extensive experiments on classification and\nretrieval tasks demonstrate the superiority and generality of the proposed CRL.\nThe code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.",
    "published": "2025-10-06T08:00:59Z",
    "updated": "2025-10-06T08:00:59Z",
    "link": "http://arxiv.org/pdf/2510.04564v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Honglin Liu",
      "Chao Sun",
      "Peng Hu",
      "Yunfan Li",
      "Xi Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23256v2",
    "title": "EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan\n  Africa using MedNeXt V2 with Deep Supervision",
    "summary": "Brain cancer affects millions worldwide, and in nearly every clinical\nsetting, doctors rely on magnetic resonance imaging (MRI) to diagnose and\nmonitor gliomas. However, the current standard for tumor quantification through\nmanual segmentation of multi-parametric MRI is time-consuming, requires expert\nradiologists, and is often infeasible in under-resourced healthcare systems.\nThis problem is especially pronounced in low-income regions, where MRI scanners\nare of lower quality and radiology expertise is scarce, leading to incorrect\nsegmentation and quantification. In addition, the number of acquired MRI scans\nin Africa is typically small. To address these challenges, the BraTS-Lighthouse\n2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa\n(SSA), where resource constraints and image quality degradation introduce\nsignificant shifts. In this study, we present EMedNeXt -- an enhanced brain\ntumor segmentation framework based on MedNeXt V2 with deep supervision and\noptimized post-processing pipelines tailored for SSA. EMedNeXt introduces three\nkey contributions: a larger region of interest, an improved nnU-Net v2-based\narchitectural skeleton, and a robust model ensembling system. Evaluated on the\nhidden validation set, our solution achieved an average LesionWise DSC of 0.897\nwith an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and\n1.0 mm, respectively.",
    "published": "2025-07-31T05:30:19Z",
    "updated": "2025-10-06T07:47:51Z",
    "link": "http://arxiv.org/pdf/2507.23256v2.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Ahmed Jaheen",
      "Abdelrahman Elsayed",
      "Damir Kim",
      "Daniil Tikhonov",
      "Matheus Scatolin",
      "Mohor Banerjee",
      "Qiankun Ji",
      "Mostafa Salem",
      "Hu Wang",
      "Sarim Hashmi",
      "Mohammad Yaqub"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.16357v3",
    "title": "Law of Vision Representation in MLLMs",
    "summary": "We present the \"Law of Vision Representation\" in multimodal large language\nmodels (MLLMs). It reveals a strong correlation between the combination of\ncross-modal alignment, correspondence in vision representation, and MLLM\nperformance. We quantify the two factors using the cross-modal Alignment and\nCorrespondence score (AC score). Through extensive experiments involving\nthirteen different vision representation settings and evaluations across eight\nbenchmarks, we find that the AC score is linearly correlated to model\nperformance. By leveraging this relationship, we are able to identify and train\nthe optimal vision representation only, which does not require finetuning the\nlanguage model every time, resulting in a 99.7% reduction in computational\ncost.",
    "published": "2024-08-29T08:56:48Z",
    "updated": "2025-10-06T07:37:19Z",
    "link": "http://arxiv.org/pdf/2408.16357v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shijia Yang",
      "Bohan Zhai",
      "Quanzeng You",
      "Jianbo Yuan",
      "Hongxia Yang",
      "Chenfeng Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04553v1",
    "title": "Fast Witness Persistence for MRI Volumes via Hybrid Landmarking",
    "summary": "We introduce a scalable witness-based persistent homology pipeline for\nfull-brain MRI volumes that couples density-aware landmark selection with a\nGPU-ready witness filtration. Candidates are scored by a hybrid metric that\nbalances geometric coverage against inverse kernel density, yielding landmark\nsets that shrink mean pairwise distances by 30-60% over random or density-only\nbaselines while preserving topological features. Benchmarks on BrainWeb, IXI,\nand synthetic manifolds execute in under ten seconds on a single NVIDIA RTX\n4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha\nfiltrations. The package is distributed on PyPI as whale-tda (installable via\npip); source and issues are hosted at https://github.com/jorgeLRW/whale. The\nrelease also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,\nand ships with reproducibility-focused scripts and artifacts for drop-in use in\nmedical imaging workflows.",
    "published": "2025-10-06T07:34:21Z",
    "updated": "2025-10-06T07:34:21Z",
    "link": "http://arxiv.org/pdf/2510.04553v1.pdf",
    "category": [
      "cs.CG",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jorge Leonardo Ruiz Williams"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04547v1",
    "title": "Post-training quantization of vision encoders needs prefixing registers",
    "summary": "Transformer-based vision encoders -- such as CLIP -- are central to\nmultimodal intelligence, powering applications from autonomous web agents to\nrobotic control. Since these applications often demand real-time processing of\nmassive visual data, reducing the inference cost of vision encoders is\ncritical. Post-training quantization offers a practical path, but remains\nchallenging even at 8-bit precision due to massive-scale activations (i.e.,\noutliers). In this work, we propose $\\textit{RegCache}$, a training-free\nalgorithm to mitigate outliers in vision encoders, enabling quantization with\nsignificantly smaller accuracy drops. The proposed RegCache introduces\noutlier-prone yet semantically meaningless prefix tokens to the target vision\nencoder, which prevents other tokens from having outliers. Notably, we observe\nthat outliers in vision encoders behave differently from those in language\nmodels, motivating two technical innovations: middle-layer prefixing and token\ndeletion. Experiments show that our method consistently improves the accuracy\nof quantized models across both text-supervised and self-supervised vision\nencoders.",
    "published": "2025-10-06T07:27:46Z",
    "updated": "2025-10-06T07:27:46Z",
    "link": "http://arxiv.org/pdf/2510.04547v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Seunghyeon Kim",
      "Jinho Kim",
      "Taesun Yeom",
      "Wonpyo Park",
      "Kyuyeun Kim",
      "Jaeho Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.08214v3",
    "title": "Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification\n  Mapping on Non-Contrast CT",
    "summary": "While total intracranial carotid artery calcification (ICAC) volume is an\nestablished stroke biomarker, growing evidence shows this aggregate metric\nignores the critical influence of plaque location, since calcification in\ndifferent segments carries distinct prognostic and procedural risks. However, a\nfiner-grained, segment-specific quantification has remained technically\ninfeasible. Conventional 3D models are forced to process downsampled volumes or\nisolated patches, sacrificing the global context required to resolve anatomical\nambiguity and render reliable landmark localization. To overcome this, we\nreformulate the 3D challenge as a \\textbf{Parallel Probabilistic Landmark\nLocalization} task along the 1D axial dimension. We propose the\n\\textbf{Depth-Sequence Transformer (DST)}, a framework that processes\nfull-resolution CT volumes as sequences of 2D slices, learning to predict $N=6$\nindependent probability distributions that pinpoint key anatomical landmarks.\nOur DST framework demonstrates exceptional accuracy and robustness. Evaluated\non a 100-patient clinical cohort with rigorous 5-fold cross-validation, it\nachieves a Mean Absolute Error (MAE) of \\textbf{0.1 slices}, with \\textbf{96\\%}\nof predictions falling within a $\\pm1$ slice tolerance. Furthermore, to\nvalidate its architectural power, the DST backbone establishes the best result\non the public Clean-CC-CCII classification benchmark under an end-to-end\nevaluation protocol. Our work delivers the first practical tool for automated\nsegment-specific ICAC analysis. The proposed framework provides a foundation\nfor further studies on the role of location-specific biomarkers in diagnosis,\nprognosis, and procedural planning.",
    "published": "2025-07-10T23:12:12Z",
    "updated": "2025-10-06T07:24:47Z",
    "link": "http://arxiv.org/pdf/2507.08214v3.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Xiangjian Hou",
      "Ebru Yaman Akcicek",
      "Xin Wang",
      "Kazem Hashemizadeh",
      "Scott Mcnally",
      "Chun Yuan",
      "Xiaodong Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04539v1",
    "title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing",
    "summary": "Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations.",
    "published": "2025-10-06T07:07:14Z",
    "updated": "2025-10-06T07:07:14Z",
    "link": "http://arxiv.org/pdf/2510.04539v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Zeng Tao",
      "Zheng Ding",
      "Zeyuan Chen",
      "Xiang Zhang",
      "Leizhi Li",
      "Zhuowen Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07510v2",
    "title": "Divergence Minimization Preference Optimization for Diffusion Model\n  Alignment",
    "summary": "Diffusion models have achieved remarkable success in generating realistic and\nversatile images from text prompts. Inspired by the recent advancements of\nlanguage models, there is an increasing interest in further improving the\nmodels by aligning with human preferences. However, we investigate alignment\nfrom a divergence minimization perspective and reveal that existing preference\noptimization methods are typically trapped in suboptimal mean-seeking\noptimization. In this paper, we introduce Divergence Minimization Preference\nOptimization (DMPO), a novel and principled method for aligning diffusion\nmodels by minimizing reverse KL divergence, which asymptotically enjoys the\nsame optimization direction as original RL. We provide rigorous analysis to\njustify the effectiveness of DMPO and conduct comprehensive experiments to\nvalidate its empirical strength across both human evaluations and automatic\nmetrics. Our extensive results show that diffusion models fine-tuned with DMPO\ncan consistently outperform or match existing techniques, specifically\nconsistently outperforming all baseline models across different base models and\ntest sets, achieving the best PickScore in every case, demonstrating the\nmethod's superiority in aligning generative behavior with desired outputs.\nOverall, DMPO unlocks a robust and elegant pathway for preference alignment,\nbridging principled theory with practical performance in diffusion models.",
    "published": "2025-07-10T07:57:30Z",
    "updated": "2025-10-06T07:01:28Z",
    "link": "http://arxiv.org/pdf/2507.07510v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Binxu Li",
      "Minkai Xu",
      "Jiaqi Han",
      "Meihua Dang",
      "Stefano Ermon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21574v2",
    "title": "Do We Need All the Synthetic Data? Targeted Synthetic Image Augmentation\n  via Diffusion Models",
    "summary": "Synthetically augmenting training datasets with diffusion models has been an\neffective strategy for improving generalization of image classifiers. However,\nexisting techniques struggle to ensure the diversity of generation and increase\nthe size of the data by up to 10-30x to improve the in-distribution\nperformance. In this work, we show that synthetically augmenting part of the\ndata that is not learned early in training with faithful images-containing same\nfeatures but different noise-outperforms augmenting the entire dataset. By\nanalyzing a two-layer CNN, we prove that this strategy improves generalization\nby promoting homogeneity in feature learning speed without amplifying noise.\nOur extensive experiments show that by augmenting only 30%-40% of the data, our\nmethod boosts generalization by up to 2.8% in a variety of scenarios, including\ntraining ResNet, ViT, ConvNeXt, and Swin Transformer on CIFAR-10/100, and\nTinyImageNet, with various optimizers including SGD and SAM. Notably, our\nmethod applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and\nTinyImageNet.",
    "published": "2025-05-27T07:27:03Z",
    "updated": "2025-10-06T06:55:59Z",
    "link": "http://arxiv.org/pdf/2505.21574v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Dang Nguyen",
      "Jiping Li",
      "Jinghao Zheng",
      "Baharan Mirzasoleiman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04533v1",
    "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
    "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
    "published": "2025-10-06T06:53:29Z",
    "updated": "2025-10-06T06:53:29Z",
    "link": "http://arxiv.org/pdf/2510.04533v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hyunmin Cho",
      "Donghoon Ahn",
      "Susung Hong",
      "Jee Eun Kim",
      "Seungryong Kim",
      "Kyong Hwan Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04510v1",
    "title": "Real-time Prediction of Urban Sound Propagation with Conditioned\n  Normalizing Flows",
    "summary": "Accurate and fast urban noise prediction is pivotal for public health and for\nregulatory workflows in cities, where the Environmental Noise Directive\nmandates regular strategic noise maps and action plans, often needed in\npermission workflows, right-of-way allocation, and construction scheduling.\nPhysics-based solvers are too slow for such time-critical, iterative \"what-if\"\nstudies. We evaluate conditional Normalizing Flows (Full-Glow) for generating\nfor generating standards-compliant urban sound-pressure maps from 2D urban\nlayouts in real time per 256x256 map on a single RTX 4090), enabling\ninteractive exploration directly on commodity hardware. On datasets covering\nBaseline, Diffraction, and Reflection regimes, our model accelerates map\ngeneration by >2000 times over a reference solver while improving NLoS accuracy\nby up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE\nwith high structural fidelity. The model reproduces diffraction and\ninterference patterns and supports instant recomputation under source or\ngeometry changes, making it a practical engine for urban planning, compliance\nmapping, and operations (e.g., temporary road closures, night-work variance\nassessments).",
    "published": "2025-10-06T06:00:08Z",
    "updated": "2025-10-06T06:00:08Z",
    "link": "http://arxiv.org/pdf/2510.04510v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Achim Eckerle",
      "Martin Spitznagel",
      "Janis Keuper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04504v1",
    "title": "Asynchronous Denoising Diffusion Models for Aligning Text-to-Image\n  Generation",
    "summary": "Diffusion models have achieved impressive results in generating high-quality\nimages. Yet, they often struggle to faithfully align the generated images with\nthe input prompts. This limitation arises from synchronous denoising, where all\npixels simultaneously evolve from random noise to clear images. As a result,\nduring generation, the prompt-related regions can only reference the unrelated\nregions at the same noise level, failing to obtain clear context and ultimately\nimpairing text-to-image alignment. To address this issue, we propose\nasynchronous diffusion models -- a novel framework that allocates distinct\ntimesteps to different pixels and reformulates the pixel-wise denoising\nprocess. By dynamically modulating the timestep schedules of individual pixels,\nprompt-related regions are denoised more gradually than unrelated regions,\nthereby allowing them to leverage clearer inter-pixel context. Consequently,\nthese prompt-related regions achieve better alignment in the final images.\nExtensive experiments demonstrate that our asynchronous diffusion models can\nsignificantly improve text-to-image alignment across diverse prompts. The code\nrepository for this work is available at https://github.com/hu-zijing/AsynDM.",
    "published": "2025-10-06T05:45:56Z",
    "updated": "2025-10-06T05:45:56Z",
    "link": "http://arxiv.org/pdf/2510.04504v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zijing Hu",
      "Yunze Tong",
      "Fengda Zhang",
      "Junkun Yuan",
      "Jun Xiao",
      "Kun Kuang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04483v1",
    "title": "TBStar-Edit: From Image Editing Pattern Shifting to Consistency\n  Enhancement",
    "summary": "Recent advances in image generation and editing technologies have enabled\nstate-of-the-art models to achieve impressive results in general domains.\nHowever, when applied to e-commerce scenarios, these general models often\nencounter consistency limitations. To address this challenge, we introduce\nTBStar-Edit, an new image editing model tailored for the e-commerce domain.\nThrough rigorous data engineering, model architecture design and training\nstrategy, TBStar-Edit achieves precise and high-fidelity image editing while\nmaintaining the integrity of product appearance and layout. Specifically, for\ndata engineering, we establish a comprehensive data construction pipeline,\nencompassing data collection, construction, filtering, and augmentation, to\nacquire high-quality, instruction-following, and strongly consistent editing\ndata to support model training. For model architecture design, we design a\nhierarchical model framework consisting of a base model, pattern shifting\nmodules, and consistency enhancement modules. For model training, we adopt a\ntwo-stage training strategy to enhance the consistency preservation: first\nstage for editing pattern shifting, and second stage for consistency\nenhancement. Each stage involves training different modules with separate\ndatasets. Finally, we conduct extensive evaluations of TBStar-Edit on a\nself-proposed e-commerce benchmark, and the results demonstrate that\nTBStar-Edit outperforms existing general-domain editing models in both\nobjective metrics (VIE Score) and subjective user preference.",
    "published": "2025-10-06T04:46:42Z",
    "updated": "2025-10-06T04:46:42Z",
    "link": "http://arxiv.org/pdf/2510.04483v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Fang",
      "Zechao Zhan",
      "Weixin Feng",
      "Ziwei Huang",
      "XuBin Li",
      "Tiezheng Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14512v2",
    "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex\n  Reasoning Tasks",
    "summary": "Large Language Models (LLMs) have undergone rapid progress, largely\nattributed to reinforcement learning on complex reasoning tasks. In contrast,\nwhile spatial intelligence is fundamental for Vision-Language Models (VLMs) in\nreal-world interaction, the systematic study of their complex spatial reasoning\nremains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark\ndesigned to evaluate VLMs' structural spatial intelligence through\nspatial-grounded reasoning tasks. SIRI-Bench comprises 9,000\nvideo-question-answer triplets, where each problem is embedded in a realistic\n3D scene. The benchmark is carefully designed so that solving each problem\nrequires both spatial comprehension and structural reasoning. To facilitate\nlarge-scale data synthesis, we develop an Automatic Scene Creation Engine that\nemploys collaborative LLM agents to translate abstract mathematical problems\ninto faithful 3D scenes. Experimental results reveal that state-of-the-art VLMs\nstruggle significantly on SIRI-Bench, underscoring the challenge of structural\nspatial reasoning. We hope that our study will bring researchers' attention to\nspatially grounded reasoning and advance VLMs in visual problem-solving.",
    "published": "2025-06-17T13:40:00Z",
    "updated": "2025-10-06T04:31:42Z",
    "link": "http://arxiv.org/pdf/2506.14512v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zijian Song",
      "Xiaoxin Lin",
      "Qiuming Huang",
      "Guangrun Wang",
      "Liang Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04479v1",
    "title": "VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery",
    "summary": "Vision-Language Models (VLMs) have achieved significant progress in\nmultimodal understanding tasks, demonstrating strong capabilities particularly\nin general tasks such as image captioning and visual reasoning. However, when\ndealing with specialized cultural heritage domains like 3D vase artifacts,\nexisting models face severe data scarcity issues and insufficient domain\nknowledge limitations. Due to the lack of targeted training data, current VLMs\nstruggle to effectively handle such culturally significant specialized tasks.\nTo address these challenges, we propose the VaseVQA-3D dataset, which serves as\nthe first 3D visual question answering dataset for ancient Greek pottery\nanalysis, collecting 664 ancient Greek vase 3D models with corresponding\nquestion-answer data and establishing a complete data construction pipeline. We\nfurther develop the VaseVLM model, enhancing model performance in vase artifact\nanalysis through domain-adaptive training. Experimental results validate the\neffectiveness of our approach, where we improve by 12.8% on R@1 metrics and by\n6.6% on lexical similarity compared with previous state-of-the-art on the\nVaseVQA-3D dataset, significantly improving the recognition and understanding\nof 3D vase artifacts, providing new technical pathways for digital heritage\npreservation research.",
    "published": "2025-10-06T04:28:39Z",
    "updated": "2025-10-06T04:28:39Z",
    "link": "http://arxiv.org/pdf/2510.04479v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nonghai Zhang",
      "Zeyu Zhang",
      "Jiazi Wang",
      "Yang Zhao",
      "Hao Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07120v3",
    "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
    "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
    "published": "2025-03-10T09:49:18Z",
    "updated": "2025-10-06T04:28:05Z",
    "link": "http://arxiv.org/pdf/2503.07120v3.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zhen Zou",
      "Feng Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12157v2",
    "title": "Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning",
    "summary": "Ultrahigh field (UHF) Magnetic Resonance Imaging (MRI) offers an elevated\nsignal-to-noise ratio (SNR), enabling exceptionally high spatial resolution\nthat benefits both clinical diagnostics and advanced research. However, the\njump to higher fields introduces complications, particularly transmit\nradiofrequency (RF) field ($B_{1}^{+}$) inhomogeneities, manifesting as uneven\nflip angles and image intensity irregularities. These artifacts can degrade\nimage quality and impede broader clinical adoption. Traditional RF shimming\nmethods, such as Magnitude Least Squares (MLS) optimization, effectively\nmitigate $B_{1}^{+}$ inhomogeneity, but remain time-consuming. Recent machine\nlearning approaches, including RF Shim Prediction by Iteratively Projected\nRidge Regression and other deep learning architectures, suggest alternative\npathways. Although these approaches show promise, challenges such as extensive\ntraining periods, limited network complexity, and practical data requirements\npersist. In this paper, we introduce a holistic learning-based framework called\nFast-RF-Shimming, which achieves a 5000x speed-up compared to the traditional\nMLS method. In the initial phase, we employ random-initialized Adaptive Moment\nEstimation (Adam) to derive the desired reference shimming weights from\nmulti-channel $B_{1}^{+}$ fields. Next, we train a Residual Network (ResNet) to\nmap $B_{1}^{+}$ fields directly to the ultimate RF shimming outputs,\nincorporating the confidence parameter into its loss function. Finally, we\ndesign Non-uniformity Field Detector (NFD), an optional post-processing step,\nto ensure the extreme non-uniform outcomes are identified. Comparative\nevaluations with standard MLS optimization underscore notable gains in both\nprocessing speed and predictive accuracy, which indicates that our technique\nshows a promising solution for addressing persistent inhomogeneity challenges.",
    "published": "2025-01-21T14:09:58Z",
    "updated": "2025-10-06T03:43:52Z",
    "link": "http://arxiv.org/pdf/2501.12157v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhengyi Lu",
      "Hao Liang",
      "Ming Lu",
      "Xiao Wang",
      "Xinqiang Yan",
      "Yuankai Huo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04450v1",
    "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer\n  Consistency Regularization",
    "summary": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).",
    "published": "2025-10-06T02:48:13Z",
    "updated": "2025-10-06T02:48:13Z",
    "link": "http://arxiv.org/pdf/2510.04450v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qiyuan He",
      "Yicong Li",
      "Haotian Ye",
      "Jinghao Wang",
      "Xinyao Liao",
      "Pheng-Ann Heng",
      "Stefano Ermon",
      "James Zou",
      "Angela Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24758v3",
    "title": "ExGS: Extreme 3D Gaussian Compression with Diffusion Priors",
    "summary": "Neural scene representations, such as 3D Gaussian Splatting (3DGS), have\nenabled high-quality neural rendering; however, their large storage and\ntransmission costs hinder deployment in resource-constrained environments.\nExisting compression methods either rely on costly optimization, which is slow\nand scene-specific, or adopt training-free pruning and quantization, which\ndegrade rendering quality under high compression ratios. In contrast, recent\ndata-driven approaches provide a promising direction to overcome this\ntrade-off, enabling efficient compression while preserving high rendering\nquality.We introduce ExGS, a novel feed-forward framework that unifies\nUniversal Gaussian Compression (UGC) with GaussPainter for Extreme 3DGS\ncompression. UGC performs re-optimization-free pruning to aggressively reduce\nGaussian primitives while retaining only essential information, whereas\nGaussPainter leverages powerful diffusion priors with mask-guided refinement to\nrestore high-quality renderings from heavily pruned Gaussian scenes. Unlike\nconventional inpainting, GaussPainter not only fills in missing regions but\nalso enhances visible pixels, yielding substantial improvements in degraded\nrenderings.To ensure practicality, it adopts a lightweight VAE and a one-step\ndiffusion design, enabling real-time restoration. Our framework can even\nachieve over 100X compression (reducing a typical 354.77 MB model to about 3.31\nMB) while preserving fidelity and significantly improving image quality under\nchallenging conditions. These results highlight the central role of diffusion\npriors in bridging the gap between extreme compression and high-quality neural\nrendering.Our code repository will be released at:\nhttps://github.com/chenttt2001/ExGS",
    "published": "2025-09-29T13:23:06Z",
    "updated": "2025-10-06T02:40:44Z",
    "link": "http://arxiv.org/pdf/2509.24758v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaqi Chen",
      "Xinhao Ji",
      "Yuanyuan Gao",
      "Hao Li",
      "Yuning Gong",
      "Yifei Liu",
      "Dan Xu",
      "Zhihang Zhong",
      "Dingwen Zhang",
      "Xiao Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.11941v2",
    "title": "TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians\n  for Robust Reconstruction",
    "summary": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Recent\nmethods extend 3D Gaussian Splatting to dynamic scenes via additional\ndeformation fields and apply explicit constraints like motion flow to guide the\ndeformation. However, they learn motion changes from individual timestamps\nindependently, making it challenging to reconstruct complex scenes,\nparticularly when dealing with violent movement, extreme-shaped geometries, or\nreflective surfaces. To address the above issue, we design a plug-and-play\nmodule called TimeFormer to enable existing deformable 3D Gaussians\nreconstruction methods with the ability to implicitly model motion patterns\nfrom a learning perspective. Specifically, TimeFormer includes a Cross-Temporal\nTransformer Encoder, which adaptively learns the temporal relationships of\ndeformable 3D Gaussians. Furthermore, we propose a two-stream optimization\nstrategy that transfers the motion knowledge learned from TimeFormer to the\nbase stream during the training phase. This allows us to remove TimeFormer\nduring inference, thereby preserving the original rendering speed. Extensive\nexperiments in the multi-view and monocular dynamic scenes validate qualitative\nand quantitative improvement brought by TimeFormer. Project Page:\nhttps://patrickddj.github.io/TimeFormer/",
    "published": "2024-11-18T17:11:11Z",
    "updated": "2025-10-06T02:13:04Z",
    "link": "http://arxiv.org/pdf/2411.11941v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "DaDong Jiang",
      "Zhihui Ke",
      "Xiaobo Zhou",
      "Zhi Hou",
      "Xianghui Yang",
      "Wenbo Hu",
      "Tie Qiu",
      "Chunchao Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04428v1",
    "title": "A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame\n  Selection For Video Question Answering",
    "summary": "Effectively applying Vision-Language Models (VLMs) to Video Question\nAnswering (VideoQA) hinges on selecting a concise yet comprehensive set of\nframes, as processing entire videos is computationally infeasible. However,\ncurrent frame selection methods face a critical trade-off: approaches relying\non lightweight similarity models, such as CLIP, often fail to capture the\nnuances of complex queries, resulting in inaccurate similarity scores that\ncannot reflect the authentic query-frame relevance, which further undermines\nframe selection. Meanwhile, methods that leverage a VLM for deeper analysis\nachieve higher accuracy but incur prohibitive computational costs. To address\nthese limitations, we propose A.I.R., a training-free approach for Adaptive,\nIterative, and Reasoning-based frame selection. We leverage a powerful VLM to\nperform deep, semantic analysis on complex queries, and this analysis is\ndeployed within a cost-effective iterative loop that processes only a small\nbatch of the most high-potential frames at a time. Extensive experiments on\nvarious VideoQA benchmarks demonstrate that our approach outperforms existing\nframe selection methods, significantly boosts the performance of the foundation\nVLM, and achieves substantial gains in computational efficiency over other\nVLM-based techniques.",
    "published": "2025-10-06T01:51:13Z",
    "updated": "2025-10-06T01:51:13Z",
    "link": "http://arxiv.org/pdf/2510.04428v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuanhao Zou",
      "Shengji Jin",
      "Andong Deng",
      "Youpeng Zhao",
      "Jun Wang",
      "Chen Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04410v1",
    "title": "CodeFormer++: Blind Face Restoration Using Deformable Registration and\n  Deep Metric Learning",
    "summary": "Blind face restoration (BFR) has attracted increasing attention with the rise\nof generative methods. Most existing approaches integrate generative priors\ninto the restoration pro- cess, aiming to jointly address facial detail\ngeneration and identity preservation. However, these methods often suffer from\na trade-off between visual quality and identity fidelity, leading to either\nidentity distortion or suboptimal degradation removal. In this paper, we\npresent CodeFormer++, a novel framework that maximizes the utility of\ngenerative priors for high-quality face restoration while preserving identity.\nWe decompose BFR into three sub-tasks: (i) identity- preserving face\nrestoration, (ii) high-quality face generation, and (iii) dynamic fusion of\nidentity features with realistic texture details. Our method makes three key\ncontributions: (1) a learning-based deformable face registration module that\nsemantically aligns generated and restored faces; (2) a texture guided\nrestoration network to dynamically extract and transfer the texture of\ngenerated face to boost the quality of identity-preserving restored face; and\n(3) the integration of deep metric learning for BFR with the generation of\ninformative positive and hard negative samples to better fuse identity-\npreserving and generative features. Extensive experiments on real-world and\nsynthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves\nsuperior performance in terms of both visual fidelity and identity consistency.",
    "published": "2025-10-06T00:53:50Z",
    "updated": "2025-10-06T00:53:50Z",
    "link": "http://arxiv.org/pdf/2510.04410v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Venkata Bharath Reddy Reddem",
      "Akshay P Sarashetti",
      "Ranjith Merugu",
      "Amit Satish Unde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01660v3",
    "title": "VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual\n  Reprogramming",
    "summary": "Existing UDA pipelines fine-tune already well-trained backbone parameters for\nevery new source-and-target pair, resulting in the number of training\nparameters and storage memory growing linearly with each new pair, and also\npreventing the reuse of these well-trained backbone parameters.\n  Inspired by recent implications that existing backbones have textural biases,\nwe propose making use of domain-specific textural bias for domain adaptation\nvia visual reprogramming, namely VirDA. Instead of fine-tuning the full\nbackbone, VirDA prepends a domain-specific visual reprogramming layer to the\nbackbone. This layer produces visual prompts that act as an added textural bias\nto the input image, adapting its \"style\" to a target domain. To optimize these\nvisual reprogramming layers, we use multiple objective functions that optimize\nthe intra- and inter-domain distribution differences when domain-adapting\nvisual prompts are applied. This process does not require modifying the\nbackbone parameters, allowing the same backbone to be reused across different\ndomains.\n  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M\ntrainable parameters. VirDA surpasses PDA, the state-of-the-art\nparameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its\nparameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans\nand FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%\nof their trainable parameters. Relative to the strongest current methods\n(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only\n2.2% and 1.1% accuracy, respectively.",
    "published": "2025-10-02T04:40:42Z",
    "updated": "2025-10-06T00:19:12Z",
    "link": "http://arxiv.org/pdf/2510.01660v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Duy Nguyen",
      "Dat Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21742v2",
    "title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning",
    "summary": "Video Question Answering (VideoQA) has made significant strides by leveraging\nmultimodal learning to align visual and textual modalities. However, current\nbenchmarks overwhelmingly focus on questions answerable through explicit visual\ncontent - actions, objects, and events directly observable within individual\nframes or short clips. In contrast, creative and cinematic videos - such as\nmovies, TV shows, and narrative-driven content - employ storytelling techniques\nthat deliberately omit certain depictions, requiring viewers to infer motives,\nrelationships across discontinuous frames with disjoint visual contexts. Humans\nnaturally excel at such implicit reasoning, seamlessly integrating information\nacross time and context to construct coherent narratives. Yet current\nbenchmarks fail to capture this essential dimension of human-like\nunderstanding. To bridge this gap, we present ImplicitQA, a novel benchmark\nspecifically designed to test VideoQA models on human-like implicit reasoning.\nImplicitQA comprises 1K meticulously annotated QA pairs drawn from 1K\nhigh-quality creative video clips covering 15 genres across 7 decades of\ncontent. Questions are systematically categorized into nine key reasoning\ndimensions: lateral and vertical spatial reasoning, depth and proximity,\nviewpoint and visibility, motion and trajectory, causal and motivational\nreasoning, social interactions, physical context, and inferred counting. These\nannotations are deliberately challenging, crafted by authors, validated through\nmultiple annotators, and benchmarked against human performance to ensure high\nquality. Our extensive evaluations on 11 leading VideoQA models reveals\nconsistent and significant performance degradation, underscoring their reliance\non surface-level visual cues and highlighting the difficulty of implicit\nreasoning. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.",
    "published": "2025-06-26T19:53:54Z",
    "updated": "2025-10-05T23:04:14Z",
    "link": "http://arxiv.org/pdf/2506.21742v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sirnam Swetha",
      "Rohit Gupta",
      "Parth Parag Kulkarni",
      "David G Shatwell",
      "Jeffrey A Chan Santiago",
      "Nyle Siddiqui",
      "Joseph Fioresi",
      "Mubarak Shah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13731v2",
    "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization",
    "summary": "Worldwide image geolocalization-the task of predicting GPS coordinates from\nimages taken anywhere on Earth-poses a fundamental challenge due to the vast\ndiversity in visual content across regions. While recent approaches adopt a\ntwo-stage pipeline of retrieving candidates and selecting the best match, they\ntypically rely on simplistic similarity heuristics and point-wise supervision,\nfailing to model spatial relationships among candidates. In this paper, we\npropose GeoRanker, a distance-aware ranking framework that leverages large\nvision-language models to jointly encode query-candidate interactions and\npredict geographic proximity. In addition, we introduce a multi-order distance\nloss that ranks both absolute and relative distances, enabling the model to\nreason over structured spatial relationships. To support this, we curate\nGeoRanking, the first dataset explicitly designed for geographic ranking tasks\nwith multimodal candidate information. GeoRanker achieves state-of-the-art\nresults on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly\noutperforming current best methods.",
    "published": "2025-05-19T21:04:46Z",
    "updated": "2025-10-05T22:54:40Z",
    "link": "http://arxiv.org/pdf/2505.13731v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pengyue Jia",
      "Seongheon Park",
      "Song Gao",
      "Xiangyu Zhao",
      "Yixuan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04382v1",
    "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
    "summary": "We propose a new image denoising model based on a variable-growth total\nvariation regularization of double-phase type with adaptive weight. It is\ndesigned to reduce staircasing with respect to the classical\nRudin--Osher--Fatemi model, while preserving the edges of the image in a\nsimilar fashion. We implement the model and test its performance on synthetic\nand natural images in 1D and 2D over a range of noise levels.",
    "published": "2025-10-05T22:26:06Z",
    "updated": "2025-10-05T22:26:06Z",
    "link": "http://arxiv.org/pdf/2510.04382v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "authors": [
      "Wojciech Górny",
      "Michał Łasica",
      "Alexandros Matsoukas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20585v2",
    "title": "Region-of-Interest Augmentation for Mammography Classification under\n  Patient-Level Cross-Validation",
    "summary": "Breast cancer screening with mammography remains central to early detection\nand mortality reduction. Deep learning has shown strong potential for\nautomating mammogram interpretation, yet limited-resolution datasets and small\nsample sizes continue to restrict performance. We revisit the Mini-DDSM dataset\n(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest\n(ROI) augmentation strategy. During training, full images are probabilistically\nreplaced with random ROI crops sampled from a precomputed, label-free\nbounding-box bank, with optional jitter to increase variability. We evaluate\nunder strict patient-level cross-validation and report ROC-AUC, PR-AUC, and\ntraining-time efficiency metrics (throughput and GPU memory). Because ROI\naugmentation is training-only, inference-time cost remains unchanged. On\nMini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest\naverage ROC-AUC gains, with performance varying across folds; PR-AUC is flat to\nslightly lower. These results demonstrate that simple, data-centric ROI\nstrategies can enhance mammography classification in constrained settings\nwithout requiring additional labels or architectural modifications.",
    "published": "2025-09-24T21:52:49Z",
    "updated": "2025-10-05T21:40:20Z",
    "link": "http://arxiv.org/pdf/2509.20585v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Farbod Bigdeli",
      "Mohsen Mohammadagha",
      "Ali Bigdeli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04369v1",
    "title": "The method of the approximate inverse for limited-angle CT",
    "summary": "Limited-angle computerized tomography stands for one of the most difficult\nchallenges in imaging. Although it opens the way to faster data acquisition in\nindustry and less dangerous scans in medicine, standard approaches, such as the\nfiltered backprojection (FBP) algorithm or the widely used total-variation\nfunctional, often produce various artefacts that hinder the diagnosis. With the\nrise of deep learning, many modern techniques have proven themselves successful\nin removing such artefacts but at the cost of large datasets. In this paper, we\npropose a new model-driven approach based on the method of the approximate\ninverse, which could serve as new starting point for learning strategies in the\nfuture. In contrast to FBP-type approaches, our reconstruction step consists in\nevaluating linear functionals on the measured data using reconstruction kernels\nthat are precomputed as solution of an auxiliary problem. With this problem\nbeing uniquely solvable, the derived limited-angle reconstruction kernel (LARK)\nis able to fully reconstruct the object without the well-known streak\nartefacts, even for large limited angles. However, it inherits severe\nill-conditioning which leads to a different kind of artefacts arising from the\nsingular functions of the limited-angle Radon transform. The problem becomes\nparticularly challenging when working on semi-discrete (real or analytical)\nmeasurements. We develop a general regularization strategy, named constrained\nlimited-angle reconstruction kernel (CLARK), by combining spectral filter, the\nmethod of the approximate inverse and custom edge-preserving denoising in order\nto stabilize the whole process. We further derive and interpret error estimates\nfor the application on real, i.e. semi-discrete, data and we validate our\napproach on synthetic and real data.",
    "published": "2025-10-05T21:24:44Z",
    "updated": "2025-10-05T21:24:44Z",
    "link": "http://arxiv.org/pdf/2510.04369v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "authors": [
      "Bernadette Hahn",
      "Gael Rigaud",
      "Richard Schmähl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04365v1",
    "title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise\n  for Momentary Trajectory Prediction",
    "summary": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and\nefficiency in autonomous driving and human-robot interaction scenarios. Earlier\nstudies primarily utilized sufficient observational data to predict future\ntrajectories. However, in real-world scenarios, such as pedestrians suddenly\nemerging from blind spots, sufficient observational data is often unavailable\n(i.e. momentary trajectory), making accurate prediction challenging and\nincreasing the risk of traffic accidents. Therefore, advancing research on\npedestrian trajectory prediction under extreme scenarios is critical for\nenhancing traffic safety. In this work, we propose a novel framework termed\nDiffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists\nof two sequentially connected diffusion models: one for backward prediction,\nwhich generates unobserved historical trajectories, and the other for forward\nprediction, which forecasts future trajectories. Given that the generated\nunobserved historical trajectories may introduce additional noise, we propose a\ndual-head parameterization mechanism to estimate their aleatoric uncertainty\nand design a temporally adaptive noise module that dynamically modulates the\nnoise scale in the forward diffusion process. Empirically, Diffusion^2 sets a\nnew state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford\nDrone datasets.",
    "published": "2025-10-05T21:19:33Z",
    "updated": "2025-10-05T21:19:33Z",
    "link": "http://arxiv.org/pdf/2510.04365v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuhao Luo",
      "Yuang Zhang",
      "Kehua Chen",
      "Xinyu Zheng",
      "Shucheng Zhang",
      "Sikai Chen",
      "Yinhai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18825v2",
    "title": "How to build a consistency model: Learning flow maps via\n  self-distillation",
    "summary": "Flow-based generative models achieve state-of-the-art sample quality, but\nrequire the expensive solution of a differential equation at inference time.\nFlow map models, commonly known as consistency models, encompass many recent\nefforts to improve inference-time efficiency by learning the solution operator\nof this differential equation. Yet despite their promise, these models lack a\nunified description that clearly explains how to learn them efficiently in\npractice. Here, building on the methodology proposed in Boffi et. al. (2024),\nwe present a systematic algorithmic framework for directly learning the flow\nmap associated with a flow or diffusion model. By exploiting a relationship\nbetween the velocity field underlying a continuous-time flow and the\ninstantaneous rate of change of the flow map, we show how to convert any\ndistillation scheme into a direct training algorithm via self-distillation,\neliminating the need for pre-trained teachers. We introduce three algorithmic\nfamilies based on different mathematical characterizations of the flow map:\nEulerian, Lagrangian, and Progressive methods, which we show encompass and\nextend all known distillation and direct training schemes for consistency\nmodels. We find that the novel class of Lagrangian methods, which avoid both\nspatial derivatives and bootstrapping from small steps by design, achieve\nsignificantly more stable training and higher performance than more standard\nEulerian and Progressive schemes. Our methodology unifies existing training\nschemes under a single common framework and reveals new design principles for\naccelerated generative modeling. Associated code is available at\nhttps://github.com/nmboffi/flow-maps.",
    "published": "2025-05-24T18:50:50Z",
    "updated": "2025-10-05T20:24:27Z",
    "link": "http://arxiv.org/pdf/2505.18825v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Nicholas M. Boffi",
      "Michael S. Albergo",
      "Eric Vanden-Eijnden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18569v2",
    "title": "VisualChef: Generating Visual Aids in Cooking via Mask Inpainting",
    "summary": "Cooking requires not only following instructions but also understanding,\nexecuting, and monitoring each step - a process that can be challenging without\nvisual guidance. Although recipe images and videos offer helpful cues, they\noften lack consistency in focus, tools, and setup. To better support the\ncooking process, we introduce VisualChef, a method for generating contextual\nvisual aids tailored to cooking scenarios. Given an initial frame and a\nspecified action, VisualChef generates images depicting both the action's\nexecution and the resulting appearance of the object, while preserving the\ninitial frame's environment. Previous work aims to integrate knowledge\nextracted from large language models by generating detailed textual\ndescriptions to guide image generation, which requires fine-grained\nvisual-textual alignment and involves additional annotations. In contrast,\nVisualChef simplifies alignment through mask-based visual grounding. Our key\ninsight is identifying action-relevant objects and classifying them to enable\ntargeted modifications that reflect the intended action and outcome while\nmaintaining a consistent environment. In addition, we propose an automated\npipeline to extract high-quality initial, action, and final state frames. We\nevaluate VisualChef quantitatively and qualitatively on three egocentric video\ndatasets and show its improvements over state-of-the-art methods.",
    "published": "2025-06-23T12:23:21Z",
    "updated": "2025-10-05T20:03:32Z",
    "link": "http://arxiv.org/pdf/2506.18569v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Oleh Kuzyk",
      "Zuoyue Li",
      "Marc Pollefeys",
      "Xi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20148v2",
    "title": "Smaller is Better: Enhancing Transparency in Vehicle AI Systems via\n  Pruning",
    "summary": "Connected and autonomous vehicles continue to heavily rely on AI systems,\nwhere transparency and security are critical for trust and operational safety.\nPost-hoc explanations provide transparency to these black-box like AI models\nbut the quality and reliability of these explanations is often questioned due\nto inconsistencies and lack of faithfulness in representing model decisions.\nThis paper systematically examines the impact of three widely used training\napproaches, namely natural training, adversarial training, and pruning, affect\nthe quality of post-hoc explanations for traffic sign classifiers. Through\nextensive empirical evaluation, we demonstrate that pruning significantly\nenhances the comprehensibility and faithfulness of explanations (using saliency\nmaps). Our findings reveal that pruning not only improves model efficiency but\nalso enforces sparsity in learned representation, leading to more interpretable\nand reliable decisions. Additionally, these insights suggest that pruning is a\npromising strategy for developing transparent deep learning models, especially\nin resource-constrained vehicular AI systems.",
    "published": "2025-09-24T14:11:59Z",
    "updated": "2025-10-05T20:01:40Z",
    "link": "http://arxiv.org/pdf/2509.20148v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sanish Suwal",
      "Shaurya Garg",
      "Dipkamal Bhusal",
      "Michael Clifford",
      "Nidhi Rastogi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.10390v4",
    "title": "Evaluating Perceptual Distance Models by Fitting Binomial Distributions\n  to Two-Alternative Forced Choice Data",
    "summary": "The Two Alternative Forced Choice (2AFC) paradigm offers advantages over the\nMean Opinion Score (MOS) paradigm in psychophysics (PF), such as simplicity and\nrobustness. However, when evaluating perceptual distance models, MOS enables\ndirect correlation between model predictions and PF data. In contrast, 2AFC\nonly allows pairwise comparisons to be converted into a quality ranking similar\nto MOS when comparisons include shared images. In large datasets, like BAPPS,\nwhere image patches and distortions are combined randomly, deriving rankings\nfrom 2AFC PF data becomes infeasible, as distorted images included in each\ncomparisons are independent. To address this, instead of relying on MOS\ncorrelation, researchers have trained ad-hoc neural networks to reproduce 2AFC\nPF data based on pairs of model distances - a black-box approach with\nconceptual and operational limitations. This paper introduces a more robust\ndistance-model evaluation method using a pure probabilistic approach, applying\nmaximum likelihood estimation to a binomial decision model. Our method\ndemonstrates superior simplicity, interpretability, flexibility, and\ncomputational efficiency, as shown through evaluations of various visual\ndistance models on two 2AFC PF datasets.",
    "published": "2024-03-15T15:21:04Z",
    "updated": "2025-10-05T19:35:52Z",
    "link": "http://arxiv.org/pdf/2403.10390v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alexander Hepburn",
      "Raul Santos-Rodriguez",
      "Javier Portilla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04333v1",
    "title": "RAP: 3D Rasterization Augmented End-to-End Planning",
    "summary": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.",
    "published": "2025-10-05T19:31:24Z",
    "updated": "2025-10-05T19:31:24Z",
    "link": "http://arxiv.org/pdf/2510.04333v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Lan Feng",
      "Yang Gao",
      "Eloi Zablocki",
      "Quanyi Li",
      "Wuyang Li",
      "Sichao Liu",
      "Matthieu Cord",
      "Alexandre Alahi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04331v1",
    "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise\n  Injection and Auxiliary Networks",
    "summary": "Parameter-efficient fine-tuning (PEFT) methods have become the standard\nparadigm for adapting large-scale models. Among these techniques,\nWeight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the\nlearning capacity and training stability of the vanilla Low-Rank Adaptation\n(LoRA) method by explicitly decomposing pre-trained weights into magnitude and\ndirectional components. In this work, we propose DoRAN, a new variant of DoRA\ndesigned to further stabilize training and boost the sample efficiency of DoRA.\nOur approach includes two key stages: (i) injecting noise into the denominator\nof DoRA's weight decomposition, which serves as an adaptive regularizer to\nmitigate instabilities; and (ii) replacing static low-rank matrices with\nauxiliary networks that generate them dynamically, enabling parameter coupling\nacross layers and yielding better sample efficiency in both theory and\npractice. Comprehensive experiments on vision and language benchmarks show that\nDoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These\nresults underscore the effectiveness of combining stabilization through\nnoise-based regularization with network-based parameter generation, offering a\npromising direction for robust and efficient fine-tuning of foundation models.",
    "published": "2025-10-05T19:27:48Z",
    "updated": "2025-10-05T19:27:48Z",
    "link": "http://arxiv.org/pdf/2510.04331v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Nghiem T. Diep",
      "Hien Dang",
      "Tuan Truong",
      "Tan Dinh",
      "Huy Nguyen",
      "Nhat Ho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11336v2",
    "title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New\n  Benchmarks",
    "summary": "Real-world user-generated videos, especially on platforms like TikTok, often\nfeature rich and intertwined audio visual content. However, existing video\ncaptioning benchmarks and models remain predominantly visual centric,\noverlooking the crucial role of audio in conveying scene dynamics, speaker\nintent, and narrative context. This lack of omni datasets and lightweight,\ncapable models hampers progress in fine grained, multimodal video\nunderstanding. To address these challenges, we introduce UGC-VideoCap, a new\nbenchmark and model framework specifically designed for detailed omnimodal\ncaptioning of short form user-generated videos. Unlike prior datasets,\nUGC-VideoCap emphasizes balanced integration of audio and visual modalities,\nfeaturing 1000 TikTok videos annotated through a structured three stage\nhuman-in-the-loop pipeline covering audio only, visual only, and joint audio\nvisual semantics. The benchmark also includes 4000 carefully crafted QA pairs\nprobing both unimodal and cross modal understanding. Alongside the dataset, we\npropose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from\nGemini 2.5 Flash. Using a novel two-stage training strategy supervised fine\ntuning followed by Group Relative Policy Optimization (GRPO), our approach\nenables efficient adaptation from limited data while maintaining competitive\nperformance. Together, our benchmark and model offer a high-quality foundation\nand a data-efficient solution for advancing omnimodal video captioning in\nunconstrained real-world UGC settings.",
    "published": "2025-07-15T14:08:29Z",
    "updated": "2025-10-05T18:41:25Z",
    "link": "http://arxiv.org/pdf/2507.11336v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peiran Wu",
      "Yunze Liu",
      "Zhengdong Zhu",
      "Enmin Zhou",
      "Junxiao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04315v1",
    "title": "GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression\n  Prediction",
    "summary": "Spatial Transcriptomics (ST) offers spatially resolved gene expression but\nremains costly. Predicting expression directly from widely available\nHematoxylin and Eosin (H&E) stained images presents a cost-effective\nalternative. However, most computational approaches (i) predict each gene\nindependently, overlooking co-expression structure, and (ii) cast the task as\ncontinuous regression despite expression being discrete counts. This mismatch\ncan yield biologically implausible outputs and complicate downstream analyses.\nWe introduce GenAR, a multi-scale autoregressive framework that refines\npredictions from coarse to fine. GenAR clusters genes into hierarchical groups\nto expose cross-gene dependencies, models expression as codebook-free discrete\ntoken generation to directly predict raw counts, and conditions decoding on\nfused histological and spatial embeddings. From an information-theoretic\nperspective, the discrete formulation avoids log-induced biases and the\ncoarse-to-fine factorization aligns with a principled conditional\ndecomposition. Extensive experimental results on four Spatial Transcriptomics\ndatasets across different tissue types demonstrate that GenAR achieves\nstate-of-the-art performance, offering potential implications for precision\nmedicine and cost-effective molecular profiling. Code is publicly available at\nhttps://github.com/oyjr/genar.",
    "published": "2025-10-05T18:28:21Z",
    "updated": "2025-10-05T18:28:21Z",
    "link": "http://arxiv.org/pdf/2510.04315v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiarui Ouyang",
      "Yihui Wang",
      "Yihang Gao",
      "Yingxue Xu",
      "Shu Yang",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04312v1",
    "title": "CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's\n  Disease Gait Assessment",
    "summary": "Objective gait assessment in Parkinson's Disease (PD) is limited by the\nabsence of large, diverse, and clinically annotated motion datasets. We\nintroduce CARE-PD, the largest publicly available archive of 3D mesh gait data\nfor PD, and the first multi-site collection spanning 9 cohorts from 8 clinical\ncenters. All recordings (RGB video or motion capture) are converted into\nanonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD\nsupports two key benchmarks: supervised clinical score prediction (estimating\nUnified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised\nmotion pretext tasks (2D-to-3D keypoint lifting and full-body 3D\nreconstruction). Clinical prediction is evaluated under four generalization\nprotocols: within-dataset, cross-dataset, leave-one-dataset-out, and\nmulti-dataset in-domain adaptation. To assess clinical relevance, we compare\nstate-of-the-art motion encoders with a traditional gait-feature baseline,\nfinding that encoders consistently outperform handcrafted features. Pretraining\non CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1\nby 17 percentage points, underscoring the value of clinically curated, diverse\ntraining data. CARE-PD and all benchmark code are released for non-commercial\nresearch at https://neurips2025.care-pd.ca/.",
    "published": "2025-10-05T18:14:50Z",
    "updated": "2025-10-05T18:14:50Z",
    "link": "http://arxiv.org/pdf/2510.04312v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Vida Adeli",
      "Ivan Klabucar",
      "Javad Rajabi",
      "Benjamin Filtjens",
      "Soroush Mehraban",
      "Diwei Wang",
      "Hyewon Seo",
      "Trung-Hieu Hoang",
      "Minh N. Do",
      "Candice Muller",
      "Claudia Oliveira",
      "Daniel Boari Coelho",
      "Pieter Ginis",
      "Moran Gilat",
      "Alice Nieuwboer",
      "Joke Spildooren",
      "Lucas Mckay",
      "Hyeokhyen Kwon",
      "Gari Clifford",
      "Christine Esper",
      "Stewart Factor",
      "Imari Genias",
      "Amirhossein Dadashzadeh",
      "Leia Shum",
      "Alan Whone",
      "Majid Mirmehdi",
      "Andrea Iaboni",
      "Babak Taati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07413v2",
    "title": "Learning Penalty for Optimal Partitioning via Automatic Feature\n  Extraction",
    "summary": "Changepoint detection identifies significant shifts in data sequences, making\nit important in areas like finance, genetics, and healthcare. The Optimal\nPartitioning algorithms efficiently detect these changes, using a penalty\nparameter to limit the changepoints count. Determining the optimal value for\nthis penalty can be challenging. Traditionally, this process involved manually\nextracting statistical features, such as sequence length or variance to make\nthe prediction. This study proposes a novel approach that uses recurrent\nnetworks to learn this penalty directly from raw sequences by automatically\nextracting features. Experiments conducted on 20 benchmark genomic datasets\nshow that this novel method generally outperforms traditional ones in\nchangepoint detection accuracy.",
    "published": "2025-05-12T10:07:55Z",
    "updated": "2025-10-06T17:53:44Z",
    "link": "http://arxiv.org/pdf/2505.07413v2.pdf",
    "category": [
      "cs.LG",
      "stat.AP"
    ],
    "authors": [
      "Tung L Nguyen",
      "Toby Hocking"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02107v2",
    "title": "PENEX: AdaBoost-Inspired Neural Network Regularization",
    "summary": "AdaBoost sequentially fits so-called weak learners to minimize an exponential\nloss, which penalizes mislabeled data points more severely than other loss\nfunctions like cross-entropy. Paradoxically, AdaBoost generalizes well in\npractice as the number of weak learners grows. In the present work, we\nintroduce Penalized Exponential Loss (PENEX), a new formulation of the\nmulti-class exponential loss that is theoretically grounded and, in contrast to\nthe existing formulation, amenable to optimization via first-order methods. We\ndemonstrate both empirically and theoretically that PENEX implicitly maximizes\nmargins of data points. Also, we show that gradient increments on PENEX\nimplicitly parameterize weak learners in the boosting framework. Across\ncomputer vision and language tasks, we show that PENEX exhibits a regularizing\neffect often better than established methods with similar computational cost.\nOur results highlight PENEX's potential as an AdaBoost-inspired alternative for\neffective training and fine-tuning of deep neural networks.",
    "published": "2025-10-02T15:13:02Z",
    "updated": "2025-10-06T17:51:59Z",
    "link": "http://arxiv.org/pdf/2510.02107v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Klaus-Rudolf Kladny",
      "Bernhard Schölkopf",
      "Michael Muehlebach"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05080v1",
    "title": "MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis",
    "summary": "This study presents a novel small-area estimation framework to enhance urban\ntransportation planning through detailed characterization of travel behavior.\nOur approach improves on the four-step travel model by employing publicly\navailable microdata files and machine learning methods to predict travel\nbehavior for a representative, synthetic population at small geographic areas.\nThis approach enables high-resolution estimation of trip generation, trip\ndistribution, mode choice, and route assignment. Validation using ACS/PUMS\nwork-commute datasets demonstrates that our framework achieves higher accuracy\ncompared to conventional approaches. The resulting granular insights enable the\ntailoring of interventions to address localized situations and support a range\nof policy applications and targeted interventions, including the optimal\nplacement of micro-fulfillment centers, effective curb-space management, and\nthe design of more inclusive transportation solutions particularly for\nvulnerable communities.",
    "published": "2025-10-06T17:50:56Z",
    "updated": "2025-10-06T17:50:56Z",
    "link": "http://arxiv.org/pdf/2510.05080v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yangyang Wang",
      "Tayo Fabusuyi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05070v1",
    "title": "ResMimic: From General Motion Tracking to Humanoid Whole-body\n  Loco-Manipulation via Residual Learning",
    "summary": "Humanoid whole-body loco-manipulation promises transformative capabilities\nfor daily service and warehouse tasks. While recent advances in general motion\ntracking (GMT) have enabled humanoids to reproduce diverse human motions, these\npolicies lack the precision and object awareness required for\nloco-manipulation. To this end, we introduce ResMimic, a two-stage residual\nlearning framework for precise and expressive humanoid control from human\nmotion data. First, a GMT policy, trained on large-scale human-only motion,\nserves as a task-agnostic base for generating human-like whole-body movements.\nAn efficient but precise residual policy is then learned to refine the GMT\noutputs to improve locomotion and incorporate object interaction. To further\nfacilitate efficient training, we design (i) a point-cloud-based object\ntracking reward for smoother optimization, (ii) a contact reward that\nencourages accurate humanoid body-object interactions, and (iii) a\ncurriculum-based virtual object controller to stabilize early training. We\nevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results\nshow substantial gains in task success, training efficiency, and robustness\nover strong baselines. Videos are available at https://resmimic.github.io/ .",
    "published": "2025-10-06T17:47:02Z",
    "updated": "2025-10-06T17:47:02Z",
    "link": "http://arxiv.org/pdf/2510.05070v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Siheng Zhao",
      "Yanjie Ze",
      "Yue Wang",
      "C. Karen Liu",
      "Pieter Abbeel",
      "Guanya Shi",
      "Rocky Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02274v2",
    "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps",
    "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.",
    "published": "2025-10-02T17:50:22Z",
    "updated": "2025-10-06T17:44:43Z",
    "link": "http://arxiv.org/pdf/2510.02274v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kyoungjun Park",
      "Yifan Yang",
      "Changhan Ge",
      "Lili Qiu",
      "Shiqi Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05064v1",
    "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
    "summary": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation.",
    "published": "2025-10-06T17:41:20Z",
    "updated": "2025-10-06T17:41:20Z",
    "link": "http://arxiv.org/pdf/2510.05064v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sara Kangaslahti",
      "Nihal V. Nayak",
      "Jonathan Geuter",
      "Marco Fumero",
      "Francesco Locatello",
      "David Alvarez-Melis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05060v1",
    "title": "ResCP: Reservoir Conformal Prediction for Time Series Forecasting",
    "summary": "Conformal prediction offers a powerful framework for building\ndistribution-free prediction intervals for exchangeable data. Existing methods\nthat extend conformal prediction to sequential data rely on fitting a\nrelatively complex model to capture temporal dependencies. However, these\nmethods can fail if the sample size is small and often require expensive\nretraining when the underlying data distribution changes. To overcome these\nlimitations, we propose Reservoir Conformal Prediction (ResCP), a novel\ntraining-free conformal prediction method for time series. Our approach\nleverages the efficiency and representation learning capabilities of reservoir\ncomputing to dynamically reweight conformity scores. In particular, we compute\nsimilarity scores among reservoir states and use them to adaptively reweight\nthe observed residuals at each step. With this approach, ResCP enables us to\naccount for local temporal dynamics when modeling the error distribution\nwithout compromising computational scalability. We prove that, under reasonable\nassumptions, ResCP achieves asymptotic conditional coverage, and we empirically\ndemonstrate its effectiveness across diverse forecasting tasks.",
    "published": "2025-10-06T17:37:44Z",
    "updated": "2025-10-06T17:37:44Z",
    "link": "http://arxiv.org/pdf/2510.05060v1.pdf",
    "category": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "authors": [
      "Roberto Neglia",
      "Andrea Cini",
      "Michael M. Bronstein",
      "Filippo Maria Bianchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05056v1",
    "title": "Modeling Student Learning with 3.8 Million Program Traces",
    "summary": "As programmers write code, they often edit and retry multiple times, creating\nrich \"interaction traces\" that reveal how they approach coding tasks and\nprovide clues about their level of skill development. For novice programmers in\nparticular, these traces reflect the diverse reasoning processes they employ to\ncode, such as exploratory behavior to understand how a programming concept\nworks, re-strategizing in response to bugs, and personalizing stylistic\nchoices. In this work, we explore what can be learned from training language\nmodels on such reasoning traces: not just about code, but about coders, and\nparticularly students learning to program. We introduce a dataset of over 3.8\nmillion programming reasoning traces from users of Pencil Code, a free online\neducational platform used by students to learn simple programming concepts.\nCompared to models trained only on final programs or synthetically-generated\ntraces, we find that models trained on real traces are stronger at modeling\ndiverse student behavior. Through both behavioral and probing analyses, we also\nfind that many properties of code traces, such as goal backtracking or number\nof comments, can be predicted from learned representations of the students who\nwrite them. Building on this result, we show that we can help students recover\nfrom mistakes by steering code generation models to identify a sequence of\nedits that will results in more correct code while remaining close to the\noriginal student's style. Together, our results suggest that many properties of\ncode are properties of individual students and that training on edit traces can\nlead to models that are more steerable, more predictive of student behavior\nwhile programming, and better at generating programs in their final states.\nCode and data is available at https://github.com/meghabyte/pencilcode-public",
    "published": "2025-10-06T17:37:17Z",
    "updated": "2025-10-06T17:37:17Z",
    "link": "http://arxiv.org/pdf/2510.05056v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Alexis Ross",
      "Megha Srivastava",
      "Jeremiah Blanchard",
      "Jacob Andreas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.01580v2",
    "title": "Learning-Augmented Robust Algorithmic Recourse",
    "summary": "Algorithmic recourse provides individuals who receive undesirable outcomes\nfrom machine learning systems with minimum-cost improvements to achieve a\ndesirable outcome. However, machine learning models often get updated, so the\nrecourse may not lead to the desired outcome. The robust recourse framework\nchooses recourses that are less sensitive to adversarial model changes, but\nthis comes at a higher cost. To address this, we initiate the study of\nlearning-augmented algorithmic recourse and evaluate the extent to which a\ndesigner equipped with a prediction of the future model can reduce the cost of\nrecourse when the prediction is accurate (consistency) while also limiting the\ncost even when the prediction is inaccurate (robustness). We propose a novel\nalgorithm, study the robustness-consistency trade-off, and analyze how\nprediction accuracy affects performance.",
    "published": "2024-10-02T14:15:32Z",
    "updated": "2025-10-06T17:35:00Z",
    "link": "http://arxiv.org/pdf/2410.01580v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kshitij Kayastha",
      "Vasilis Gkatzelis",
      "Shahin Jabbari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05049v1",
    "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code\n  Embeddings",
    "summary": "Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments.",
    "published": "2025-10-06T17:27:54Z",
    "updated": "2025-10-06T17:27:54Z",
    "link": "http://arxiv.org/pdf/2510.05049v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ahmed Elhussein",
      "Paul Meddeb",
      "Abigail Newbury",
      "Jeanne Mirone",
      "Martin Stoll",
      "Gamze Gursoy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05047v1",
    "title": "A Unified Optimization Framework for Multiclass Classification with\n  Structured Hyperplane Arrangements",
    "summary": "In this paper, we propose a new mathematical optimization model for\nmulticlass classification based on arrangements of hyperplanes. Our approach\npreserves the core support vector machine (SVM) paradigm of maximizing class\nseparation while minimizing misclassification errors, and it is computationally\nmore efficient than a previous formulation. We present a kernel-based extension\nthat allows it to construct nonlinear decision boundaries. Furthermore, we show\nhow the framework can naturally incorporate alternative geometric structures,\nincluding classification trees, $\\ell_p$-SVMs, and models with discrete feature\nselection. To address large-scale instances, we develop a dynamic clustering\nmatheuristic that leverages the proposed MIP formulation. Extensive\ncomputational experiments demonstrate the efficiency of the proposed model and\ndynamic clustering heuristic, and we report competitive classification\nperformance on both synthetic datasets and real-world benchmarks from the UCI\nMachine Learning Repository, comparing our method with state-of-the-art\nimplementations available in scikit-learn.",
    "published": "2025-10-06T17:26:56Z",
    "updated": "2025-10-06T17:26:56Z",
    "link": "http://arxiv.org/pdf/2510.05047v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Víctor Blanco",
      "Harshit Kothari",
      "James Luedtke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05033v1",
    "title": "Causal Abstractions, Categorically Unified",
    "summary": "We present a categorical framework for relating causal models that represent\nthe same system at different levels of abstraction. We define a causal\nabstraction as natural transformations between appropriate Markov functors,\nwhich concisely consolidate desirable properties a causal abstraction should\nexhibit. Our approach unifies and generalizes previously considered causal\nabstractions, and we obtain categorical proofs and generalizations of existing\nresults on causal abstractions. Using string diagrammatical tools, we can\nexplicitly describe the graphs that serve as consistent abstractions of a\nlow-level graph under interventions. We discuss how methods from mechanistic\ninterpretability, such as circuit analysis and sparse autoencoders, fit within\nour categorical framework. We also show how applying do-calculus on a\nhigh-level graphical abstraction of an acyclic-directed mixed graph (ADMG),\nwhen unobserved confounders are present, gives valid results on the low-level\ngraph, thus generalizing an earlier statement by Anand et al. (2023). We argue\nthat our framework is more suitable for modeling causal abstractions compared\nto existing categorical frameworks. Finally, we discuss how notions such as\n$\\tau$-consistency and constructive $\\tau$-abstractions can be recovered with\nour framework.",
    "published": "2025-10-06T17:09:30Z",
    "updated": "2025-10-06T17:09:30Z",
    "link": "http://arxiv.org/pdf/2510.05033v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Markus Englberger",
      "Devendra Singh Dhami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05024v1",
    "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment",
    "summary": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.",
    "published": "2025-10-06T17:02:59Z",
    "updated": "2025-10-06T17:02:59Z",
    "link": "http://arxiv.org/pdf/2510.05024v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Nevan Wichers",
      "Aram Ebtekar",
      "Ariana Azarbal",
      "Victor Gillioz",
      "Christine Ye",
      "Emil Ryd",
      "Neil Rathi",
      "Henry Sleight",
      "Alex Mallen",
      "Fabien Roger",
      "Samuel Marks"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05013v1",
    "title": "Curiosity-Driven Co-Development of Action and Language in Robots Through\n  Self-Exploration",
    "summary": "Human infants acquire language and action co-developmentally, achieving\nremarkable generalization capabilities from only a minimal number of learning\nexamples. In contrast, recent large language models require exposure to\nbillions of training tokens to achieve such generalization. What mechanisms\nunderlie such efficient developmental learning in humans? This study addresses\nthis question through simulation experiments in which robots learn to perform\nvarious actions corresponding to imperative sentences (e.g., \\textit{push red\ncube}) via trials of self-guided exploration. Our approach integrates the\nactive inference framework with reinforcement learning, enabling\ncuriosity-driven developmental learning. The simulations yielded several\nnontrivial findings: i) Curiosity-driven exploration combined with motor noise\nsubstantially outperforms learning without curiosity. ii) Simpler,\nprerequisite-like actions emerge earlier in development, while more complex\nactions involving these prerequisites develop later. iii) Rote pairing of\nsentences and actions occurs before the emergence of compositional\ngeneralization. iv) Generalization is drastically improved as the number of\ncompositional elements increases. These results shed light into possible\nmechanisms underlying efficient co-developmental learning in infants and\nprovide computational parallels to findings in developmental psychology.",
    "published": "2025-10-06T16:53:39Z",
    "updated": "2025-10-06T16:53:39Z",
    "link": "http://arxiv.org/pdf/2510.05013v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Theodore Jerome Tinker",
      "Kenji Doya",
      "Jun Tani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.09320v2",
    "title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning",
    "summary": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance.",
    "published": "2025-01-16T06:22:35Z",
    "updated": "2025-10-06T16:41:32Z",
    "link": "http://arxiv.org/pdf/2501.09320v2.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Seohyun Lee",
      "Wenzhi Fang",
      "Anindya Bijoy Das",
      "Seyyedali Hosseinalipour",
      "David J. Love",
      "Christopher G. Brinton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04995v1",
    "title": "Power Transform Revisited: Numerically Stable, and Federated",
    "summary": "Power transforms are popular parametric techniques for making data more\nGaussian-like, and are widely used as preprocessing steps in statistical\nanalysis and machine learning. However, we find that direct implementations of\npower transforms suffer from severe numerical instabilities, which can lead to\nincorrect results or even crashes. In this paper, we provide a comprehensive\nanalysis of the sources of these instabilities and propose effective remedies.\nWe further extend power transforms to the federated learning setting,\naddressing both numerical and distributional challenges that arise in this\ncontext. Experiments on real-world datasets demonstrate that our methods are\nboth effective and robust, substantially improving stability compared to\nexisting approaches.",
    "published": "2025-10-06T16:32:22Z",
    "updated": "2025-10-06T16:32:22Z",
    "link": "http://arxiv.org/pdf/2510.04995v1.pdf",
    "category": [
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "authors": [
      "Xuefeng Xu",
      "Graham Cormode"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.13831v3",
    "title": "Data-Driven Performance Guarantees for Classical and Learned Optimizers",
    "summary": "We introduce a data-driven approach to analyze the performance of continuous\noptimization algorithms using generalization guarantees from statistical\nlearning theory. We study classical and learned optimizers to solve families of\nparametric optimization problems. We build generalization guarantees for\nclassical optimizers, using a sample convergence bound, and for learned\noptimizers, using the Probably Approximately Correct (PAC)-Bayes framework. To\ntrain learned optimizers, we use a gradient-based algorithm to directly\nminimize the PAC-Bayes upper bound. Numerical experiments in signal processing,\ncontrol, and meta-learning showcase the ability of our framework to provide\nstrong generalization guarantees for both classical and learned optimizers\ngiven a fixed budget of iterations. For classical optimizers, our bounds are\nmuch tighter than those that worst-case guarantees provide. For learned\noptimizers, our bounds outperform the empirical outcomes observed in their\nnon-learned counterparts.",
    "published": "2024-04-22T02:06:35Z",
    "updated": "2025-10-06T16:30:05Z",
    "link": "http://arxiv.org/pdf/2404.13831v3.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Rajiv Sambharya",
      "Bartolomeo Stellato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17000v2",
    "title": "Critical Points of Random Neural Networks",
    "summary": "This work investigates the expected number of critical points of random\nneural networks with different activation functions as the depth increases in\nthe infinite-width limit. Under suitable regularity conditions, we derive\nprecise asymptotic formulas for the expected number of critical points of fixed\nindex and those exceeding a given threshold. Our analysis reveals three\ndistinct regimes depending on the value of the first derivative of the\ncovariance evaluated at 1: the expected number of critical points may converge,\ngrow polynomially, or grow exponentially with depth. The theoretical\npredictions are supported by numerical experiments. Moreover, we provide\nnumerical evidence suggesting that, when the regularity condition is not\nsatisfied (e.g. for neural networks with ReLU as activation function), the\nnumber of critical points increases as the map resolution increases, indicating\na potential divergence in the number of critical points.",
    "published": "2025-05-22T17:57:30Z",
    "updated": "2025-10-06T16:27:45Z",
    "link": "http://arxiv.org/pdf/2505.17000v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "60G60, 62B10, 62M45"
    ],
    "authors": [
      "Simmaco Di Lillo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06782v2",
    "title": "Physics-informed Value Learner for Offline Goal-Conditioned\n  Reinforcement Learning",
    "summary": "Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise\nfor domains such as autonomous navigation and locomotion, where collecting\ninteractive data is costly and unsafe. However, it remains challenging in\npractice due to the need to learn from datasets with limited coverage of the\nstate-action space and to generalize across long-horizon tasks. To improve on\nthese challenges, we propose a \\emph{Physics-informed (Pi)} regularized loss\nfor value learning, derived from the Eikonal Partial Differential Equation\n(PDE) and which induces a geometric inductive bias in the learned value\nfunction. Unlike generic gradient penalties that are primarily used to\nstabilize training, our formulation is grounded in continuous-time optimal\ncontrol and encourages value functions to align with cost-to-go structures. The\nproposed regularizer is broadly compatible with temporal-difference-based value\nlearning and can be integrated into existing Offline GCRL algorithms. When\ncombined with Hierarchical Implicit Q-Learning (HIQL), the resulting method,\nEikonal-regularized HIQL (Eik-HIQL), yields significant improvements in both\nperformance and generalization, with pronounced gains in stitching regimes and\nlarge-scale navigation tasks.",
    "published": "2025-09-08T15:08:42Z",
    "updated": "2025-10-06T16:26:44Z",
    "link": "http://arxiv.org/pdf/2509.06782v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Vittorio Giammarino",
      "Ruiqi Ni",
      "Ahmed H. Qureshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04988v1",
    "title": "Adaptive Memory Momentum via a Model-Based Framework for Deep Learning\n  Optimization",
    "summary": "The vast majority of modern deep learning models are trained with\nmomentum-based first-order optimizers. The momentum term governs the\noptimizer's memory by determining how much each past gradient contributes to\nthe current convergence direction. Fundamental momentum methods, such as\nNesterov Accelerated Gradient and the Heavy Ball method, as well as more recent\noptimizers such as AdamW and Lion, all rely on the momentum coefficient that is\ncustomarily set to $\\beta = 0.9$ and kept constant during model training, a\nstrategy widely used by practitioners, yet suboptimal. In this paper, we\nintroduce an \\textit{adaptive memory} mechanism that replaces constant momentum\nwith a dynamic momentum coefficient that is adjusted online during\noptimization. We derive our method by approximating the objective function\nusing two planes: one derived from the gradient at the current iterate and the\nother obtained from the accumulated memory of the past gradients. To the best\nof our knowledge, such a proximal framework was never used for momentum-based\noptimization. Our proposed approach is novel, extremely simple to use, and does\nnot rely on extra assumptions or hyperparameter tuning. We implement adaptive\nmemory variants of both SGD and AdamW across a wide range of learning tasks,\nfrom simple convex problems to large-scale deep learning scenarios,\ndemonstrating that our approach can outperform standard SGD and Adam with\nhand-tuned momentum coefficients. Finally, our work opens doors for new ways of\ninducing adaptivity in optimization.",
    "published": "2025-10-06T16:24:57Z",
    "updated": "2025-10-06T16:24:57Z",
    "link": "http://arxiv.org/pdf/2510.04988v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kristi Topollai",
      "Anna Choromanska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.19908v5",
    "title": "Another look at inference after prediction",
    "summary": "From structural biology to epidemiology, predictions from machine learning\n(ML) models increasingly complement costly gold-standard data to enable faster,\nmore affordable, and scalable scientific inquiry. In response, prediction-based\n(PB) inference has emerged to accommodate statistical analysis using a large\nvolume of predictions together with a small amount of gold-standard data. The\ngoals of PB inference are two-fold: (i) to mitigate bias from errors in\npredictions and (ii) to improve efficiency relative to classical inference\nusing only the gold-standard data. While early PB inference methods focused on\nbias, their ability to enhance efficiency remains a focus of ongoing research.\nWe revisit a foundational PB inference method and show that a simple\nmodification can be applied to guarantee provable improvements in efficiency.\nIn doing so, we establish new connections between augmented inverse probability\nweighted estimators (AIPW) and several recently proposed PB inference methods\nwith a similar focus. The utility of our proposal, which leverages\nprediction-based outcomes to enhance efficiency, is demonstrated through\nextensive simulation studies and an application to real data from the UK\nBiobank. Further, we contextualize PB inference by drawing connections to\nhistorical literature from economics and statistics, highlighting how classic\nmethods directly inform this contemporary problem.",
    "published": "2024-11-29T18:12:50Z",
    "updated": "2025-10-06T16:21:56Z",
    "link": "http://arxiv.org/pdf/2411.19908v5.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Jessica Gronsbell",
      "Jianhui Gao",
      "Yaqi Shi",
      "Zachary R. McCaw",
      "David Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04979v1",
    "title": "Federated Computation of ROC and PR Curves",
    "summary": "Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are\nfundamental tools for evaluating machine learning classifiers, offering\ndetailed insights into the trade-offs between true positive rate vs. false\npositive rate (ROC) or precision vs. recall (PR). However, in Federated\nLearning (FL) scenarios, where data is distributed across multiple clients,\ncomputing these curves is challenging due to privacy and communication\nconstraints. Specifically, the server cannot access raw prediction scores and\nclass labels, which are used to compute the ROC and PR curves in a centralized\nsetting. In this paper, we propose a novel method for approximating ROC and PR\ncurves in a federated setting by estimating quantiles of the prediction score\ndistribution under distributed differential privacy. We provide theoretical\nbounds on the Area Error (AE) between the true and estimated curves,\ndemonstrating the trade-offs between approximation accuracy, privacy, and\ncommunication cost. Empirical results on real-world datasets demonstrate that\nour method achieves high approximation accuracy with minimal communication and\nstrong privacy guarantees, making it practical for privacy-preserving model\nevaluation in federated systems.",
    "published": "2025-10-06T16:16:46Z",
    "updated": "2025-10-06T16:16:46Z",
    "link": "http://arxiv.org/pdf/2510.04979v1.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Xuefeng Xu",
      "Graham Cormode"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04974v1",
    "title": "StructuralDecompose: A Modular Framework for Robust Time Series\n  Decomposition in R",
    "summary": "We present StructuralDecompose, an R package for modular and interpretable\ntime series decomposition. Unlike existing approaches that treat decomposition\nas a monolithic process, StructuralDecompose separates the analysis into\ndistinct components: changepoint detection, anomaly detection, smoothing, and\ndecomposition. This design provides flexibility and robust- ness, allowing\nusers to tailor methods to specific time series characteristics. We demonstrate\nthe package on simulated and real-world datasets, benchmark its performance\nagainst state-of-the- art tools such as Rbeast and autostsm, and discuss its\nrole in interpretable machine learning workflows.",
    "published": "2025-10-06T16:11:49Z",
    "updated": "2025-10-06T16:11:49Z",
    "link": "http://arxiv.org/pdf/2510.04974v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Allen Daniel Sunny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.03389v4",
    "title": "A Deterministic Information Bottleneck Method for Clustering Mixed-Type\n  Data",
    "summary": "In this paper, we present an information-theoretic method for clustering\nmixed-type data, that is, data consisting of both continuous and categorical\nvariables. The proposed approach extends the Information Bottleneck principle\nto heterogeneous data through generalised product kernels, integrating\ncontinuous, nominal, and ordinal variables within a unified optimization\nframework. We address the following challenges: developing a systematic\nbandwidth selection strategy that equalises contributions across variable\ntypes, and proposing an adaptive hyperparameter updating scheme that ensures a\nvalid solution into a predetermined number of potentially imbalanced clusters.\nThrough simulations on 28,800 synthetic data sets and ten publicly available\nbenchmarks, we demonstrate that the proposed method, named DIBmix, achieves\nsuperior performance compared to four established methods (KAMILA,\nK-Prototypes, FAMD with K-Means, and PAM with Gower's dissimilarity). Results\nshow DIBmix particularly excels when clusters exhibit size imbalances, data\ncontain low or moderate cluster overlap, and categorical and continuous\nvariables are equally represented. The method presents a significant advantage\nover traditional centroid-based algorithms, establishing DIBmix as a\ncompetitive and theoretically grounded alternative for mixed-type data\nclustering.",
    "published": "2024-07-03T09:06:19Z",
    "updated": "2025-10-06T16:07:58Z",
    "link": "http://arxiv.org/pdf/2407.03389v4.pdf",
    "category": [
      "stat.ME",
      "cs.LG",
      "stat.ML",
      "62H30"
    ],
    "authors": [
      "Efthymios Costa",
      "Ioanna Papatsouma",
      "Angelos Markos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04972v1",
    "title": "Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent\n  Random Fields",
    "summary": "In this paper, we study fluctuations of conditionally centered statistics of\nthe form $$N^{-1/2}\\sum_{i=1}^N\nc_i(g(\\sigma_i)-\\mathbb{E}_N[g(\\sigma_i)|\\sigma_j,j\\neq i])$$ where\n$(\\sigma_1,\\ldots ,\\sigma_N)$ are sampled from a dependent random field, and\n$g$ is some bounded function. Our first main result shows that under weak\nsmoothness assumptions on the conditional means (which cover both sparse and\ndense interactions), the above statistic converges to a Gaussian \\emph{scale\nmixture} with a random scale determined by a \\emph{quadratic variance} and an\n\\emph{interaction component}. We also show that under appropriate\nstudentization, the limit becomes a pivotal Gaussian. We leverage this theory\nto develop a general asymptotic framework for maximum pseudolikelihood (MPLE)\ninference in dependent random fields. We apply our results to Ising models with\npairwise as well as higher-order interactions and exponential random graph\nmodels (ERGMs). In particular, we obtain a joint central limit theorem for the\ninverse temperature and magnetization parameters via the joint MPLE (to our\nknowledge, the first such result in dense, irregular regimes), and we derive\nconditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without\nrestricting to the ``sub-critical\" region. Our proof is based on a method of\nmoments approach via combinatorial decision-tree pruning, which may be of\nindependent interest.",
    "published": "2025-10-06T16:06:45Z",
    "updated": "2025-10-06T16:06:45Z",
    "link": "http://arxiv.org/pdf/2510.04972v1.pdf",
    "category": [
      "math.ST",
      "cs.LG",
      "math.PR",
      "stat.TH",
      "82B20, 82B26"
    ],
    "authors": [
      "Nabarun Deb"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04930v1",
    "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking",
    "summary": "Grokking is the phenomenon whereby, unlike the training performance, which\npeaks early in the training process, the test/generalization performance of a\nmodel stagnates over arbitrarily many epochs and then suddenly jumps to usually\nclose to perfect levels. In practice, it is desirable to reduce the length of\nsuch plateaus, that is to make the learning process \"grok\" faster. In this\nwork, we provide new insights into grokking. First, we show both empirically\nand theoretically that grokking can be induced by asymmetric speeds of\n(stochastic) gradient descent, along different principal (i.e singular\ndirections) of the gradients. We then propose a simple modification that\nnormalizes the gradients so that dynamics along all the principal directions\nevolves at exactly the same speed. Then, we establish that this modified\nmethod, which we call egalitarian gradient descent (EGD) and can be seen as a\ncarefully modified form of natural gradient descent, groks much faster. In\nfact, in some cases the stagnation is completely removed. Finally, we\nempirically show that on classical arithmetic problems such as modular addition\nand sparse parity problem which this stagnation has been widely observed and\nintensively studied, that our proposed method eliminates the plateaus.",
    "published": "2025-10-06T15:40:36Z",
    "updated": "2025-10-06T15:40:36Z",
    "link": "http://arxiv.org/pdf/2510.04930v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ali Saheb Pasand",
      "Elvis Dohmatob"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04926v1",
    "title": "Set to Be Fair: Demographic Parity Constraints for Set-Valued\n  Classification",
    "summary": "Set-valued classification is used in multiclass settings where confusion\nbetween classes can occur and lead to misleading predictions. However, its\napplication may amplify discriminatory bias motivating the development of\nset-valued approaches under fairness constraints. In this paper, we address the\nproblem of set-valued classification under demographic parity and expected size\nconstraints. We propose two complementary strategies: an oracle-based method\nthat minimizes classification risk while satisfying both constraints, and a\ncomputationally efficient proxy that prioritizes constraint satisfaction. For\nboth strategies, we derive closed-form expressions for the (optimal) fair\nset-valued classifiers and use these to build plug-in, data-driven procedures\nfor empirical predictions. We establish distribution-free convergence rates for\nviolations of the size and fairness constraints for both methods, and under\nmild assumptions we also provide excess-risk bounds for the oracle-based\napproach. Empirical results demonstrate the effectiveness of both strategies\nand highlight the efficiency of our proxy method.",
    "published": "2025-10-06T15:36:45Z",
    "updated": "2025-10-06T15:36:45Z",
    "link": "http://arxiv.org/pdf/2510.04926v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Eyal Cohen",
      "Christophe Denis",
      "Mohamed Hebiri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04908v1",
    "title": "How Different from the Past? Spatio-Temporal Time Series Forecasting\n  with Self-Supervised Deviation Learning",
    "summary": "Spatio-temporal forecasting is essential for real-world applications such as\ntraffic management and urban computing. Although recent methods have shown\nimproved accuracy, they often fail to account for dynamic deviations between\ncurrent inputs and historical patterns. These deviations contain critical\nsignals that can significantly affect model performance. To fill this gap, we\npropose ST-SSDL, a Spatio-Temporal time series forecasting framework that\nincorporates a Self-Supervised Deviation Learning scheme to capture and utilize\nsuch deviations. ST-SSDL anchors each input to its historical average and\ndiscretizes the latent space using learnable prototypes that represent typical\nspatio-temporal patterns. Two auxiliary objectives are proposed to refine this\nstructure: a contrastive loss that enhances inter-prototype discriminability\nand a deviation loss that regularizes the distance consistency between input\nrepresentations and corresponding prototypes to quantify deviation. Optimized\njointly with the forecasting objective, these components guide the model to\norganize its hidden space and improve generalization across diverse input\nconditions. Experiments on six benchmark datasets show that ST-SSDL\nconsistently outperforms state-of-the-art baselines across multiple metrics.\nVisualizations further demonstrate its ability to adaptively respond to varying\nlevels of deviation in complex spatio-temporal scenarios. Our code and datasets\nare available at https://github.com/Jimmy-7664/ST-SSDL.",
    "published": "2025-10-06T15:21:13Z",
    "updated": "2025-10-06T15:21:13Z",
    "link": "http://arxiv.org/pdf/2510.04908v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Haotian Gao",
      "Zheng Dong",
      "Jiawei Yong",
      "Shintaro Fukushima",
      "Kenjiro Taura",
      "Renhe Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04902v1",
    "title": "DP-HYPE: Distributed Differentially Private Hyperparameter Search",
    "summary": "The tuning of hyperparameters in distributed machine learning can\nsubstantially impact model performance. When the hyperparameters are tuned on\nsensitive data, privacy becomes an important challenge and to this end,\ndifferential privacy has emerged as the de facto standard for provable privacy.\nA standard setting when performing distributed learning tasks is that clients\nagree on a shared setup, i.e., find a compromise from a set of hyperparameters,\nlike the learning rate of the model to be trained. Yet, prior work on\ndifferentially private hyperparameter tuning either uses computationally\nexpensive cryptographic protocols, determines hyperparameters separately for\neach client, or applies differential privacy locally, which can lead to\nundesirable utility-privacy trade-offs.\n  In this work, we present our algorithm DP-HYPE, which performs a distributed\nand privacy-preserving hyperparameter search by conducting a distributed voting\nbased on local hyperparameter evaluations of clients. In this way, DP-HYPE\nselects hyperparameters that lead to a compromise supported by the majority of\nclients, while maintaining scalability and independence from specific learning\ntasks. We prove that DP-HYPE preserves the strong notion of differential\nprivacy called client-level differential privacy and, importantly, show that\nits privacy guarantees do not depend on the number of hyperparameters. We also\nprovide bounds on its utility guarantees, that is, the probability of reaching\na compromise, and implement DP-HYPE as a submodule in the popular Flower\nframework for distributed machine learning. In addition, we evaluate\nperformance on multiple benchmark data sets in iid as well as multiple non-iid\nsettings and demonstrate high utility of DP-HYPE even under small privacy\nbudgets.",
    "published": "2025-10-06T15:18:34Z",
    "updated": "2025-10-06T15:18:34Z",
    "link": "http://arxiv.org/pdf/2510.04902v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Johannes Liebenow",
      "Thorsten Peinemann",
      "Esfandiar Mohammadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04900v1",
    "title": "Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of\n  Multivariate Long Time Series Forecasting Models",
    "summary": "Understanding the robustness of deep learning models for multivariate\nlong-term time series forecasting (M-LTSF) remains challenging, as evaluations\ntypically rely on real-world datasets with unknown noise properties. We propose\na simulation-based evaluation framework that generates parameterizable\nsynthetic datasets, where each dataset instance corresponds to a different\nconfiguration of signal components, noise types, signal-to-noise ratios, and\nfrequency characteristics. These configurable components aim to model\nreal-world multivariate time series data without the ambiguity of unknown\nnoise. This framework enables fine-grained, systematic evaluation of M-LTSF\nmodels under controlled and diverse scenarios. We benchmark four representative\narchitectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear\n(linear), and Autoformer (decomposition-based). Our analysis reveals that all\nmodels degrade severely when lookback windows cannot capture complete periods\nof seasonal patters in the data. S-Mamba and Autoformer perform best on\nsawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.\nWhite and Brownian noise universally degrade performance with lower\nsignal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer\nshows seasonal-noise vulnerability. Further spectral analysis shows that\nS-Mamba and iTransformer achieve superior frequency reconstruction. This\ncontrolled approach, based on our synthetic and principle-driven testbed,\noffers deeper insights into model-specific strengths and limitations through\nthe aggregation of MSE scores and provides concrete guidance for model\nselection based on signal characteristics and noise conditions.",
    "published": "2025-10-06T15:16:52Z",
    "updated": "2025-10-06T15:16:52Z",
    "link": "http://arxiv.org/pdf/2510.04900v1.pdf",
    "category": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Nick Janßen",
      "Melanie Schaller",
      "Bodo Rosenhahn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04885v1",
    "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning\n  Recipe for Strong Prompt Injection",
    "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector.",
    "published": "2025-10-06T15:06:04Z",
    "updated": "2025-10-06T15:06:04Z",
    "link": "http://arxiv.org/pdf/2510.04885v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Yuxin Wen",
      "Arman Zharmagambetov",
      "Ivan Evtimov",
      "Narine Kokhlikyan",
      "Tom Goldstein",
      "Kamalika Chaudhuri",
      "Chuan Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04878v1",
    "title": "Flow-Matching Based Refiner for Molecular Conformer Generation",
    "summary": "Low-energy molecular conformers generation (MCG) is a foundational yet\nchallenging problem in drug discovery. Denoising-based methods include\ndiffusion and flow-matching methods that learn mappings from a simple base\ndistribution to the molecular conformer distribution. However, these approaches\noften suffer from error accumulation during sampling, especially in the low SNR\nsteps, which are hard to train. To address these challenges, we propose a\nflow-matching refiner for the MCG task. The proposed method initializes\nsampling from mixed-quality outputs produced by upstream denoising models and\nreschedules the noise scale to bypass the low-SNR phase, thereby improving\nsample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the\ngenerator-refiner pipeline improves quality with fewer total denoising steps\nwhile preserving diversity.",
    "published": "2025-10-06T15:00:36Z",
    "updated": "2025-10-06T15:00:36Z",
    "link": "http://arxiv.org/pdf/2510.04878v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.QM"
    ],
    "authors": [
      "Xiangyang Xu",
      "Hongyang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.10123v3",
    "title": "Asynchronous Federated Stochastic Optimization for Heterogeneous\n  Objectives Under Arbitrary Delays",
    "summary": "Federated learning (FL) was recently proposed to securely train models with\ndata held over multiple locations (``clients'') under the coordination of a\ncentral server. Prolonged training times caused by slow clients may hinder the\nperformance of FL; while asynchronous communication is a promising solution,\nhighly heterogeneous client response times under non-IID local data may\nintroduce significant bias to the global model, particularly in client-driven\nsetups where sampling is infeasible. To address this issue, we propose\n\\underline{A}synch\\underline{R}onous \\underline{E}xact \\underline{A}veraging\n(\\textsc{AREA}), a stochastic (sub)gradient method that leverages asynchrony\nfor scalability and uses client-side memory to correct the bias induced by\nuneven participation, without client sampling or prior knowledge of client\nlatencies. \\textsc{AREA} communicates model residuals rather than gradient\nestimates, reducing exposure to gradient inversion, and is compatible with\nsecure aggregation. Under standard assumptions and unbounded, heterogeneous\ndelays with finite mean, AREA achieves optimal convergence rates:\n$\\mathcal{O}(1/K)$ in the strongly convex, smooth regime and\n$\\mathcal{O}(1/\\sqrt{K})$ in the convex, nonsmooth regime. For strongly convex,\nsmooth objectives, we demonstrate theoretically and empirically that AREA\naccommodates larger step sizes than existing methods, enabling fast convergence\nwithout adversely impacting model generalization. In the convex, nonsmooth\nsetting, to our knowledge we are the first to obtain rates that scale with the\naverage client update frequency rather than the minimum or maximum, indicating\nincreased robustness to outliers.",
    "published": "2024-05-16T14:22:49Z",
    "updated": "2025-10-06T14:53:25Z",
    "link": "http://arxiv.org/pdf/2405.10123v3.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Charikleia Iakovidou",
      "Kibaek Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04861v1",
    "title": "A Clinical-grade Universal Foundation Model for Intraoperative Pathology",
    "summary": "Intraoperative pathology is pivotal to precision surgery, yet its clinical\nimpact is constrained by diagnostic complexity and the limited availability of\nhigh-quality frozen-section data. While computational pathology has made\nsignificant strides, the lack of large-scale, prospective validation has\nimpeded its routine adoption in surgical workflows. Here, we introduce CRISP, a\nclinical-grade foundation model developed on over 100,000 frozen sections from\neight medical centers, specifically designed to provide Clinical-grade Robust\nIntraoperative Support for Pathology (CRISP). CRISP was comprehensively\nevaluated on more than 15,000 intraoperative slides across nearly 100\nretrospective diagnostic tasks, including benign-malignant discrimination, key\nintraoperative decision-making, and pan-cancer detection, etc. The model\ndemonstrated robust generalization across diverse institutions, tumor types,\nand anatomical sites-including previously unseen sites and rare cancers. In a\nprospective cohort of over 2,000 patients, CRISP sustained high diagnostic\naccuracy under real-world conditions, directly informing surgical decisions in\n92.6% of cases. Human-AI collaboration further reduced diagnostic workload by\n35%, avoided 105 ancillary tests and enhanced detection of micrometastases with\n87.5% accuracy. Together, these findings position CRISP as a clinical-grade\nparadigm for AI-driven intraoperative pathology, bridging computational\nadvances with surgical precision and accelerating the translation of artificial\nintelligence into routine clinical practice.",
    "published": "2025-10-06T14:48:43Z",
    "updated": "2025-10-06T14:48:43Z",
    "link": "http://arxiv.org/pdf/2510.04861v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zihan Zhao",
      "Fengtao Zhou",
      "Ronggang Li",
      "Bing Chu",
      "Xinke Zhang",
      "Xueyi Zheng",
      "Ke Zheng",
      "Xiaobo Wen",
      "Jiabo Ma",
      "Yihui Wang",
      "Jiewei Chen",
      "Chengyou Zheng",
      "Jiangyu Zhang",
      "Yongqin Wen",
      "Jiajia Meng",
      "Ziqi Zeng",
      "Xiaoqing Li",
      "Jing Li",
      "Dan Xie",
      "Yaping Ye",
      "Yu Wang",
      "Hao Chen",
      "Muyan Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04855v1",
    "title": "Synthesising Counterfactual Explanations via Label-Conditional Gaussian\n  Mixture Variational Autoencoders",
    "summary": "Counterfactual explanations (CEs) provide recourse recommendations for\nindividuals affected by algorithmic decisions. A key challenge is generating\nCEs that are robust against various perturbation types (e.g. input and model\nperturbations) while simultaneously satisfying other desirable properties.\nThese include plausibility, ensuring CEs reside on the data manifold, and\ndiversity, providing multiple distinct recourse options for single inputs.\nExisting methods, however, mostly struggle to address these multifaceted\nrequirements in a unified, model-agnostic manner. We address these limitations\nby proposing a novel generative framework. First, we introduce the\nLabel-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model\ntrained to learn a structured latent space where each class label is\nrepresented by a set of Gaussian components with diverse, prototypical\ncentroids. Building on this, we present LAPACE (LAtent PAth Counterfactual\nExplanations), a model-agnostic algorithm that synthesises entire paths of CE\npoints by interpolating from inputs' latent representations to those learned\nlatent centroids. This approach inherently ensures robustness to input changes,\nas all paths for a given target class converge to the same fixed centroids.\nFurthermore, the generated paths provide a spectrum of recourse options,\nallowing users to navigate the trade-off between proximity and plausibility\nwhile also encouraging robustness against model changes. In addition,\nuser-specified actionability constraints can also be easily incorporated via\nlightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive\nexperiments show that LAPACE is computationally efficient and achieves\ncompetitive performance across eight quantitative metrics.",
    "published": "2025-10-06T14:42:23Z",
    "updated": "2025-10-06T14:42:23Z",
    "link": "http://arxiv.org/pdf/2510.04855v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Junqi Jiang",
      "Francesco Leofante",
      "Antonio Rago",
      "Francesca Toni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20020v2",
    "title": "The Syntax and Semantics of einsum",
    "summary": "In 2011, einsum was introduced to NumPy as a practical and convenient\nnotation for tensor expressions in machine learning, quantum circuit\nsimulation, and other fields. It has since been implemented in additional\nPython frameworks such as PyTorch and TensorFlow, as well as in other\nprogramming languages such as Julia. Despite its practical success, the einsum\nnotation still lacks a solid theoretical basis, and is not unified across the\ndifferent frameworks, limiting opportunities for formal reasoning and\nsystematic optimization. In this work, we discuss the terminology of tensor\nexpressions and provide a formal definition of the einsum language. Based on\nthis definition, we formalize and prove important equivalence rules for tensor\nexpressions and highlight their relevance in practical applications.",
    "published": "2025-09-24T11:36:02Z",
    "updated": "2025-10-06T14:35:42Z",
    "link": "http://arxiv.org/pdf/2509.20020v2.pdf",
    "category": [
      "cs.PL",
      "cs.LG",
      "cs.MS",
      "cs.SC",
      "F.2.2; I.1.2; I.1.3"
    ],
    "authors": [
      "Maurice Wenig",
      "Paul G. Rump",
      "Mark Blacher",
      "Joachim Giesen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.06674v5",
    "title": "Impact of Dataset Properties on Membership Inference Vulnerability of\n  Deep Transfer Learning",
    "summary": "Membership inference attacks (MIAs) are used to test practical privacy of\nmachine learning models. MIAs complement formal guarantees from differential\nprivacy (DP) under a more realistic adversary model. We analyse MIA\nvulnerability of fine-tuned neural networks both empirically and theoretically,\nthe latter using a simplified model of fine-tuning. We show that the\nvulnerability of non-DP models when measured as the attacker advantage at a\nfixed false positive rate reduces according to a simple power law as the number\nof examples per class increases. A similar power-law applies even for the most\nvulnerable points, but the dataset size needed for adequate protection of the\nmost vulnerable points is very large.",
    "published": "2024-02-07T14:23:01Z",
    "updated": "2025-10-06T14:33:31Z",
    "link": "http://arxiv.org/pdf/2402.06674v5.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Marlon Tobaben",
      "Hibiki Ito",
      "Joonas Jälkö",
      "Yuan He",
      "Antti Honkela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04834v1",
    "title": "On the Hardness of Learning Regular Expressions",
    "summary": "Despite the theoretical significance and wide practical use of regular\nexpressions, the computational complexity of learning them has been largely\nunexplored. We study the computational hardness of improperly learning regular\nexpressions in the PAC model and with membership queries. We show that PAC\nlearning is hard even under the uniform distribution on the hypercube, and also\nprove hardness of distribution-free learning with membership queries.\nFurthermore, if regular expressions are extended with complement or\nintersection, we establish hardness of learning with membership queries even\nunder the uniform distribution. We emphasize that these results do not follow\nfrom existing hardness results for learning DFAs or NFAs, since the descriptive\ncomplexity of regular languages can differ exponentially between DFAs, NFAs,\nand regular expressions.",
    "published": "2025-10-06T14:17:55Z",
    "updated": "2025-10-06T14:17:55Z",
    "link": "http://arxiv.org/pdf/2510.04834v1.pdf",
    "category": [
      "cs.LG",
      "cs.CC"
    ],
    "authors": [
      "Idan Attias",
      "Lev Reyzin",
      "Nathan Srebro",
      "Gal Vardi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04811v1",
    "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation",
    "summary": "Understanding signal behavior across scales is vital in areas such as natural\nphenomena analysis and financial modeling. A key property is self-similarity,\nquantified by the Hurst exponent (H), which reveals long-term dependencies.\nWavelet-based methods are effective for estimating H due to their multi-scale\nanalysis capability, but additive noise in real-world measurements often\ndegrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an\nenhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),\nincorporating noise mitigation and generating multiple level-pairwise estimates\nfrom signal energy pairs. A neural network (NN) combines these estimates,\nreplacing traditional averaging. This adaptive learning maintains ALPHEE's\nbehavior in noise-free cases while improving performance in noisy conditions.\nExtensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's\naccuracy using both averaging and NN-based methods. Under noise, however,\ntraditional averaging deteriorates and requires impractical level restrictions,\nwhile NC-ALPHEE consistently outperforms existing techniques without such\nconstraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,\nsignificantly enhancing the reliability of wavelet-based methods in noisy\nenvironments.",
    "published": "2025-10-06T13:54:23Z",
    "updated": "2025-10-06T13:54:23Z",
    "link": "http://arxiv.org/pdf/2510.04811v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Malith Premarathna",
      "Fabrizio Ruggeri",
      "Dixon Vimalajeewa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.08224v3",
    "title": "Joint Diffusion models in Continual Learning",
    "summary": "In this work, we introduce JDCL - a new method for continual learning with\ngenerative rehearsal based on joint diffusion models. Neural networks suffer\nfrom catastrophic forgetting defined as abrupt loss in the model's performance\nwhen retrained with additional data coming from a different distribution.\nGenerative-replay-based continual learning methods try to mitigate this issue\nby retraining a model with a combination of new and rehearsal data sampled from\na generative model. In this work, we propose to extend this idea by combining a\ncontinually trained classifier with a diffusion-based generative model into a\nsingle - jointly optimized neural network. We show that such shared\nparametrization, combined with the knowledge distillation technique allows for\nstable adaptation to new tasks without catastrophic forgetting. We evaluate our\napproach on several benchmarks, where it outperforms recent state-of-the-art\ngenerative replay techniques. Additionally, we extend our method to the\nsemi-supervised continual learning setup, where it outperforms competing\nbuffer-based replay techniques, and evaluate, in a self-supervised manner, the\nquality of trained representations.",
    "published": "2024-11-12T22:35:44Z",
    "updated": "2025-10-06T13:30:32Z",
    "link": "http://arxiv.org/pdf/2411.08224v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Paweł Skierś",
      "Kamil Deja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04780v1",
    "title": "Kernel ridge regression under power-law data: spectrum and\n  generalization",
    "summary": "In this work, we investigate high-dimensional kernel ridge regression (KRR)\non i.i.d. Gaussian data with anisotropic power-law covariance. This setting\ndiffers fundamentally from the classical source & capacity conditions for KRR,\nwhere power-law assumptions are typically imposed on the kernel eigen-spectrum\nitself. Our contributions are twofold. First, we derive an explicit\ncharacterization of the kernel spectrum for polynomial inner-product kernels,\ngiving a precise description of how the kernel eigen-spectrum inherits the data\ndecay. Second, we provide an asymptotic analysis of the excess risk in the\nhigh-dimensional regime for a particular kernel with this spectral behavior,\nshowing that the sample complexity is governed by the effective dimension of\nthe data rather than the ambient dimension. These results establish a\nfundamental advantage of learning with power-law anisotropic data over\nisotropic data. To our knowledge, this is the first rigorous treatment of\nnon-linear KRR under power-law data.",
    "published": "2025-10-06T12:58:35Z",
    "updated": "2025-10-06T12:58:35Z",
    "link": "http://arxiv.org/pdf/2510.04780v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Arie Wortsman",
      "Bruno Loureiro"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03013v2",
    "title": "Distributional Inverse Reinforcement Learning",
    "summary": "We propose a distributional framework for offline Inverse Reinforcement\nLearning (IRL) that jointly models uncertainty over reward functions and full\ndistributions of returns. Unlike conventional IRL approaches that recover a\ndeterministic reward estimate or match only expected returns, our method\ncaptures richer structure in expert behavior, particularly in learning the\nreward distribution, by minimizing first-order stochastic dominance (FSD)\nviolations and thus integrating distortion risk measures (DRMs) into policy\nlearning, enabling the recovery of both reward distributions and\ndistribution-aware policies. This formulation is well-suited for behavior\nanalysis and risk-aware imitation learning. Empirical results on synthetic\nbenchmarks, real-world neurobehavioral data, and MuJoCo control tasks\ndemonstrate that our method recovers expressive reward representations and\nachieves state-of-the-art imitation performance.",
    "published": "2025-10-03T13:58:09Z",
    "updated": "2025-10-06T12:56:00Z",
    "link": "http://arxiv.org/pdf/2510.03013v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Feiyang Wu",
      "Ye Zhao",
      "Anqi Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04776v1",
    "title": "MetaMP: Seamless Metadata Enrichment and AI Application Framework for\n  Enhanced Membrane Protein Visualization and Analysis",
    "summary": "Structural biology has made significant progress in determining membrane\nproteins, leading to a remarkable increase in the number of available\nstructures in dedicated databases. The inherent complexity of membrane protein\nstructures, coupled with challenges such as missing data, inconsistencies, and\ncomputational barriers from disparate sources, underscores the need for\nimproved database integration. To address this gap, we present MetaMP, a\nframework that unifies membrane-protein databases within a web application and\nuses machine learning for classification. MetaMP improves data quality by\nenriching metadata, offering a user-friendly interface, and providing eight\ninteractive views for streamlined exploration. MetaMP was effective across\ntasks of varying difficulty, demonstrating advantages across different levels\nwithout compromising speed or accuracy, according to user evaluations.\nMoreover, MetaMP supports essential functions such as structure classification\nand outlier detection.\n  We present three practical applications of Artificial Intelligence (AI) in\nmembrane protein research: predicting transmembrane segments, reconciling\nlegacy databases, and classifying structures with explainable AI support. In a\nvalidation focused on statistics, MetaMP resolved 77% of data discrepancies and\naccurately predicted the class of newly identified membrane proteins 98% of the\ntime and overtook expert curation. Altogether, MetaMP is a much-needed resource\nthat harmonizes current knowledge and empowers AI-driven exploration of\nmembrane-protein architecture.",
    "published": "2025-10-06T12:52:50Z",
    "updated": "2025-10-06T12:52:50Z",
    "link": "http://arxiv.org/pdf/2510.04776v1.pdf",
    "category": [
      "cs.LG",
      "cs.DB"
    ],
    "authors": [
      "Ebenezer Awotoro",
      "Chisom Ezekannagha",
      "Florian Schwarz",
      "Johannes Tauscher",
      "Dominik Heider",
      "Katharina Ladewig",
      "Christel Le Bon",
      "Karine Moncoq",
      "Bruno Miroux",
      "Georges Hattab"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04767v1",
    "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
    "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.",
    "published": "2025-10-06T12:41:31Z",
    "updated": "2025-10-06T12:41:31Z",
    "link": "http://arxiv.org/pdf/2510.04767v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Wonjun Kang",
      "Kevin Galim",
      "Seunghyuk Oh",
      "Minjae Lee",
      "Yuchen Zeng",
      "Shuibai Zhang",
      "Coleman Hooper",
      "Yuezhou Hu",
      "Hyung Il Koo",
      "Nam Ik Cho",
      "Kangwook Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04758v1",
    "title": "Provable Affine Identifiability of Nonlinear CCA under Latent\n  Distributional Priors",
    "summary": "In this work, we establish conditions under which nonlinear CCA recovers the\nground-truth latent factors up to an orthogonal transform after whitening.\nBuilding on the classical result that linear mappings maximize canonical\ncorrelations under Gaussian priors, we prove affine identifiability for a broad\nclass of latent distributions in the population setting. Central to our proof\nis a reparameterization result that transports the analysis from observation\nspace to source space, where identifiability becomes tractable. We further show\nthat whitening is essential for ensuring boundedness and well-conditioning,\nthereby underpinning identifiability. Beyond the population setting, we prove\nthat ridge-regularized empirical CCA converges to its population counterpart,\ntransferring these guarantees to the finite-sample regime. Experiments on a\ncontrolled synthetic dataset and a rendered image dataset validate our theory\nand demonstrate the necessity of its assumptions through systematic ablations.",
    "published": "2025-10-06T12:35:07Z",
    "updated": "2025-10-06T12:35:07Z",
    "link": "http://arxiv.org/pdf/2510.04758v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhiwei Han",
      "Stefan Matthes",
      "Hao Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04728v1",
    "title": "EVaR-Optimal Arm Identification in Bandits",
    "summary": "We study the fixed-confidence best arm identification (BAI) problem within\nthe multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)\ncriterion. Our analysis considers a nonparametric setting, allowing for general\nreward distributions bounded in [0,1]. This formulation addresses the critical\nneed for risk-averse decision-making in high-stakes environments, such as\nfinance, moving beyond simple expected value optimization. We propose a\n$\\delta$-correct, Track-and-Stop based algorithm and derive a corresponding\nlower bound on the expected sample complexity, which we prove is asymptotically\nmatched. The implementation of our algorithm and the characterization of the\nlower bound both require solving a complex convex optimization problem and a\nrelated, simpler non-convex one.",
    "published": "2025-10-06T11:49:56Z",
    "updated": "2025-10-06T11:49:56Z",
    "link": "http://arxiv.org/pdf/2510.04728v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mehrasa Ahmadipour",
      "Aurélien Garivier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04727v1",
    "title": "Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and\n  Undirected Hypergraphs",
    "summary": "Hypergraphs provide a natural way to represent higher-order interactions\namong multiple entities. While undirected hypergraphs have been extensively\nstudied, the case of directed hypergraphs, which can model oriented group\ninteractions, remains largely under-explored despite its relevance for many\napplications. Recent approaches in this direction often exhibit an implicit\nbias toward homophily, which limits their effectiveness in heterophilic\nsettings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf\nNeural Networks (SNNs) were introduced as an effective solution to circumvent\nsuch a drawback. While a generalization to hypergraphs is known, it is only\nsuitable for undirected hypergraphs, failing to tackle the directed case. In\nthis work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a\nframework integrating sheaf theory with a principled treatment of asymmetric\nrelations within a hypergraph. From it, we construct the Directed Sheaf\nHypergraph Laplacian, a complex-valued operator by which we unify and\ngeneralize many existing Laplacian matrices proposed in the graph- and\nhypergraph-learning literature. Across 7 real-world datasets and against 13\nbaselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how\na principled treatment of directionality in hypergraphs, combined with the\nexpressive power of sheaves, can substantially improve performance.",
    "published": "2025-10-06T11:46:53Z",
    "updated": "2025-10-06T11:46:53Z",
    "link": "http://arxiv.org/pdf/2510.04727v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Emanuele Mule",
      "Stefano Fiorini",
      "Antonio Purificato",
      "Federico Siciliano",
      "Stefano Coniglio",
      "Fabrizio Silvestri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04726v1",
    "title": "Predictive economics: Rethinking economic methodology with machine\n  learning",
    "summary": "This article proposes predictive economics as a distinct analytical\nperspective within economics, grounded in machine learning and centred on\npredictive accuracy rather than causal identification. Drawing on the\ninstrumentalist tradition (Friedman), the explanation-prediction divide\n(Shmueli), and the contrast between modelling cultures (Breiman), we formalise\nprediction as a valid epistemological and methodological objective. Reviewing\nrecent applications across economic subfields, we show how predictive models\ncontribute to empirical analysis, particularly in complex or data-rich\ncontexts. This perspective complements existing approaches and supports a more\npluralistic methodology - one that values out-of-sample performance alongside\ninterpretability and theoretical structure.",
    "published": "2025-10-06T11:46:03Z",
    "updated": "2025-10-06T11:46:03Z",
    "link": "http://arxiv.org/pdf/2510.04726v1.pdf",
    "category": [
      "econ.GN",
      "cs.LG",
      "q-fin.EC"
    ],
    "authors": [
      "Miguel Alves Pereira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00264v2",
    "title": "Low Resource Audio Codec Challenge Baseline Systems",
    "summary": "The Low-Resource Audio Codec (LRAC) Challenge aims to advance neural audio\ncoding for deployment in resource-constrained environments. The first edition\nfocuses on low-resource neural speech codecs that must operate reliably under\neveryday noise and reverberation, while satisfying strict constraints on\ncomputational complexity, latency, and bitrate. Track 1 targets transparency\ncodecs, which aim to preserve the perceptual transparency of input speech under\nmild noise and reverberation. Track 2 addresses enhancement codecs, which\ncombine coding and compression with denoising and dereverberation. This paper\npresents the official baseline systems for both tracks in the 2025 LRAC\nChallenge. The baselines are convolutional neural codec models with Residual\nVector Quantization, trained end-to-end using a combination of adversarial and\nreconstruction objectives. We detail the data filtering and augmentation\nstrategies, model architectures, optimization procedures, and checkpoint\nselection criteria.",
    "published": "2025-09-30T20:36:58Z",
    "updated": "2025-10-06T11:39:10Z",
    "link": "http://arxiv.org/pdf/2510.00264v2.pdf",
    "category": [
      "cs.SD",
      "cs.LG"
    ],
    "authors": [
      "Yusuf Ziya Isik",
      "Rafał Łaganowski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03697v2",
    "title": "RhoDARTS: Differentiable Quantum Architecture Search with Density Matrix\n  Simulations",
    "summary": "Variational Quantum Algorithms (VQAs) are a promising approach to leverage\nNoisy Intermediate-Scale Quantum (NISQ) computers. However, choosing optimal\nquantum circuits that efficiently solve a given VQA problem is a non-trivial\ntask. Quantum Architecture Search (QAS) algorithms enable automatic generation\nof quantum circuits tailored to the provided problem. Existing QAS approaches\ntypically adapt classical neural architecture search techniques, training\nmachine learning models to sample relevant circuits, but often overlook the\ninherent quantum nature of the circuits they produce. By reformulating QAS from\na quantum perspective, we propose a sampling-free differentiable QAS algorithm\nthat models the search process as the evolution of a quantum mixed state, which\nemerges from the search space of quantum circuits. The mixed state formulation\nalso enables our method to incorporate generic noise models, for example the\ndepolarizing channel, which cannot be modeled by state vector simulation. We\nvalidate our method by finding circuits for state initialization and\nHamiltonian optimization tasks, namely the variational quantum eigensolver and\nthe unweighted max-cut problems. We show our approach to be comparable to, if\nnot outperform, existing QAS techniques while requiring significantly fewer\nquantum simulations during training, and also show improved robustness levels\nto noise.",
    "published": "2025-06-04T08:30:35Z",
    "updated": "2025-10-06T11:34:48Z",
    "link": "http://arxiv.org/pdf/2506.03697v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Swagat Kumar",
      "Jan-Nico Zaech",
      "Colin Michael Wilmott",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19506v2",
    "title": "Frame-based Equivariant Diffusion Models for 3D Molecular Generation",
    "summary": "Recent methods for molecular generation face a trade-off: they either enforce\nstrict equivariance with costly architectures or relax it to gain scalability\nand flexibility. We propose a frame-based diffusion paradigm that achieves\ndeterministic E(3)-equivariance while decoupling symmetry handling from the\nbackbone. Building on this paradigm, we investigate three variants: Global\nFrame Diffusion (GFD), which assigns a shared molecular frame; Local Frame\nDiffusion (LFD), which constructs node-specific frames and benefits from\nadditional alignment constraints; and Invariant Frame Diffusion (IFD), which\nrelies on pre-canonicalized invariant representations. To enhance expressivity,\nwe further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention.\n  On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance,\nwith a test NLL of -137.97 at standard scale and -141.85 at double scale,\nalongside atom stability of 98.98%, and molecular stability of 90.51%. These\nresults surpass all equivariant baselines while maintaining high validity and\nuniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study\nestablishes frame-based diffusion as a scalable, flexible, and physically\ngrounded paradigm for molecular generation, highlighting the critical role of\nglobal structure preservation.",
    "published": "2025-09-23T19:23:37Z",
    "updated": "2025-10-06T11:26:19Z",
    "link": "http://arxiv.org/pdf/2509.19506v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mohan Guo",
      "Cong Liu",
      "Patrick Forré"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04710v1",
    "title": "ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts",
    "summary": "Web service administrators must ensure the stability of multiple systems by\npromptly detecting anomalies in Key Performance Indicators (KPIs). Achieving\nthe goal of \"train once, infer across scenarios\" remains a fundamental\nchallenge for time series anomaly detection models. Beyond improving zero-shot\ngeneralization, such models must also flexibly handle sequences of varying\nlengths during inference, ranging from one hour to one week, without\nretraining. Conventional approaches rely on sliding-window encoding and\nself-supervised learning, which restrict inference to fixed-length inputs.\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot\ncapabilities across general domains. However, when applied to time series data,\nthey face inherent limitations due to context length. To address this issue, we\npropose ViTs, a Vision-Language Model (VLM)-based framework that converts time\nseries curves into visual representations. By rescaling time series images,\ntemporal dependencies are preserved while maintaining a consistent input size,\nthereby enabling efficient processing of arbitrarily long sequences without\ncontext constraints. Training VLMs for this purpose introduces unique\nchallenges, primarily due to the scarcity of aligned time series image-text\ndata. To overcome this, we employ an evolutionary algorithm to automatically\ngenerate thousands of high-quality image-text pairs and design a three-stage\ntraining pipeline consisting of: (1) time series knowledge injection, (2)\nanomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive\nexperiments demonstrate that ViTs substantially enhance the ability of VLMs to\nunderstand and detect anomalies in time series data. All datasets and code will\nbe publicly released at: https://anonymous.4open.science/r/ViTs-C484/.",
    "published": "2025-10-06T11:24:53Z",
    "updated": "2025-10-06T11:24:53Z",
    "link": "http://arxiv.org/pdf/2510.04710v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zexin Wang",
      "Changhua Pei",
      "Yang Liu",
      "Hengyue Jiang",
      "Quan Zhou",
      "Haotian Si",
      "Hang Cui",
      "Jianhui Li",
      "Gaogang Xie",
      "Jingjing Li",
      "Dan Pei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04688v1",
    "title": "A Study on the Data Distribution Gap in Music Emotion Recognition",
    "summary": "Music Emotion Recognition (MER) is a task deeply connected to human\nperception, relying heavily on subjective annotations collected from\ncontributors. Prior studies tend to focus on specific musical styles rather\nthan incorporating a diverse range of genres, such as rock and classical,\nwithin a single framework. In this paper, we address the task of recognizing\nemotion from audio content by investigating five datasets with dimensional\nemotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span\nvarious musical styles. We demonstrate the problem of out-of-distribution\ngeneralization in a systematic experiment. By closely looking at multiple data\nand feature sets, we provide insight into genre-emotion relationships in\nexisting data and examine potential genre dominance and dataset biases in\ncertain feature representations. Based on these experiments, we arrive at a\nsimple yet effective framework that combines embeddings extracted from the\nJukebox model with chroma features and demonstrate how, alongside a combination\nof several diverse training sets, this permits us to train models with\nsubstantially improved cross-dataset generalization capabilities.",
    "published": "2025-10-06T10:57:05Z",
    "updated": "2025-10-06T10:57:05Z",
    "link": "http://arxiv.org/pdf/2510.04688v1.pdf",
    "category": [
      "cs.SD",
      "cs.LG"
    ],
    "authors": [
      "Joann Ching",
      "Gerhard Widmer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04685v1",
    "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial\n  Model",
    "summary": "We develop the first parameter-free algorithms for the Stochastically\nExtended Adversarial (SEA) model, a framework that bridges adversarial and\nstochastic online convex optimization. Existing approaches for the SEA model\nrequire prior knowledge of problem-specific parameters, such as the diameter of\nthe domain $D$ and the Lipschitz constant of the loss functions $G$, which\nlimits their practical applicability. Addressing this, we develop\nparameter-free methods by leveraging the Optimistic Online Newton Step (OONS)\nalgorithm to eliminate the need for these parameters. We first establish a\ncomparator-adaptive algorithm for the scenario with unknown domain diameter but\nknown Lipschitz constant, achieving an expected regret bound of\n$\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} +\n\\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and\n$\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic\nvariance and cumulative adversarial variation, respectively. We then extend\nthis to the more general setting where both $D$ and $G$ are unknown, attaining\nthe comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound\nexhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$,\ndemonstrating the efficacy of our proposed methods even when both parameters\nare unknown in the SEA model.",
    "published": "2025-10-06T10:53:37Z",
    "updated": "2025-10-06T10:53:37Z",
    "link": "http://arxiv.org/pdf/2510.04685v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Shuche Wang",
      "Adarsh Barik",
      "Peng Zhao",
      "Vincent Y. F. Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04676v1",
    "title": "Counterfactual Credit Guided Bayesian Optimization",
    "summary": "Bayesian optimization has emerged as a prominent methodology for optimizing\nexpensive black-box functions by leveraging Gaussian process surrogates, which\nfocus on capturing the global characteristics of the objective function.\nHowever, in numerous practical scenarios, the primary objective is not to\nconstruct an exhaustive global surrogate, but rather to quickly pinpoint the\nglobal optimum. Due to the aleatoric nature of the sequential optimization\nproblem and its dependence on the quality of the surrogate model and the\ninitial design, it is restrictive to assume that all observed samples\ncontribute equally to the discovery of the optimum in this context. In this\npaper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),\na novel framework that explicitly quantifies the contribution of individual\nhistorical observations through counterfactual credit. By incorporating\ncounterfactual credit into the acquisition function, our approach can\nselectively allocate resources in areas where optimal solutions are most likely\nto occur. We prove that CCGBO retains sublinear regret. Empirical evaluations\non various synthetic and real-world benchmarks demonstrate that CCGBO\nconsistently reduces simple regret and accelerates convergence to the global\noptimum.",
    "published": "2025-10-06T10:34:50Z",
    "updated": "2025-10-06T10:34:50Z",
    "link": "http://arxiv.org/pdf/2510.04676v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Qiyu Wei",
      "Haowei Wang",
      "Richard Allmendinger",
      "Mauricio A. Álvarez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.06867v3",
    "title": "Optimal Bound for PCA with Outliers using Higher-Degree Voronoi Diagrams",
    "summary": "In this paper, we introduce new algorithms for Principal Component Analysis\n(PCA) with outliers. Utilizing techniques from computational geometry,\nspecifically higher-degree Voronoi diagrams, we navigate to the optimal\nsubspace for PCA even in the presence of outliers. This approach achieves an\noptimal solution with a time complexity of\n$n^{d+\\mathcal{O}(1)}\\text{poly}(n,d)$. Additionally, we present a randomized\nalgorithm with a complexity of $2^{\\mathcal{O}(r(d-r))} \\times \\text{poly}(n,\nd)$. This algorithm samples subspaces characterized in terms of a Grassmannian\nmanifold. By employing such sampling method, we ensure a high likelihood of\ncapturing the optimal subspace, with the success probability $(1 - \\delta)^T$.\nWhere $\\delta$ represents the probability that a sampled subspace does not\ncontain the optimal solution, and $T$ is the number of subspaces sampled,\nproportional to $2^{r(d-r)}$. Our use of higher-degree Voronoi diagrams and\nGrassmannian based sampling offers a clearer conceptual pathway and practical\nadvantages, particularly in handling large datasets or higher-dimensional\nsettings.",
    "published": "2024-08-13T13:05:36Z",
    "updated": "2025-10-06T10:23:52Z",
    "link": "http://arxiv.org/pdf/2408.06867v3.pdf",
    "category": [
      "cs.LG",
      "68W01"
    ],
    "authors": [
      "Sajjad Hashemian",
      "Mohammad Saeed Arvenaghi",
      "Ebrahim Ardeshir-Larijani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04660v1",
    "title": "IMLP: An Energy-Efficient Continual Learning Method for Tabular Data\n  Streams",
    "summary": "Tabular data streams are rapidly emerging as a dominant modality for\nreal-time decision-making in healthcare, finance, and the Internet of Things\n(IoT). These applications commonly run on edge and mobile devices, where energy\nbudgets, memory, and compute are strictly limited. Continual learning (CL)\naddresses such dynamics by training models sequentially on task streams while\npreserving prior knowledge and consolidating new knowledge. While recent CL\nwork has advanced in mitigating catastrophic forgetting and improving knowledge\ntransfer, the practical requirements of energy and memory efficiency for\ntabular data streams remain underexplored. In particular, existing CL solutions\nmostly depend on replay mechanisms whose buffers grow over time and exacerbate\nresource costs.\n  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a\ncompact continual learner for tabular data streams. IMLP incorporates a\nwindowed scaled dot-product attention over a sliding latent feature buffer,\nenabling constant-size memory and avoiding storing raw data. The attended\ncontext is concatenated with current features and processed by shared\nfeed-forward layers, yielding lightweight per-segment updates. To assess\npractical deployability, we introduce NetScore-T, a tunable metric coupling\nbalanced accuracy with energy for Pareto-aware comparison across models and\ndatasets. IMLP achieves up to $27.6\\times$ higher energy efficiency than TabNet\nand $85.5\\times$ higher than TabPFN, while maintaining competitive average\naccuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient\nalternative to full retraining for tabular data streams.",
    "published": "2025-10-06T10:05:44Z",
    "updated": "2025-10-06T10:05:44Z",
    "link": "http://arxiv.org/pdf/2510.04660v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuandou Wang",
      "Filip Gunnarsson",
      "Rihan Hai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19417v2",
    "title": "Analyzing Uncertainty Quantification in Statistical and Deep Learning\n  Models for Probabilistic Electricity Price Forecasting",
    "summary": "Precise probabilistic forecasts are fundamental for energy risk management,\nand there is a wide range of both statistical and machine learning models for\nthis purpose. Inherent to these probabilistic models is some form of\nuncertainty quantification. However, most models do not capture the full extent\nof uncertainty, which arises not only from the data itself but also from model\nand distributional choices. In this study, we examine uncertainty\nquantification in state-of-the-art statistical and deep learning probabilistic\nforecasting models for electricity price forecasting in the German market. In\nparticular, we consider deep distributional neural networks (DDNNs) and augment\nthem with an ensemble approach, Monte Carlo (MC) dropout, and conformal\nprediction to account for model uncertainty. Additionally, we consider the\nLASSO-estimated autoregressive (LEAR) approach combined with quantile\nregression averaging (QRA), generalized autoregressive conditional\nheteroskedasticity (GARCH), and conformal prediction. Across a range of\nperformance metrics, we find that the LEAR-based models perform well in terms\nof probabilistic forecasting, irrespective of the uncertainty quantification\nmethod. Furthermore, we find that DDNNs benefit from incorporating both data\nand model uncertainty, improving both point and probabilistic forecasting.\nUncertainty itself appears to be best captured by the models using conformal\nprediction. Overall, our extensive study shows that all models under\nconsideration perform competitively. However, their relative performance\ndepends on the choice of metrics for point and probabilistic forecasting.",
    "published": "2025-09-23T15:20:03Z",
    "updated": "2025-10-06T09:55:49Z",
    "link": "http://arxiv.org/pdf/2509.19417v2.pdf",
    "category": [
      "cs.LG",
      "math.ST",
      "stat.TH",
      "I.6.4; I.6.5; I.6.6"
    ],
    "authors": [
      "Andreas Lebedev",
      "Abhinav Das",
      "Sven Pappert",
      "Stephan Schlüter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04626v1",
    "title": "Compressed Concatenation of Small Embedding Models",
    "summary": "Embedding models are central to dense retrieval, semantic search, and\nrecommendation systems, but their size often makes them impractical to deploy\nin resource-constrained environments such as browsers or edge devices. While\nsmaller embedding models offer practical advantages, they typically\nunderperform compared to their larger counterparts. To bridge this gap, we\ndemonstrate that concatenating the raw embedding vectors of multiple small\nmodels can outperform a single larger baseline on standard retrieval\nbenchmarks. To overcome the resulting high dimensionality of naive\nconcatenation, we introduce a lightweight unified decoder trained with a\nMatryoshka Representation Learning (MRL) loss. This decoder maps the\nhigh-dimensional joint representation to a low-dimensional space, preserving\nmost of the original performance without fine-tuning the base models. We also\nshow that while concatenating more base models yields diminishing gains, the\nrobustness of the decoder's representation under compression and quantization\nimproves. Our experiments show that, on a subset of MTEB retrieval tasks, our\nconcat-encode-quantize pipeline recovers 89\\% of the original performance with\na 48x compression factor when the pipeline is applied to a concatenation of\nfour small embedding models.",
    "published": "2025-10-06T09:32:54Z",
    "updated": "2025-10-06T09:32:54Z",
    "link": "http://arxiv.org/pdf/2510.04626v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mohamed Ayoub Ben Ayad",
      "Michael Dinzinger",
      "Kanishka Ghosh Dastidar",
      "Jelena Mitrovic",
      "Michael Granitzer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04622v1",
    "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data\n  and Robust AI",
    "summary": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research.",
    "published": "2025-10-06T09:32:10Z",
    "updated": "2025-10-06T09:32:10Z",
    "link": "http://arxiv.org/pdf/2510.04622v1.pdf",
    "category": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Youngjoon Lee",
      "Seongmin Cho",
      "Yehhyun Jo",
      "Jinu Gong",
      "Hyunjoo Jenny Lee",
      "Joonhyuk Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.18925v4",
    "title": "Federated Continual Learning Goes Online: Uncertainty-Aware Memory\n  Management for Vision Tasks and Beyond",
    "summary": "Given the ability to model more realistic and dynamic problems, Federated\nContinual Learning (FCL) has been increasingly investigated recently. A\nwell-known problem encountered in this setting is the so-called catastrophic\nforgetting, for which the learning model is inclined to focus on more recent\ntasks while forgetting the previously learned knowledge. The majority of the\ncurrent approaches in FCL propose generative-based solutions to solve said\nproblem. However, this setting requires multiple training epochs over the data,\nimplying an offline setting where datasets are stored locally and remain\nunchanged over time. Furthermore, the proposed solutions are tailored for\nvision tasks solely. To overcome these limitations, we propose a new approach\nto deal with different modalities in the online scenario where new data arrive\nin streams of mini-batches that can only be processed once. To solve\ncatastrophic forgetting, we propose an uncertainty-aware memory-based approach.\nSpecifically, we suggest using an estimator based on the Bregman Information\n(BI) to compute the model's variance at the sample level. Through measures of\npredictive uncertainty, we retrieve samples with specific characteristics, and\n- by retraining the model on such samples - we demonstrate the potential of\nthis approach to reduce the forgetting effect in realistic settings while\nmaintaining data confidentiality and competitive communication efficiency\ncompared to state-of-the-art approaches.",
    "published": "2024-05-29T09:29:39Z",
    "updated": "2025-10-06T09:24:25Z",
    "link": "http://arxiv.org/pdf/2405.18925v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Giuseppe Serra",
      "Florian Buettner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20095v2",
    "title": "Spurious Privacy Leakage in Neural Networks",
    "summary": "Neural networks trained on real-world data often exhibit biases while\nsimultaneously being vulnerable to privacy attacks aimed at extracting\nsensitive information. Despite extensive research on each problem individually,\ntheir intersection remains poorly understood. In this work, we investigate the\nprivacy impact of spurious correlation bias. We introduce \\emph{spurious\nprivacy leakage}, a phenomenon in which spurious groups are significantly more\nvulnerable to privacy attacks than non-spurious groups. We observe that privacy\ndisparity between groups increases in tasks with simpler objectives (e.g. fewer\nclasses) due to spurious features. Counterintuitively, we demonstrate that\nspurious robust methods, designed to reduce spurious bias, fail to mitigate\nprivacy disparity. Our analysis reveals that this occurs because robust methods\ncan reduce reliance on spurious features for prediction, but do not prevent\ntheir memorization during training. Finally, we systematically compare the\nprivacy of different model architectures trained with spurious data,\ndemonstrating that, contrary to previous work, architectural choice can affect\nprivacy evaluation.",
    "published": "2025-05-26T15:04:39Z",
    "updated": "2025-10-06T09:21:16Z",
    "link": "http://arxiv.org/pdf/2505.20095v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chenxiang Zhang",
      "Jun Pang",
      "Sjouke Mauw"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04606v1",
    "title": "Closed-Form Last Layer Optimization",
    "summary": "Neural networks are typically optimized with variants of stochastic gradient\ndescent. Under a squared loss, however, the optimal solution to the linear last\nlayer weights is known in closed-form. We propose to leverage this during\noptimization, treating the last layer as a function of the backbone parameters,\nand optimizing solely for these parameters. We show this is equivalent to\nalternating between gradient descent steps on the backbone and closed-form\nupdates on the last layer. We adapt the method for the setting of stochastic\ngradient descent, by trading off the loss on the current batch against the\naccumulated information from previous batches. Further, we prove that, in the\nNeural Tangent Kernel regime, convergence of this method to an optimal solution\nis guaranteed. Finally, we demonstrate the effectiveness of our approach\ncompared with standard SGD on a squared loss in several supervised tasks --\nboth regression and classification -- including Fourier Neural Operators and\nInstrumental Variable Regression.",
    "published": "2025-10-06T09:14:39Z",
    "updated": "2025-10-06T09:14:39Z",
    "link": "http://arxiv.org/pdf/2510.04606v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Alexandre Galashov",
      "Nathaël Da Costa",
      "Liyuan Xu",
      "Philipp Hennig",
      "Arthur Gretton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05126v4",
    "title": "Taming OOD Actions for Offline Reinforcement Learning: An\n  Advantage-Based Approach",
    "summary": "Offline reinforcement learning (RL) learns policies from fixed datasets\nwithout online interactions, but suffers from distribution shift, causing\ninaccurate evaluation and overestimation of out-of-distribution (OOD) actions.\nExisting methods counter this by conservatively discouraging all OOD actions,\nwhich limits generalization. We propose Advantage-based Diffusion Actor-Critic\n(ADAC), which evaluates OOD actions via an advantage-like function and uses it\nto modulate the Q-function update discriminatively. Our key insight is that the\n(state) value function is generally learned more reliably than the action-value\nfunction; we thus use the next-state value to indirectly assess each action. We\ndevelop a PointMaze environment to clearly visualize that advantage modulation\neffectively selects superior OOD actions while discouraging inferior ones.\nMoreover, extensive experiments on the D4RL benchmark show that ADAC achieves\nstate-of-the-art performance, with especially strong gains on challenging\ntasks.",
    "published": "2025-05-08T10:57:28Z",
    "updated": "2025-10-06T09:11:08Z",
    "link": "http://arxiv.org/pdf/2505.05126v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xuyang Chen",
      "Keyu Yan",
      "Wenhan Cao",
      "Lin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.04446v3",
    "title": "Sampling-aware Adversarial Attacks Against Large Language Models",
    "summary": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in single-point\ngreedy generations, overlooking the inherently stochastic nature of LLMs and\noverestimating robustness. We show that for the goal of eliciting harmful\nresponses, repeated sampling of model outputs during the attack complements\nprompt optimization and serves as a strong and efficient attack vector. By\ncasting attacks as a resource allocation problem between optimization and\nsampling, we determine compute-optimal trade-offs and show that integrating\nsampling into existing attacks boosts success rates by up to 37\\% and improves\nefficiency by up to two orders of magnitude. We further analyze how\ndistributions of output harmfulness evolve during an adversarial attack,\ndiscovering that many common optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a label-free proof-of-concept\nobjective based on entropy maximization, demonstrating how our sampling-aware\nperspective enables new optimization targets. Overall, our findings establish\nthe importance of sampling in attacks to accurately assess and strengthen LLM\nsafety at scale.",
    "published": "2025-07-06T16:13:33Z",
    "updated": "2025-10-06T09:02:24Z",
    "link": "http://arxiv.org/pdf/2507.04446v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Tim Beyer",
      "Yan Scholten",
      "Leo Schwinn",
      "Stephan Günnemann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01727v2",
    "title": "Mamba base PKD for efficient knowledge compression",
    "summary": "Deep neural networks (DNNs) have remarkably succeeded in various image\nprocessing tasks. However, their large size and computational complexity\npresent significant challenges for deploying them in resource-constrained\nenvironments. This paper presents an innovative approach for integrating Mamba\nArchitecture within a Progressive Knowledge Distillation (PKD) process to\naddress the challenge of reducing model complexity while maintaining accuracy\nin image classification tasks. The proposed framework distills a large teacher\nmodel into progressively smaller student models, designed using Mamba blocks.\nEach student model is trained using Selective-State-Space Models (S-SSM) within\nthe Mamba blocks, focusing on important input aspects while reducing\ncomputational complexity. The work's preliminary experiments use MNIST and\nCIFAR-10 as datasets to demonstrate the effectiveness of this approach. For\nMNIST, the teacher model achieves 98% accuracy. A set of seven student models\nas a group retained 63% of the teacher's FLOPs, approximating the teacher's\nperformance with 98% accuracy. The weak student used only 1% of the teacher's\nFLOPs and maintained 72% accuracy. Similarly, for CIFAR-10, the students\nachieved 1% less accuracy compared to the teacher, with the small student\nretaining 5% of the teacher's FLOPs to achieve 50% accuracy. These results\nconfirm the flexibility and scalability of Mamba Architecture, which can be\nintegrated into PKD, succeeding in the process of finding students as weak\nlearners. The framework provides a solution for deploying complex neural\nnetworks in real-time applications with a reduction in computational cost.",
    "published": "2025-03-03T16:44:23Z",
    "updated": "2025-10-06T08:56:55Z",
    "link": "http://arxiv.org/pdf/2503.01727v2.pdf",
    "category": [
      "cs.LG",
      "I.2.6; I.5.1"
    ],
    "authors": [
      "José Medina",
      "Amnir Hadachi",
      "Paul Honeine",
      "Abdelaziz Bensrhair"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04591v1",
    "title": "Data-Driven Adaptive PID Control Based on Physics-Informed Neural\n  Networks",
    "summary": "This article proposes a data-driven PID controller design based on the\nprinciple of adaptive gain optimization, leveraging Physics-Informed Neural\nNetworks (PINNs) generated for predictive modeling purposes. The proposed\ncontrol design method utilizes gradients of the PID gain optimization, achieved\nthrough the automatic differentiation of PINNs, to apply model predictive\ncontrol using a cost function based on tracking error and control inputs. By\noptimizing PINNs-based PID gains, the method achieves adaptive gain tuning that\nensures stability while accounting for system nonlinearities. The proposed\nmethod features a systematic framework for integrating PINNs-based models of\ndynamical control systems into closed-loop control systems, enabling direct\napplication to PID control design. A series of numerical experiments is\nconducted to demonstrate the effectiveness of the proposed method from the\ncontrol perspectives based on both time and frequency domains.",
    "published": "2025-10-06T08:46:20Z",
    "updated": "2025-10-06T08:46:20Z",
    "link": "http://arxiv.org/pdf/2510.04591v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "authors": [
      "Junsei Ito",
      "Yasuaki Wasa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04583v1",
    "title": "Improved probabilistic regression using diffusion models",
    "summary": "Probabilistic regression models the entire predictive distribution of a\nresponse variable, offering richer insights than classical point estimates and\ndirectly allowing for uncertainty quantification. While diffusion-based\ngenerative models have shown remarkable success in generating complex,\nhigh-dimensional data, their usage in general regression tasks often lacks\nuncertainty-related evaluation and remains limited to domain-specific\napplications. We propose a novel diffusion-based framework for probabilistic\nregression that learns predictive distributions in a nonparametric way. More\nspecifically, we propose to model the full distribution of the diffusion noise,\nenabling adaptation to diverse tasks and enhanced uncertainty quantification.\nWe investigate different noise parameterizations, analyze their trade-offs, and\nevaluate our framework across a broad range of regression tasks, covering low-\nand high-dimensional settings. For several experiments, our approach shows\nsuperior performance against existing baselines, while delivering calibrated\nuncertainty estimates, demonstrating its versatility as a tool for\nprobabilistic prediction.",
    "published": "2025-10-06T08:36:05Z",
    "updated": "2025-10-06T08:36:05Z",
    "link": "http://arxiv.org/pdf/2510.04583v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Carlo Kneissl",
      "Christopher Bülte",
      "Philipp Scholl",
      "Gitta Kutyniok"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04579v1",
    "title": "Busemann Functions in the Wasserstein Space: Existence, Closed-Forms,\n  and Applications to Slicing",
    "summary": "The Busemann function has recently found much interest in a variety of\ngeometric machine learning problems, as it naturally defines projections onto\ngeodesic rays of Riemannian manifolds and generalizes the notion of\nhyperplanes. As several sources of data can be conveniently modeled as\nprobability distributions, it is natural to study this function in the\nWasserstein space, which carries a rich formal Riemannian structure induced by\nOptimal Transport metrics. In this work, we investigate the existence and\ncomputation of Busemann functions in Wasserstein space, which admits geodesic\nrays. We establish closed-form expressions in two important cases:\none-dimensional distributions and Gaussian measures. These results enable\nexplicit projection schemes for probability distributions on $\\mathbb{R}$,\nwhich in turn allow us to define novel Sliced-Wasserstein distances over\nGaussian mixtures and labeled datasets. We demonstrate the efficiency of those\noriginal schemes on synthetic datasets as well as transfer learning problems.",
    "published": "2025-10-06T08:31:14Z",
    "updated": "2025-10-06T08:31:14Z",
    "link": "http://arxiv.org/pdf/2510.04579v1.pdf",
    "category": [
      "cs.LG",
      "math.MG",
      "stat.ML"
    ],
    "authors": [
      "Clément Bonet",
      "Elsa Cazelles",
      "Lucas Drumetz",
      "Nicolas Courty"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04577v1",
    "title": "Language Model Based Text-to-Audio Generation: Anti-Causally Aligned\n  Collaborative Residual Transformers",
    "summary": "While language models (LMs) paired with residual vector quantization (RVQ)\ntokenizers have shown promise in text-to-audio (T2A) generation, they still lag\nbehind diffusion-based models by a non-trivial margin. We identify a critical\ndilemma underpinning this gap: incorporating more RVQ layers improves audio\nreconstruction fidelity but exceeds the generation capacity of conventional\nLMs. To address this, we first analyze RVQ dynamics and uncover two key\nlimitations: 1) orthogonality of features across RVQ layers hinders effective\nLMs training, and 2) descending semantic richness in tokens from deeper RVQ\nlayers exacerbates exposure bias during autoregressive decoding. Based on these\ninsights, we propose Siren, a novel LM-based framework that employs multiple\nisolated transformers with causal conditioning and anti-causal alignment via\nreinforcement learning. Extensive experiments demonstrate that Siren\noutperforms both existing LM-based and diffusion-based T2A systems, achieving\nstate-of-the-art results. By bridging the representational strengths of LMs\nwith the fidelity demands of audio synthesis, our approach repositions LMs as\ncompetitive contenders against diffusion models in T2A tasks. Moreover, by\naligning audio representations with linguistic structures, Siren facilitates a\npromising pathway toward unified multi-modal generation frameworks.",
    "published": "2025-10-06T08:26:55Z",
    "updated": "2025-10-06T08:26:55Z",
    "link": "http://arxiv.org/pdf/2510.04577v1.pdf",
    "category": [
      "cs.SD",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Juncheng Wang",
      "Chao Xu",
      "Cheng Yu",
      "Zhe Hu",
      "Haoyu Xie",
      "Guoqi Yu",
      "Lei Shang",
      "Shujun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02578v2",
    "title": "FLOWR.root: A flow matching based foundation model for joint\n  multi-purpose structure-aware 3D ligand generation and affinity prediction",
    "summary": "We present FLOWR:root, an equivariant flow-matching model for pocket-aware 3D\nligand generation with joint binding affinity prediction and confidence\nestimation. The model supports de novo generation, pharmacophore-conditional\nsampling, fragment elaboration, and multi-endpoint affinity prediction (pIC50,\npKi, pKd, pEC50). Training combines large-scale ligand libraries with\nmixed-fidelity protein-ligand complexes, followed by refinement on curated\nco-crystal datasets and parameter-efficient finetuning for project-specific\nadaptation. FLOWR:root achieves state-of-the-art performance in unconditional\n3D molecule generation and pocket-conditional ligand design, producing\ngeometrically realistic, low-strain structures. The integrated affinity\nprediction module demonstrates superior accuracy on the SPINDR test set and\noutperforms recent models on the Schrodinger FEP+/OpenFE benchmark with\nsubstantial speed advantages. As a foundation model, FLOWR:root requires\nfinetuning on project-specific datasets to account for unseen\nstructure-activity landscapes, yielding strong correlation with experimental\ndata. Joint generation and affinity prediction enable inference-time scaling\nthrough importance sampling, steering molecular design toward higher-affinity\ncompounds. Case studies validate this: selective CK2$\\alpha$ ligand generation\nagainst CLK3 shows significant correlation between predicted and\nquantum-mechanical binding energies, while ER$\\alpha$ and TYK2 scaffold\nelaboration demonstrates strong agreement with QM calculations. By integrating\nstructure-aware generation, affinity estimation, and property-guided sampling,\nFLOWR:root provides a comprehensive foundation for structure-based drug design\nspanning hit identification through lead optimization.",
    "published": "2025-10-02T21:38:26Z",
    "updated": "2025-10-06T08:20:22Z",
    "link": "http://arxiv.org/pdf/2510.02578v2.pdf",
    "category": [
      "q-bio.BM",
      "cs.LG"
    ],
    "authors": [
      "Julian Cremer",
      "Tuan Le",
      "Mohammad M. Ghahremanpour",
      "Emilia Sługocka",
      "Filipe Menezes",
      "Djork-Arné Clevert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04563v1",
    "title": "Stochastic Approximation Methods for Distortion Risk Measure\n  Optimization",
    "summary": "Distortion Risk Measures (DRMs) capture risk preferences in decision-making\nand serve as general criteria for managing uncertainty. This paper proposes\ngradient descent algorithms for DRM optimization based on two dual\nrepresentations: the Distortion-Measure (DM) form and Quantile-Function (QF)\nform. The DM-form employs a three-timescale algorithm to track quantiles,\ncompute their gradients, and update decision variables, utilizing the\nGeneralized Likelihood Ratio and kernel-based density estimation. The QF-form\nprovides a simpler two-timescale approach that avoids the need for complex\nquantile gradient estimation. A hybrid form integrates both approaches,\napplying the DM-form for robust performance around distortion function jumps\nand the QF-form for efficiency in smooth regions. Proofs of strong convergence\nand convergence rates for the proposed algorithms are provided. In particular,\nthe DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form\nattains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their\neffectiveness and demonstrate substantial improvements over baselines in robust\nportfolio selection tasks. The method's scalability is further illustrated\nthrough integration into deep reinforcement learning. Specifically, a DRM-based\nProximal Policy Optimization algorithm is developed and applied to\nmulti-echelon dynamic inventory management, showcasing its practical\napplicability.",
    "published": "2025-10-06T07:59:09Z",
    "updated": "2025-10-06T07:59:09Z",
    "link": "http://arxiv.org/pdf/2510.04563v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Jinyang Jiang",
      "Bernd Heidergott",
      "Jiaqiao Hu",
      "Yijie Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04559v1",
    "title": "Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM\n  Systems",
    "summary": "This paper investigates the identification of the top-m user-scheduling sets\nin multi-user MIMO downlink, which is cast as a combinatorial pure-exploration\nproblem in stochastic linear bandits. Because the action space grows\nexponentially, exhaustive search is infeasible. We therefore adopt a linear\nutility model to enable efficient exploration and reliable selection of\npromising user subsets. We introduce a gap-index framework that maintains a\nshortlist of current estimates of champion arms (top-m sets) and a rotating\nshortlist of challenger arms that pose the greatest threat to the champions.\nThis design focuses on measurements that yield the most informative\ngap-index-based comparisons, resulting in significant reductions in runtime and\ncomputation compared to state-of-the-art linear bandit methods, with high\nidentification accuracy. The method also exposes a tunable trade-off between\nspeed and accuracy. Simulations on a realistic OFDM downlink show that\nshortlist-driven pure exploration makes online, measurement-efficient\nsubcarrier selection practical for AI-enabled communication systems.",
    "published": "2025-10-06T07:48:44Z",
    "updated": "2025-10-06T07:48:44Z",
    "link": "http://arxiv.org/pdf/2510.04559v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mohsen Amiri",
      "V Venktesh",
      "Sindri Magnússon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04556v1",
    "title": "Gini-based Model Monitoring: A General Framework with an Application to\n  Non-life Insurance Pricing",
    "summary": "In a dynamic landscape where portfolios and environments evolve, maintaining\nthe accuracy of pricing models is critical. To the best of our knowledge, this\nis the first study to systematically examine concept drift in non-life\ninsurance pricing. We (i) provide an overview of the relevant literature and\ncommonly used methodologies, clarify the distinction between virtual drift and\nconcept drift, and explain their implications for long-run model performance;\n(ii) review and formalize common performance measures, including the Gini index\nand deviance loss, and articulate their interpretation; (iii) derive the\nasymptotic distribution of the Gini index, enabling valid inference and\nhypothesis testing; and (iv) present a standardized monitoring procedure that\nindicates when refitting is warranted. We illustrate the framework using a\nmodified real-world portfolio with induced concept drift and discuss practical\nconsiderations and pitfalls.",
    "published": "2025-10-06T07:41:09Z",
    "updated": "2025-10-06T07:41:09Z",
    "link": "http://arxiv.org/pdf/2510.04556v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "q-fin.ST",
      "stat.AP",
      "stat.TH",
      "62, 68",
      "G.3"
    ],
    "authors": [
      "Alexej Brauer",
      "Paul Menzel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04555v1",
    "title": "Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning\n  with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets",
    "summary": "We introduce Tail-Safe, a deployability-oriented framework for derivatives\nhedging that unifies distributional, risk-sensitive reinforcement learning with\na white-box control-barrier-function (CBF) quadratic-program (QP) safety layer\ntailored to financial constraints. The learning component combines an IQN-based\ndistributional critic with a CVaR objective (IQN--CVaR--PPO) and a\nTail-Coverage Controller that regulates quantile sampling through temperature\ntilting and tail boosting to stabilize small-$\\alpha$ estimation. The safety\ncomponent enforces discrete-time CBF inequalities together with domain-specific\nconstraints -- ellipsoidal no-trade bands, box and rate limits, and a\nsign-consistency gate -- solved as a convex QP whose telemetry (active sets,\ntightness, rate utilization, gate scores, slack, and solver status) forms an\nauditable trail for governance. We provide guarantees of robust forward\ninvariance of the safe set under bounded model mismatch, a minimal-deviation\nprojection interpretation of the QP, a KL-to-DRO upper bound linking per-state\nKL regularization to worst-case CVaR, concentration and sample-complexity\nresults for the temperature-tilted CVaR estimator, and a CVaR trust-region\nimprovement inequality under KL limits, together with feasibility persistence\nunder expiry-aware tightening. Empirically, in arbitrage-free,\nmicrostructure-aware synthetic markets (SSVI $\\to$ Dupire $\\to$ VIX with\nABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading\ncentral performance and yields zero hard-constraint violations whenever the QP\nis feasible with zero slack. Telemetry is mapped to governance dashboards and\nincident workflows to support explainability and auditability. Limitations\ninclude reliance on synthetic data and simplified execution to isolate\nmethodological contributions.",
    "published": "2025-10-06T07:39:45Z",
    "updated": "2025-10-06T07:39:45Z",
    "link": "http://arxiv.org/pdf/2510.04555v1.pdf",
    "category": [
      "cs.LG",
      "q-fin.TR",
      "68T05, 90C20, 91G60",
      "I.2.6; I.2.8; G.3"
    ],
    "authors": [
      "Jian'an Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13881v4",
    "title": "TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias\n  Intrinsically from Regression Models in Recommender Systems",
    "summary": "Regression models are crucial in recommender systems. However,\nretransformation bias problem has been conspicuously neglected within the\ncommunity. While many works in other fields have devised effective bias\ncorrection methods, all of them are post-hoc cures externally to the model,\nfacing practical challenges when applied to real-world recommender systems.\nHence, we propose a preemptive paradigm to eradicate the bias intrinsically\nfrom the models via minor model refinement. Specifically, a novel TranSUN\nmethod is proposed with a joint bias learning manner to offer theoretically\nguaranteed unbiasedness under empirical superior convergence. It is further\ngeneralized into a novel generic regression model family, termed Generalized\nTranSUN (GTS), which not only offers more theoretical insights but also serves\nas a generic framework for flexibly developing various bias-free models.\nComprehensive experimental results demonstrate the superiority of our methods\nacross data from various domains, which have been successfully deployed in two\nreal-world industrial recommendation scenarios, i.e. product and short video\nrecommendation scenarios in Guess What You Like business domain in the homepage\nof Taobao App (a leading e-commerce platform with DAU > 300M), to serve the\nmajor online traffic.",
    "published": "2025-05-20T03:36:54Z",
    "updated": "2025-10-06T07:30:32Z",
    "link": "http://arxiv.org/pdf/2505.13881v4.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Jiahao Yu",
      "Haozhuang Liu",
      "Yeqiu Yang",
      "Lu Chen",
      "Jian Wu",
      "Yuning Jiang",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04548v1",
    "title": "Learning Linear Regression with Low-Rank Tasks in-Context",
    "summary": "In-context learning (ICL) is a key building block of modern large language\nmodels, yet its theoretical mechanisms remain poorly understood. It is\nparticularly mysterious how ICL operates in real-world applications where tasks\nhave a common structure. In this work, we address this problem by analyzing a\nlinear attention model trained on low-rank regression tasks. Within this\nsetting, we precisely characterize the distribution of predictions and the\ngeneralization error in the high-dimensional limit. Moreover, we find that\nstatistical fluctuations in finite pre-training data induce an implicit\nregularization. Finally, we identify a sharp phase transition of the\ngeneralization error governed by task structure. These results provide a\nframework for understanding how transformers learn to learn the task structure.",
    "published": "2025-10-06T07:27:49Z",
    "updated": "2025-10-06T07:27:49Z",
    "link": "http://arxiv.org/pdf/2510.04548v1.pdf",
    "category": [
      "cond-mat.dis-nn",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Kaito Takanami",
      "Takashi Takahashi",
      "Yoshiyuki Kabashima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04543v1",
    "title": "Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not\n  Just Make Predictions",
    "summary": "Despite recent progress, deep learning methods for tabular data still\nstruggle to compete with traditional tree-based models. A key challenge lies in\nmodeling complex, dataset-specific feature interactions that are central to\ntabular data. Graph-based tabular deep learning (GTDL) methods aim to address\nthis by representing features and their interactions as graphs. However,\nexisting methods predominantly optimize predictive accuracy, neglecting\naccurate modeling of the graph structure. This position paper argues that GTDL\nshould move beyond prediction-centric objectives and prioritize the explicit\nlearning and evaluation of feature interactions. Using synthetic datasets with\nknown ground-truth graph structures, we show that existing GTDL methods fail to\nrecover meaningful feature interactions. Moreover, enforcing the true\ninteraction structure improves predictive performance. This highlights the need\nfor GTDL methods to prioritize quantitative evaluation and accurate structural\nlearning. We call for a shift toward structure-aware modeling as a foundation\nfor building GTDL systems that are not only accurate but also interpretable,\ntrustworthy, and grounded in domain understanding.",
    "published": "2025-10-06T07:16:42Z",
    "updated": "2025-10-06T07:16:42Z",
    "link": "http://arxiv.org/pdf/2510.04543v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Elias Dubbeldam",
      "Reza Mohammadi",
      "Marit Schoonhoven",
      "S. Ilker Birbil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04525v1",
    "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
    "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
    "published": "2025-10-06T06:30:22Z",
    "updated": "2025-10-06T06:30:22Z",
    "link": "http://arxiv.org/pdf/2510.04525v1.pdf",
    "category": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "authors": [
      "Satoshi Hayakawa",
      "Yuhta Takida",
      "Masaaki Imaizumi",
      "Hiromi Wakaki",
      "Yuki Mitsufuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04512v1",
    "title": "Quantum generative model on bicycle-sharing system and an application",
    "summary": "Recently, bicycle-sharing systems have been implemented in numerous cities,\nbecoming integral to daily life. However, a prevalent issue arises when\nintensive commuting demand leads to bicycle shortages in specific areas and at\nparticular times. To address this challenge, we employ a novel quantum machine\nlearning model that analyzes time series data by fitting quantum time evolution\nto observed sequences. This model enables us to capture actual trends in\nbicycle counts at individual ports and identify correlations between different\nports. Utilizing the trained model, we simulate the impact of proactively\nadding bicycles to high-demand ports on the overall rental number across the\nsystem. Given that the core of this method lies in a Monte Carlo simulation, it\nis anticipated to have a wide range of industrial applications.",
    "published": "2025-10-06T06:02:13Z",
    "updated": "2025-10-06T06:02:13Z",
    "link": "http://arxiv.org/pdf/2510.04512v1.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Fumio Nemoto",
      "Nobuyuki Koike",
      "Daichi Sato",
      "Yuuta Kawaai",
      "Masayuki Ohzeki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04507v1",
    "title": "Wavelet Predictive Representations for Non-Stationary Reinforcement\n  Learning",
    "summary": "The real world is inherently non-stationary, with ever-changing factors, such\nas weather conditions and traffic flows, making it challenging for agents to\nadapt to varying environmental dynamics. Non-Stationary Reinforcement Learning\n(NSRL) addresses this challenge by training agents to adapt rapidly to\nsequences of distinct Markov Decision Processes (MDPs). However, existing NSRL\napproaches often focus on tasks with regularly evolving patterns, leading to\nlimited adaptability in highly dynamic settings. Inspired by the success of\nWavelet analysis in time series modeling, specifically its ability to capture\nsignal trends at multiple scales, we propose WISDOM to leverage wavelet-domain\npredictive task representations to enhance NSRL. WISDOM captures these\nmulti-scale features in evolving MDP sequences by transforming task\nrepresentation sequences into the wavelet domain, where wavelet coefficients\nrepresent both global trends and fine-grained variations of non-stationary\nchanges. In addition to the auto-regressive modeling commonly employed in time\nseries forecasting, we devise a wavelet temporal difference (TD) update\noperator to enhance tracking and prediction of MDP evolution. We theoretically\nprove the convergence of this operator and demonstrate policy improvement with\nwavelet task representations. Experiments on diverse benchmarks show that\nWISDOM significantly outperforms existing baselines in both sample efficiency\nand asymptotic performance, demonstrating its remarkable adaptability in\ncomplex environments characterized by non-stationary and stochastically\nevolving tasks.",
    "published": "2025-10-06T05:49:18Z",
    "updated": "2025-10-06T05:49:18Z",
    "link": "http://arxiv.org/pdf/2510.04507v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Min Wang",
      "Xin Li",
      "Ye He",
      "Yao-Hui Li",
      "Hasnaa Bennis",
      "Riashat Islam",
      "Mingzhong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04502v1",
    "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity\n  Debiasing in Top-K Recommendation",
    "summary": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED.",
    "published": "2025-10-06T05:33:37Z",
    "updated": "2025-10-06T05:33:37Z",
    "link": "http://arxiv.org/pdf/2510.04502v1.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Yue Que",
      "Yingyi Zhang",
      "Xiangyu Zhao",
      "Chen Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03589v2",
    "title": "VITA: Variational Pretraining of Transformers for Climate-Robust Crop\n  Yield Forecasting",
    "summary": "Accurate crop yield forecasting is essential for global food security.\nHowever, current AI models systematically underperform when yields deviate from\nhistorical trends. We attribute this to the lack of rich, physically grounded\ndatasets directly linking atmospheric states to yields. To address this, we\nintroduce VITA (Variational Inference Transformer for Asymmetric data), a\nvariational pretraining framework that learns representations from large\nsatellite-based weather datasets and transfers to the ground-based limited\nmeasurements available for yield prediction. VITA is trained using detailed\nmeteorological variables as proxy targets during pretraining and learns to\npredict latent atmospheric states under a seasonality-aware sinusoidal prior.\nThis allows the model to be fine-tuned using limited weather statistics during\ndeployment. Applied to 763 counties in the U.S. Corn Belt, VITA achieves\nstate-of-the-art performance in predicting corn and soybean yields across all\nevaluation scenarios, particularly during extreme years, with statistically\nsignificant improvements (paired t-test, $p < 0.0001$). Importantly, VITA\noutperforms prior frameworks like GNN-RNN without soil data, and bigger\nfoundational models (e.g., Chronos-Bolt) with less compute, making it practical\nfor real-world use--especially in data-scarce regions. This work highlights how\ndomain-aware AI design can overcome data limitations and support resilient\nagricultural forecasting in a changing climate.",
    "published": "2025-08-05T15:56:36Z",
    "updated": "2025-10-06T05:27:56Z",
    "link": "http://arxiv.org/pdf/2508.03589v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Adib Hasan",
      "Mardavij Roozbehani",
      "Munther Dahleh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04500v1",
    "title": "Expand Neurons, Not Parameters",
    "summary": "This work demonstrates how increasing the number of neurons in a network\nwithout increasing its number of non-zero parameters improves performance. We\nshow that this gain corresponds with a decrease in interference between\nmultiple features that would otherwise share the same neurons. To reduce such\nentanglement at a fixed non-zero parameter count, we introduce Fixed Parameter\nExpansion (FPE): replace a neuron with multiple children and partition the\nparent's weights disjointly across them, so that each child inherits a\nnon-overlapping subset of connections. On symbolic tasks, specifically Boolean\ncode problems, clause-aligned FPE systematically reduces polysemanticity\nmetrics and yields higher task accuracy. Notably, random splits of neuron\nweights approximate these gains, indicating that reduced collisions, not\nprecise assignment, are a primary driver. Consistent with the superposition\nhypothesis, the benefits of FPE grow with increasing interference: when\npolysemantic load is high, accuracy improvements are the largest. Transferring\nthese insights to real models (classifiers over CLIP embeddings and deeper\nmultilayer networks) we find that widening networks while maintaining a\nconstant non-zero parameter count consistently increases accuracy. These\nresults identify an interpretability-grounded mechanism to leverage width\nagainst superposition, improving performance without increasing the number of\nnon-zero parameters. Such a direction is well matched to modern accelerators,\nwhere memory movement of non-zero parameters, rather than raw compute, is the\ndominant bottleneck.",
    "published": "2025-10-06T05:26:52Z",
    "updated": "2025-10-06T05:26:52Z",
    "link": "http://arxiv.org/pdf/2510.04500v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Linghao Kong",
      "Inimai Subramanian",
      "Yonadav Shavit",
      "Micah Adler",
      "Dan Alistarh",
      "Nir Shavit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.08390v2",
    "title": "Inference-Time Scaling of Diffusion Language Models with Particle Gibbs\n  Sampling",
    "summary": "Discrete diffusion models have recently emerged as strong alternatives to\nautoregressive language models, matching their performance through large-scale\ntraining. However, inference-time control remains relatively underexplored. In\nthis work, we study how to steer generation toward desired rewards without\nretraining the models. Prior methods typically resample or filter within a\nsingle denoising trajectory, optimizing rewards step-by-step without\ntrajectory-level refinement. We introduce particle Gibbs sampling for diffusion\nlanguage models (PG-DLM), a novel inference-time algorithm enabling\ntrajectory-level refinement while preserving generation perplexity under reward\noptimization. PG-DLM constructs a Markov chain over full denoising trajectories\nand applies a conditional sequential Monte Carlo kernel to resample them. We\nderive theoretical guarantees for convergence, including asymptotic consistency\nand variance bounds. Within this framework, we further analyze trade-offs\nacross four key axes for inference-time scaling under fixed budgets:\niterations, samples, denoising steps, and reward estimation. Our analysis shows\nscaling iterations achieves the best reward-perplexity trade-off. Empirically,\nPG-DLM consistently outperforms prior methods using MDLM and LLaDA-8B as base\nmodels across a wide range of compute budgets for reward-guided generation\ntasks including toxicity and sentiment control as well as linguistic\nacceptability.",
    "published": "2025-07-11T08:00:47Z",
    "updated": "2025-10-06T05:26:50Z",
    "link": "http://arxiv.org/pdf/2507.08390v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Meihua Dang",
      "Jiaqi Han",
      "Minkai Xu",
      "Kai Xu",
      "Akash Srivastava",
      "Stefano Ermon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04490v1",
    "title": "Deep vs. Shallow: Benchmarking Physics-Informed Neural Architectures on\n  the Biharmonic Equation",
    "summary": "Partial differential equation (PDE) solvers are fundamental to engineering\nsimulation. Classical mesh-based approaches (finite difference/volume/element)\nare fast and accurate on high-quality meshes but struggle with higher-order\noperators and complex, hard-to-mesh geometries. Recently developed\nphysics-informed neural networks (PINNs) and their variants are mesh-free and\nflexible, yet compute-intensive and often less accurate. This paper\nsystematically benchmarks RBF-PIELM, a rapid PINN variant-an extreme learning\nmachine with radial-basis activations-for higher-order PDEs. RBF-PIELM replaces\nPINNs' time-consuming gradient descent with a single-shot least-squares solve.\nWe test RBF-PIELM on the fourth-order biharmonic equation using two benchmarks:\nlid-driven cavity flow (streamfunction formulation) and a manufactured\noscillatory solution. Our results show up to $(350\\times)$ faster training than\nPINNs and over $(10\\times)$ fewer parameters for comparable solution accuracy.\nDespite surpassing PINNs, RBF-PIELM still lags mature mesh-based solvers and\nits accuracy degrades on highly oscillatory solutions, highlighting remaining\nchallenges for practical deployment.",
    "published": "2025-10-06T04:54:04Z",
    "updated": "2025-10-06T04:54:04Z",
    "link": "http://arxiv.org/pdf/2510.04490v1.pdf",
    "category": [
      "cs.CE",
      "cs.ET",
      "cs.LG",
      "G.1.7; G.1.8; G.1.10; J.2"
    ],
    "authors": [
      "Akshay Govind Srinivasan",
      "Vikas Dwivedi",
      "Balaji Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04487v1",
    "title": "Forking-Sequences",
    "summary": "While accuracy is a critical requirement for time series forecasting models,\nan equally important (yet often overlooked) desideratum is forecast stability\nacross forecast creation dates (FCDs). Even highly accurate models can produce\nerratic revisions between FCDs, undermining stakeholder trust and disrupting\ndownstream decision-making. To improve forecast stability, models like MQCNN,\nMQT, and SPADE employ a little-known but highly effective technique:\nforking-sequences. Unlike standard statistical and neural forecasting methods\nthat treat each FCD independently, the forking-sequences method jointly encodes\nand decodes the entire time series across all FCDs, in a way mirroring time\nseries cross-validation. Since forking sequences remains largely unknown in the\nbroader neural forecasting community, in this work, we formalize the\nforking-sequences approach, and we make a case for its broader adoption. We\ndemonstrate three key benefits of forking-sequences: (i) more stable and\nconsistent gradient updates during training; (ii) reduced forecast variance\nthrough ensembling; and (iii) improved inference computational efficiency. We\nvalidate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and\nTourism competitions, showing improvements in forecast percentage change\nstability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,\nRNN, LSTM, CNN, and Transformer-based architectures, respectively.",
    "published": "2025-10-06T04:51:06Z",
    "updated": "2025-10-06T04:51:06Z",
    "link": "http://arxiv.org/pdf/2510.04487v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Willa Potosnak",
      "Malcolm Wolff",
      "Boris Oreshkin",
      "Mengfei Cao",
      "Michael W. Mahoney",
      "Dmitry Efimov",
      "Kin G. Olivares"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03167v2",
    "title": "Improving Online-to-Nonconvex Conversion for Smooth Optimization via\n  Double Optimism",
    "summary": "A recent breakthrough in nonconvex optimization is the online-to-nonconvex\nconversion framework of [Cutkosky et al., 2023], which reformulates the task of\nfinding an $\\varepsilon$-first-order stationary point as an online learning\nproblem. When both the gradient and the Hessian are Lipschitz continuous,\ninstantiating this framework with two different online learners achieves a\ncomplexity of $O(\\varepsilon^{-1.75}\\log(1/\\varepsilon))$ in the deterministic\ncase and a complexity of $O(\\varepsilon^{-3.5})$ in the stochastic case.\nHowever, this approach suffers from several limitations: (i) the deterministic\nmethod relies on a complex double-loop scheme that solves a fixed-point\nequation to construct hint vectors for an optimistic online learner,\nintroducing an extra logarithmic factor; (ii) the stochastic method assumes a\nbounded second-order moment of the stochastic gradient, which is stronger than\nstandard variance bounds; and (iii) different online learning algorithms are\nused in the two settings. In this paper, we address these issues by introducing\nan online optimistic gradient method based on a novel doubly optimistic hint\nfunction. Specifically, we use the gradient at an extrapolated point as the\nhint, motivated by two optimistic assumptions: that the difference between the\nhint and the target gradient remains near constant, and that consecutive update\ndirections change slowly due to smoothness. Our method eliminates the need for\na double loop and removes the logarithmic factor. Furthermore, by simply\nreplacing full gradients with stochastic gradients and under the standard\nassumption that their variance is bounded by $\\sigma^2$, we obtain a unified\nalgorithm with complexity $O(\\varepsilon^{-1.75} + \\sigma^2\n\\varepsilon^{-3.5})$, smoothly interpolating between the best-known\ndeterministic rate and the optimal stochastic rate.",
    "published": "2025-10-03T16:41:24Z",
    "updated": "2025-10-06T03:45:44Z",
    "link": "http://arxiv.org/pdf/2510.03167v2.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Francisco Patitucci",
      "Ruichen Jiang",
      "Aryan Mokhtari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04466v1",
    "title": "Benchmarking atmospheric circulation variability in an AI emulator,\n  ACE2, and a hybrid model, NeuralGCM",
    "summary": "Physics-based atmosphere-land models with prescribed sea surface temperature\nhave notable successes but also biases in their ability to represent\natmospheric variability compared to observations. Recently, AI emulators and\nhybrid models have emerged with the potential to overcome these biases, but\nstill require systematic evaluation against metrics grounded in fundamental\natmospheric dynamics. Here, we evaluate the representation of four atmospheric\nvariability benchmarking metrics in a fully data-driven AI emulator (ACE2-ERA5)\nand hybrid model (NeuralGCM). The hybrid model and emulator can capture the\nspectra of large-scale tropical waves and extratropical eddy-mean flow\ninteractions, including critical levels. However, both struggle to capture the\ntimescales associated with quasi-biennial oscillation (QBO, $\\sim 28$ months)\nand Southern annular mode propagation ($\\sim 150$ days). These dynamical\nmetrics serve as an initial benchmarking tool to inform AI model development\nand understand their limitations, which may be essential for\nout-of-distribution applications (e.g., extrapolating to unseen climates).",
    "published": "2025-10-06T03:42:18Z",
    "updated": "2025-10-06T03:42:18Z",
    "link": "http://arxiv.org/pdf/2510.04466v1.pdf",
    "category": [
      "physics.ao-ph",
      "cs.LG"
    ],
    "authors": [
      "Ian Baxter",
      "Hamid Pahlavan",
      "Pedram Hassanzadeh",
      "Katharine Rucker",
      "Tiffany Shaw"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2306.12658v3",
    "title": "Fitted value iteration methods for bicausal optimal transport",
    "summary": "We develop a fitted value iteration (FVI) method to compute bicausal optimal\ntransport (OT) where couplings have an adapted structure. Based on the dynamic\nprogramming formulation, FVI adopts a function class to approximate the value\nfunctions in bicausal OT. Under the concentrability condition and approximate\ncompleteness assumption, we prove the sample complexity using (local)\nRademacher complexity. Furthermore, we demonstrate that multilayer neural\nnetworks with appropriate structures satisfy the crucial assumptions required\nin sample complexity proofs. Numerical experiments reveal that FVI outperforms\nlinear programming and adapted Sinkhorn methods in scalability as the time\nhorizon increases, while still maintaining acceptable accuracy.",
    "published": "2023-06-22T03:55:36Z",
    "updated": "2025-10-06T03:35:15Z",
    "link": "http://arxiv.org/pdf/2306.12658v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "q-fin.MF",
      "49Q99, 90C39, 68T07, 90C59"
    ],
    "authors": [
      "Erhan Bayraktar",
      "Bingyan Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04460v1",
    "title": "Perspectives on Stochastic Localization",
    "summary": "We survey different perspectives on the stochastic localization process of\n[Eld13], a powerful construction that has had many exciting recent applications\nin high-dimensional probability and algorithm design. Unlike prior surveys on\nthis topic, our focus is on giving a self-contained presentation of all known\nalternative constructions of Eldan's stochastic localization, with an emphasis\non connections between different constructions. Our hope is that by collecting\nthese perspectives, some of which had primarily arisen within a particular\ncommunity (e.g., probability theory, theoretical computer science, information\ntheory, or machine learning), we can broaden the accessibility of stochastic\nlocalization, and ease its future use.",
    "published": "2025-10-06T03:18:41Z",
    "updated": "2025-10-06T03:18:41Z",
    "link": "http://arxiv.org/pdf/2510.04460v1.pdf",
    "category": [
      "math.PR",
      "cs.DS",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Bobby Shi",
      "Kevin Tian",
      "Matthew S. Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21261v2",
    "title": "Owen Sampling Accelerates Contribution Estimation in Federated Learning",
    "summary": "Federated Learning (FL) aggregates information from multiple clients to train\na shared global model without exposing raw data. Accurately estimating each\nclient's contribution is essential not just for fair rewards, but for selecting\nthe most useful clients so the global model converges faster. The Shapley value\nis a principled choice, yet exact computation scales exponentially with the\nnumber of clients, making it infeasible for large federations. We propose\nFedOwen, an efficient framework that uses Owen sampling to approximate Shapley\nvalues under the same total evaluation budget as existing methods while keeping\nthe approximation error small. In addition, FedOwen uses an adaptive client\nselection strategy that balances exploiting high-value clients with exploring\nunder-sampled ones, reducing bias and uncovering rare but informative data.\nUnder a fixed valuation cost, FedOwen achieves up to 23 percent higher final\naccuracy within the same number of communication rounds compared to\nstate-of-the-art baselines on non-IID benchmarks.",
    "published": "2025-08-28T23:22:37Z",
    "updated": "2025-10-06T02:49:38Z",
    "link": "http://arxiv.org/pdf/2508.21261v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hossein KhademSohi",
      "Hadi Hemmati",
      "Jiayu Zhou",
      "Steve Drew"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07696v2",
    "title": "Conformalized Generative Bayesian Imaging: An Uncertainty Quantification\n  Framework for Computational Imaging",
    "summary": "Uncertainty quantification plays an important role in achieving trustworthy\nand reliable learning-based computational imaging. Recent advances in\ngenerative modeling and Bayesian neural networks have enabled the development\nof uncertainty-aware image reconstruction methods. Current generative\nmodel-based methods seek to quantify the inherent (aleatoric) uncertainty on\nthe underlying image for given measurements by learning to sample from the\nposterior distribution of the underlying image. On the other hand, Bayesian\nneural network-based approaches aim to quantify the model (epistemic)\nuncertainty on the parameters of a deep neural network-based reconstruction\nmethod by approximating the posterior distribution of those parameters.\nUnfortunately, an ongoing need for an inversion method that can jointly\nquantify complex aleatoric uncertainty and epistemic uncertainty patterns still\npersists. In this paper, we present a scalable framework that can quantify both\naleatoric and epistemic uncertainties. The proposed framework accepts an\nexisting generative model-based posterior sampling method as an input and\nintroduces an epistemic uncertainty quantification capability through Bayesian\nneural networks with latent variables and deep ensembling. Furthermore, by\nleveraging the conformal prediction methodology, the proposed framework can be\neasily calibrated to ensure rigorous uncertainty quantification. We evaluated\nthe proposed framework on magnetic resonance imaging, computed tomography, and\nimage inpainting problems and showed that the epistemic and aleatoric\nuncertainty estimates produced by the proposed framework display the\ncharacteristic features of true epistemic and aleatoric uncertainties.\nFurthermore, our results demonstrated that the use of conformal prediction on\ntop of the proposed framework enables marginal coverage guarantees consistent\nwith frequentist principles.",
    "published": "2025-04-10T12:30:46Z",
    "updated": "2025-10-06T02:42:53Z",
    "link": "http://arxiv.org/pdf/2504.07696v2.pdf",
    "category": [
      "eess.IV",
      "cs.LG"
    ],
    "authors": [
      "Canberk Ekmekci",
      "Mujdat Cetin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04446v1",
    "title": "Zeroth-Order Methods for Stochastic Nonconvex Nonsmooth Composite\n  Optimization",
    "summary": "This work aims to solve a stochastic nonconvex nonsmooth composite\noptimization problem. Previous works on composite optimization problem requires\nthe major part to satisfy Lipschitz smoothness or some relaxed smoothness\nconditions, which excludes some machine learning examples such as regularized\nReLU network and sparse support matrix machine. In this work, we focus on\nstochastic nonconvex composite optimization problem without any smoothness\nassumptions. In particular, we propose two new notions of approximate\nstationary points for such optimization problem and obtain finite-time\nconvergence results of two zeroth-order algorithms to these two approximate\nstationary points respectively. Finally, we demonstrate that these algorithms\nare effective using numerical experiments.",
    "published": "2025-10-06T02:35:42Z",
    "updated": "2025-10-06T02:35:42Z",
    "link": "http://arxiv.org/pdf/2510.04446v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Ziyi Chen",
      "Peiran Yu",
      "Heng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.10523v2",
    "title": "Machine Learning for Inverse Problems and Data Assimilation",
    "summary": "The aim of these notes is to demonstrate the potential for ideas in machine\nlearning to impact on the fields of inverse problems and data assimilation. The\nperspective is one that is primarily aimed at researchers from inverse problems\nand/or data assimilation who wish to see a mathematical presentation of machine\nlearning as it pertains to their fields. As a by-product, we include a succinct\nmathematical treatment of various fundamental underpinning topics in machine\nlearning, and adjacent areas of (computational) mathematics.",
    "published": "2024-10-14T14:01:35Z",
    "updated": "2025-10-06T02:31:53Z",
    "link": "http://arxiv.org/pdf/2410.10523v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Eviatar Bach",
      "Ricardo Baptista",
      "Daniel Sanz-Alonso",
      "Andrew Stuart"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.03508v4",
    "title": "On-Demand Growth of Semiconductor Heterostructures Guided by\n  Physics-Informed Machine Learning",
    "summary": "Developing tailored semiconductor heterostructures on demand represents a\ncritical capability for addressing the escalating performance demands in\nelectronic and optoelectronic devices. However, traditional fabrication methods\nremain constrained by simulation-based design and iterative trial-and-error\noptimization. Here, we introduce SemiEpi, a self-driving platform designed for\nmolecular beam epitaxy (MBE) to perform multi-step semiconductor\nheterostructure growth through in-situ monitoring and on-the-fly feedback\ncontrol. By integrating standard MBE reactors, physics-informed machine\nlearning (ML) models, and parameter initialization, SemiEpi identifies optimal\ninitial conditions and proposes experiments for heterostructure growth,\neliminating the need for extensive expertise in MBE processes. As a proof of\nconcept, we demonstrate the optimization of high-density InAs quantum dot (QD)\ngrowth with a target emission wavelength of 1240 nm, showcasing the power of\nSemiEpi. We achieve a QD density of 5 x 10^10 cm^-2, a 1.6-fold increase in\nphotoluminescence (PL) intensity, and a reduced full width at half maximum\n(FWHM) of 29.13 meV, leveraging in-situ reflective high-energy electron\ndiffraction monitoring with feedback control for adjusting growth temperatures.\nTaken together, our results highlight the potential of ML-guided systems to\naddress challenges in multi-step heterostructure growth, facilitate the\ndevelopment of a hardware-independent framework, and enhance process\nrepeatability and stability, even without exhaustive knowledge of growth\nparameters.",
    "published": "2024-08-07T02:19:17Z",
    "updated": "2025-10-06T02:26:28Z",
    "link": "http://arxiv.org/pdf/2408.03508v4.pdf",
    "category": [
      "cond-mat.mtrl-sci",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Chao Shen",
      "Yuan Li",
      "Wenkang Zhan",
      "Shujie Pan",
      "Fuxin Lin",
      "Kaiyao Xin",
      "Hui Cong",
      "Chi Xu",
      "Xiaotian Cheng",
      "Ruixiang Liu",
      "Zhibo Ni",
      "Chaoyuan Jin",
      "Bo Xu",
      "Siming Chen",
      "Zhongming Wei",
      "Chunlai Xue",
      "Zhanguo Wang",
      "Chao Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05145v3",
    "title": "From Restless to Contextual: A Thresholding Bandit Reformulation For\n  Finite-horizon Performance",
    "summary": "This paper addresses the poor finite-horizon performance of existing online\n\\emph{restless bandit} (RB) algorithms, which stems from the prohibitive sample\ncomplexity of learning a full \\emph{Markov decision process} (MDP) for each\nagent. We argue that superior finite-horizon performance requires \\emph{rapid\nconvergence} to a \\emph{high-quality} policy. Thus motivated, we introduce a\nreformulation of online RBs as a \\emph{budgeted thresholding contextual\nbandit}, which simplifies the learning problem by encoding long-term state\ntransitions into a scalar reward. We prove the first non-asymptotic optimality\nof an oracle policy for a simplified finite-horizon setting. We propose a\npractical learning policy under a heterogeneous-agent, multi-state setting, and\nshow that it achieves a sublinear regret, achieving \\emph{faster convergence}\nthan existing methods. This directly translates to higher cumulative reward, as\nempirically validated by significant gains over state-of-the-art algorithms in\nlarge-scale heterogeneous environments. Our work provides a new pathway for\nachieving practical, sample-efficient learning in finite-horizon RBs.",
    "published": "2025-02-07T18:23:43Z",
    "updated": "2025-10-06T02:24:15Z",
    "link": "http://arxiv.org/pdf/2502.05145v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jiamin Xu",
      "Ivan Nazarov",
      "Aditya Rastogi",
      "África Periáñez",
      "Kyra Gan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16204v2",
    "title": "Directional Convergence, Benign Overfitting of Gradient Descent in leaky\n  ReLU two-layer Neural Networks",
    "summary": "In this paper, we study benign overfitting of fixed width leaky ReLU\ntwo-layer neural network classifiers trained on mixture data via gradient\ndescent. We provide both, upper and lower classification error bounds, and\ndiscover a phase transition in the bound as a function of signal strength. The\nlower bound leads to a characterization of cases when benign overfitting\nprovably fails even if directional convergence occurs. Our analysis allows us\nto considerably relax the distributional assumptions that are made in existing\nwork on benign overfitting of leaky ReLU two-layer neural network classifiers.\nWe can allow for non-sub-Gaussian data and do not require near orthogonality.\nOur results are derived by establishing directional convergence of the network\nparameters and studying classification error bounds for the convergent\ndirection. Previously, directional convergence in (leaky) ReLU neural networks\nwas established only for gradient flow. By first establishing directional\nconvergence, we are able to study benign overfitting of fixed width leaky ReLU\ntwo-layer neural network classifiers in a much wider range of scenarios than\nwas done before.",
    "published": "2025-05-22T04:11:58Z",
    "updated": "2025-10-06T02:21:14Z",
    "link": "http://arxiv.org/pdf/2505.16204v2.pdf",
    "category": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T07 (primary)"
    ],
    "authors": [
      "Ichiro Hashimoto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04441v1",
    "title": "Domain Generalization: A Tale of Two ERMs",
    "summary": "Domain generalization (DG) is the problem of generalizing from several\ndistributions (or domains), for which labeled training data are available, to a\nnew test domain for which no labeled data is available. A common finding in the\nDG literature is that it is difficult to outperform empirical risk minimization\n(ERM) on the pooled training data.\n  In this work, we argue that this finding has primarily been reported for\ndatasets satisfying a \\emph{covariate shift} assumption. When the dataset\nsatisfies a \\emph{posterior drift} assumption instead, we show that\n``domain-informed ERM,'' wherein feature vectors are augmented with\ndomain-specific information, outperforms pooling ERM. These claims are\nsupported by a theoretical framework and experiments on language and vision\ntasks.",
    "published": "2025-10-06T02:17:12Z",
    "updated": "2025-10-06T02:17:12Z",
    "link": "http://arxiv.org/pdf/2510.04441v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Yilun Zhu",
      "Naihao Deng",
      "Naichen Shi",
      "Aditya Gangrade",
      "Clayton Scott"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04440v1",
    "title": "Fractional Heat Kernel for Semi-Supervised Graph Learning with Small\n  Training Sample Size",
    "summary": "In this work, we introduce novel algorithms for label propagation and\nself-training using fractional heat kernel dynamics with a source term. We\nmotivate the methodology through the classical correspondence of information\ntheory with the physics of parabolic evolution equations. We integrate the\nfractional heat kernel into Graph Neural Network architectures such as Graph\nConvolutional Networks and Graph Attention, enhancing their expressiveness\nthrough adaptive, multi-hop diffusion. By applying Chebyshev polynomial\napproximations, large graphs become computationally feasible. Motivating\nvariational formulations demonstrate that by extending the classical diffusion\nmodel to fractional powers of the Laplacian, nonlocal interactions deliver more\nglobally diffusing labels. The particular balance between supervision of known\nlabels and diffusion across the graph is particularly advantageous in the case\nwhere only a small number of labeled training examples are present. We\ndemonstrate the effectiveness of this approach on standard datasets.",
    "published": "2025-10-06T02:15:46Z",
    "updated": "2025-10-06T02:15:46Z",
    "link": "http://arxiv.org/pdf/2510.04440v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Farid Bozorgnia",
      "Vyacheslav Kungurtsev",
      "Shirali Kadyrov",
      "Mohsen Yousefnezhad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04438v1",
    "title": "spd-metrics-id: A Python Package for SPD-Aware Distance Metrics in\n  Connectome Fingerprinting and Beyond",
    "summary": "We present spd-metrics-id, a Python package for computing distances and\ndivergences between symmetric positive-definite (SPD) matrices. Unlike\ntraditional toolkits that focus on specific applications, spd-metrics-id\nprovides a unified, extensible, and reproducible framework for SPD distance\ncomputation. The package supports a wide variety of geometry-aware metrics,\nincluding Alpha-z Bures-Wasserstein, Alpha-Procrustes, affine-invariant\nRiemannian, log-Euclidean, and others, and is accessible both via a\ncommand-line interface and a Python API. Reproducibility is ensured through\nDocker images and Zenodo archiving. We illustrate usage through a connectome\nfingerprinting example, but the package is broadly applicable to covariance\nanalysis, diffusion tensor imaging, and other domains requiring SPD matrix\ncomparison. The package is openly available at\nhttps://pypi.org/project/spd-metrics-id/.",
    "published": "2025-10-06T02:12:55Z",
    "updated": "2025-10-06T02:12:55Z",
    "link": "http://arxiv.org/pdf/2510.04438v1.pdf",
    "category": [
      "stat.CO",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Kaosar Uddin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04432v1",
    "title": "Trade-off in Estimating the Number of Byzantine Clients in Federated\n  Learning",
    "summary": "Federated learning has attracted increasing attention at recent large-scale\noptimization and machine learning research and applications, but is also\nvulnerable to Byzantine clients that can send any erroneous signals. Robust\naggregators are commonly used to resist Byzantine clients. This usually\nrequires to estimate the unknown number $f$ of Byzantine clients, and thus\naccordingly select the aggregators with proper degree of robustness (i.e., the\nmaximum number $\\hat{f}$ of Byzantine clients allowed by the aggregator). Such\nan estimation should have important effect on the performance, which has not\nbeen systematically studied to our knowledge. This work will fill in the gap by\ntheoretically analyzing the worst-case error of aggregators as well as its\ninduced federated learning algorithm for any cases of $\\hat{f}$ and $f$.\nSpecifically, we will show that underestimation ($\\hat{f}<f$) can lead to\narbitrarily poor performance for both aggregators and federated learning. For\nnon-underestimation ($\\hat{f}\\ge f$), we have proved optimal lower and upper\nbounds of the same order on the errors of both aggregators and federated\nlearning. All these optimal bounds are proportional to $\\hat{f}/(n-f-\\hat{f})$\nwith $n$ clients, which monotonically increases with larger $\\hat{f}$. This\nindicates a fundamental trade-off: while an aggregator with a larger robustness\ndegree $\\hat{f}$ can solve federated learning problems of wider range $f\\in\n[0,\\hat{f}]$, the performance can deteriorate when there are actually fewer or\neven no Byzantine clients (i.e., $f\\in [0,\\hat{f})$).",
    "published": "2025-10-06T02:01:56Z",
    "updated": "2025-10-06T02:01:56Z",
    "link": "http://arxiv.org/pdf/2510.04432v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Ziyi Chen",
      "Su Zhang",
      "Heng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04430v1",
    "title": "Achieve Performatively Optimal Policy for Performative Reinforcement\n  Learning",
    "summary": "Performative reinforcement learning is an emerging dynamical decision making\nframework, which extends reinforcement learning to the common applications\nwhere the agent's policy can change the environmental dynamics. Existing works\non performative reinforcement learning only aim at a performatively stable (PS)\npolicy that maximizes an approximate value function. However, there is a\nprovably positive constant gap between the PS policy and the desired\nperformatively optimal (PO) policy that maximizes the original value function.\nIn contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)\nalgorithm with a zeroth-order approximation of the performative policy gradient\nin the Frank-Wolfe framework, and obtains \\textbf{the first polynomial-time\nconvergence to the desired PO} policy under the standard regularizer dominance\ncondition. For the convergence analysis, we prove two important properties of\nthe nonconvex value function. First, when the policy regularizer dominates the\nenvironmental shift, the value function satisfies a certain gradient dominance\nproperty, so that any stationary point (not PS) of the value function is a\ndesired PO. Second, though the value function has unbounded gradient, we prove\nthat all the sufficiently stationary points lie in a convex and compact policy\nsubspace $\\Pi_{\\Delta}$, where the policy value has a constant lower bound\n$\\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.\nExperimental results also demonstrate that our 0-FW algorithm is more effective\nthan the existing algorithms in finding the desired PO policy.",
    "published": "2025-10-06T01:56:31Z",
    "updated": "2025-10-06T01:56:31Z",
    "link": "http://arxiv.org/pdf/2510.04430v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Ziyi Chen",
      "Heng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04421v1",
    "title": "Learning Survival Models with Right-Censored Reporting Delays",
    "summary": "Survival analysis is a statistical technique used to estimate the time until\nan event occurs. Although it is applied across a wide range of fields,\nadjusting for reporting delays under practical constraints remains a\nsignificant challenge in the insurance industry. Such delays render event\noccurrences unobservable when their reports are subject to right censoring.\nThis issue becomes particularly critical when estimating hazard rates for newly\nenrolled cohorts with limited follow-up due to administrative censoring. Our\nstudy addresses this challenge by jointly modeling the parametric hazard\nfunctions of event occurrences and report timings. The joint probability\ndistribution is marginalized over the latent event occurrence status. We\nconstruct an estimator for the proposed survival model and establish its\nasymptotic consistency. Furthermore, we develop an expectation-maximization\nalgorithm to compute its estimates. Using these findings, we propose a\ntwo-stage estimation procedure based on a parametric proportional hazards model\nto evaluate observations subject to administrative censoring. Experimental\nresults demonstrate that our method effectively improves the timeliness of risk\nevaluation for newly enrolled cohorts.",
    "published": "2025-10-06T01:16:57Z",
    "updated": "2025-10-06T01:16:57Z",
    "link": "http://arxiv.org/pdf/2510.04421v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Yuta Shikuri",
      "Hironori Fujisawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01072v2",
    "title": "Vector Copula Variational Inference and Dependent Block Posterior\n  Approximations",
    "summary": "The key to VI is the selection of a tractable density to approximate the\nBayesian posterior. For large and complex models a common choice is to assume\nindependence between multivariate blocks in a partition of the parameter space.\nWhile this simplifies the problem it can reduce accuracy. This paper proposes\nusing vector copulas to capture dependence between the blocks parsimoniously.\nTailored multivariate marginals are constructed using learnable transport maps.\nWe call the resulting joint distribution a ``dependent block posterior''\napproximation. Vector copula models are suggested that make tractable and\nflexible variational approximations. They allow for differing marginals,\nnumbers of blocks, block sizes and forms of between block dependence. They also\nallow for solution of the variational optimization using efficient stochastic\ngradient methods. The approach is demonstrated using four different statistical\nmodels and 16 datasets which have posteriors that are challenging to\napproximate. This includes models that use global-local shrinkage priors for\nregularization, and hierarchical models for smoothing and heteroscedastic time\nseries. In all cases, our method produces more accurate posterior\napproximations than benchmark VI methods that either assume block independence\nor factor-based dependence, at limited additional computational cost. A python\npackage implementing the method is available on GitHub at\nhttps://github.com/YuFuOliver/VCVI_Rep_PyPackage.",
    "published": "2025-03-03T00:24:54Z",
    "updated": "2025-10-06T00:50:29Z",
    "link": "http://arxiv.org/pdf/2503.01072v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.ME"
    ],
    "authors": [
      "Yu Fu",
      "Michael Stanley Smith",
      "Anastasios Panagiotelis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23365v2",
    "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of\n  Continuous Thought",
    "summary": "Previous work shows that the chain of continuous thought (continuous CoT)\nimproves the reasoning capability of large language models (LLMs) by enabling\nimplicit parallel thinking, and a subsequent work provided theoretical insight\nby showing that a two-layer transformer equipped with continuous CoT can\nefficiently solve directed graph reachability by maintaining a superposition of\nmultiple reasoning traces in the continuous thought. However, it remains\nunclear how the superposition mechanism is naturally learned from\ngradient-based training methods. To fill this gap, we theoretically analyze the\ntraining dynamics of a simplified two-layer transformer on the directed graph\nreachability problem to unveil how the superposition mechanism emerges during\ntraining in two training stages -- (i) a thought-generation stage that\nautoregressively expands the continuous thought, and (ii) a prediction stage\nthat converts the thought into the final answer. Our analysis reveals that\nduring training using continuous thought, the index-matching logit, an\nimportant quantity which reflects the strength of the model's local search\nability, will first increase and then remain bounded under mild assumptions.\nThe bounded index-matching logit effectively balances exploration and\nexploitation during the reasoning process: the model will exploit local problem\nstructures to identify plausible search traces, and assign comparable weights\nto multiple such traces to explore when it is uncertain about which solution is\ncorrect, which results in superposition. Our experimental results tracking the\ngrowth of logits further validate our theory.",
    "published": "2025-09-27T15:23:46Z",
    "updated": "2025-10-06T00:40:29Z",
    "link": "http://arxiv.org/pdf/2509.23365v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hanlin Zhu",
      "Shibo Hao",
      "Zhiting Hu",
      "Jiantao Jiao",
      "Stuart Russell",
      "Yuandong Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.03378v3",
    "title": "Understanding and Improving Shampoo and SOAP via Kullback-Leibler\n  Minimization",
    "summary": "Shampoo and its efficient variant, SOAP, employ structured second-moment\nestimations and have shown strong performance for training neural networks\n(NNs). In practice, however, Shampoo typically requires step-size grafting with\nAdam to be competitive, and SOAP mitigates this by applying Adam in Shampoo's\neigenbasis -- at the cost of additional memory overhead from Adam in both\nmethods. Prior analyses have largely relied on the Frobenius norm to motivate\nthese estimation schemes. We instead recast their estimation procedures as\ncovariance estimation under Kullback-Leibler (KL) divergence minimization,\nrevealing a previously overlooked theoretical limitation and motivating\nprincipled redesigns. Building on this perspective, we develop\n$\\textbf{KL-Shampoo}$ and $\\textbf{KL-SOAP}$, practical schemes that match or\nexceed the performance of Shampoo and SOAP in NN pre-training while achieving\nSOAP-level per-iteration runtime. Notably, KL-Shampoo does not rely on Adam to\nattain competitive performance, eliminating the memory overhead introduced by\nAdam. Across our experiments, KL-Shampoo consistently outperforms SOAP,\nShampoo, and even KL-SOAP, establishing the KL-based approach as a compelling\nfoundation for designing structured methods in NN optimization.",
    "published": "2025-09-03T14:55:15Z",
    "updated": "2025-10-06T00:39:27Z",
    "link": "http://arxiv.org/pdf/2509.03378v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Wu Lin",
      "Scott C. Lowe",
      "Felix Dangel",
      "Runa Eschenhagen",
      "Zikun Xu",
      "Roger B. Grosse"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04407v1",
    "title": "Scale-Invariant Regret Matching and Online Learning with Optimal\n  Convergence: Bridging Theory and Practice in Zero-Sum Games",
    "summary": "A considerable chasm has been looming for decades between theory and practice\nin zero-sum game solving through first-order methods. Although a convergence\nrate of $T^{-1}$ has long been established since Nemirovski's mirror-prox\nalgorithm and Nesterov's excessive gap technique in the early 2000s, the most\neffective paradigm in practice is *counterfactual regret minimization*, which\nis based on *regret matching* and its modern variants. In particular, the state\nof the art across most benchmarks is *predictive* regret matching$^+$\n(PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can\nexhibit slower $\\Omega(T^{-1/2})$ convergence even in self-play.\n  In this paper, we close the gap between theory and practice. We propose a new\nscale-invariant and parameter-free variant of PRM$^+$, which we call\nIREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$\n(i.e., optimal) average-iterate convergence guarantees, while also being on par\nwith PRM$^+$ on benchmark games. From a technical standpoint, we draw an\nanalogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive*\nlearning rate. The basic flaw of PRM$^+$ is that the ($\\ell_2$-)norm of the\nregret vector -- which can be thought of as the inverse of the learning rate --\ncan decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the\ninvariance that the norm of the regret vector is nondecreasing. This enables us\nto derive an RVU-type bound for IREG-PRM$^+$, the first such property that does\nnot rely on introducing additional hyperparameters to enforce smoothness.\n  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive\nversion of optimistic gradient descent that we introduce whose learning rate\ndepends on the misprediction error, demystifying the effectiveness of the\nregret matching family *vis-a-vis* more standard optimization techniques.",
    "published": "2025-10-06T00:33:20Z",
    "updated": "2025-10-06T00:33:20Z",
    "link": "http://arxiv.org/pdf/2510.04407v1.pdf",
    "category": [
      "cs.GT",
      "cs.LG"
    ],
    "authors": [
      "Brian Hu Zhang",
      "Ioannis Anagnostides",
      "Tuomas Sandholm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04406v1",
    "title": "Modular and Adaptive Conformal Prediction for Sequential Models via\n  Residual Decomposition",
    "summary": "Conformal prediction offers finite-sample coverage guarantees under minimal\nassumptions. However, existing methods treat the entire modeling process as a\nblack box, overlooking opportunities to exploit modular structure. We introduce\na conformal prediction framework for two-stage sequential models, where an\nupstream predictor generates intermediate representations for a downstream\nmodel. By decomposing the overall prediction residual into stage-specific\ncomponents, our method enables practitioners to attribute uncertainty to\nspecific pipeline stages. We develop a risk-controlled parameter selection\nprocedure using family-wise error rate (FWER) control to calibrate stage-wise\nscaling parameters, and propose an adaptive extension for non-stationary\nsettings that preserves long-run coverage guarantees. Experiments on synthetic\ndistribution shifts, as well as real-world supply chain and stock market data,\ndemonstrate that our approach maintains coverage under conditions that degrade\nstandard conformal methods, while providing interpretable stage-wise\nuncertainty attribution. This framework offers diagnostic advantages and robust\ncoverage that standard conformal methods lack.",
    "published": "2025-10-06T00:33:18Z",
    "updated": "2025-10-06T00:33:18Z",
    "link": "http://arxiv.org/pdf/2510.04406v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "William Zhang",
      "Saurabh Amin",
      "Georgia Perakis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19396v4",
    "title": "Uniform convergence of the smooth calibration error and its relationship\n  with functional gradient",
    "summary": "Calibration is a critical requirement for reliable probabilistic prediction,\nespecially in high-risk applications. However, the theoretical understanding of\nwhich learning algorithms can simultaneously achieve high accuracy and good\ncalibration remains limited, and many existing studies provide empirical\nvalidation or a theoretical guarantee in restrictive settings. To address this\nissue, in this work, we focus on the smooth calibration error (CE) and provide\na uniform convergence bound, showing that the smooth CE is bounded by the sum\nof the smooth CE over the training dataset and a generalization gap. We further\nprove that the functional gradient of the loss function can effectively control\nthe training smooth CE. Based on this framework, we analyze three\nrepresentative algorithms: gradient boosting trees, kernel boosting, and\ntwo-layer neural networks. For each, we derive conditions under which both\nclassification and calibration performances are simultaneously guaranteed. Our\nresults offer new theoretical insights and practical guidance for designing\nreliable probabilistic models with provable calibration guarantees.",
    "published": "2025-05-26T01:23:56Z",
    "updated": "2025-10-05T23:51:42Z",
    "link": "http://arxiv.org/pdf/2505.19396v4.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Futoshi Futami",
      "Atsushi Nitanda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.20117v3",
    "title": "Exact and Linear Convergence for Federated Learning under Arbitrary\n  Client Participation is Attainable",
    "summary": "This work tackles the fundamental challenges in Federated Learning (FL) posed\nby arbitrary client participation and data heterogeneity, prevalent\ncharacteristics in practical FL settings. It is well-established that popular\nFedAvg-style algorithms struggle with exact convergence and can suffer from\nslow convergence rates since a decaying learning rate is required to mitigate\nthese scenarios. To address these issues, we introduce the concept of\nstochastic matrix and the corresponding time-varying graphs as a novel modeling\ntool to accurately capture the dynamics of arbitrary client participation and\nthe local update procedure. Leveraging this approach, we offer a fresh\ndecentralized perspective on designing FL algorithms and present FOCUS,\nFederated Optimization with Exact Convergence via Push-pull Strategy, a\nprovably convergent algorithm designed to effectively overcome the previously\nmentioned two challenges. More specifically, we provide a rigorous proof\ndemonstrating that FOCUS achieves exact convergence with a linear rate\nregardless of the arbitrary client participation, establishing it as the first\nwork to demonstrate this significant result.",
    "published": "2025-03-25T23:54:23Z",
    "updated": "2025-10-05T23:26:38Z",
    "link": "http://arxiv.org/pdf/2503.20117v3.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Bicheng Ying",
      "Zhe Li",
      "Haibo Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04386v1",
    "title": "SSM-CGM: Interpretable State-Space Forecasting Model of Continuous\n  Glucose Monitoring for Personalized Diabetes Management",
    "summary": "Continuous glucose monitoring (CGM) generates dense data streams critical for\ndiabetes management, but most used forecasting models lack interpretability for\nclinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting\nmodel that integrates CGM and wearable activity signals from the AI-READI\ncohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer\nbaseline, adds interpretability through variable selection and temporal\nattribution, and enables counterfactual forecasts simulating how planned\nchanges in physiological signals (e.g., heart rate, respiration) affect\nnear-term glucose. Together, these features make SSM-CGM an interpretable,\nphysiologically grounded framework for personalized diabetes management.",
    "published": "2025-10-05T22:37:28Z",
    "updated": "2025-10-05T22:37:28Z",
    "link": "http://arxiv.org/pdf/2510.04386v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shakson Isaac",
      "Yentl Collin",
      "Chirag Patel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04378v1",
    "title": "Score-based Greedy Search for Structure Identification of Partially\n  Observed Linear Causal Models",
    "summary": "Identifying the structure of a partially observed causal system is essential\nto various scientific fields. Recent advances have focused on constraint-based\ncausal discovery to solve this problem, and yet in practice these methods often\nface challenges related to multiple testing and error propagation. These issues\ncould be mitigated by a score-based method and thus it has raised great\nattention whether there exists a score-based greedy search method that can\nhandle the partially observed scenario. In this work, we propose the first\nscore-based greedy search method for the identification of structure involving\nlatent variables with identifiability guarantees. Specifically, we propose\nGeneralized N Factor Model and establish the global consistency:\n  the true structure including latent variables can be identified up to the\nMarkov equivalence class by using score. We then design\n  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm\nfor this class of model with well-defined operators,\n  which search very efficiently over the graph space to find the optimal\nstructure. Our experiments on both synthetic and real-life data validate the\neffectiveness of our method (code will be publicly available).",
    "published": "2025-10-05T21:50:17Z",
    "updated": "2025-10-05T21:50:17Z",
    "link": "http://arxiv.org/pdf/2510.04378v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xinshuai Dong",
      "Ignavier Ng",
      "Haoyue Dai",
      "Jiaqi Sun",
      "Xiangchen Song",
      "Peter Spirtes",
      "Kun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04377v1",
    "title": "TCR-EML: Explainable Model Layers for TCR-pMHC Prediction",
    "summary": "T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a\ncentral component of adaptive immunity, with implications for vaccine design,\ncancer immunotherapy, and autoimmune disease. While recent advances in machine\nlearning have improved prediction of TCR-pMHC binding, the most effective\napproaches are black-box transformer models that cannot provide a rationale for\npredictions. Post-hoc explanation methods can provide insight with respect to\nthe input but do not explicitly model biochemical mechanisms (e.g. known\nbinding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e.,\nwith architectural components that can be examined directly after training)\nhave been explored in other domains, but have not been used for TCR-pMHC\nbinding. We propose explainable model layers (TCR-EML) that can be incorporated\ninto protein-language model backbones for TCR-pMHC modeling. Our approach uses\nprototype layers for amino acid residue contacts drawn from known TCR-pMHC\nbinding mechanisms, enabling high-quality explanations for predicted TCR-pMHC\nbinding. Experiments of our proposed method on large-scale datasets demonstrate\ncompetitive predictive accuracy and generalization, and evaluation on the\nTCR-XAI benchmark demonstrates improved explainability compared with existing\napproaches.",
    "published": "2025-10-05T21:47:48Z",
    "updated": "2025-10-05T21:47:48Z",
    "link": "http://arxiv.org/pdf/2510.04377v1.pdf",
    "category": [
      "q-bio.QM",
      "cs.CE",
      "cs.LG"
    ],
    "authors": [
      "Jiarui Li",
      "Zixiang Yin",
      "Zhengming Ding",
      "Samuel J. Landry",
      "Ramgopal R. Mettu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04376v1",
    "title": "Categorical Invariants of Learning Dynamics",
    "summary": "Neural network training is typically viewed as gradient descent on a loss\nsurface. We propose a fundamentally different perspective: learning is a\nstructure-preserving transformation (a functor L) between the space of network\nparameters (Param) and the space of learned representations (Rep). This\ncategorical framework reveals that different training runs producing similar\ntest performance often belong to the same homotopy class (continuous\ndeformation family) of optimization paths. We show experimentally that networks\nconverging via homotopic trajectories generalize within 0.5% accuracy of each\nother, while non-homotopic paths differ by over 3%. The theory provides\npractical tools: persistent homology identifies stable minima predictive of\ngeneralization (R^2 = 0.82 correlation), pullback constructions formalize\ntransfer learning, and 2-categorical structures explain when different\noptimization algorithms yield functionally equivalent models. These categorical\ninvariants offer both theoretical insight into why deep learning works and\nconcrete algorithmic principles for training more robust networks.",
    "published": "2025-10-05T21:45:36Z",
    "updated": "2025-10-05T21:45:36Z",
    "link": "http://arxiv.org/pdf/2510.04376v1.pdf",
    "category": [
      "cs.LG",
      "68T07, 18B99, 55N35",
      "I.2.6; F.4.1; G.2.2"
    ],
    "authors": [
      "Abdulrahman Tamim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.00384v2",
    "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
    "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
    "published": "2025-05-31T04:27:22Z",
    "updated": "2025-10-05T21:29:28Z",
    "link": "http://arxiv.org/pdf/2506.00384v2.pdf",
    "category": [
      "cs.LG",
      "cs.DC",
      "cs.OS"
    ],
    "authors": [
      "Yutong Huang",
      "Zhiyuan Guo",
      "Yiying Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04366v1",
    "title": "Quantifying Ambiguity in Categorical Annotations: A Measure and\n  Statistical Inference Framework",
    "summary": "Human-generated categorical annotations frequently produce empirical response\ndistributions (soft labels) that reflect ambiguity rather than simple annotator\nerror. We introduce an ambiguity measure that maps a discrete response\ndistribution to a scalar in the unit interval, designed to quantify aleatoric\nuncertainty in categorical tasks. The measure bears a close relationship to\nquadratic entropy (Gini-style impurity) but departs from those indices by\ntreating an explicit \"can't solve\" category asymmetrically, thereby separating\nuncertainty arising from class-level indistinguishability from uncertainty due\nto explicit unresolvability. We analyze the measure's formal properties and\ncontrast its behavior with a representative ambiguity measure from the\nliterature. Moving beyond description, we develop statistical tools for\ninference: we propose frequentist point estimators for population ambiguity and\nderive the Bayesian posterior over ambiguity induced by Dirichlet priors on the\nunderlying probability vector, providing a principled account of epistemic\nuncertainty. Numerical examples illustrate estimation, calibration, and\npractical use for dataset-quality assessment and downstream machine-learning\nworkflows.",
    "published": "2025-10-05T21:19:42Z",
    "updated": "2025-10-05T21:19:42Z",
    "link": "http://arxiv.org/pdf/2510.04366v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Christopher Klugmann",
      "Daniel Kondermann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04357v1",
    "title": "From News to Returns: A Granger-Causal Hypergraph Transformer on the\n  Sphere",
    "summary": "We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel\narchitecture for interpretable financial time-series forecasting that unifies\n\\emph{Granger-causal hypergraph structure}, \\emph{Riemannian geometry}, and\n\\emph{causally masked Transformer attention}. CSHT models the directional\ninfluence of financial news and sentiment on asset returns by extracting\nmultivariate Granger-causal dependencies, which are encoded as directional\nhyperedges on the surface of a hypersphere. Attention is constrained via\nangular masks that preserve both temporal directionality and geometric\nconsistency. Evaluated on S\\&P 500 data from 2018 to 2023, including the 2020\nCOVID-19 shock, CSHT consistently outperforms baselines across return\nprediction, regime classification, and top-asset ranking tasks. By enforcing\npredictive causal structure and embedding variables in a Riemannian manifold,\nCSHT delivers both \\emph{robust generalisation across market regimes} and\n\\emph{transparent attribution pathways} from macroeconomic events to\nstock-level responses. These results suggest that CSHT is a principled and\npractical solution for trustworthy financial forecasting under uncertainty.",
    "published": "2025-10-05T20:51:59Z",
    "updated": "2025-10-05T20:51:59Z",
    "link": "http://arxiv.org/pdf/2510.04357v1.pdf",
    "category": [
      "cs.LG",
      "q-fin.CP"
    ],
    "authors": [
      "Anoushka Harit",
      "Zhongtian Sun",
      "Jongmin Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04355v1",
    "title": "Quantizer Design for Finite Model Approximations, Model Learning, and\n  Quantized Q-Learning for MDPs with Unbounded Spaces",
    "summary": "In this paper, for Markov decision processes (MDPs) with unbounded state\nspaces we present refined upper bounds presented in [Kara et. al. JMLR'23] on\nfinite model approximation errors via optimizing the quantizers used for finite\nmodel approximations. We also consider implications on quantizer design for\nquantized Q-learning and empirical model learning, and the performance of\npolicies obtained via Q-learning where the quantized state is treated as the\nstate itself. We highlight the distinctions between planning, where\napproximating MDPs can be independently designed, and learning (either via\nQ-learning or empirical model learning), where approximating MDPs are\nrestricted to be defined by invariant measures of Markov chains under\nexploration policies, leading to significant subtleties on quantizer design\nperformance, even though asymptotic near optimality can be established under\nboth setups. In particular, under Lyapunov growth conditions, we obtain\nexplicit upper bounds which decay to zero as the number of bins approaches\ninfinity.",
    "published": "2025-10-05T20:39:52Z",
    "updated": "2025-10-05T20:39:52Z",
    "link": "http://arxiv.org/pdf/2510.04355v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Osman Bicer",
      "Ali D. Kara",
      "Serdar Yuksel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04346v1",
    "title": "Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression\n  Comparisons, Shadow Fading, and Calibrated Fade Margins",
    "summary": "Indoor LoRaWAN propagation is shaped by structural and time-varying context\nfactors, which challenge log-distance models and the assumption of log-normal\nshadowing. We present an environment-aware, statistically disciplined path loss\nframework evaluated using leakage-safe cross-validation on a 12-month campaign\nin an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is\naugmented with environmental covariates (relative humidity, temperature, carbon\ndioxide, particulate matter, and barometric pressure), as well as the\nsignal-to-noise ratio. We compare multiple linear regression with regularized\nvariants, Bayesian linear regression, and a selective second-order polynomial\napplied to continuous drivers. Predictor relevance is established using\nheteroscedasticity-robust Type II and III analysis of variance and nested\npartial F tests. Shadow fading is profiled with kernel density estimation and\nnon-parametric families, including Normal, Skew-Normal, Student's t, and\nGaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07\nto 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are\nnon-Gaussian; a 3-component mixture captures a sharp core with a light, broad\ntail. We convert accuracy into reliability by prescribing the fade margin as\nthe upper-tail quantile of cross-validated residuals, quantifying uncertainty\nvia a moving-block bootstrap, and validating on a held-out set. At 99% packet\ndelivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7\nto 27.9 dB for linear baselines. This result presents a deployment-ready,\ninterpretable workflow with calibrated reliability control for indoor Internet\nof Things planning, aligned with 6G targets.",
    "published": "2025-10-05T20:14:48Z",
    "updated": "2025-10-05T20:14:48Z",
    "link": "http://arxiv.org/pdf/2510.04346v1.pdf",
    "category": [
      "cs.NI",
      "cs.LG",
      "cs.NA",
      "eess.SP",
      "math.NA"
    ],
    "authors": [
      "Nahshon Mokua Obiri",
      "Kristof Van Laerhoven"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04342v1",
    "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust\n  Forecasting of Chaotic Dynamics",
    "summary": "Forecasting chaotic systems is a cornerstone challenge in many scientific\nfields, complicated by the exponential amplification of even infinitesimal\nprediction errors. Modern machine learning approaches often falter due to two\nopposing pitfalls: over-specializing on a single, well-known chaotic system\n(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing\nvast, unrelated time-series, which prevents the model from learning the nuances\nof any specific dynamical regime. We propose Curriculum Chaos Forecasting\n(CCF), a training paradigm that bridges this gap. CCF organizes training data\nbased on fundamental principles of dynamical systems theory, creating a\ncurriculum that progresses from simple, periodic behaviors to highly complex,\nchaotic dynamics. We quantify complexity using the largest Lyapunov exponent\nand attractor dimension, two well-established metrics of chaos. By first\ntraining a sequence model on predictable systems and gradually introducing more\nchaotic trajectories, CCF enables the model to build a robust and generalizable\nrepresentation of dynamical behaviors. We curate a library of over 50 synthetic\nODE/PDE systems to build this curriculum. Our experiments show that\npre-training with CCF significantly enhances performance on unseen, real-world\nbenchmarks. On datasets including Sunspot numbers, electricity demand, and\nhuman ECG signals, CCF extends the valid prediction horizon by up to 40%\ncompared to random-order training and more than doubles it compared to training\non real-world data alone. We demonstrate that this benefit is consistent across\nvarious neural architectures (GRU, Transformer) and provide extensive ablations\nto validate the importance of the curriculum's structure.",
    "published": "2025-10-05T20:06:16Z",
    "updated": "2025-10-05T20:06:16Z",
    "link": "http://arxiv.org/pdf/2510.04342v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Harshil Vejendla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04327v1",
    "title": "Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate\n  Scale for CNNs and ResNets",
    "summary": "Choosing an appropriate learning rate remains a key challenge in scaling\ndepth of modern deep networks. The classical maximal update parameterization\n($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to\nhomogeneous multilayer perceptrons (MLPs) but becomes ill-posed in\nheterogeneous architectures where residual accumulation and convolutions\nintroduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P\n(AM-$\\mu$P), which constrains not each individual layer but the network-wide\naverage one-step pre-activation second moment to a constant scale. Combined\nwith a residual-aware He fan-in initialization - scaling residual-branch\nweights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot\n\\mathrm{fan\\text{-}in})$) - AM-$\\mu$P yields width-robust depth laws that\ntransfer consistently across depths. We prove that, for one- and\ntwo-dimensional convolutional networks, the maximal-update learning rate\nsatisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects\nare constant-level as $N\\gg k$. For standard residual networks with general\nconv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the\nminimal depth. Empirical results across a range of depths confirm the $-3/2$\nscaling law and enable zero-shot learning-rate transfer, providing a unified\nand practical LR principle for convolutional and deep residual networks without\nadditional tuning overhead.",
    "published": "2025-10-05T19:22:50Z",
    "updated": "2025-10-05T19:22:50Z",
    "link": "http://arxiv.org/pdf/2510.04327v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Haosong Zhang",
      "Shenxi Wu",
      "Yichi Zhang",
      "Wei Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04325v1",
    "title": "FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of\n  2D Airfoil Flow Fields",
    "summary": "The accurate prediction of flow fields around airfoils is crucial for\naerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models\nare effective but computationally expensive, thus inspiring the development of\nsurrogate models to enable quicker predictions. These surrogate models can be\nbased on deep learning architectures, such as Convolutional Neural Networks\n(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion\nmodels have shown significant promise in predicting complex flow fields. In\nthis work, we propose FoilDiff, a diffusion-based surrogate model with a\nhybrid-backbone denoising network. This hybrid design combines the power of\nconvolutional feature extraction and transformer-based global attention to\ngenerate more adaptable and accurate representations of flow structures.\nFoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling\nto optimise the efficiency of the sampling process at no additional cost to\nmodel generalisation. We used encoded representations of Reynolds number, angle\nof attack, and airfoil geometry to define the input space for generalisation\nacross a wide range of aerodynamic conditions. When evaluated against\nstate-of-the-art models, FoilDiff shows significant performance improvements,\nwith mean prediction errors reducing by up to 85\\% on the same datasets. The\nresults have demonstrated that FoilDiff can provide both more accurate\npredictions and better-calibrated predictive uncertainty than existing\ndiffusion-based models.",
    "published": "2025-10-05T19:10:38Z",
    "updated": "2025-10-05T19:10:38Z",
    "link": "http://arxiv.org/pdf/2510.04325v1.pdf",
    "category": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "authors": [
      "Kenechukwu Ogbuagu",
      "Sepehr Maleki",
      "Giuseppe Bruni",
      "Senthil Krishnababu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.05323v2",
    "title": "Graph Alignment via Birkhoff Relaxation",
    "summary": "We consider the graph alignment problem, wherein the objective is to find a\nvertex correspondence between two graphs that maximizes the edge overlap. The\ngraph alignment problem is an instance of the quadratic assignment problem\n(QAP), known to be NP-hard in the worst case even to approximately solve. In\nthis paper, we analyze Birkhoff relaxation, a tight convex relaxation of QAP,\nand present theoretical guarantees on its performance when the inputs follow\nthe Gaussian Wigner Model. More specifically, the weighted adjacency matrices\nare correlated Gaussian Orthogonal Ensemble with correlation\n$1/\\sqrt{1+\\sigma^2}$. Denote the optimal solutions of the QAP and Birkhoff\nrelaxation by $\\Pi^\\star$ and $X^\\star$ respectively. We show that\n$\\|X^\\star-\\Pi^\\star\\|_F^2 = o(n)$ when $\\sigma = o(n^{-1.25})$ and\n$\\|X^\\star-\\Pi^\\star\\|_F^2 = \\Omega(n)$ when $\\sigma = \\Omega(n^{-0.5})$. Thus,\nthe optimal solution $X^\\star$ transitions from a small perturbation of\n$\\Pi^\\star$ for small $\\sigma$ to being well separated from $\\Pi^\\star$ as\n$\\sigma$ becomes larger than $n^{-0.5}$. This result allows us to guarantee\nthat simple rounding procedures on $X^\\star$ align $1-o(1)$ fraction of\nvertices correctly whenever $\\sigma = o(n^{-1.25})$. This condition on $\\sigma$\nto ensure the success of the Birkhoff relaxation is state-of-the-art.",
    "published": "2025-03-07T11:01:35Z",
    "updated": "2025-10-05T19:06:29Z",
    "link": "http://arxiv.org/pdf/2503.05323v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.SP",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Sushil Mahavir Varma",
      "Irène Waldspurger",
      "Laurent Massoulié"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04322v1",
    "title": "Towards Fast Option Pricing PDE Solvers Powered by PIELM",
    "summary": "Partial differential equation (PDE) solvers underpin modern quantitative\nfinance, governing option pricing and risk evaluation. Physics-Informed Neural\nNetworks (PINNs) have emerged as a promising approach for solving the forward\nand inverse problems of partial differential equations (PDEs) using deep\nlearning. However they remain computationally expensive due to their iterative\ngradient descent based optimization and scale poorly with increasing model\nsize. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)\nas fast alternative to PINNs for solving both forward and inverse problems in\nfinancial PDEs. PIELMs replace iterative optimization with a single\nleast-squares solve, enabling deterministic and efficient training. We\nbenchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward\npricing and demonstrate its capability in inverse model calibration to recover\nvolatility and interest rate parameters from noisy data. From experiments we\nobserve that PIELM achieve accuracy comparable to PINNs while being up to\n$30\\times$ faster, highlighting their potential for real-time financial\nmodeling.",
    "published": "2025-10-05T18:50:49Z",
    "updated": "2025-10-05T18:50:49Z",
    "link": "http://arxiv.org/pdf/2510.04322v1.pdf",
    "category": [
      "cs.CE",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "J.2; I.6.3; G.1.7; G.1.8"
    ],
    "authors": [
      "Akshay Govind Srinivasan",
      "Anuj Jagannath Said",
      "Sathwik Pentela",
      "Vikas Dwivedi",
      "Balaji Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12460v2",
    "title": "A Case for Library-Level k-Means Binning in Histogram Gradient-Boosted\n  Trees",
    "summary": "Modern Gradient Boosted Decision Trees (GBDTs) accelerate split finding with\nhistogram-based binning, which reduces complexity from $O(N\\log N)$ to $O(N)$\nby aggregating gradients into fixed-size bins. However, the predominant\nquantile binning strategy - designed to distribute data points evenly among\nbins -- may overlook critical boundary values that could enhance predictive\nperformance. In this work, we consider a novel approach that replaces quantile\nbinning with a $k$-means discretizer initialized with quantile bins, and\njustify the swap with a proof showing how, for any $L$-Lipschitz function,\nk-means maximizes the worst-case explained variance of Y obtained when treating\nall values in a given bin as equivalent. We test this swap against quantile and\nuniform binning on 33 OpenML datasets plus synthetics that control for\nmodality, skew, and bin budget. Across 18 regression datasets, k-means shows no\nstatistically significant losses at the 5% level and wins in three cases-most\nstrikingly a 55% MSE drop on one particularly skewed dataset-even though\nk-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15\nclassification datasets the two methods are statistically tied (MRR 0.70 vs\n0.68) with gaps $\\leq$0.2 pp. Synthetic experiments confirm consistently large\nMSE gains - typically >20% and rising to 90% as outlier magnitude increases or\nbin budget drops. We find that k-means keeps error on par with exhaustive\n(no-binning) splitting when extra cuts add little value, yet still recovers key\nsplit points that quantile overlooks. As such, we advocate for a built-in\nbin_method=k-means flag, especially in regression tasks and in tight-budget\nsettings such as the 32-64-bin GPU regime - because it is a \"safe default\" with\nlarge upside, yet adds only a one-off, cacheable overhead ($\\approx$ 3.5s per\nfeature to bin 10M rows on one Apple M1 thread).",
    "published": "2025-05-18T15:28:06Z",
    "updated": "2025-10-05T18:46:23Z",
    "link": "http://arxiv.org/pdf/2505.12460v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Asher Labovich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04318v1",
    "title": "Adaptive Coverage Policies in Conformal Prediction",
    "summary": "Traditional conformal prediction methods construct prediction sets such that\nthe true label falls within the set with a user-specified coverage level.\nHowever, poorly chosen coverage levels can result in uninformative predictions,\neither producing overly conservative sets when the coverage level is too high,\nor empty sets when it is too low. Moreover, the fixed coverage level cannot\nadapt to the specific characteristics of each individual example, limiting the\nflexibility and efficiency of these methods. In this work, we leverage recent\nadvances in e-values and post-hoc conformal inference, which allow the use of\ndata-dependent coverage levels while maintaining valid statistical guarantees.\nWe propose to optimize an adaptive coverage policy by training a neural network\nusing a leave-one-out procedure on the calibration set, allowing the coverage\nlevel and the resulting prediction set size to vary with the difficulty of each\nindividual example. We support our approach with theoretical coverage\nguarantees and demonstrate its practical benefits through a series of\nexperiments.",
    "published": "2025-10-05T18:37:22Z",
    "updated": "2025-10-05T18:37:22Z",
    "link": "http://arxiv.org/pdf/2510.04318v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Etienne Gauthier",
      "Francis Bach",
      "Michael I. Jordan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02218v2",
    "title": "Quantum Fisher information matrices from Rényi relative entropies",
    "summary": "Quantum generalizations of the Fisher information are important in quantum\ninformation science, with applications in high energy and condensed matter\nphysics and in quantum estimation theory, machine learning, and optimization.\nOne can derive a quantum generalization of the Fisher information matrix in a\nnatural way as the Hessian matrix arising in a Taylor expansion of a smooth\ndivergence. Such an approach is appealing for quantum information theorists,\ngiven the ubiquity of divergences in quantum information theory. In contrast to\nthe classical case, there is not a unique quantum generalization of the Fisher\ninformation matrix, similar to how there is not a unique quantum generalization\nof the relative entropy or the R\\'enyi relative entropy. In this paper, I\nderive information matrices arising from the log-Euclidean, $\\alpha$-$z$, and\ngeometric R\\'enyi relative entropies, with the main technical tool for doing so\nbeing the method of divided differences for calculating matrix derivatives.\nInterestingly, for all non-negative values of the R\\'enyi parameter $\\alpha$,\nthe log-Euclidean R\\'enyi relative entropy leads to the Kubo-Mori information\nmatrix, and the geometric R\\'enyi relative entropy leads to the\nright-logarithmic derivative Fisher information matrix. Thus, the resulting\ninformation matrices obey the data-processing inequality for all non-negative\nvalues of the R\\'enyi parameter $\\alpha$ even though the original quantities do\nnot. Additionally, I derive and establish basic properties of $\\alpha$-$z$\ninformation matrices resulting from the $\\alpha$-$z$ R\\'enyi relative\nentropies. For parameterized thermal states and time-evolved states, I\nestablish formulas for their $\\alpha$-$z$ information matrices and hybrid\nquantum-classical algorithms for estimating them, with applications in quantum\nBoltzmann machine learning.",
    "published": "2025-10-02T17:02:48Z",
    "updated": "2025-10-05T18:35:10Z",
    "link": "http://arxiv.org/pdf/2510.02218v2.pdf",
    "category": [
      "quant-ph",
      "cond-mat.stat-mech",
      "cs.IT",
      "cs.LG",
      "hep-th",
      "math.IT"
    ],
    "authors": [
      "Mark M. Wilde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04316v1",
    "title": "Crash Severity Prediction Using Deep Learning Approaches: A Hybrid\n  CNN-RNN Framework",
    "summary": "Accurate and timely prediction of crash severity is crucial in mitigating the\nsevere consequences of traffic accidents. Accurate and timely prediction of\ncrash severity is crucial in mitigating the severe consequences of traffic\naccidents. In order to provide appropriate levels of medical assistance and\ntransportation services, an intelligent transportation system relies on\neffective prediction methods. Deep learning models have gained popularity in\nthis domain due to their capability to capture non-linear relationships among\nvariables. In this research, we have implemented a hybrid CNN-RNN deep learning\nmodel for crash severity prediction and compared its performance against widely\nused statistical and machine learning models such as logistic regression,\nna\\\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and\nindividual deep learning models: RNN and CNN. This study employs a methodology\nthat considers the interconnected relationships between various features of\ntraffic accidents. The study was conducted using a dataset of 15,870 accident\nrecords gathered over a period of seven years between 2015 and 2021 on Virginia\nhighway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model\nhas outperformed all benchmark models in terms of predicting crash severity.\nThis result illustrates the effectiveness of the hybrid model as it combines\nthe advantages of both RNN and CNN models in order to achieve greater accuracy\nin the prediction process.",
    "published": "2025-10-05T18:31:45Z",
    "updated": "2025-10-05T18:31:45Z",
    "link": "http://arxiv.org/pdf/2510.04316v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Sahar Koohfar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04309v1",
    "title": "Activation Steering with a Feedback Controller",
    "summary": "Controlling the behaviors of large language models (LLM) is fundamental to\ntheir safety alignment and reliable deployment. However, existing steering\nmethods are primarily driven by empirical insights and lack theoretical\nperformance guarantees. In this work, we develop a control-theoretic foundation\nfor activation steering by showing that popular steering methods correspond to\nthe proportional (P) controllers, with the steering vector serving as the\nfeedback signal. Building on this finding, we propose\nProportional-Integral-Derivative (PID) Steering, a principled framework that\nleverages the full PID controller for activation steering in LLMs. The\nproportional (P) term aligns activations with target semantic directions, the\nintegral (I) term accumulates errors to enforce persistent corrections across\nlayers, and the derivative (D) term mitigates overshoot by counteracting rapid\nactivation changes. This closed-loop design yields interpretable error dynamics\nand connects activation steering to classical stability guarantees in control\ntheory. Moreover, PID Steering is lightweight, modular, and readily integrates\nwith state-of-the-art steering methods. Extensive experiments across multiple\nLLM families and benchmarks demonstrate that PID Steering consistently\noutperforms existing approaches, achieving more robust and reliable behavioral\ncontrol.",
    "published": "2025-10-05T18:05:28Z",
    "updated": "2025-10-05T18:05:28Z",
    "link": "http://arxiv.org/pdf/2510.04309v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Dung V. Nguyen",
      "Hieu M. Vu",
      "Nhi Y. Pham",
      "Lei Zhang",
      "Tan M. Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01698v2",
    "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling",
    "summary": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems.",
    "published": "2025-10-02T06:08:54Z",
    "updated": "2025-10-06T16:03:03Z",
    "link": "http://arxiv.org/pdf/2510.01698v2.pdf",
    "category": [
      "cs.IR",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Seungheon Doh",
      "Keunwoo Choi",
      "Juhan Nam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04396v1",
    "title": "Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous\n  Collections",
    "summary": "Multimodal deep-learning models power interactive video retrieval by ranking\nkeyframes in response to textual queries. Despite these advances, users must\nstill browse ranked candidates manually to locate a target. Keyframe\narrangement within the search grid highly affects browsing effectiveness and\nuser efficiency, yet remains underexplored. We report a study with 49\nparticipants evaluating seven keyframe layouts for the Visual Known-Item Search\ntask. Beyond efficiency and accuracy, we relate browsing phenomena, such as\noverlooks, to layout characteristics. Our results show that a video-grouped\nlayout is the most efficient, while a four-column, rank-preserving grid\nachieves the highest accuracy. Sorted grids reveal potentials and trade-offs,\nenabling rapid scanning of uninteresting regions but down-ranking relevant\ntargets to less prominent positions, delaying first arrival times and\nincreasing overlooks.\n  These findings motivate hybrid designs that preserve positions of top-ranked\nitems while sorting or grouping the remainder, and offer guidance for searching\nin grids beyond video retrieval.",
    "published": "2025-10-05T23:30:33Z",
    "updated": "2025-10-05T23:30:33Z",
    "link": "http://arxiv.org/pdf/2510.04396v1.pdf",
    "category": [
      "cs.MM",
      "cs.IR",
      "H.3.3; H.5.2; H.5.1"
    ],
    "authors": [
      "Bastian Jäckl",
      "Jiří Kruchina",
      "Lucas Joos",
      "Daniel A. Keim",
      "Ladislav Peška",
      "Jakub Lokoč"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.06702v2",
    "title": "CHOICE: Coordinated Human-Object Interaction in Cluttered Environments\n  for Pick-and-Place Actions",
    "summary": "Animating human-scene interactions such as pick-and-place tasks in cluttered,\ncomplex layouts is a challenging task, with objects of a wide variation of\ngeometries and articulation under scenarios with various obstacles. The main\ndifficulty lies in the sparsity of the motion data compared to the wide\nvariation of the objects and environments as well as the poor availability of\ntransition motions between different tasks, increasing the complexity of the\ngeneralization to arbitrary conditions. To cope with this issue, we develop a\nsystem that tackles the interaction synthesis problem as a hierarchical\ngoal-driven task. Firstly, we develop a bimanual scheduler that plans a set of\nkeyframes for simultaneously controlling the two hands to efficiently achieve\nthe pick-and-place task from an abstract goal signal such as the target object\nselected by the user. Next, we develop a neural implicit planner that generates\nguidance hand trajectories under diverse object shape/types and obstacle\nlayouts. Finally, we propose a linear dynamic model for our DeepPhase\ncontroller that incorporates a Kalman filter to enable smooth transitions in\nthe frequency domain, resulting in a more realistic and effective\nmulti-objective control of the character.Our system can produce a wide range of\nnatural pick-and-place movements with respect to the geometry of objects, the\narticulation of containers and the layout of the objects in the scene.",
    "published": "2024-12-09T17:49:00Z",
    "updated": "2025-10-05T20:18:41Z",
    "link": "http://arxiv.org/pdf/2412.06702v2.pdf",
    "category": [
      "cs.GR",
      "cs.RO"
    ],
    "authors": [
      "Jintao Lu",
      "He Zhang",
      "Yuting Ye",
      "Takaaki Shiratori",
      "Sebastian Starke",
      "Taku Komura"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05061v1",
    "title": "Automaton Constrained Q-Learning",
    "summary": "Real-world robotic tasks often require agents to achieve sequences of goals\nwhile respecting time-varying safety constraints. However, standard\nReinforcement Learning (RL) paradigms are fundamentally limited in these\nsettings. A natural approach to these problems is to combine RL with\nLinear-time Temporal Logic (LTL), a formal language for specifying complex,\ntemporally extended tasks and safety constraints. Yet, existing RL methods for\nLTL objectives exhibit poor empirical performance in complex and continuous\nenvironments. As a result, no scalable methods support both temporally ordered\ngoals and safety simultaneously, making them ill-suited for realistic robotics\nscenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm\nthat addresses this gap by combining goal-conditioned value learning with\nautomaton-guided reinforcement. ACQL supports most LTL task specifications and\nleverages their automaton representation to explicitly encode stage-wise goal\nprogression and both stationary and non-stationary safety constraints. We show\nthat ACQL outperforms existing methods across a range of continuous control\ntasks, including cases where prior methods fail to satisfy either goal-reaching\nor safety constraints. We further validate its real-world applicability by\ndeploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a\ncluttered, cabinet-like space with safety constraints. Our results demonstrate\nthat ACQL is a robust and scalable solution for learning robotic behaviors\naccording to rich temporal specifications.",
    "published": "2025-10-06T17:38:05Z",
    "updated": "2025-10-06T17:38:05Z",
    "link": "http://arxiv.org/pdf/2510.05061v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Anastasios Manganaris",
      "Vittorio Giammarino",
      "Ahmed H. Qureshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01862v3",
    "title": "ReLI: A Language-Agnostic Approach to Human-Robot Interaction",
    "summary": "Adapting autonomous agents for real-world industrial, domestic, and other\ndaily tasks is currently gaining momentum. However, in global or cross-lingual\napplication contexts, ensuring effective interaction with the environment and\nexecuting unrestricted human-specified tasks regardless of the language remains\nan unsolved problem. To address this, we propose ReLI, a language-agnostic\napproach that enables autonomous agents to converse naturally, semantically\nreason about their environment, and perform downstream tasks, regardless of the\ntask instruction's modality or linguistic origin. First, we ground large-scale\npre-trained foundation models and transform them into language-to-action models\nthat can directly provide common-sense reasoning and high-level robot control\nthrough natural, free-flow conversational interactions. Further, we perform\ncross-lingual adaptation of the models to ensure that ReLI generalises across\nthe global languages. To demonstrate ReLI's robustness, we conducted extensive\nexperiments on various short- and long-horizon tasks, including zero- and\nfew-shot spatial navigation, scene information retrieval, and query-oriented\ntasks. We benchmarked the performance on $140$ languages involving $70K+$\nmulti-turn conversations. On average, ReLI achieved over $90\\%\\pm0.2$ accuracy\nin cross-lingual instruction parsing and task execution success. These results\ndemonstrate its potential to advance natural human-agent interaction in the\nreal world while championing inclusive and linguistic diversity. Demos and\nresources will be public at: https://linusnep.github.io/ReLI/.",
    "published": "2025-05-03T16:48:05Z",
    "updated": "2025-10-06T17:09:04Z",
    "link": "http://arxiv.org/pdf/2505.01862v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Linus Nwankwo",
      "Bjoern Ellensohn",
      "Ozan Özdenizci",
      "Elmar Rueckert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05001v1",
    "title": "Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a\n  TARS-Inspired Robot",
    "summary": "Robotic locomotion research typically draws from biologically inspired leg\ndesigns, yet many human-engineered settings can benefit from\nnon-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from\nInterstellar into a 0.25 m, 0.99 kg research platform with seven actuated\ndegrees of freedom. The film shows two primary gaits: a bipedal-like walk and a\nhigh-speed rolling mode. For TARS3D, we build reduced-order models for each,\nderive closed-form limit-cycle conditions, and validate the predictions on\nhardware. Experiments confirm that the robot respects its +/-150 degree hip\nlimits, alternates left-right contacts without interference, and maintains an\neight-step hybrid limit cycle in rolling mode. Because each telescopic leg\nprovides four contact corners, the rolling gait is modeled as an eight-spoke\ndouble rimless wheel. The robot's telescopic leg redundancy implies a far\nricher gait repertoire than the two limit cycles treated analytically. So, we\nused deep reinforcement learning (DRL) in simulation to search the unexplored\nspace. We observed that the learned policy can recover the analytic gaits under\nthe right priors and discover novel behaviors as well. Our findings show that\nTARS3D's fiction-inspired bio-transcending morphology can realize multiple\npreviously unexplored locomotion modes and that further learning-driven search\nis likely to reveal more. This combination of analytic synthesis and\nreinforcement learning opens a promising pathway for multimodal robotics.",
    "published": "2025-10-06T16:39:25Z",
    "updated": "2025-10-06T16:39:25Z",
    "link": "http://arxiv.org/pdf/2510.05001v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Aditya Sripada",
      "Abhishek Warrier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04991v1",
    "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language\n  Models",
    "summary": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average.",
    "published": "2025-10-06T16:26:16Z",
    "updated": "2025-10-06T16:26:16Z",
    "link": "http://arxiv.org/pdf/2510.04991v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "D. Schwartz",
      "K. Kondo",
      "J. P. How"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23651v2",
    "title": "HeLoM: Hierarchical Learning for Whole-Body Loco-Manipulation in Hexapod\n  Robot",
    "summary": "Robots in real-world environments are often required to move/manipulate\nobjects comparable in weight to their own bodies. Compared to grasping and\ncarrying, pushing provides a more straightforward and efficient non-prehensile\nmanipulation strategy, avoiding complex grasp design while leveraging direct\ncontact to regulate an object's pose. Achieving effective pushing, however,\ndemands both sufficient manipulation forces and the ability to maintain\nstability, which is particularly challenging when dealing with heavy or\nirregular objects. To address these challenges, we propose HeLoM, a\nlearning-based hierarchical whole-body manipulation framework for a hexapod\nrobot that exploits coordinated multi-limb control. Inspired by the cooperative\nstrategies of multi-legged insects, our framework leverages redundant contact\npoints and high degrees of freedom to enable dynamic redistribution of contact\nforces. HeLoM's high-level planner plans pushing behaviors and target object\nposes, while its low-level controller maintains locomotion stability and\ngenerates dynamically consistent joint actions. Our policies trained in\nsimulation are directly deployed on real robots without additional fine-tuning.\nThis design allows the robot to maintain balance while exerting continuous and\ncontrollable pushing forces through coordinated foreleg interaction and\nsupportive hind-leg propulsion. We validate the effectiveness of HeLoM through\nboth simulation and real-world experiments. Results show that our framework can\nstably push boxes of varying sizes and unknown physical properties to\ndesignated goal poses in the real world.",
    "published": "2025-09-28T05:34:39Z",
    "updated": "2025-10-06T16:00:38Z",
    "link": "http://arxiv.org/pdf/2509.23651v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xinrong Yang",
      "Peizhuo Li",
      "Hongyi Li",
      "Junkai Lu",
      "Linnan Chang",
      "Yuhong Cao",
      "Yifeng Zhang",
      "Ge Sun",
      "Guillaume Sartoretti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23650v2",
    "title": "KiVi: Kinesthetic-Visuospatial Integration for Dynamic and Safe\n  Egocentric Legged Locomotion",
    "summary": "Vision-based locomotion has shown great promise in enabling legged robots to\nperceive and adapt to complex environments. However, visual information is\ninherently fragile, being vulnerable to occlusions, reflections, and lighting\nchanges, which often cause instability in locomotion. Inspired by animal\nsensorimotor integration, we propose KiVi, a Kinesthetic-Visuospatial\nintegration framework, where kinesthetics encodes proprioceptive sensing of\nbody motion and visuospatial reasoning captures visual perception of\nsurrounding terrain. Specifically, KiVi separates these pathways, leveraging\nproprioception as a stable backbone while selectively incorporating vision for\nterrain awareness and obstacle avoidance. This modality-balanced, yet\nintegrative design, combined with memory-enhanced attention, allows the robot\nto robustly interpret visual cues while maintaining fallback stability through\nproprioception. Extensive experiments show that our method enables quadruped\nrobots to stably traverse diverse terrains and operate reliably in unstructured\noutdoor environments, remaining robust to out-of-distribution (OOD) visual\nnoise and occlusion unseen during training, thereby highlighting its\neffectiveness and applicability to real-world legged locomotion.",
    "published": "2025-09-28T05:31:07Z",
    "updated": "2025-10-06T15:09:33Z",
    "link": "http://arxiv.org/pdf/2509.23650v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Peizhuo Li",
      "Hongyi Li",
      "Yuxuan Ma",
      "Linnan Chang",
      "Xinrong Yang",
      "Ruiqi Yu",
      "Yifeng Zhang",
      "Yuhong Cao",
      "Qiuguo Zhu",
      "Guillaume Sartoretti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04839v1",
    "title": "TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and\n  Performant Online Inertial Parameter Estimation",
    "summary": "Accurate online inertial parameter estimation is essential for adaptive\nrobotic control, enabling real-time adjustment to payload changes,\nenvironmental interactions, and system wear. Traditional methods such as\nRecursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to\ntrack abrupt parameter shifts or incur high computational costs, limiting their\neffectiveness in dynamic environments and for computationally constrained\nrobotic systems. As such, we introduce TAG-K, a lightweight extension of the\nKaczmarz method that combines greedy randomized row selection for rapid\nconvergence with tail averaging for robustness under noise and inconsistency.\nThis design enables fast, stable parameter adaptation while retaining the low\nper-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K\nin synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other\nKaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class\nCPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More\nimportantly, these speedups are paired with improved resilience to measurement\nnoise and a 25% reduction in estimation error, leading to nearly 2x better\nend-to-end tracking performance.",
    "published": "2025-10-06T14:25:01Z",
    "updated": "2025-10-06T14:25:01Z",
    "link": "http://arxiv.org/pdf/2510.04839v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shuo Sha",
      "Anupam Bhakta",
      "Zhenyuan Jiang",
      "Kevin Qiu",
      "Ishaan Mahajan",
      "Gabriel Bravo",
      "Brian Plancher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04807v1",
    "title": "Efficient Probabilistic Planning with Maximum-Coverage Distributionally\n  Robust Backward Reachable Trees",
    "summary": "This paper presents a new multi-query motion planning algorithm for linear\nGaussian systems with the goal of reaching a Euclidean ball with high\nprobability. We develop a new formulation for ball-shaped ambiguity sets of\nGaussian distributions and leverage it to develop a distributionally robust\nbelief roadmap construction algorithm. This algorithm synthe- sizes robust\ncontrollers which are certified to be safe for maximal size ball-shaped\nambiguity sets of Gaussian distributions. Our algorithm achieves better\ncoverage than the maximal coverage algorithm for planning over Gaussian\ndistributions [1], and we identify mild conditions under which our algorithm\nachieves strictly better coverage. For the special case of no process noise or\nstate constraints, we formally prove that our algorithm achieves maximal\ncoverage. In addition, we present a second multi-query motion planning\nalgorithm for linear Gaussian systems with the goal of reaching a region\nparameterized by the Minkowski sum of an ellipsoid and a Euclidean ball with\nhigh probability. This algorithm plans over ellipsoidal sets of maximal size\nball-shaped ambiguity sets of Gaussian distributions, and provably achieves\nequal or better coverage than the best-known algorithm for planning over\nellipsoidal ambiguity sets of Gaussian distributions [2]. We demonstrate the\nefficacy of both methods in a wide range of conditions via extensive simulation\nexperiments.",
    "published": "2025-10-06T13:46:55Z",
    "updated": "2025-10-06T13:46:55Z",
    "link": "http://arxiv.org/pdf/2510.04807v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Alex Rose",
      "Naman Aggarwal",
      "Christopher Jewison",
      "Jonathan P. How"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04724v1",
    "title": "Performance-guided Task-specific Optimization for Multirotor Design",
    "summary": "This paper introduces a methodology for task-specific design optimization of\nmultirotor Micro Aerial Vehicles. By leveraging reinforcement learning,\nBayesian optimization, and covariance matrix adaptation evolution strategy, we\noptimize aerial robot designs guided exclusively by their closed-loop\nperformance in a considered task. Our approach systematically explores the\ndesign space of motor pose configurations while ensuring manufacturability\nconstraints and minimal aerodynamic interference. Results demonstrate that\noptimized designs achieve superior performance compared to conventional\nmultirotor configurations in agile waypoint navigation tasks, including against\nfully actuated designs from the literature. We build and test one of the\noptimized designs in the real world to validate the sim2real transferability of\nour approach.",
    "published": "2025-10-06T11:45:23Z",
    "updated": "2025-10-06T11:45:23Z",
    "link": "http://arxiv.org/pdf/2510.04724v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Etor Arza",
      "Welf Rehberg",
      "Philipp Weiss",
      "Mihir Kulkarni",
      "Kostas Alexis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04696v1",
    "title": "Building Gradient by Gradient: Decentralised Energy Functions for\n  Bimanual Robot Assembly",
    "summary": "There are many challenges in bimanual assembly, including high-level\nsequencing, multi-robot coordination, and low-level, contact-rich operations\nsuch as component mating. Task and motion planning (TAMP) methods, while\neffective in this domain, may be prohibitively slow to converge when adapting\nto disturbances that require new task sequencing and optimisation. These events\nare common during tight-tolerance assembly, where difficult-to-model dynamics\nsuch as friction or deformation require rapid replanning and reattempts.\nMoreover, defining explicit task sequences for assembly can be cumbersome,\nlimiting flexibility when task replanning is required. To simplify this\nplanning, we introduce a decentralised gradient-based framework that uses a\npiecewise continuous energy function through the automatic composition of\nadaptive potential functions. This approach generates sub-goals using only\nmyopic optimisation, rather than long-horizon planning. It demonstrates\neffectiveness at solving long-horizon tasks due to the structure and adaptivity\nof the energy function. We show that our approach scales to physical bimanual\nassembly tasks for constructing tight-tolerance assemblies. In these\nexperiments, we discover that our gradient-based rapid replanning framework\ngenerates automatic retries, coordinated motions and autonomous handovers in an\nemergent fashion.",
    "published": "2025-10-06T11:10:11Z",
    "updated": "2025-10-06T11:10:11Z",
    "link": "http://arxiv.org/pdf/2510.04696v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Alexander L. Mitchell",
      "Joe Watson",
      "Ingmar Posner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04666v1",
    "title": "Learning a Shape-adaptive Assist-as-needed Rehabilitation Policy from\n  Therapist-informed Input",
    "summary": "Therapist-in-the-loop robotic rehabilitation has shown great promise in\nenhancing rehabilitation outcomes by integrating the strengths of therapists\nand robotic systems. However, its broader adoption remains limited due to\ninsufficient safe interaction and limited adaptation capability. This article\nproposes a novel telerobotics-mediated framework that enables therapists to\nintuitively and safely deliver assist-as-needed~(AAN) therapy based on two\nprimary contributions. First, our framework encodes the therapist-informed\ncorrective force into via-points in a latent space, allowing the therapist to\nprovide only minimal assistance while encouraging patient maintaining own\nmotion preferences. Second, a shape-adaptive ANN rehabilitation policy is\nlearned to partially and progressively deform the reference trajectory for\nmovement therapy based on encoded patient motion preferences and\ntherapist-informed via-points. The effectiveness of the proposed shape-adaptive\nAAN strategy was validated on a telerobotic rehabilitation system using two\nrepresentative tasks. The results demonstrate its practicality for remote AAN\ntherapy and its superiority over two state-of-the-art methods in reducing\ncorrective force and improving movement smoothness.",
    "published": "2025-10-06T10:21:00Z",
    "updated": "2025-10-06T10:21:00Z",
    "link": "http://arxiv.org/pdf/2510.04666v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Zhimin Hou",
      "Jiacheng Hou",
      "Xiao Chen",
      "Hamid Sadeghian",
      "Tianyu Ren",
      "Sami Haddadin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00770v2",
    "title": "Tele-rehabilitation with online skill transfer and adaptation in\n  $\\mathbb{R}^3 \\times \\mathit{S}^3$",
    "summary": "This paper proposes a tele-teaching framework for the domain of\nrobot-assisted tele-rehabilitation. The system connects two robotic\nmanipulators on therapist and patient side via bilateral teleoperation,\nenabling a therapist to remotely demonstrate rehabilitation exercises that are\nexecuted by the patient-side robot. A 6-DoF Dynamical Movement Primitives\nformulation is employed to jointly encode translational and rotational motions\nin $\\mathbb{R}^3 \\times \\mathit{S}^3$ space, ensuring accurate trajectory\nreproduction. The framework supports smooth transitions between therapist-led\nguidance and patient passive training, while allowing adaptive adjustment of\nmotion. Experiments with 7-DoF manipulators demonstrate the feasibility of the\napproach, highlighting its potential for personalized and remotely supervised\nrehabilitation.",
    "published": "2025-10-01T11:02:25Z",
    "updated": "2025-10-06T10:19:57Z",
    "link": "http://arxiv.org/pdf/2510.00770v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Tianle Ni",
      "Xiao Chen",
      "Hamid Sadeghian",
      "Sami Haddadin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06787v4",
    "title": "Digital-physical testbed for ship autonomy studies in the Marine\n  Cybernetics Laboratory basin",
    "summary": "The algorithms developed for Maritime Autonomous Surface Ships (MASS) are\noften challenging to test on actual vessels due to high operational costs and\nsafety considerations. Simulations offer a cost-effective alternative and\neliminate risks, but they may not accurately represent real-world dynamics for\nthe given tasks. Utilizing small-scale model ships and robotic vessels in\nconjunction with a laboratory basin provides an accessible testing environment\nfor the early stages of validation processes. However, designing and developing\na model vessel for a single test can be costly and cumbersome, and researchers\noften lack access to such infrastructure. To address these challenges and\nenable streamlined testing, we have developed an in-house testbed that\nfacilitates the development, testing, verification, and validation of MASS\nalgorithms in a digital-physical laboratory. This infrastructure includes a set\nof small-scale model vessels, a simulation environment for each vessel, a\ncomprehensive testbed environment, and a digital twin in Unity. With this, we\naim to establish a full design and verification pipeline that starts with\nhigh-fidelity simulation models of each model vessel, to the model-scale\ntesting in the laboratory basin, allowing possibilities for moving towards\nsemi-fullscale validation with R/V milliAmpere1 and full-scale validation with\nR/V Gunnerus. In this work, we present our progress on the development of this\ntestbed environment and its components, demonstrating its effectiveness in\nenabling ship guidance, navigation, and control (GNC), including autonomy.",
    "published": "2025-05-10T23:48:32Z",
    "updated": "2025-10-06T10:16:42Z",
    "link": "http://arxiv.org/pdf/2505.06787v4.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Emir Cem Gezer",
      "Mael Korentin Ivan Moreau",
      "Anders Sandneseng Høgden",
      "Dong Trong Nguyen",
      "Roger Skjetne",
      "Asgeir Sørensen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04612v1",
    "title": "OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with\n  Dense Depth or LiDAR, and GNSS",
    "summary": "To empower mobile robots with usable maps as well as highest state estimation\naccuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor\nSimultaneous Localization and Mapping (SLAM) system building dense volumetric\noccupancy maps, while scalable to large environments and operating in realtime.\nOur unified SLAM framework seamlessly integrates different sensor modalities:\nvisual, inertial, measured or learned depth, LiDAR and Global Navigation\nSatellite System (GNSS) measurements. Unlike most state-of-the-art SLAM\nsystems, we advocate using dense volumetric map representations when leveraging\ndepth or range-sensing capabilities. We employ an efficient submapping strategy\nthat allows our system to scale to large environments, showcased in sequences\nof up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by\ntightly-coupling the estimator and submaps through map alignment factors. Our\nsystem provides globally consistent maps, directly usable for autonomous\nnavigation. To further improve the accuracy of OKVIS2-X, we also incorporate\nthe option of performing online calibration of camera extrinsics. Our system\nachieves the highest trajectory accuracy in EuRoC against state-of-the-art\nalternatives, outperforms all competitors in the Hilti22 VI-only benchmark,\nwhile also proving competitive in the LiDAR version, and showcases state of the\nart accuracy in the diverse and large-scale sequences from the VBR dataset.",
    "published": "2025-10-06T09:23:36Z",
    "updated": "2025-10-06T09:23:36Z",
    "link": "http://arxiv.org/pdf/2510.04612v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Simon Boche",
      "Jaehyung Jung",
      "Sebastián Barbas Laina",
      "Stefan Leutenegger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22955v2",
    "title": "A Hierarchical Control Architecture for Space Robots in On-Orbit\n  Servicing Operations",
    "summary": "In-Orbit Servicing and Active Debris Removal require advanced robotic\ncapabilities for capturing and detumbling uncooperative targets. This work\npresents a hierarchical control framework for autonomous robotic capture of\ntumbling objects in space. A simulation environment is developed, incorporating\nsloshing dynamics of the chaser, a rarely studied effect in space robotics. The\nproposed controller combines an inner Lyapunov-based robust control loop for\nmulti-body dynamics with an outer loop addressing an extended inverse\nkinematics problem. Simulation results show improved robustness and\nadaptability compared to existing control schemes.",
    "published": "2025-09-26T21:36:00Z",
    "updated": "2025-10-06T09:00:36Z",
    "link": "http://arxiv.org/pdf/2509.22955v2.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Pietro Bruschi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04592v1",
    "title": "MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile\n  Manipulation",
    "summary": "Recent advances in robotics have been largely driven by imitation learning,\nwhich depends critically on large-scale, high-quality demonstration data.\nHowever, collecting such data remains a significant challenge-particularly for\nmobile manipulators, which must coordinate base locomotion and arm manipulation\nin high-dimensional, dynamic, and partially observable environments.\nConsequently, most existing research remains focused on simpler tabletop\nscenarios, leaving mobile manipulation relatively underexplored. To bridge this\ngap, we present \\textit{MobRT}, a digital twin-based framework designed to\nsimulate two primary categories of complex, whole-body tasks: interaction with\narticulated objects (e.g., opening doors and drawers) and mobile-base\npick-and-place operations. \\textit{MobRT} autonomously generates diverse and\nrealistic demonstrations through the integration of virtual kinematic control\nand whole-body motion planning, enabling coherent and physically consistent\nexecution. We evaluate the quality of \\textit{MobRT}-generated data across\nmultiple baseline algorithms, establishing a comprehensive benchmark and\ndemonstrating a strong correlation between task success and the number of\ngenerated trajectories. Experiments integrating both simulated and real-world\ndemonstrations confirm that our approach markedly improves policy\ngeneralization and performance, achieving robust results in both simulated and\nreal-world environments.",
    "published": "2025-10-06T08:46:56Z",
    "updated": "2025-10-06T08:46:56Z",
    "link": "http://arxiv.org/pdf/2510.04592v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yilin Mei",
      "Peng Qiu",
      "Wei Zhang",
      "WenChao Zhang",
      "Wenjie Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04585v1",
    "title": "Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic\n  Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation",
    "summary": "Grasping objects across vastly different sizes and physical states-including\nboth solids and liquids-with a single robotic gripper remains a fundamental\nchallenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a\nsoft end-effector that synergistically integrates distributed surface suction\nwith internal granular jamming, enabling cross-scale and cross-state\nmanipulation without requiring airtight sealing at the contact interface with\ntarget objects. The EG Gripper can handle objects with surface areas ranging\nfrom sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized\npaper and woven bag), enabling manipulation of objects nearly 3,500X smaller\nand 88X larger than its own contact area (approximated at 707 mm2 for a 30\nmm-diameter base). We further introduce a tactile sensing framework that\ncombines liquid detection and pressure-based suction feedback, enabling\nreal-time differentiation between solid and liquid targets. Guided by the\nactile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper\nautonomously selects grasping modes based on distributed pressure and voltage\nsignals. Experiments across diverse tasks-including underwater grasping,\nfragile object handling, and liquid capture-demonstrate robust and repeatable\nperformance. To our knowledge, this is the first soft gripper to reliably grasp\nboth solid and liquid objects across scales using a unified compliant\narchitecture.",
    "published": "2025-10-06T08:37:40Z",
    "updated": "2025-10-06T08:37:40Z",
    "link": "http://arxiv.org/pdf/2510.04585v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jianshu Zhou",
      "Jing Shu",
      "Tianle Pan",
      "Puchen Zhu",
      "Jiajun An",
      "Huayu Zhang",
      "Junda Huang",
      "Upinder Kaur",
      "Xin Ma",
      "Masayoshi Tomizuka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.06122v2",
    "title": "NDOB-Based Control of a UAV with Delta-Arm Considering Manipulator\n  Dynamics",
    "summary": "Aerial Manipulators (AMs) provide a versatile platform for various\napplications, including 3D printing, architecture, and aerial grasping\nmissions. However, their operational speed is often sacrificed to uphold\nprecision. Existing control strategies for AMs often regard the manipulator as\na disturbance and employ robust control methods to mitigate its influence. This\nresearch focuses on elevating the precision of the end-effector and enhancing\nthe agility of aerial manipulator movements. We present a composite control\nscheme to address these challenges. Initially, a Nonlinear Disturbance Observer\n(NDOB) is utilized to compensate for internal coupling effects and external\ndisturbances. Subsequently, manipulator dynamics are processed through a high\npass filter to facilitate agile movements. By integrating the proposed control\nmethod into a fully autonomous delta-arm-based AM system, we substantiate the\ncontroller's efficacy through extensive real-world experiments. The outcomes\nillustrate that the end-effector can achieve accuracy at the millimeter level.",
    "published": "2025-01-10T17:21:04Z",
    "updated": "2025-10-06T07:28:07Z",
    "link": "http://arxiv.org/pdf/2501.06122v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hongming Chen",
      "Biyu Ye",
      "Xianqi Liang",
      "Weiliang Deng",
      "Ximin Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14712v2",
    "title": "BiDexHand: Design and Evaluation of an Open-Source 16-DoF Biomimetic\n  Dexterous Hand",
    "summary": "Achieving human-level dexterity in robotic hands remains a fundamental\nchallenge for enabling versatile manipulation across diverse applications. This\nextended abstract presents BiDexHand, a cable-driven biomimetic robotic hand\nthat combines human-like dexterity with accessible and efficient mechanical\ndesign. The robotic hand features 16 independently actuated degrees of freedom\nand 5 mechanically coupled joints through novel phalange designs that replicate\nnatural finger motion. Performance validation demonstrated success across all\n33 grasp types in the GRASP Taxonomy, 9 of 11 positions in the Kapandji thumb\nopposition test, a measured fingertip force of 2.14\\,N, and the capability to\nlift a 10\\,lb weight. As an open-source platform supporting multiple control\nmodes including vision-based teleoperation, BiDexHand aims to democratize\naccess to advanced manipulation capabilities for the broader robotics research\ncommunity.",
    "published": "2025-04-20T18:56:20Z",
    "updated": "2025-10-06T06:30:03Z",
    "link": "http://arxiv.org/pdf/2504.14712v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zhengyang Kris Weng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04509v1",
    "title": "Velocity-Form Data-Enabled Predictive Control of Soft Robots under\n  Unknown External Payloads",
    "summary": "Data-driven control methods such as data-enabled predictive control (DeePC)\nhave shown strong potential in efficient control of soft robots without\nexplicit parametric models. However, in object manipulation tasks, unknown\nexternal payloads and disturbances can significantly alter the system dynamics\nand behavior, leading to offset error and degraded control performance. In this\npaper, we present a novel velocity-form DeePC framework that achieves robust\nand optimal control of soft robots under unknown payloads. The proposed\nframework leverages input-output data in an incremental representation to\nmitigate performance degradation induced by unknown payloads, eliminating the\nneed for weighted datasets or disturbance estimators. We validate the method\nexperimentally on a planar soft robot and demonstrate its superior performance\ncompared to standard DeePC in scenarios involving unknown payloads.",
    "published": "2025-10-06T05:57:55Z",
    "updated": "2025-10-06T05:57:55Z",
    "link": "http://arxiv.org/pdf/2510.04509v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Huanqing Wang",
      "Kaixiang Zhang",
      "Kyungjoon Lee",
      "Yu Mei",
      "Vaibhav Srivastava",
      "Jun Sheng",
      "Ziyou Song",
      "Zhaojian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.00893v4",
    "title": "ToddlerBot: Open-Source ML-Compatible Humanoid Platform for\n  Loco-Manipulation",
    "summary": "Learning-based robotics research driven by data demands a new approach to\nrobot hardware design-one that serves as both a platform for policy execution\nand a tool for embodied data collection to train policies. We introduce\nToddlerBot, a low-cost, open-source humanoid robot platform designed for\nscalable policy learning and research in robotics and AI. ToddlerBot enables\nseamless acquisition of high-quality simulation and real-world data. The\nplug-and-play zero-point calibration and transferable motor system\nidentification ensure a high-fidelity digital twin, enabling zero-shot policy\ntransfer from simulation to the real world. A user-friendly teleoperation\ninterface facilitates streamlined real-world data collection for learning motor\nskills from human demonstrations. Utilizing its data collection ability and\nanthropomorphic design, ToddlerBot is an ideal platform to perform whole-body\nloco-manipulation. Additionally, ToddlerBot's compact size (0.56m, 3.4kg)\nensures safe operation in real-world environments. Reproducibility is achieved\nwith an entirely 3D-printed, open-source design and commercially available\ncomponents, keeping the total cost under 6,000 USD. Comprehensive documentation\nallows assembly and maintenance with basic technical expertise, as validated by\na successful independent replication of the system. We demonstrate ToddlerBot's\ncapabilities through arm span, payload, endurance tests, loco-manipulation\ntasks, and a collaborative long-horizon scenario where two robots tidy a toy\nsession together. By advancing ML-compatibility, capability, and\nreproducibility, ToddlerBot provides a robust platform for scalable learning\nand dynamic policy execution in robotics research.",
    "published": "2025-02-02T20:05:32Z",
    "updated": "2025-10-06T04:52:40Z",
    "link": "http://arxiv.org/pdf/2502.00893v4.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Haochen Shi",
      "Weizhuo Wang",
      "Shuran Song",
      "C. Karen Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23075v2",
    "title": "In-Hand Manipulation of Articulated Tools with Dexterous Robot Hands\n  with Sim-to-Real Transfer",
    "summary": "Reinforcement learning (RL) and sim-to-real transfer have advanced robotic\nmanipulation of rigid objects. Yet, policies remain brittle when applied to\narticulated mechanisms due to contact-rich dynamics and under-modeled joint\nphenomena such as friction, stiction, backlash, and clearances. We address this\nchallenge through dexterous in-hand manipulation of articulated tools using a\nrobotic hand with reduced articulation and kinematic redundancy relative to the\nhuman hand. Our controller augments a simulation-trained base policy with a\nsensor-driven refinement learned from hardware demonstrations, conditioning on\nproprioception and target articulation states while fusing whole-hand tactile\nand force feedback with the policy's internal action intent via\ncross-attention-based integration. This design enables online adaptation to\ninstance-specific articulation properties, stabilizes contact interactions,\nregulates internal forces, and coordinates coupled-link motion under\nperturbations. We validate our approach across a diversity of real-world\nexamples, including scissors, pliers, minimally invasive surgical tools, and\nstaplers. We achieve robust transfer from simulation to hardware, improved\ndisturbance resilience, and generalization to previously unseen articulated\ntools, thereby reducing reliance on precise physical modeling in contact-rich\nsettings.",
    "published": "2025-09-27T02:56:57Z",
    "updated": "2025-10-06T03:40:13Z",
    "link": "http://arxiv.org/pdf/2509.23075v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Soofiyan Atar",
      "Daniel Huang",
      "Florian Richter",
      "Michael Yip"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.18149v2",
    "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
    "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
    "published": "2024-03-26T23:17:05Z",
    "updated": "2025-10-06T02:46:01Z",
    "link": "http://arxiv.org/pdf/2403.18149v2.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "authors": [
      "Ishaan Mahajan",
      "Khai Nguyen",
      "Sam Schoedel",
      "Elakhya Nedumaran",
      "Moises Mata",
      "Brian Plancher",
      "Zachary Manchester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.11829v2",
    "title": "Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent\n  Collaborative Field Coverage",
    "summary": "Multi-agent reinforcement learning is a challenging and active field of\nresearch due to the inherent nonstationary property and coupling between\nagents. A popular approach to modeling the multi-agent interactions underlying\nthe multi-agent RL problem is the Markov Game. There is a special type of\nMarkov Game, termed Markov Potential Game, which allows us to reduce the Markov\nGame to a single-objective optimal control problem where the objective function\nis a potential function. In this work, we prove that a multi-agent\ncollaborative field coverage problem, which is found in many engineering\napplications, can be formulated as a Markov Potential Game, and we can learn a\nparameterized closed-loop Nash Equilibrium by solving an equivalent\nsingle-objective optimal control problem. As a result, our algorithm is 10x\nfaster during training compared to a game-theoretic baseline and converges\nfaster during policy execution.",
    "published": "2025-03-14T19:46:37Z",
    "updated": "2025-10-06T02:23:10Z",
    "link": "http://arxiv.org/pdf/2503.11829v2.pdf",
    "category": [
      "cs.MA",
      "cs.GT",
      "cs.RO"
    ],
    "authors": [
      "Jushan Chen",
      "Santiago Paternain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04436v1",
    "title": "PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory\n  Optimization",
    "summary": "Recently, diffusion models have gained popularity and attention in trajectory\noptimization due to their capability of modeling multi-modal probability\ndistributions. However, addressing nonlinear equality constraints, i.e, dynamic\nfeasi- bility, remains a great challenge in diffusion-based trajectory\noptimization. Recent diffusion-based trajectory optimization frameworks rely on\na single-shooting style approach where the denoised control sequence is applied\nto forward propagate the dynamical system, which cannot explicitly enforce\nconstraints on the states and frequently leads to sub-optimal solutions. In\nthis work, we propose a novel direct trajectory optimization approach via\nmodel-based diffusion, which directly generates a sequence of states. To ensure\ndynamic feasibility, we propose a gradient-free projection mechanism that is\nincorporated into the reverse diffusion process. Our results show that,\ncompared to a recent state-of-the-art baseline, our approach leads to zero\ndynamic feasibility error and approximately 4x higher success rate in a\nquadrotor waypoint navigation scenario involving dense static obstacles.",
    "published": "2025-10-06T02:06:58Z",
    "updated": "2025-10-06T02:06:58Z",
    "link": "http://arxiv.org/pdf/2510.04436v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Jushan Chen",
      "Santiago Paternain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18455v2",
    "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous\n  Hands",
    "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.",
    "published": "2025-09-22T22:25:35Z",
    "updated": "2025-10-05T21:46:13Z",
    "link": "http://arxiv.org/pdf/2509.18455v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yunshuang Li",
      "Yiyang Ling",
      "Gaurav S. Sukhatme",
      "Daniel Seita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2307.11622v2",
    "title": "A Benchmarking Study of Vision-Based Robotic Grasping Algorithms: A\n  Comparative Analysis",
    "summary": "We present a benchmarking study of vision-based robotic grasping algorithms\nand provide a comparative analysis. In particular, we compare two\nmachine-learning-based and two analytical algorithms using an existing\nbenchmarking protocol from the literature and determine the algorithms\nstrengths and weaknesses under different experimental conditions. These\nconditions include variations in lighting, background textures, cameras with\ndifferent noise levels, and grippers. We also run analogous experiments in\nsimulations and with real robots and present the discrepancies. Some\nexperiments are also run in two different laboratories using the same protocols\nto further analyze the repeatability of our results. We believe that this\nstudy, comprising 5040 experiments, provides important insights into the role\nand challenges of systematic experimentation in robotic manipulation and guides\nthe development of new algorithms by considering the factors that could impact\nthe performance. The experiment recordings and our benchmarking software are\npublicly available.",
    "published": "2023-07-21T14:41:19Z",
    "updated": "2025-10-05T20:53:38Z",
    "link": "http://arxiv.org/pdf/2307.11622v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Bharath K Rameshbabu",
      "Sumukh S Balakrishna",
      "Brian Flynn",
      "Vinayak Kapoor",
      "Adam Norton",
      "Holly Yanco",
      "Berk Calli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04353v1",
    "title": "Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation",
    "summary": "Teleoperation is a powerful method to generate reference motions and enable\nhumanoid robots to perform a broad range of tasks. However, teleoperation\nbecomes challenging when using hand contacts and non-coplanar surfaces, often\nleading to motor torque saturation or loss of stability through slipping. We\npropose a centroidal stability-based retargeting method that dynamically\nadjusts contact points and posture during teleoperation to enhance stability in\nthese difficult scenarios. Central to our approach is an efficient analytical\ncalculation of the stability margin gradient. This gradient is used to identify\nscenarios for which stability is highly sensitive to teleoperation setpoints\nand inform the local adjustment of these setpoints. We validate the framework\nin simulation and hardware by teleoperating manipulation tasks on a humanoid,\ndemonstrating increased stability margins. We also demonstrate empirically that\nhigher stability margins correlate with improved impulse resilience and joint\ntorque margin.",
    "published": "2025-10-05T20:33:21Z",
    "updated": "2025-10-05T20:33:21Z",
    "link": "http://arxiv.org/pdf/2510.04353v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Stephen McCrory",
      "Romeo Orsolino",
      "Dhruv Thanki",
      "Luigi Penco",
      "Robert Griffin"
    ]
  }
]