[
  {
    "id": "http://arxiv.org/abs/2509.19646v1",
    "title": "Where 6G Stands Today: Evolution, Enablers, and Research Gaps",
    "summary": "As the fifth-generation (5G) mobile communication system continues its global\ndeployment, both industry and academia have started conceptualizing the 6th\ngeneration (6G) to address the growing need for a progressively advanced and\ndigital society. Even while 5G offers considerable advancements over LTE, it\ncould struggle to be sufficient to meet all of the requirements, including\nultra-high reliability, seamless automation, and ubiquitous coverage. In\nresponse, 6G is supposed to bring out a highly intelligent, automated, and\nultra-reliable communication system that can handle a vast number of connected\ndevices. This paper offers a comprehensive overview of 6G, beginning with its\nmain stringent requirements while focusing on key enabling technologies such as\nterahertz (THz) communications, intelligent reflecting surfaces, massive MIMO\nand AI-driven networking that will shape the 6G networks. Furthermore, the\npaper lists various 6G applications and usage scenarios that will benefit from\nthese advancements. At the end, we outline the potential challenges that must\nbe addressed to achieve the 6G promises.",
    "published": "2025-09-23T23:52:47Z",
    "link": "http://arxiv.org/pdf/2509.19646v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Salma Tika",
      "Abdelkrim Haqiq",
      "Essaid Sabir",
      "Elmahdi Driouch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19645v1",
    "title": "Are We Scaling the Right Thing? A System Perspective on Test-Time\n  Scaling",
    "summary": "Test-time scaling (TTS) has recently emerged as a promising direction to\nexploit the hidden reasoning capabilities of pre-trained large language models\n(LLMs). However, existing scaling methods narrowly focus on the compute-optimal\nPareto-frontier, ignoring the simple fact that compute-optimal is not always\nsystem-optimal. In this work, we propose a system-driven perspective on TTS,\nanalyzing how reasoning models scale against practical metrics, such as latency\nand cost-per-token. By evaluating the impact of popular optimizations such as\ntensor parallelism and speculative decoding, our preliminary analysis reveals\nthe limitations of current methods and calls for a paradigm shift toward\nholistic, system-aware evaluations that capture the true essence of scaling\nlaws at inference time.",
    "published": "2025-09-23T23:52:07Z",
    "link": "http://arxiv.org/pdf/2509.19645v1.pdf",
    "category": [
      "cs.PF",
      "cs.AI"
    ],
    "authors": [
      "Youpeng Zhao",
      "Jinpeng LV",
      "Di Wu",
      "Jun Wang",
      "Christopher Gooley"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19633v1",
    "title": "Mamba Modulation: On the Length Generalization of Mamba",
    "summary": "The quadratic complexity of the attention mechanism in Transformer models has\nmotivated the development of alternative architectures with sub-quadratic\nscaling, such as state-space models. Among these, Mamba has emerged as a\nleading architecture, achieving state-of-the-art results across a range of\nlanguage modeling tasks. However, Mamba's performance significantly\ndeteriorates when applied to contexts longer than those seen during\npre-training, revealing a sharp sensitivity to context length extension.\nThrough detailed analysis, we attribute this limitation to the\nout-of-distribution behaviour of its state-space dynamics, particularly within\nthe parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent\nworks which attribute this sensitivity to the vanished accumulation of\ndiscretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a\nconnection between state convergence behavior as the input length approaches\ninfinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a\nwell-founded explanation of its role in length extension. Next, to overcome\nthis challenge, we propose an approach that applies spectrum scaling to\npre-trained Mamba models to enable robust long-context generalization by\nselectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We\nshow that this can significantly improve performance in settings where simply\nmodulating $\\Delta_t$ fails, validating our insights and providing avenues for\nbetter length generalization of state-space models with structured transition\nmatrices.",
    "published": "2025-09-23T22:46:19Z",
    "link": "http://arxiv.org/pdf/2509.19633v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Peng Lu",
      "Jerry Huang",
      "Qiuhao Zeng",
      "Xinyu Wang",
      "Boxing Wang",
      "Philippe Langlais",
      "Yufei Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19631v1",
    "title": "Advancing Speech Summarization in Multi-modal LLMs with Reinforcement\n  Learning",
    "summary": "Speech summarization is a critical component of spoken content understanding,\nparticularly in the era of rapidly growing spoken and audiovisual data. Recent\nadvances in multi-modal large language models (MLLMs), leveraging the power of\nLLMs, enable generating textual summaries directly from speech without\nintermediate transcriptions, while supporting controllable styles and zero-shot\ngeneralization. However, open-source MLLMs continue to lag behind the\nstate-of-the-art text-based LLMs, limiting their practical deployment for\nspeech summarization. In this work, we present a novel multi-stage\nreinforcement learning training framework to enhance the speech summarization\ncapabilities in MLLMs. Our model delivers substantial improvements over strong\nbaselines, outperforms much larger MLLMs, and significantly narrows the gap\nwith state-of-the-art text-based LLMs.",
    "published": "2025-09-23T22:45:13Z",
    "link": "http://arxiv.org/pdf/2509.19631v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shaoshi Ling",
      "Gang Liu",
      "Guoli Ye",
      "Jinyu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19623v1",
    "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL\n  Generation",
    "summary": "Large Language Models (LLMs) struggle with complex Text-to-SQL queries that\ndemand both sophisticated mathematical reasoning and intricate schema\nnavigation. Existing methods often tackle these challenges in isolation,\ncreating a fractured reasoning process that compromises logical and structural\ncorrectness. To resolve this, we introduce SteinerSQL, a framework that unifies\nthese dual challenges into a single, graph-centric optimization problem.\nSteinerSQL operates in three stages: mathematical decomposition to identify\nrequired tables (terminals), optimal reasoning scaffold construction via a\nSteiner tree problem, and multi-level validation to ensure correctness. On the\nchallenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a\nnew state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,\nusing Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified\nparadigm for Text-to-SQL, paving the way for more robust and principled\nsolutions to complex reasoning tasks.",
    "published": "2025-09-23T22:30:52Z",
    "link": "http://arxiv.org/pdf/2509.19623v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xutao Mao",
      "Tao Liu",
      "Hongying Zan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19599v1",
    "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
    "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
    "published": "2025-09-23T21:46:38Z",
    "link": "http://arxiv.org/pdf/2509.19599v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Danilo Trombino",
      "Vincenzo Pecorella",
      "Alessandro de Giulii",
      "Davide Tresoldi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19593v1",
    "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in\n  Large Language Models",
    "summary": "We introduce GuessingGame, a protocol for evaluating large language models\n(LLMs) as strategic question-askers in open-ended, open-domain settings. A\nGuesser LLM identifies a hidden object by posing free-form questions to an\nOracle without predefined choices or candidate lists. To measure question\nquality, we propose two information gain (IG) metrics: a Bayesian method that\ntracks belief updates over semantic concepts using LLM-scored relevance, and an\nentropy-based method that filters candidates via ConceptNet. Both metrics are\nmodel-agnostic and support post hoc analysis. Across 858 games with multiple\nmodels and prompting strategies, higher IG strongly predicts efficiency: a\none-standard-deviation IG increase reduces expected game length by 43\\%.\nPrompting constraints guided by IG, such as enforcing question diversity,\nenable weaker models to significantly improve performance. These results show\nthat question-asking in LLMs is both measurable and improvable, and crucial for\ninteractive reasoning.",
    "published": "2025-09-23T21:31:14Z",
    "link": "http://arxiv.org/pdf/2509.19593v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Dylan Hutson",
      "Daniel Vennemeyer",
      "Aneesh Deshmukh",
      "Justin Zhan",
      "Tianyu Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19592v1",
    "title": "Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech\n  Generation",
    "summary": "Speech generation models based on large language models (LLMs) typically\noperate on discrete acoustic codes, which differ fundamentally from text tokens\ndue to their multicodebook structure. At each timestep, models must predict N\ncodebook entries jointly, introducing dependencies that challenge simple\nparallel prediction approaches. Parallel prediction assumes independence among\ncodebooks, yielding efficient decoding but often at the cost of reduced\nfidelity. To address this, hierarchical strategies employ a local transformer\n(LT) to refine predictions and capture intra-timestep dependencies. In this\nwork, we systematically investigate two LT architectures: an autoregressive\ntransformer that generates codebooks sequentially, and a MaskGIT-based\ntransformer that performs iterative masked prediction. Both designs further\nenable frame stacking, where the primary transformer predicts multiple frames\njointly, and the LT decodes their codebooks, offering improvements in speed\nwithout compromising perceptual quality. Through extensive analysis, we\ncharacterize the tradeoffs between parallel and iterative sampling strategies\nacross different throughput and quality regimes. Finally, we propose practical\nguidelines for selecting decoding strategies based on deployment priorities\nsuch as computational efficiency and synthesis fidelity.",
    "published": "2025-09-23T21:31:00Z",
    "link": "http://arxiv.org/pdf/2509.19592v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      "Roy Fejgin",
      "Paarth Neekhara",
      "Xuesong Yang",
      "Edresson Casanova",
      "Ryan Langman Jaehyeon Kim",
      "Subhankar Ghosh",
      "Shehzeen Hussain",
      "Jason Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19590v1",
    "title": "What Does Your Benchmark Really Measure? A Framework for Robust\n  Inference of AI Capabilities",
    "summary": "Evaluations of generative models on benchmark data are now ubiquitous, and\ntheir outcomes critically shape public and scientific expectations of AI's\ncapabilities. Yet growing skepticism surrounds their reliability. How can we\nknow that a reported accuracy genuinely reflects a model's true performance?\nEvaluations are often presented as simple measurements, but in reality they are\ninferences: to treat benchmark scores as evidence of capability is already to\nassume a theory of what capability is and how it manifests in a test. We make\nthis step explicit by proposing a principled framework for evaluation as\ninference: begin from a theory of capability, and then derive methods for\nestimating it. This perspective, familiar in fields such as psychometrics, has\nnot yet become commonplace in AI evaluation. As a proof of concept, we address\na central challenge that undermines reliability: sensitivity to perturbations.\nAfter formulating a model of ability, we introduce methods that infer ability\nwhile accounting for uncertainty from sensitivity and finite samples, including\nan adaptive algorithm that significantly reduces sample complexity. Together,\nthese contributions lay the groundwork for more reliable and trustworthy\nestimates of AI capabilities as measured through benchmarks.",
    "published": "2025-09-23T21:29:04Z",
    "link": "http://arxiv.org/pdf/2509.19590v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Nathanael Jo",
      "Ashia Wilson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19587v1",
    "title": "Reverse Engineering User Stories from Code using Large Language Models",
    "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models.",
    "published": "2025-09-23T21:23:37Z",
    "link": "http://arxiv.org/pdf/2509.19587v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Mohamed Ouf",
      "Haoyu Li",
      "Michael Zhang",
      "Mariam Guizani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19586v1",
    "title": "A Foundation Chemical Language Model for Comprehensive Fragment-Based\n  Drug Discovery",
    "summary": "We introduce FragAtlas-62M, a specialized foundation model trained on the\nlargest fragment dataset to date. Built on the complete ZINC-22 fragment subset\ncomprising over 62 million molecules, it achieves unprecedented coverage of\nfragment chemical space. Our GPT-2 based model (42.7M parameters) generates\n99.90% chemically valid fragments. Validation across 12 descriptors and three\nfingerprint methods shows generated fragments closely match the training\ndistribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC\nfragments while producing 22% novel structures with practical relevance. We\nrelease FragAtlas-62M with training code, preprocessed data, documentation, and\nmodel weights to accelerate adoption.",
    "published": "2025-09-23T21:23:36Z",
    "link": "http://arxiv.org/pdf/2509.19586v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "authors": [
      "Alexander Ho",
      "Sukyeong Lee",
      "Francis T. F. Tsai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19566v1",
    "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics",
    "summary": "We investigate the application of Small Language Models (<10 billion\nparameters) for genomics question answering via agentic framework to address\nhallucination issues and computational cost challenges. The Nano Bio-Agent\n(NBA) framework we implemented incorporates task decomposition, tool\norchestration, and API access into well-established systems such as NCBI and\nAlphaGenome. Results show that SLMs combined with such agentic framework can\nachieve comparable and in many cases superior performance versus existing\napproaches utilising larger models, with our best model-agent combination\nachieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B\nparameter models consistently achieve 85-97% accuracy while requiring much\nlower computational resources than conventional approaches. This demonstrates\npromising potential for efficiency gains, cost savings, and democratization of\nML-powered genomics tools while retaining highly robust and accurate\nperformance.",
    "published": "2025-09-23T20:44:31Z",
    "link": "http://arxiv.org/pdf/2509.19566v1.pdf",
    "category": [
      "cs.AI",
      "q-bio.GN"
    ],
    "authors": [
      "George Hong",
      "Daniel Trejo Banos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19554v1",
    "title": "Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural\n  Networks",
    "summary": "This thesis explores how deep learning models learn over time, using ideas\ninspired by force analysis. Specifically, we zoom in on the model's training\nprocedure to see how one training example affects another during learning, like\nanalyzing how forces move objects. We break this influence into two parts: how\nsimilar the two examples are, and how strong the updating force is. This\nframework helps us understand a wide range of the model's behaviors in\ndifferent real systems. For example, it explains why certain examples have\nnon-trivial learning paths, why (and why not) some LLM finetuning methods work,\nand why simpler, more structured patterns tend to be learned more easily. We\napply this approach to various learning tasks and uncover new strategies for\nimproving model training. While the method is still developing, it offers a new\nway to interpret models' behaviors systematically.",
    "published": "2025-09-23T20:27:19Z",
    "link": "http://arxiv.org/pdf/2509.19554v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yi Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19538v1",
    "title": "DAWM: Diffusion Action World Models for Offline Reinforcement Learning\n  via Action-Inferred Transitions",
    "summary": "Diffusion-based world models have demonstrated strong capabilities in\nsynthesizing realistic long-horizon trajectories for offline reinforcement\nlearning (RL). However, many existing methods do not directly generate actions\nalongside states and rewards, limiting their compatibility with standard\nvalue-based offline RL algorithms that rely on one-step temporal difference\n(TD) learning. While prior work has explored joint modeling of states, rewards,\nand actions to address this issue, such formulations often lead to increased\ntraining complexity and reduced performance in practice. We propose\n\\textbf{DAWM}, a diffusion-based world model that generates future state-reward\ntrajectories conditioned on the current state, action, and return-to-go, paired\nwith an inverse dynamics model (IDM) for efficient action inference. This\nmodular design produces complete synthetic transitions suitable for one-step\nTD-based offline RL, enabling effective and computationally efficient training.\nEmpirically, we show that conservative offline RL algorithms such as TD3BC and\nIQL benefit significantly from training on these augmented trajectories,\nconsistently outperforming prior diffusion-based baselines across multiple\ntasks in the D4RL benchmark.",
    "published": "2025-09-23T20:06:26Z",
    "link": "http://arxiv.org/pdf/2509.19538v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zongyue Li",
      "Xiao Han",
      "Yusong Li",
      "Niklas Strauss",
      "Matthias Schubert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19533v1",
    "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided,\n  Reasoning-Driven Input Mutation",
    "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork.",
    "published": "2025-09-23T19:57:29Z",
    "link": "http://arxiv.org/pdf/2509.19533v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Mengdi Lu",
      "Steven Ding",
      "Furkan Alaca",
      "Philippe Charland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19524v1",
    "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for\n  Robotic Manipulation",
    "summary": "Robot learning papers typically report a single binary success rate (SR),\nwhich obscures where a policy succeeds or fails along a multi-step manipulation\ntask. We argue that subgoal-level reporting should become routine: for each\ntrajectory, a vector of per-subgoal SRs that makes partial competence visible\n(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware\nplug-in evaluation framework that utilizes vision-language models (VLMs) as\nautomated judges of subgoal outcomes from recorded images or videos. Rather\nthan proposing new benchmarks or APIs, our contribution is to outline design\nprinciples for a scalable, community-driven open-source project. In StepEval,\nthe primary artifact for policy evaluation is the per-subgoal SR vector;\nhowever, other quantities (e.g., latency or cost estimates) are also considered\nfor framework-optimization diagnostics to help the community tune evaluation\nefficiency and accuracy when ground-truth subgoal success labels are available.\nWe discuss how such a framework can remain model-agnostic, support single- or\nmulti-view inputs, and be lightweight enough to adopt across labs. The intended\ncontribution is a shared direction: a minimal, extensible seed that invites\nopen-source contributions, so that scoring the steps, not just the final goal,\nbecomes a standard and reproducible practice.",
    "published": "2025-09-23T19:42:14Z",
    "link": "http://arxiv.org/pdf/2509.19524v1.pdf",
    "category": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Ramy ElMallah",
      "Krish Chhajer",
      "Chi-Guhn Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19517v1",
    "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop\n  Reasoning",
    "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems.",
    "published": "2025-09-23T19:36:56Z",
    "link": "http://arxiv.org/pdf/2509.19517v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7; I.2.6"
    ],
    "authors": [
      "Sai Teja Reddy Adapala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19515v1",
    "title": "A Longitudinal Randomized Control Study of Companion Chatbot Use:\n  Anthropomorphism and Its Mediating Role on Social Impacts",
    "summary": "Relationships with social artificial intelligence (AI) agents are on the\nrise. People report forming friendships, mentorships, and romantic partnerships\nwith chatbots such as Replika, a type of social AI agent that is designed\nspecifically for companionship. Concerns that companion chatbot relationships\nmay harm or replace human ones have been raised, but whether and how these\nsocial consequences occur remains unclear. Prior research suggests that\npeople's states of social need and their anthropomorphism of the AI agent may\nplay a role in how human-AI interaction impacts human-human interaction. In\nthis longitudinal study (N = 183), participants were randomly assigned to\nconverse with a companion chatbot over text or to play text-based word games\nfor 10 minutes a day for 21 consecutive days. During these 21 days,\nparticipants also completed four surveys and two audio-recorded interviews. We\nfound that people's social health and relationships were not significantly\nimpacted by interacting with a companion chatbot across 21 days compared to the\ncontrol group. However, people who had a higher desire to socially connect\nanthropomorphized the chatbot more. Those who anthropomorphized the chatbot\nmore indicated that the human-chatbot interaction had greater impacts on their\nsocial interactions and relationships with family and friends. A mediation\nanalysis suggested that the impact of human-AI interaction on human-human\nsocial outcomes was mediated by the extent to which people anthropomorphized\nthe AI agent, which itself was related to the desire to socially connect.",
    "published": "2025-09-23T19:33:41Z",
    "link": "http://arxiv.org/pdf/2509.19515v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Rose E. Guingrich",
      "Michael S. A. Graziano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19512v1",
    "title": "The Heterogeneous Multi-Agent Challenge",
    "summary": "Multi-Agent Reinforcement Learning (MARL) is a growing research area which\ngained significant traction in recent years, extending Deep RL applications to\na much wider range of problems. A particularly challenging class of problems in\nthis domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where\nagents with different sensors, resources, or capabilities must cooperate based\non local information. The large number of real-world situations involving\nheterogeneous agents makes it an attractive research area, yet underexplored,\nas most MARL research focuses on homogeneous agents (e.g., a swarm of identical\nrobots). In MARL and single-agent RL, standardized environments such as ALE and\nSMAC have allowed to establish recognized benchmarks to measure progress.\nHowever, there is a clear lack of such standardized testbed for cooperative\nHeMARL. As a result, new research in this field often uses simple environments,\nwhere most algorithms perform near optimally, or uses weakly heterogeneous MARL\nenvironments.",
    "published": "2025-09-23T19:30:30Z",
    "link": "http://arxiv.org/pdf/2509.19512v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI"
    ],
    "authors": [
      "Charles Dansereau",
      "Junior-Samuel Lopez-Yepez",
      "Karthik Soma",
      "Antoine Fagette"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19509v1",
    "title": "AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit\n  Claims on Social Media with Dual Encoders and Neural Re-Ranking",
    "summary": "Linking implicit scientific claims made on social media to their original\npublications is crucial for evidence-based fact-checking and scholarly\ndiscourse, yet it is hindered by lexical sparsity, very short queries, and\ndomain-specific language. Team AIRwaves ranked second in Subtask 4b of the\nCLEF-2025 CheckThat! Lab with an evidence-retrieval approach that markedly\noutperforms the competition baseline. The optimized sparse-retrieval\nbaseline(BM25) achieves MRR@5 = 0.5025 on the gold label blind test set. To\nsurpass this baseline, a two-stage retrieval pipeline is introduced: (i) a\nfirst stage that uses a dual encoder based on E5-large, fine-tuned using\nin-batch and mined hard negatives and enhanced through chunked tokenization and\nrich document metadata; and (ii) a neural re-ranking stage using a SciBERT\ncross-encoder. Replacing purely lexical matching with neural representations\nlifts performance to MRR@5 = 0.6174, and the complete pipeline further improves\nto MRR@5 = 0.6828. The findings demonstrate that coupling dense retrieval with\nneural re-rankers delivers a powerful and efficient solution for tweet-to-study\nmatching and provides a practical blueprint for future evidence-retrieval\npipelines.",
    "published": "2025-09-23T19:26:31Z",
    "link": "http://arxiv.org/pdf/2509.19509v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Cem Ashbaugh",
      "Leon Baumgärtner",
      "Tim Gress",
      "Nikita Sidorov",
      "Daniel Werner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19497v1",
    "title": "Generative AI as a catalyst for democratic Innovation: Enhancing citizen\n  engagement in participatory budgeting",
    "summary": "This research examines the role of Generative Artificial Intelligence (AI) in\nenhancing citizen engagement in participatory budgeting. In response to\nchallenges like declining civic participation and increased societal\npolarization, the study explores how online political participation can\nstrengthen democracy and promote social equity. By integrating Generative AI\ninto public consultation platforms, the research aims to improve citizen\nproposal formulation and foster effective dialogue between citizens and\ngovernment. It assesses the capacities governments need to implement\nAI-enhanced participatory tools, considering technological dependencies and\nvulnerabilities. Analyzing technological structures, actors, interests, and\nstrategies, the study contributes to understanding how technological\nadvancements can reshape participatory institutions to better facilitate\ncitizen involvement. Ultimately, the research highlights how Generative AI can\ntransform participatory institutions, promoting inclusive, democratic\nengagement and empowering citizens.",
    "published": "2025-09-23T19:09:31Z",
    "link": "http://arxiv.org/pdf/2509.19497v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Italo Alberto do Nascimento Sousa",
      "Jorge Machado",
      "Jose Carlos Vaz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19495v1",
    "title": "ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based\n  Speech Enhancement",
    "summary": "Diffusion-based speech enhancement (SE) achieves natural-sounding speech and\nstrong generalization, yet suffers from key limitations like generative\nartifacts and high inference latency. In this work, we systematically study\nartifact prediction and reduction in diffusion-based SE. We show that variance\nin speech embeddings can be used to predict phonetic errors during inference.\nBuilding on these findings, we propose an ensemble inference method guided by\nsemantic consistency across multiple diffusion runs. This technique reduces WER\nby 15% in low-SNR conditions, effectively improving phonetic accuracy and\nsemantic plausibility. Finally, we analyze the effect of the number of\ndiffusion steps, showing that adaptive diffusion steps balance artifact\nsuppression and latency. Our findings highlight semantic priors as a powerful\ntool to guide generative SE toward artifact-free outputs.",
    "published": "2025-09-23T19:04:18Z",
    "link": "http://arxiv.org/pdf/2509.19495v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Bhawana Chhaglani",
      "Yang Gao",
      "Julius Richter",
      "Xilin Li",
      "Syavosh Zadissa",
      "Tarun Pruthi",
      "Andrew Lovitt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19489v1",
    "title": "Estimating the Self-Consistency of LLMs",
    "summary": "Systems often repeat the same prompt to large language models (LLMs) and\naggregate responses to improve reliability. This short note analyzes an\nestimator of the self-consistency of LLMs and the tradeoffs it induces under a\nfixed compute budget $B=mn$, where $m$ is the number of prompts sampled from\nthe task distribution and $n$ is the number of repeated LLM calls per prompt;\nthe resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$.",
    "published": "2025-09-23T18:51:56Z",
    "link": "http://arxiv.org/pdf/2509.19489v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Robert Nowak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19485v1",
    "title": "Identifying and Addressing User-level Security Concerns in Smart Homes\n  Using \"Smaller\" LLMs",
    "summary": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models.",
    "published": "2025-09-23T18:47:59Z",
    "link": "http://arxiv.org/pdf/2509.19485v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Hafijul Hoque Chowdhury",
      "Riad Ahmed Anonto",
      "Sourov Jajodia",
      "Suryadipta Majumdar",
      "Md. Shohrab Hossain"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19465v1",
    "title": "A Realistic Evaluation of Cross-Frequency Transfer Learning and\n  Foundation Forecasting Models",
    "summary": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework\nfor curating large-scale time series datasets to pre-train foundation\nforecasting models (FFMs). Although CFTL has shown promise, current\nbenchmarking practices fall short of accurately assessing its performance. This\nshortcoming stems from many factors: an over-reliance on small-scale evaluation\ndatasets; inadequate treatment of sample size when computing summary\nstatistics; reporting of suboptimal statistical models; and failing to account\nfor non-negligible risks of overlap between pre-training and test datasets. To\naddress these limitations, we introduce a unified reimplementation of\nwidely-adopted neural forecasting networks, adapting them for the CFTL setup;\nwe pre-train only on proprietary and synthetic data, being careful to prevent\ntest leakage; and we evaluate on 15 large, diverse public forecast competition\ndatasets. Our empirical analysis reveals that statistical models' accuracy is\nfrequently underreported. Notably, we confirm that statistical models and their\nensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and\nby more than 20% MASE, across datasets. However, we also find that synthetic\ndataset pre-training does improve the accuracy of a FFM by 7% percent.",
    "published": "2025-09-23T18:19:50Z",
    "link": "http://arxiv.org/pdf/2509.19465v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "authors": [
      "Kin G. Olivares",
      "Malcolm Wolff",
      "Tatiana Konstantinova",
      "Shankar Ramasubramanian",
      "Andrew Gordon Wilson",
      "Andres Potapczynski",
      "Willa Potosnak",
      "Mengfei Cao",
      "Boris Oreshkin",
      "Dmitry Efimov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19464v1",
    "title": "Evaluation-Aware Reinforcement Learning",
    "summary": "Policy evaluation is often a prerequisite for deploying safety- and\nperformance-critical systems. Existing evaluation approaches frequently suffer\nfrom high variance due to limited data and long-horizon tasks, or high bias due\nto unequal support or inaccurate environmental models. We posit that these\nchallenges arise, in part, from the standard reinforcement learning (RL)\nparadigm of policy learning without explicit consideration of evaluation. As an\nalternative, we propose evaluation-aware reinforcement learning (EvA-RL), in\nwhich a policy is trained to maximize expected return while simultaneously\nminimizing expected evaluation error under a given value prediction scheme --\nin other words, being \"easy\" to evaluate. We formalize a framework for EvA-RL\nand design an instantiation that enables accurate policy evaluation,\nconditioned on a small number of rollouts in an assessment environment that can\nbe different than the deployment environment. However, our theoretical analysis\nand empirical results show that there is often a tradeoff between evaluation\naccuracy and policy performance when using a fixed value-prediction scheme\nwithin EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an\nassessment-conditioned state-value predictor alongside the policy. Empirical\nresults across diverse discrete and continuous action domains demonstrate that\nEvA-RL can substantially reduce evaluation error while maintaining competitive\nreturns. This work lays the foundation for a broad new class of RL methods that\ntreat reliable evaluation as a first-class principle during training.",
    "published": "2025-09-23T18:17:21Z",
    "link": "http://arxiv.org/pdf/2509.19464v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shripad Vilasrao Deshmukh",
      "Will Schwarzer",
      "Scott Niekum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19460v1",
    "title": "Self-evolved Imitation Learning in Simulated World",
    "summary": "Imitation learning has been a trend recently, yet training a generalist agent\nacross multiple tasks still requires large-scale expert demonstrations, which\nare costly and labor-intensive to collect. To address the challenge of limited\nsupervision, we propose Self-Evolved Imitation Learning (SEIL), a framework\nthat progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasksin the simulator, from which successful\ntrajectories are collected as new demonstrations for iterative refinement. To\nenhance the diversity of these demonstrations, SEIL employs dual-level\naugmentation: (i) Model-level, using an Exponential Moving Average (EMA) model\nto collaborate with the primary model, and (ii) Environment-level, introducing\nslight variations in initial object positions. We further introduce a\nlightweight selector that filters complementary and informative trajectories\nfrom the generated pool to ensure demonstration quality. These curated samples\nenable the model to achieve competitive performance with far fewer training\nexamples. Extensive experiments on the LIBERO benchmark show that SEIL achieves\na new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git.",
    "published": "2025-09-23T18:15:32Z",
    "link": "http://arxiv.org/pdf/2509.19460v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yifan Ye",
      "Jun Cen",
      "Jing Chen",
      "Zhihe Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19456v1",
    "title": "The Indispensable Role of User Simulation in the Pursuit of AGI",
    "summary": "Progress toward Artificial General Intelligence (AGI) faces significant\nbottlenecks, particularly in rigorously evaluating complex interactive systems\nand acquiring the vast interaction data needed for training adaptive agents.\nThis paper posits that user simulation -- creating computational agents that\nmimic human interaction with AI systems -- is not merely a useful tool, but is\na critical catalyst required to overcome these bottlenecks and accelerate AGI\ndevelopment. We argue that realistic simulators provide the necessary\nenvironments for scalable evaluation, data generation for interactive learning,\nand fostering the adaptive capabilities central to AGI. Therefore, research\ninto user simulation technology and intelligent task agents are deeply\nsynergistic and must advance hand-in-hand. This article elaborates on the\ncritical role of user simulation for AGI, explores the interdisciplinary nature\nof building realistic simulators, identifies key challenges including those\nposed by large language models, and proposes a future research agenda.",
    "published": "2025-09-23T18:12:45Z",
    "link": "http://arxiv.org/pdf/2509.19456v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Krisztian Balog",
      "ChengXiang Zhai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19454v1",
    "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data\n  Augmentation",
    "summary": "Training robust bimanual manipulation policies via imitation learning\nrequires demonstration data with broad coverage over robot poses, contacts, and\nscene contexts. However, collecting diverse and precise real-world\ndemonstrations is costly and time-consuming, which hinders scalability. Prior\nworks have addressed this with data augmentation, typically for either\neye-in-hand (wrist camera) setups with RGB inputs or for generating novel\nimages without paired actions, leaving augmentation for eye-to-hand\n(third-person) RGB-D training with new action labels less explored. In this\npaper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data\nAugmentation (ROPA), an offline imitation learning data augmentation method\nthat fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D\nobservations of novel robot poses. Our approach simultaneously generates\ncorresponding joint-space action labels while employing constrained\noptimization to enforce physical consistency through appropriate\ngripper-to-object contact constraints in bimanual scenarios. We evaluate our\nmethod on 5 simulated and 3 real-world tasks. Our results across 2625\nsimulation trials and 300 real-world trials demonstrate that ROPA outperforms\nbaselines and ablations, showing its potential for scalable RGB and RGB-D data\naugmentation in eye-to-hand bimanual manipulation. Our project website is\navailable at: https://ropaaug.github.io/.",
    "published": "2025-09-23T18:11:53Z",
    "link": "http://arxiv.org/pdf/2509.19454v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Jason Chen",
      "I-Chun Arthur Liu",
      "Gaurav Sukhatme",
      "Daniel Seita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19295v1",
    "title": "Audio-Based Pedestrian Detection in the Presence of Vehicular Noise",
    "summary": "Audio-based pedestrian detection is a challenging task and has, thus far,\nonly been explored in noise-limited environments. We present a new dataset,\nresults, and a detailed analysis of the state-of-the-art in audio-based\npedestrian detection in the presence of vehicular noise. In our study, we\nconduct three analyses: (i) cross-dataset evaluation between noisy and\nnoise-limited environments, (ii) an assessment of the impact of noisy data on\nmodel performance, highlighting the influence of acoustic context, and (iii) an\nevaluation of the model's predictive robustness on out-of-domain sounds. The\nnew dataset is a comprehensive 1321-hour roadside dataset. It incorporates\ntraffic-rich soundscapes. Each recording includes 16kHz audio synchronized with\nframe-level pedestrian annotations and 1fps video thumbnails.",
    "published": "2025-09-23T17:57:44Z",
    "link": "http://arxiv.org/pdf/2509.19295v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "authors": [
      "Yonghyun Kim",
      "Chaeyeon Han",
      "Akash Sarode",
      "Noah Posner",
      "Subhrajit Guhathakurta",
      "Alexander Lerch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19292v1",
    "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold\n  Exploration",
    "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE",
    "published": "2025-09-23T17:54:47Z",
    "link": "http://arxiv.org/pdf/2509.19292v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yang Jin",
      "Jun Lv",
      "Han Xue",
      "Wendi Chen",
      "Chuan Wen",
      "Cewu Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19277v2",
    "title": "MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion\n  interactive segmentation of neurofibromas in whole-body MRI",
    "summary": "Background and Objectives: Neurofibromatosis type 1 is a genetic disorder\ncharacterized by the development of numerous neurofibromas (NFs) throughout the\nbody. Whole-body MRI (WB-MRI) is the clinical standard for detection and\nlongitudinal surveillance of NF tumor growth. Existing interactive segmentation\nmethods fail to combine high lesion-wise precision with scalability to hundreds\nof lesions. This study proposes a novel interactive segmentation model tailored\nto this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation\nmodel that extends the state-of-the-art, transformer-based, promptable Segment\nAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was\ntrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using\nT2-weighted fat-suppressed sequences. The dataset was split at the patient\nlevel into a training set and four test sets (one in-domain and three\nreflecting different domain shift scenarios, e.g., MRI field strength\nvariation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of\n0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:\n0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained\nunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:\n0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1\nscores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader\nvariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),\ncomparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable\ninteractive segmentation of NFs in WB-MRI with minimal user input and strong\ngeneralization, supporting integration into clinical workflows.",
    "published": "2025-09-23T17:42:24Z",
    "link": "http://arxiv.org/pdf/2509.19277v2.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Georgii Kolokolnikov",
      "Marie-Lena Schmalhofer",
      "Sophie Goetz",
      "Lennart Well",
      "Said Farschtschi",
      "Victor-Felix Mautner",
      "Inka Ristow",
      "Rene Werner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19271v1",
    "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset",
    "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code.",
    "published": "2025-09-23T17:34:10Z",
    "link": "http://arxiv.org/pdf/2509.19271v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Abdou Karim Kandji",
      "Frédéric Precioso",
      "Cheikh Ba",
      "Samba Ndiaye",
      "Augustin Ndione"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19270v1",
    "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data",
    "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models.",
    "published": "2025-09-23T17:33:57Z",
    "link": "http://arxiv.org/pdf/2509.19270v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Erik Božík",
      "Marek Šuppa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19766v1",
    "title": "Dynamicasome: a molecular dynamics-guided and AI-driven pathogenicity\n  prediction catalogue for all genetic mutations",
    "summary": "Advances in genomic medicine accelerate the identi cation of mutations in\ndisease-associated genes, but the pathogenicity of many mutations remains\nunknown, hindering their use in diagnostics and clinical decision-making.\nPredictive AI models are generated to combat this issue, but current tools\ndisplay low accuracy when tested against functionally validated datasets. We\nshow that integrating detailed conformational data extracted from molecular\ndynamics simulations (MDS) into advanced AI-based models increases their\npredictive power. We carry out an exhaustive mutational analysis of the disease\ngene PMM2 and subject structural models of each variant to MDS. AI models\ntrained on this dataset outperform existing tools when predicting the known\npathogenicity of mutations. Our best performing model, a neuronal networks\nmodel, also predicts the pathogenicity of several PMM2 mutations currently\nconsidered of unknown signi cance. We believe this model helps alleviate the\nburden of unknown variants in genomic medicine.",
    "published": "2025-09-23T17:33:05Z",
    "link": "http://arxiv.org/pdf/2509.19766v1.pdf",
    "category": [
      "q-bio.QM",
      "cs.AI",
      "physics.bio-ph",
      "q-bio.MN"
    ],
    "authors": [
      "Naeyma N Islam",
      "Mathew A Coban",
      "Jessica M Fuller",
      "Caleb Weber",
      "Rohit Chitale",
      "Benjamin Jussila",
      "Trisha J. Brock",
      "Cui Tao",
      "Thomas R Caulfield"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19265v1",
    "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from\n  the Arab World",
    "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.",
    "published": "2025-09-23T17:24:14Z",
    "link": "http://arxiv.org/pdf/2509.19265v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Saeed Almheiri",
      "Rania Hossam",
      "Mena Attia",
      "Chenxi Wang",
      "Preslav Nakov",
      "Timothy Baldwin",
      "Fajri Koto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20399v1",
    "title": "Defending against Stegomalware in Deep Neural Networks with Permutation\n  Symmetry",
    "summary": "Deep neural networks are being utilized in a growing number of applications,\nboth in production systems and for personal use. Network checkpoints are as a\nconsequence often shared and distributed on various platforms to ease the\ndevelopment process. This work considers the threat of neural network\nstegomalware, where malware is embedded in neural network checkpoints at a\nnegligible cost to network accuracy. This constitutes a significant security\nconcern, but is nevertheless largely neglected by the deep learning\npractitioners and security specialists alike. We propose the first effective\ncountermeasure to these attacks. In particular, we show that state-of-the-art\nneural network stegomalware can be efficiently and effectively neutralized\nthrough shuffling the column order of the weight- and bias-matrices, or\nequivalently the channel-order of convolutional layers. We show that this\neffectively corrupts payloads that have been embedded by state-of-the-art\nmethods in neural network steganography at no cost to network accuracy,\noutperforming competing methods by a significant margin. We then discuss\npossible means by which to bypass this defense, additional defense methods, and\nadvocate for continued research into the security of machine learning systems.",
    "published": "2025-09-23T17:15:38Z",
    "link": "http://arxiv.org/pdf/2509.20399v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Birk Torpmann-Hagen",
      "Michael A. Riegler",
      "Pål Halvorsen",
      "Dag Johansen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19252v1",
    "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for\n  Spatio-Temporal Heatmaps",
    "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.",
    "published": "2025-09-23T17:12:20Z",
    "link": "http://arxiv.org/pdf/2509.19252v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Gabriel Maldonado",
      "Narges Rashvand",
      "Armin Danesh Pazho",
      "Ghazal Alinezhad Noghre",
      "Vinit Katariya",
      "Hamed Tabkhi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19249v2",
    "title": "Reinforcement Learning on Pre-Training Data",
    "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.",
    "published": "2025-09-23T17:10:40Z",
    "link": "http://arxiv.org/pdf/2509.19249v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Siheng Li",
      "Kejiao Li",
      "Zenan Xu",
      "Guanhua Huang",
      "Evander Yang",
      "Kun Li",
      "Haoyuan Wu",
      "Jiajia Wu",
      "Zihao Zheng",
      "Chenchen Zhang",
      "Kun Shi",
      "Kyrierl Deng",
      "Qi Yi",
      "Ruibin Xiong",
      "Tingqiang Xu",
      "Yuhao Jiang",
      "Jianfeng Yan",
      "Yuyuan Zeng",
      "Guanghui Xu",
      "Jinbao Xue",
      "Zhijiang Xu",
      "Zheng Fang",
      "Shuai Li",
      "Qibin Liu",
      "Xiaoxue Li",
      "Zhuoyu Li",
      "Yangyu Tao",
      "Fei Gao",
      "Cheng Jiang",
      "Bo Chao Wang",
      "Kai Liu",
      "Jianchen Zhu",
      "Wai Lam",
      "Wayyt Wang",
      "Bo Zhou",
      "Di Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19236v1",
    "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and\n  Expertise Orchestration for Effective and Efficient Collaboration",
    "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.",
    "published": "2025-09-23T16:58:54Z",
    "link": "http://arxiv.org/pdf/2509.19236v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chunhao Tian",
      "Yutong Wang",
      "Xuebo Liu",
      "Zhexuan Wang",
      "Liang Ding",
      "Miao Zhang",
      "Min Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19231v1",
    "title": "Finding My Voice: Generative Reconstruction of Disordered Speech for\n  Automated Clinical Evaluation",
    "summary": "We present ChiReSSD, a speech reconstruction framework that preserves\nchildren speaker's identity while suppressing mispronunciations. Unlike prior\napproaches trained on healthy adult speech, ChiReSSD adapts to the voices of\nchildren with speech sound disorders (SSD), with particular emphasis on pitch\nand prosody. We evaluate our method on the STAR dataset and report substantial\nimprovements in lexical accuracy and speaker identity preservation.\nFurthermore, we automatically predict the phonetic content in the original and\nreconstructed pairs, where the proportion of corrected consonants is comparable\nto the percentage of correct consonants (PCC), a clinical speech assessment\nmetric. Our experiments show Pearson correlation of 0.63 between automatic and\nhuman expert annotations, highlighting the potential to reduce the manual\ntranscription burden. In addition, experiments on the TORGO dataset demonstrate\neffective generalization for reconstructing adult dysarthric speech. Our\nresults indicate that disentangled, style-based TTS reconstruction can provide\nidentity-preserving speech across diverse clinical populations.",
    "published": "2025-09-23T16:53:07Z",
    "link": "http://arxiv.org/pdf/2509.19231v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Karen Rosero",
      "Eunjung Yeo",
      "David R. Mortensen",
      "Cortney Van't Slot",
      "Rami R. Hallac",
      "Carlos Busso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19227v1",
    "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident\n  Anticipation",
    "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.",
    "published": "2025-09-23T16:49:25Z",
    "link": "http://arxiv.org/pdf/2509.19227v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Tongshuai Wu",
      "Chao Lu",
      "Ze Song",
      "Yunlong Lin",
      "Sizhe Fan",
      "Xuemei Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19224v1",
    "title": "Systematic Comparative Analysis of Large Pretrained Language Models on\n  Contextualized Medication Event Extraction",
    "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual\nrepresentations of language. In this research a comparative analysis is done\namongst pre-trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Longformer on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Processing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre-trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.",
    "published": "2025-09-23T16:48:28Z",
    "link": "http://arxiv.org/pdf/2509.19224v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tariq Abdul-Quddoos",
      "Xishuang Dong",
      "Lijun Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19220v1",
    "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders\n  for Robust Adaptation under Label Scarcity",
    "summary": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints.",
    "published": "2025-09-23T16:46:06Z",
    "link": "http://arxiv.org/pdf/2509.19220v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Ferdinand Kahenga",
      "Antoine Bagula",
      "Patrick Sello",
      "Sajal K. Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19218v1",
    "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and\n  Choroid Plexus in Pediatric Hydrocephalus",
    "summary": "Evaluation of hydrocephalus in children is challenging, and the related\nresearch is limited by a lack of publicly available, expert-annotated datasets,\nparticularly those with segmentation of the choroid plexus. To address this, we\npresent HyKid, an open-source dataset from 48 pediatric patients with\nhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was\nreconstructed from routine low-resolution images using a slice-to-volume\nalgorithm. Manually corrected segmentations of brain tissues, including white\nmatter, grey matter, lateral ventricle, external CSF, and the choroid plexus,\nwere provided by an experienced neurologist. Additionally, structured data was\nextracted from clinical radiology reports using a Retrieval-Augmented\nGeneration framework. The strong correlation between choroid plexus volume and\ntotal CSF volume provided a potential biomarker for hydrocephalus evaluation,\nachieving excellent performance in a predictive model (AUC = 0.87). The\nproposed HyKid dataset provided a high-quality benchmark for neuroimaging\nalgorithms development, and it revealed the choroid plexus-related features in\nhydrocephalus assessments. Our datasets are publicly available at\nhttps://www.synapse.org/Synapse:syn68544889.",
    "published": "2025-09-23T16:42:16Z",
    "link": "http://arxiv.org/pdf/2509.19218v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yunzhi Xu",
      "Yushuang Ding",
      "Hu Sun",
      "Hongxi Zhang",
      "Li Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19212v1",
    "title": "Steering Multimodal Large Language Models Decoding for Context-Aware\n  Safety",
    "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.",
    "published": "2025-09-23T16:32:25Z",
    "link": "http://arxiv.org/pdf/2509.19212v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zheyuan Liu",
      "Zhangchen Xu",
      "Guangyao Dou",
      "Xiangchi Yuan",
      "Zhaoxuan Tan",
      "Radha Poovendran",
      "Meng Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19419v1",
    "title": "Probabilistic Runtime Verification, Evaluation and Risk Assessment of\n  Visual Deep Learning Systems",
    "summary": "Despite achieving excellent performance on benchmarks, deep neural networks\noften underperform in real-world deployment due to sensitivity to minor, often\nimperceptible shifts in input data, known as distributional shifts. These\nshifts are common in practical scenarios but are rarely accounted for during\nevaluation, leading to inflated performance metrics. To address this gap, we\npropose a novel methodology for the verification, evaluation, and risk\nassessment of deep learning systems. Our approach explicitly models the\nincidence of distributional shifts at runtime by estimating their probability\nfrom outputs of out-of-distribution detectors. We combine these estimates with\nconditional probabilities of network correctness, structuring them in a binary\ntree. By traversing this tree, we can compute credible and precise estimates of\nnetwork accuracy. We assess our approach on five different datasets, with which\nwe simulate deployment conditions characterized by differing frequencies of\ndistributional shift. Our approach consistently outperforms conventional\nevaluation, with accuracy estimation errors typically ranging between 0.01 and\n0.1. We further showcase the potential of our approach on a medical\nsegmentation benchmark, wherein we apply our methods towards risk assessment by\nassociating costs with tree nodes, informing cost-benefit analyses and\nvalue-judgments. Ultimately, our approach offers a robust framework for\nimproving the reliability and trustworthiness of deep learning systems,\nparticularly in safety-critical applications, by providing more accurate\nperformance estimates and actionable risk assessments.",
    "published": "2025-09-23T16:16:02Z",
    "link": "http://arxiv.org/pdf/2509.19419v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Birk Torpmann-Hagen",
      "Pål Halvorsen",
      "Michael A. Riegler",
      "Dag Johansen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19182v1",
    "title": "YAC: Bridging Natural Language and Interactive Visual Exploration with\n  Generative AI for Biomedical Data Discovery",
    "summary": "Incorporating natural language input has the potential to improve the\ncapabilities of biomedical data discovery interfaces. However, user interface\nelements and visualizations are still powerful tools for interacting with data,\neven in the new world of generative AI. In our prototype system, YAC, Yet\nAnother Chatbot, we bridge the gap between natural language and interactive\nvisualizations by generating structured declarative output with a multi-agent\nsystem and interpreting that output to render linked interactive visualizations\nand apply data filters. Furthermore, we include widgets, which allow users to\nadjust the values of that structured output through user interface elements. We\nreflect on the capabilities and design of this system with an analysis of its\ntechnical dimensions and illustrate the capabilities through four usage\nscenarios.",
    "published": "2025-09-23T15:57:42Z",
    "link": "http://arxiv.org/pdf/2509.19182v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Devin Lange",
      "Shanghua Gao",
      "Pengwei Sui",
      "Austen Money",
      "Priya Misner",
      "Marinka Zitnik",
      "Nils Gehlenborg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19170v2",
    "title": "Soft Tokens, Hard Truths",
    "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.",
    "published": "2025-09-23T15:43:47Z",
    "link": "http://arxiv.org/pdf/2509.19170v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Natasha Butt",
      "Ariel Kwiatkowski",
      "Ismail Labiad",
      "Julia Kempe",
      "Yann Ollivier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19165v1",
    "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather\n  Conditions",
    "summary": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.",
    "published": "2025-09-23T15:41:40Z",
    "link": "http://arxiv.org/pdf/2509.19165v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yun Wang",
      "Junjie Hu",
      "Junhui Hou",
      "Chenghao Zhang",
      "Renwei Yang",
      "Dapeng Oliver Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19153v1",
    "title": "LLMs as verification oracles for Solidity",
    "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing.",
    "published": "2025-09-23T15:32:13Z",
    "link": "http://arxiv.org/pdf/2509.19153v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Massimo Bartoletti",
      "Enrico Lipparini",
      "Livio Pompianu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19147v1",
    "title": "Generative Propaganda",
    "summary": "Generative propaganda is the use of generative artificial intelligence (AI)\nto shape public opinion. To characterize its use in real-world settings, we\nconducted interviews with defenders (e.g., factcheckers, journalists,\nofficials) in Taiwan and creators (e.g., influencers, political consultants,\nadvertisers) as well as defenders in India, centering two places characterized\nby high levels of online propaganda. The term \"deepfakes\", we find, exerts\noutsized discursive power in shaping defenders' expectations of misuse and, in\nturn, the interventions that are prioritized. To better characterize the space\nof generative propaganda, we develop a taxonomy that distinguishes between\nobvious versus hidden and promotional versus derogatory use. Deception was\nneither the main driver nor the main impact vector of AI's use; instead, Indian\ncreators sought to persuade rather than to deceive, often making AI's use\nobvious in order to reduce legal and reputational risks, while Taiwan's\ndefenders saw deception as a subset of broader efforts to distort the\nprevalence of strategic narratives online. AI was useful and used, however, in\nproducing efficiency gains in communicating across languages and modes, and in\nevading human and algorithmic detection. Security researchers should reconsider\nthreat models to clearly differentiate deepfakes from promotional and obvious\nuses, to complement and bolster the social factors that constrain misuse by\ninternal actors, and to counter efficiency gains globally.",
    "published": "2025-09-23T15:27:00Z",
    "link": "http://arxiv.org/pdf/2509.19147v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.SI",
      "K.4.2"
    ],
    "authors": [
      "Madeleine I. G. Daepp",
      "Alejandro Cuevas",
      "Robert Osazuwa Ness",
      "Vickie Yu-Ping Wang",
      "Bharat Kumar Nayak",
      "Dibyendu Mishra",
      "Ti-Chung Cheng",
      "Shaily Desai",
      "Joyojeet Pal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19143v1",
    "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place",
    "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.",
    "published": "2025-09-23T15:26:13Z",
    "link": "http://arxiv.org/pdf/2509.19143v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Alejandro Cuevas",
      "Saloni Dash",
      "Bharat Kumar Nayak",
      "Dan Vann",
      "Madeleine I. G. Daepp"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19136v1",
    "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language",
    "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.",
    "published": "2025-09-23T15:20:40Z",
    "link": "http://arxiv.org/pdf/2509.19136v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "D.2.4; D.2.5; F.3.1"
    ],
    "authors": [
      "Sébastien Salva",
      "Redha Taguelmimt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19135v1",
    "title": "GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility\n  Understanding",
    "summary": "Human mobility traces, often recorded as sequences of check-ins, provide a\nunique window into both short-term visiting patterns and persistent lifestyle\nregularities. In this work we introduce GSTM-HMU, a generative spatio-temporal\nframework designed to advance mobility analysis by explicitly modeling the\nsemantic and temporal complexity of human movement. The framework consists of\nfour key innovations. First, a Spatio-Temporal Concept Encoder (STCE)\nintegrates geographic location, POI category semantics, and periodic temporal\nrhythms into unified vector representations. Second, a Cognitive Trajectory\nMemory (CTM) adaptively filters historical visits, emphasizing recent and\nbehaviorally salient events in order to capture user intent more effectively.\nThird, a Lifestyle Concept Bank (LCB) contributes structured human preference\ncues, such as activity types and lifestyle patterns, to enhance\ninterpretability and personalization. Finally, task-oriented generative heads\ntransform the learned representations into predictions for multiple downstream\ntasks. We conduct extensive experiments on four widely used real-world\ndatasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate\nperformance on three benchmark tasks: next-location prediction, trajectory-user\nidentification, and time estimation. The results demonstrate consistent and\nsubstantial improvements over strong baselines, confirming the effectiveness of\nGSTM-HMU in extracting semantic regularities from complex mobility data. Beyond\nraw performance gains, our findings also suggest that generative modeling\nprovides a promising foundation for building more robust, interpretable, and\ngeneralizable systems for human mobility intelligence.",
    "published": "2025-09-23T15:20:38Z",
    "link": "http://arxiv.org/pdf/2509.19135v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wenying Luo",
      "Zhiyuan Lin",
      "Wenhao Xu",
      "Minghao Liu",
      "Zhi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19122v1",
    "title": "Analysis on distribution and clustering of weight",
    "summary": "The study on architecture and parameter characteristics remains the hot topic\nin the research of large language models. In this paper we concern with the\ncharacteristics of weight which are used to analyze the correlations and\ndifferences between models. Two kinds of vectors-standard deviation vector and\nclustering vector-are proposed to describe features of models. In the first\ncase, the weights are assumed to follow normal distribution. The standard\ndeviation values of projection matrices are normalized to form\nStandard-Deviation Vector, representing the distribution characteristics of\nmodels. In the second case, the singular values from each weight projection\nmatrix are extracted and grouped by K-Means algorithm. The grouped data with\nthe same type matrix are combined as Clustering Vector to represent the\ncorrelation characteristics of models' weights. The study reveals that these\ntwo vectors can effectively distinguish between different models and clearly\nshow the similarities among models of the same family. Moreover, after\nconducting LoRA fine-tuning with different datasets and models, it is found\nthat the distribution of weights represented by standard deviation vector is\ndirectly influenced by the dataset, but the correlations between different\nweights represented by clustering vector remain unaffected and maintain a high\nconsistency with the pre-trained model.",
    "published": "2025-09-23T15:08:25Z",
    "link": "http://arxiv.org/pdf/2509.19122v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Chunming Ye",
      "Wenquan Tian",
      "Yalan Gao",
      "Songzhou Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19120v1",
    "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy\n  Federated Learning in Healthcare AI",
    "summary": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments.",
    "published": "2025-09-23T15:06:04Z",
    "link": "http://arxiv.org/pdf/2509.19120v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Ferdinand Kahenga",
      "Antoine Bagula",
      "Sajal K. Das",
      "Patrick Sello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19112v1",
    "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event\n  Sequences via One-Shot Graph Aggregation",
    "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.",
    "published": "2025-09-23T14:58:50Z",
    "link": "http://arxiv.org/pdf/2509.19112v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hugo Math",
      "Rainer Lienhart"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19102v1",
    "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object\n  Canonicalization for Generalizable Robotic Manipulation",
    "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.",
    "published": "2025-09-23T14:49:05Z",
    "link": "http://arxiv.org/pdf/2509.19102v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Hongli Xu",
      "Lei Zhang",
      "Xiaoyue Hu",
      "Boyang Zhong",
      "Kaixin Bai",
      "Zoltán-Csaba Márton",
      "Zhenshan Bing",
      "Zhaopeng Chen",
      "Alois Christian Knoll",
      "Jianwei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19100v1",
    "title": "Algorithms for Adversarially Robust Deep Learning",
    "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.",
    "published": "2025-09-23T14:48:58Z",
    "link": "http://arxiv.org/pdf/2509.19100v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alexander Robey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19412v1",
    "title": "EngravingGNN: A Hybrid Graph Neural Network for End-to-End Piano Score\n  Engraving",
    "summary": "This paper focuses on automatic music engraving, i.e., the creation of a\nhumanly-readable musical score from musical content. This step is fundamental\nfor all applications that include a human player, but it remains a mostly\nunexplored topic in symbolic music processing. In this work, we formalize the\nproblem as a collection of interdependent subtasks, and propose a unified graph\nneural network (GNN) framework that targets the case of piano music and\nquantized symbolic input. Our method employs a multi-task GNN to jointly\npredict voice connections, staff assignments, pitch spelling, key signature,\nstem direction, octave shifts, and clef signs. A dedicated postprocessing\npipeline generates print-ready MusicXML/MEI outputs. Comprehensive evaluation\non two diverse piano corpora (J-Pop and DCML Romantic) demonstrates that our\nunified model achieves good accuracy across all subtasks, compared to existing\nsystems that only specialize in specific subtasks. These results indicate that\na shared GNN encoder with lightweight task-specific decoders in a multi-task\nsetting offers a scalable and effective solution for automatic music engraving.",
    "published": "2025-09-23T14:48:35Z",
    "link": "http://arxiv.org/pdf/2509.19412v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI"
    ],
    "authors": [
      "Emmanouil Karystinaios",
      "Francesco Foscarin",
      "Gerhard Widmer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19094v1",
    "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form\n  Personalized Question Answering",
    "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.",
    "published": "2025-09-23T14:44:46Z",
    "link": "http://arxiv.org/pdf/2509.19094v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Alireza Salemi",
      "Cheng Li",
      "Mingyang Zhang",
      "Qiaozhu Mei",
      "Zhuowan Li",
      "Spurthi Amba Hombaiah",
      "Weize Kong",
      "Tao Chen",
      "Hamed Zamani",
      "Michael Bendersky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19091v1",
    "title": "Training Flow Matching Models with Reliable Labels via Self-Purification",
    "summary": "Training datasets are inherently imperfect, often containing mislabeled\nsamples due to human annotation errors, limitations of tagging models, and\nother sources of noise. Such label contamination can significantly degrade the\nperformance of a trained model. In this work, we introduce Self-Purifying Flow\nMatching (SPFM), a principled approach to filtering unreliable data within the\nflow-matching framework. SPFM identifies suspicious data using the model itself\nduring the training process, bypassing the need for pretrained models or\nadditional modules. Our experiments demonstrate that models trained with SPFM\ngenerate samples that accurately adhere to the specified conditioning, even\nwhen trained on noisy labels. Furthermore, we validate the robustness of SPFM\non the TITW dataset, which consists of in-the-wild speech data, achieving\nperformance that surpasses existing baselines.",
    "published": "2025-09-23T14:43:27Z",
    "link": "http://arxiv.org/pdf/2509.19091v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Hyeongju Kim",
      "Yechan Yu",
      "June Young Yi",
      "Juheon Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19090v2",
    "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image\n  Grounding for Clinical Reasoning",
    "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.",
    "published": "2025-09-23T14:42:31Z",
    "link": "http://arxiv.org/pdf/2509.19090v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Guoxin Wang",
      "Jun Zhao",
      "Xinyi Liu",
      "Yanbo Liu",
      "Xuyang Cao",
      "Chao Li",
      "Zhuoyun Liu",
      "Qintian Sun",
      "Fangru Zhou",
      "Haoqiang Xing",
      "Zhenhong Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19088v1",
    "title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and\n  Opportunities for Further Improvement",
    "summary": "Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines.",
    "published": "2025-09-23T14:42:14Z",
    "link": "http://arxiv.org/pdf/2509.19088v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "stat.AP"
    ],
    "authors": [
      "Tiany Peng",
      "George Gui",
      "Daniel J. Merlau",
      "Grace Jiarui Fan",
      "Malek Ben Sliman",
      "Melanie Brucks",
      "Eric J. Johnson",
      "Vicki Morwitz",
      "Abdullah Althenayyan",
      "Silvia Bellezza",
      "Dante Donati",
      "Hortense Fong",
      "Elizabeth Friedman",
      "Ariana Guevara",
      "Mohamed Hussein",
      "Kinshuk Jerath",
      "Bruce Kogut",
      "Kristen Lane",
      "Hannah Li",
      "Patryk Perkowski",
      "Oded Netzer",
      "Olivier Toubia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19084v1",
    "title": "Graph Neural Networks with Similarity-Navigated Probabilistic Feature\n  Copying",
    "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success across\nvarious graph-based tasks. However, they face some fundamental limitations:\nfeature oversmoothing can cause node representations to become\nindistinguishable in deeper networks, they struggle to effectively manage\nheterogeneous relationships where connected nodes differ significantly, and\nthey process entire feature vectors as indivisible units, which limits\nflexibility. We seek to address these limitations. We propose AxelGNN, a novel\nGNN architecture inspired by Axelrod's cultural dissemination model that\naddresses these limitations through a unified framework. AxelGNN incorporates\nsimilarity-gated probabilistic interactions that adaptively promote convergence\nor divergence based on node similarity, implements trait-level copying\nmechanisms for fine-grained feature aggregation at the segment level, and\nmaintains global polarization to preserve node distinctiveness across multiple\nrepresentation clusters. The model's bistable convergence dynamics naturally\nhandle both homophilic and heterophilic graphs within a single architecture.\nExtensive experiments on node classification and influence estimation\nbenchmarks demonstrate that AxelGNN consistently outperforms or matches\nstate-of-the-art GNN methods across diverse graph structures with varying\nhomophily-heterophily characteristics.",
    "published": "2025-09-23T14:39:09Z",
    "link": "http://arxiv.org/pdf/2509.19084v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Asela Hevapathige"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19080v1",
    "title": "World4RL: Diffusion World Models for Policy Refinement with\n  Reinforcement Learning for Robotic Manipulation",
    "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.",
    "published": "2025-09-23T14:38:15Z",
    "link": "http://arxiv.org/pdf/2509.19080v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Zhennan Jiang",
      "Kai Liu",
      "Yuxin Qin",
      "Shuai Tian",
      "Yupeng Zheng",
      "Mingcai Zhou",
      "Chao Yu",
      "Haoran Li",
      "Dongbin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19077v1",
    "title": "Code Driven Planning with Domain-Adaptive Critic",
    "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.",
    "published": "2025-09-23T14:36:12Z",
    "link": "http://arxiv.org/pdf/2509.19077v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zikang Tian",
      "Shaohui Peng",
      "Du Huang",
      "Jiaming Guo",
      "Ruizhi Chen",
      "Rui Zhang",
      "Xishan Zhang",
      "Yuxuan Guo",
      "Zidong Du",
      "Qi Guo",
      "Ling Li",
      "Yewen Pu",
      "Xing Hu",
      "Yunji Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19063v1",
    "title": "Beyond Backpropagation: Exploring Innovative Algorithms for\n  Energy-Efficient Deep Neural Network Training",
    "summary": "The rising computational and energy demands of deep neural networks (DNNs),\ndriven largely by backpropagation (BP), challenge sustainable AI development.\nThis paper rigorously investigates three BP-free training methods: the\nForward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)\nalgorithms, tracing their progression from foundational concepts to a\ndemonstrably superior solution.\n  A robust comparative framework was established: each algorithm was\nimplemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and\nbenchmarked against an equivalent BP-trained model. Hyperparameters were\noptimized with Optuna, and consistent early stopping criteria were applied\nbased on validation performance, ensuring all models were optimally tuned\nbefore comparison.\n  Results show that MF not only competes with but consistently surpasses BP in\nclassification accuracy on its native MLPs. Its superior generalization stems\nfrom converging to a more favorable minimum in the validation loss landscape,\nchallenging the assumption that global optimization is required for\nstate-of-the-art results. Measured at the hardware level using the NVIDIA\nManagement Library (NVML) API, MF reduces energy consumption by up to 41% and\nshortens training time by up to 34%, translating to a measurably smaller carbon\nfootprint as estimated by CodeCarbon.\n  Beyond this primary result, we present a hardware-level analysis that\nexplains the efficiency gains: exposing FF's architectural inefficiencies,\nvalidating MF's computationally lean design, and challenging the assumption\nthat all BP-free methods are inherently more memory-efficient. By documenting\nthe evolution from FF's conceptual groundwork to MF's synthesis of accuracy and\nsustainability, this work offers a clear, data-driven roadmap for future\nenergy-efficient deep learning.",
    "published": "2025-09-23T14:27:44Z",
    "link": "http://arxiv.org/pdf/2509.19063v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "68T07"
    ],
    "authors": [
      "Przemysław Spyra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19058v1",
    "title": "Towards Causal Representation Learning with Observable Sources as\n  Auxiliaries",
    "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches.",
    "published": "2025-09-23T14:22:39Z",
    "link": "http://arxiv.org/pdf/2509.19058v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Kwonho Kim",
      "Heejeong Nam",
      "Inwoo Hwang",
      "Sanghack Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19030v1",
    "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to\n  Action",
    "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation",
    "published": "2025-09-23T14:03:54Z",
    "link": "http://arxiv.org/pdf/2509.19030v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Victoire Hervé",
      "Henrik Warpefelt",
      "Christoph Salge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19023v1",
    "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free\n  Humanoid Locomotion",
    "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.",
    "published": "2025-09-23T13:58:36Z",
    "link": "http://arxiv.org/pdf/2509.19023v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Shuai Liu",
      "Meng Cheng Lau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19017v1",
    "title": "Fully Learnable Neural Reward Machines",
    "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs).",
    "published": "2025-09-23T13:57:13Z",
    "link": "http://arxiv.org/pdf/2509.19017v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hazem Dewidar",
      "Elena Umili"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19012v2",
    "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
    "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.",
    "published": "2025-09-23T13:53:52Z",
    "link": "http://arxiv.org/pdf/2509.19012v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Dapeng Zhang",
      "Jing Sun",
      "Chenghui Hu",
      "Xiaoyan Wu",
      "Zhenlong Yuan",
      "Rui Zhou",
      "Fei Shen",
      "Qingguo Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19002v1",
    "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction",
    "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.",
    "published": "2025-09-23T13:46:31Z",
    "link": "http://arxiv.org/pdf/2509.19002v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Hao Wang",
      "Eiki Murata",
      "Lingfang Zhang",
      "Ayako Sato",
      "So Fukuda",
      "Ziqi Yin",
      "Wentao Hu",
      "Keisuke Nakao",
      "Yusuke Nakamura",
      "Sebastian Zwirner",
      "Yi-Chia Chen",
      "Hiroyuki Otomo",
      "Hiroki Ouchi",
      "Daisuke Kawahara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20397v1",
    "title": "Variational Low-Rank Adaptation for Personalized Impaired Speech\n  Recognition",
    "summary": "Speech impairments resulting from congenital disorders, such as cerebral\npalsy, down syndrome, or apert syndrome, as well as acquired brain injuries due\nto stroke, traumatic accidents, or tumors, present major challenges to\nautomatic speech recognition (ASR) systems. Despite recent advancements,\nstate-of-the-art ASR models like Whisper still struggle with non-normative\nspeech due to limited training data availability and high acoustic variability.\nMoreover, collecting and annotating non-normative speech is burdensome:\nspeaking is effortful for many affected individuals, while laborious annotation\noften requires caregivers familiar with the speaker. This work introduces a\nnovel ASR personalization method based on Bayesian Low-rank Adaptation for\ndata-efficient fine-tuning. We validate our method on the English UA-Speech\ndataset and a newly collected German speech dataset, BF-Sprache, from a child\nwith structural speech impairment. The dataset and approach are designed to\nreflect the challenges of low-resource settings that include individuals with\nspeech impairments. Our method significantly improves ASR accuracy for impaired\nspeech while maintaining data and annotation efficiency, offering a practical\npath toward inclusive ASR.",
    "published": "2025-09-23T13:44:58Z",
    "link": "http://arxiv.org/pdf/2509.20397v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI"
    ],
    "authors": [
      "Niclas Pokel",
      "Pehuén Moure",
      "Roman Boehringer",
      "Shih-Chii Liu",
      "Yingqiang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18986v1",
    "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study\n  (Short Paper)",
    "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.",
    "published": "2025-09-23T13:37:09Z",
    "link": "http://arxiv.org/pdf/2509.18986v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Erik Penther",
      "Michael Grohs",
      "Jana-Rebecca Rehse"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18980v1",
    "title": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system",
    "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.",
    "published": "2025-09-23T13:30:03Z",
    "link": "http://arxiv.org/pdf/2509.18980v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "H.3.3; H.5.2; I.2.7"
    ],
    "authors": [
      "Maxime Manderlier",
      "Fabian Lecron",
      "Olivier Vu Thanh",
      "Nicolas Gillis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18970v1",
    "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,\n  Methods, and Directions",
    "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.",
    "published": "2025-09-23T13:24:48Z",
    "link": "http://arxiv.org/pdf/2509.18970v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xixun Lin",
      "Yucheng Ning",
      "Jingwen Zhang",
      "Yan Dong",
      "Yilong Liu",
      "Yongxuan Wu",
      "Xiaohua Qi",
      "Nan Sun",
      "Yanmin Shang",
      "Pengfei Cao",
      "Lixin Zou",
      "Xu Chen",
      "Chuan Zhou",
      "Jia Wu",
      "Shirui Pan",
      "Bin Wang",
      "Yanan Cao",
      "Kai Chen",
      "Songlin Hu",
      "Li Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18953v1",
    "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under\n  Real-World Physical Variations",
    "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.",
    "published": "2025-09-23T13:02:23Z",
    "link": "http://arxiv.org/pdf/2509.18953v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Hanqing Liu",
      "Jiahuan Long",
      "Junqi Wu",
      "Jiacheng Hou",
      "Huili Tang",
      "Tingsong Jiang",
      "Weien Zhou",
      "Wen Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18949v1",
    "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach",
    "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.",
    "published": "2025-09-23T12:58:32Z",
    "link": "http://arxiv.org/pdf/2509.18949v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Niccolò Rocchi",
      "Fabio Stella",
      "Cassio de Campos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18942v1",
    "title": "Data Efficient Adaptation in Large Language Models via Continuous\n  Low-Rank Fine-Tuning",
    "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, conventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adaptation (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the limitations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outperforms baseline methods, yielding substantial\ngains in task accuracy and resource efficiency. These findings demonstrate the\npotential of our approach to advance continual adaptation in LLMs by enhancing\ntask performance while improving resource efficiency.",
    "published": "2025-09-23T12:55:57Z",
    "link": "http://arxiv.org/pdf/2509.18942v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xiao Han",
      "Zimo Zhao",
      "Wanyu Wang",
      "Maolin Wang",
      "Zitao Liu",
      "Yi Chang",
      "Xiangyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18938v1",
    "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative\n  Self-Learning",
    "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.",
    "published": "2025-09-23T12:54:52Z",
    "link": "http://arxiv.org/pdf/2509.18938v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Matheus Vinícius Todescato",
      "Joel Luís Carbonera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20396v1",
    "title": "Data-Efficient ASR Personalization for Non-Normative Speech Using an\n  Uncertainty-Based Phoneme Difficulty Score for Guided Sampling",
    "summary": "Automatic speech recognition (ASR) systems struggle with non-normative speech\nfrom individuals with impairments caused by conditions like cerebral palsy or\nstructural anomalies. The high acoustic variability and scarcity of training\ndata severely degrade model performance. This work introduces a data-efficient\npersonalization method that quantifies phoneme-level uncertainty to guide\nfine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model\nfinds most difficult and use these estimates for a targeted oversampling\nstrategy. We validate our method on English and German datasets. Crucially, we\ndemonstrate that our model-derived uncertainty strongly correlates with\nphonemes identified as challenging in an expert clinical logopedic report,\nmarking, to our knowledge, the first work to successfully align model\nuncertainty with expert assessment of speech difficulty. Our results show that\nthis clinically-validated, uncertainty-guided sampling significantly improves\nASR accuracy, delivering a practical framework for personalized and inclusive\nASR.",
    "published": "2025-09-23T12:54:30Z",
    "link": "http://arxiv.org/pdf/2509.20396v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Niclas Pokel",
      "Pehuén Moure",
      "Roman Boehringer",
      "Yingqiang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18933v1",
    "title": "Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine\n  Learning",
    "summary": "Wireless communications are characterized by their unpredictability, posing\nchallenges for maintaining consistent communication quality. This paper\npresents a comprehensive analysis of various prediction models, with a focus on\nachieving accurate and efficient Wi-Fi link quality forecasts using machine\nlearning techniques. Specifically, the paper evaluates the performance of\ndata-driven models based on the linear combination of exponential moving\naverages, which are designed for low-complexity implementations and are then\nsuitable for hardware platforms with limited processing resources. Accuracy of\nthe proposed approaches was assessed using experimental data from a real-world\nWi-Fi testbed, considering both channel-dependent and channel-independent\ntraining data. Remarkably, channel-independent models, which allow for\ngeneralized training by equipment manufacturers, demonstrated competitive\nperformance. Overall, this study provides insights into the practical\ndeployment of machine learning-based prediction models for enhancing Wi-Fi\ndependability in industrial environments.",
    "published": "2025-09-23T12:52:01Z",
    "link": "http://arxiv.org/pdf/2509.18933v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Gabriele Formis",
      "Gianluca Cena",
      "Lukasz Wisniewski",
      "Stefano Scanzio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18930v1",
    "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined\n  through Reinforcement Learning",
    "summary": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks\nto execute classic algorithms by supervised learning. Despite its successes,\nimportant limitations remain: inability to construct valid solutions without\npost-processing and to reason about multiple correct ones, poor performance on\ncombinatorial NP-hard problems, and inapplicability to problems for which\nstrong algorithms are not yet known. To address these limitations, we reframe\nthe problem of learning algorithm trajectories as a Markov Decision Process,\nwhich imposes structure on the solution construction procedure and unlocks the\npowerful tools of imitation and reinforcement learning (RL). We propose the\nGNARL framework, encompassing the methodology to translate problem formulations\nfrom NAR to RL and a learning architecture suitable for a wide range of\ngraph-based problems. We achieve very high graph accuracy results on several\nCLRS-30 problems, performance matching or exceeding much narrower NAR\napproaches for NP-hard problems and, remarkably, applicability even when\nlacking an expert algorithm.",
    "published": "2025-09-23T12:49:25Z",
    "link": "http://arxiv.org/pdf/2509.18930v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alex Schutz",
      "Victor-Alexandru Darvariu",
      "Efimia Panagiotaki",
      "Bruno Lacerda",
      "Nick Hawes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18917v1",
    "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion\n  Probabilistic Models",
    "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.",
    "published": "2025-09-23T12:35:07Z",
    "link": "http://arxiv.org/pdf/2509.18917v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Amirhesam Aghanouri",
      "Cristina Olaverri-Monreal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18905v1",
    "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven\n  Perspective",
    "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.",
    "published": "2025-09-23T12:00:14Z",
    "link": "http://arxiv.org/pdf/2509.18905v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Songsong Yu",
      "Yuxin Chen",
      "Hao Ju",
      "Lianjie Jia",
      "Fuxi Zhang",
      "Shaofei Huang",
      "Yuhan Wu",
      "Rundi Cui",
      "Binghao Ran",
      "Zaibin Zhang",
      "Zhedong Zheng",
      "Zhipeng Zhang",
      "Yifan Wang",
      "Lin Song",
      "Lijun Wang",
      "Yanwei Li",
      "Ying Shan",
      "Huchuan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18900v1",
    "title": "The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher\n  Education",
    "summary": "The integrative literature review addresses the conceptualization and\nimplementation of AI Literacy (AIL) in Higher Education (HE) by examining\nrecent research literature. Through an analysis of publications (2021-2024), we\nexplore (1) how AIL is defined and conceptualized in current research,\nparticularly in HE, and how it can be delineated from related concepts such as\nData Literacy, Media Literacy, and Computational Literacy; (2) how various\ndefinitions can be synthesized into a comprehensive working definition, and (3)\nhow scientific insights can be effectively translated into educational\npractice. Our analysis identifies seven central dimensions of AIL: technical,\napplicational, critical thinking, ethical, social, integrational, and legal.\nThese are synthesized in the AI Literacy Heptagon, deepening conceptual\nunderstanding and supporting the structured development of AIL in HE. The study\naims to bridge the gap between theoretical AIL conceptualizations and the\npractical implementation in academic curricula.",
    "published": "2025-09-23T11:28:30Z",
    "link": "http://arxiv.org/pdf/2509.18900v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Veronika Hackl",
      "Alexandra Mueller",
      "Maximilian Sailer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18883v1",
    "title": "LongCat-Flash-Thinking Technical Report",
    "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.",
    "published": "2025-09-23T10:25:48Z",
    "link": "http://arxiv.org/pdf/2509.18883v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      " Meituan LongCat Team",
      "Anchun Gui",
      "Bei Li",
      "Bingyang Tao",
      "Bole Zhou",
      "Borun Chen",
      "Chao Zhang",
      "Chao Zhang",
      "Chengcheng Han",
      "Chenhui Yang",
      "Chi Zhang",
      "Chong Peng",
      "Chuyu Zhang",
      "Cong Chen",
      "Fengcun Li",
      "Gang Xu",
      "Guoyuan Lin",
      "Hao Jiang",
      "Hao Liang",
      "Haomin Fu",
      "Haoxiang Ma",
      "Hong Liu",
      "Hongyan Hao",
      "Hongyin Tang",
      "Hongyu Zang",
      "Hongzhi Ni",
      "Hui Su",
      "Jiahao Liu",
      "Jiahuan Li",
      "Jialin Liu",
      "Jianfei Zhang",
      "Jianhao Xu",
      "Jianing Wang",
      "Jiaqi Sun",
      "Jiaqi Zhang",
      "Jiarong Shi",
      "Jiawei Yang",
      "Jingang Wang",
      "Jinrui Ding",
      "Jun Kuang",
      "Jun Xu",
      "Ke He",
      "Kefeng Zhang",
      "Keheng Wang",
      "Keqing He",
      "Li Wei",
      "Liang Shi",
      "Lin Qiu",
      "Lingbin Kong",
      "Lingchuan Liu",
      "Linsen Guo",
      "Longfei An",
      "Mai Xia",
      "Meng Zhou",
      "Mengshen Zhu",
      "Peng Pei",
      "Pengcheng Jia",
      "Qi Gu",
      "Qi Guo",
      "Qiong Huang",
      "Quan Chen",
      "Quanchi Weng",
      "Rongxiang Weng",
      "Ruichen Shao",
      "Rumei Li",
      "Shanglin Lei",
      "Shuai Du",
      "Shuaikang Liu",
      "Shuang Zhou",
      "Shuhao Hu",
      "Siyu Xu",
      "Songshan Gong",
      "Tao Liang",
      "Tianhao Hu",
      "Wei He",
      "Wei Shi",
      "Wei Wang",
      "Wei Wu",
      "Wei Zhuo",
      "Weifeng Tang",
      "Wenjie Shi",
      "Wenlong Zhu",
      "Xi Su",
      "Xiangcheng Liu",
      "Xiangyu Xi",
      "Xiangzhou Huang",
      "Xiao Liu",
      "Xiaochen Jiang",
      "Xiaowei Shi",
      "Xiaowen Shi",
      "Xiaoyu Li",
      "Xin Chen",
      "Xinyue Zhao",
      "Xuan Huang",
      "Xuemiao Zhang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Yajie Zhang",
      "Yang Chen",
      "Yang Liu",
      "Yang Liu",
      "Yang Zheng",
      "Yaoming Wang",
      "Yaqi Huo",
      "Yerui Sun",
      "Yifan Lu",
      "Yiyang Li",
      "Youshao Xiao",
      "Yuanzhe Lei",
      "Yuchen Xie",
      "Yueqing Sun",
      "Yufei Zhang",
      "Yuhuai Wei",
      "Yulei Qian",
      "Yunke Zhao",
      "Yuqing Ding",
      "Yuwei Jiang",
      "Zhaohua Yang",
      "Zhengyu Chen",
      "Zhijian Liu",
      "Zhikang Xia",
      "Zhongda Su",
      "Ziran Li",
      "Ziwen Wang",
      "Ziyuan Zhuang",
      "Zongyu Wang",
      "Zunyuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18880v1",
    "title": "Diversity Boosts AI-Generated Text Detection",
    "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.",
    "published": "2025-09-23T10:21:22Z",
    "link": "http://arxiv.org/pdf/2509.18880v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Advik Raj Basani",
      "Pin-Yu Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18874v1",
    "title": "When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and\n  LLM Profiling Risks",
    "summary": "Automated ad targeting on social media is opaque, creating risks of\nexploitation and invisibility to external scrutiny. Users may be steered toward\nharmful content while independent auditing of these processes remains blocked.\nLarge Language Models (LLMs) raise a new concern: the potential to\nreverse-engineer sensitive user attributes from exposure alone. We introduce a\nmulti-stage auditing framework to investigate these risks. First, a large-scale\naudit of over 435,000 ad impressions delivered to 891 Australian Facebook users\nreveals algorithmic biases, including disproportionate Gambling and Politics\nads shown to socioeconomically vulnerable and politically aligned groups.\nSecond, a multimodal LLM can reconstruct users' demographic profiles from ad\nstreams, outperforming census-based baselines and matching or exceeding human\nperformance. Our results provide the first empirical evidence that ad streams\nconstitute rich digital footprints for public AI inference, highlighting urgent\nprivacy risks and the need for content-level auditing and governance.",
    "published": "2025-09-23T10:10:37Z",
    "link": "http://arxiv.org/pdf/2509.18874v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Baiyu Chen",
      "Benjamin Tag",
      "Hao Xue",
      "Daniel Angus",
      "Flora Salim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18868v1",
    "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
    "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.",
    "published": "2025-09-23T10:06:58Z",
    "link": "http://arxiv.org/pdf/2509.18868v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Dianxing Zhang",
      "Wendong Li",
      "Kani Song",
      "Jiaye Lu",
      "Gang Li",
      "Liuchun Yang",
      "Sheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18864v1",
    "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User\n  Profiling",
    "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.",
    "published": "2025-09-23T09:58:37Z",
    "link": "http://arxiv.org/pdf/2509.18864v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yingxin Li",
      "Jianbo Zhao",
      "Xueyu Ren",
      "Jie Tang",
      "Wangjie You",
      "Xu Chen",
      "Kan Zhou",
      "Chao Feng",
      "Jiao Ran",
      "Yuan Meng",
      "Zhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18851v1",
    "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
    "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
    "published": "2025-09-23T09:38:10Z",
    "link": "http://arxiv.org/pdf/2509.18851v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Gongrui Nan",
      "Siye Chen",
      "Jing Huang",
      "Mengyu Lu",
      "Dexun Wang",
      "Chunmei Xie",
      "Weiqi Xiong",
      "Xianzhou Zeng",
      "Qixuan Zhou",
      "Yadong Li",
      "Xingzhong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18849v3",
    "title": "MAPO: Mixed Advantage Policy Optimization",
    "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.",
    "published": "2025-09-23T09:37:16Z",
    "link": "http://arxiv.org/pdf/2509.18849v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Wenke Huang",
      "Quan Zhang",
      "Yiyang Fang",
      "Jian Liang",
      "Xuankun Rong",
      "Huanjin Yao",
      "Guancheng Wan",
      "Ke Liang",
      "Wenwen He",
      "Mingjun Li",
      "Leszek Rutkowski",
      "Mang Ye",
      "Bo Du",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18847v2",
    "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured\n  Reflection for Reliable Tool Interactions",
    "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.",
    "published": "2025-09-23T09:35:49Z",
    "link": "http://arxiv.org/pdf/2509.18847v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Junhao Su",
      "Yuanliang Wan",
      "Junwei Yang",
      "Hengyu Shi",
      "Tianyang Han",
      "Junfeng Luo",
      "Yurui Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18846v1",
    "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM\n  prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and\n  section-aware fine-tuning",
    "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting.",
    "published": "2025-09-23T09:35:05Z",
    "link": "http://arxiv.org/pdf/2509.18846v1.pdf",
    "category": [
      "cs.AI",
      "I.2.6; I.2.7; J.3"
    ],
    "authors": [
      "Hong-Jie Dai",
      "Zheng-Hao Li",
      "An-Tai Lu",
      "Bo-Tsz Shain",
      "Ming-Ta Li",
      "Tatheer Hussain Mir",
      "Kuang-Te Wang",
      "Min-I Su",
      "Pei-Kang Liu",
      "Ming-Ju Tsai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19406v1",
    "title": "TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via\n  Adaptive Granularity Patch and Segment-wise Decoding",
    "summary": "Multivariate time series forecasting is essential in domains such as finance,\ntransportation, climate, and energy. However, existing patch-based methods\ntypically adopt fixed-length segmentation, overlooking the heterogeneity of\nlocal temporal dynamics and the decoding heterogeneity of forecasting. Such\ndesigns lose details in information-dense regions, introduce redundancy in\nstable segments, and fail to capture the distinct complexities of short-term\nand long-term horizons. We propose TimeMosaic, a forecasting framework that\naims to address temporal heterogeneity. TimeMosaic employs adaptive patch\nembedding to dynamically adjust granularity according to local information\ndensity, balancing motif reuse with structural clarity while preserving\ntemporal continuity. In addition, it introduces segment-wise decoding that\ntreats each prediction horizon as a related subtask and adapts to\nhorizon-specific difficulty and information requirements, rather than applying\na single uniform decoder. Extensive evaluations on benchmark datasets\ndemonstrate that TimeMosaic delivers consistent improvements over existing\nmethods, and our model trained on the large-scale corpus with 321 billion\nobservations achieves performance competitive with state-of-the-art TSFMs.",
    "published": "2025-09-23T09:20:00Z",
    "link": "http://arxiv.org/pdf/2509.19406v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kuiye Ding",
      "Fanda Fan",
      "Chunyi Hou",
      "Zheya Wang",
      "Lei Wang",
      "Zhengxin Yang",
      "Jianfeng Zhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18836v1",
    "title": "Bounded PCTL Model Checking of Large Language Model Outputs",
    "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.",
    "published": "2025-09-23T09:19:37Z",
    "link": "http://arxiv.org/pdf/2509.18836v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Dennis Gross",
      "Helge Spieker",
      "Arnaud Gotlieb"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18831v1",
    "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for\n  Image/Video Synthesis via LoRA Adapters",
    "summary": "Recent advances in diffusion models have significantly improved image and\nvideo synthesis. In addition, several concept control methods have been\nproposed to enable fine-grained, continuous, and flexible control over\nfree-form text prompts. However, these methods not only require intensive\ntraining time and GPU memory usage to learn the sliders or embeddings but also\nneed to be retrained for different diffusion backbones, limiting their\nscalability and adaptability. To address these limitations, we introduce Text\nSlider, a lightweight, efficient and plug-and-play framework that identifies\nlow-rank directions within a pre-trained text encoder, enabling continuous\ncontrol of visual concepts while significantly reducing training time, GPU\nmemory consumption, and the number of trainable parameters. Furthermore, Text\nSlider supports multi-concept composition and continuous control, enabling\nfine-grained and flexible manipulation in both image and video synthesis. We\nshow that Text Slider enables smooth and continuous modulation of specific\nattributes while preserving the original spatial layout and structure of the\ninput. Text Slider achieves significantly better efficiency: 5$\\times$ faster\ntraining than Concept Slider and 47$\\times$ faster than Attribute Control,\nwhile reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$,\nrespectively.",
    "published": "2025-09-23T09:17:18Z",
    "link": "http://arxiv.org/pdf/2509.18831v1.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Pin-Yen Chiu",
      "I-Sheng Fang",
      "Jun-Cheng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19405v1",
    "title": "Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile\n  Data Augmentation",
    "summary": "Accurate outdoor positioning in cellular networks is hindered by sparse,\nheterogeneous measurement collections and the high cost of exhaustive site\nsurveys. This paper introduces a lightweight, modular mobile data augmentation\nframework designed to enhance multi-cell fingerprinting-based positioning using\noperator-collected minimization of drive test (MDT) records. The proposed\napproach decouples spatial and radio-feature synthesis: kernel density\nestimation (KDE) models the empirical spatial distribution to generate\ngeographically coherent synthetic locations, while a k-nearest-neighbor\n(KNN)-based block produces augmented per-cell radio fingerprints. The\narchitecture is intentionally training-free, interpretable, and suitable for\ndistributed or on-premise operator deployments, supporting privacy-aware\nworkflows. We both validate each augmentation module independently and assess\nits end-to-end impact on fingerprinting-based positioning using a real-world\nMDT dataset provided by an Italian mobile network operator across diverse urban\nand peri-urban scenarios. Results show that the proposed KDE-KNN augmentation\nconsistently improves positioning performance, with the largest benefits in\nsparsely sampled or structurally complex regions; we also observe\nregion-dependent saturation effects as augmentation increases. The framework\noffers a practical, low-complexity path to enhance operator positioning\nservices using existing mobile data traces.",
    "published": "2025-09-23T09:09:45Z",
    "link": "http://arxiv.org/pdf/2509.19405v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Tony Chahoud",
      "Lorenzo Mario Amorosa",
      "Riccardo Marini",
      "Luca De Nardis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20395v1",
    "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look",
    "summary": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security.",
    "published": "2025-09-23T08:54:55Z",
    "link": "http://arxiv.org/pdf/2509.20395v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "authors": [
      "Noam Schmitt",
      "Marc Antoine Lacoste"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18801v1",
    "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image\n  Denoising",
    "summary": "Achieving high image quality for temporal frames in dynamic positron emission\ntomography (PET) is challenging due to the limited statistic especially for the\nshort frames. Recent studies have shown that deep learning (DL) is useful in a\nwide range of medical image denoising tasks. In this paper, we propose a\nmodel-based neural network for dynamic PET image denoising. The inter-frame\nspatial correlation and intra-frame structural consistency in dynamic PET are\nused to establish the kernel space-based multidimensional sparse (KMDS) model.\nWe then substitute the inherent forms of the parameter estimation with neural\nnetworks to enable adaptive parameters optimization, forming the end-to-end\nneural KMDS-Net. Extensive experimental results from simulated and real data\ndemonstrate that the neural KMDS-Net exhibits strong denoising performance for\ndynamic PET, outperforming previous baseline methods. The proposed method may\nbe used to effectively achieve high temporal and spatial resolution for dynamic\nPET. Our source code is available at\nhttps://github.com/Kuangxd/Neural-KMDS-Net/tree/main.",
    "published": "2025-09-23T08:48:36Z",
    "link": "http://arxiv.org/pdf/2509.18801v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kuang Xiaodong",
      "Li Bingxuan",
      "Li Yuan",
      "Rao Fan",
      "Ma Gege",
      "Xie Qingguo",
      "Mok Greta S P",
      "Liu Huafeng",
      "Zhu Wentao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18790v1",
    "title": "Detection of security smells in IaC scripts through semantics-aware code\n  and language processing",
    "summary": "Infrastructure as Code (IaC) automates the provisioning and management of IT\ninfrastructure through scripts and tools, streamlining software deployment.\nPrior studies have shown that IaC scripts often contain recurring security\nmisconfigurations, and several detection and mitigation approaches have been\nproposed. Most of these rely on static analysis, using statistical code\nrepresentations or Machine Learning (ML) classifiers to distinguish insecure\nconfigurations from safe code.\n  In this work, we introduce a novel approach that enhances static analysis\nwith semantic understanding by jointly leveraging natural language and code\nrepresentations. Our method builds on two complementary ML models: CodeBERT, to\ncapture semantics across code and text, and LongFormer, to represent long IaC\nscripts without losing contextual information. We evaluate our approach on\nmisconfiguration datasets from two widely used IaC tools, Ansible and Puppet.\nTo validate its effectiveness, we conduct two ablation studies (removing code\ntext from the natural language input and truncating scripts to reduce context)\nand compare against four large language models (LLMs) and prior work. Results\nshow that semantic enrichment substantially improves detection, raising\nprecision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from\n0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.",
    "published": "2025-09-23T08:28:49Z",
    "link": "http://arxiv.org/pdf/2509.18790v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Aicha War",
      "Adnan A. Rawass",
      "Abdoul K. Kabore",
      "Jordan Samhi",
      "Jacques Klein",
      "Tegawende F. Bissyande"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18787v1",
    "title": "The AGNTCY Agent Directory Service: Architecture and Implementation",
    "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.",
    "published": "2025-09-23T08:25:33Z",
    "link": "http://arxiv.org/pdf/2509.18787v1.pdf",
    "category": [
      "cs.AI",
      "C.2.4"
    ],
    "authors": [
      "Luca Muscariello",
      "Vijoy Pandey",
      "Ramiz Polic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18778v1",
    "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
    "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.",
    "published": "2025-09-23T08:15:30Z",
    "link": "http://arxiv.org/pdf/2509.18778v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Shijia Ge",
      "Yinxin Zhang",
      "Shuzhao Xie",
      "Weixiang Zhang",
      "Mingcai Zhou",
      "Zhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18776v1",
    "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large\n  Language Models in the AEC Field",
    "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices.",
    "published": "2025-09-23T08:09:58Z",
    "link": "http://arxiv.org/pdf/2509.18776v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chen Liang",
      "Zhaoqi Huang",
      "Haofen Wang",
      "Fu Chai",
      "Chunying Yu",
      "Huanhuan Wei",
      "Zhengjie Liu",
      "Yanpeng Li",
      "Hongjun Wang",
      "Ruifeng Luo",
      "Xianzhong Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18775v1",
    "title": "Financial Risk Relation Identification through Dual-view Adaptation",
    "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.",
    "published": "2025-09-23T08:09:30Z",
    "link": "http://arxiv.org/pdf/2509.18775v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Wei-Ning Chiu",
      "Yu-Hsiang Wang",
      "Andy Hsiao",
      "Yu-Shiang Huang",
      "Chuan-Ju Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18771v1",
    "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
    "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress.",
    "published": "2025-09-23T08:04:58Z",
    "link": "http://arxiv.org/pdf/2509.18771v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xingkun Yin",
      "Kaibin Huang",
      "Dong In Kim",
      "Hongyang Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18765v1",
    "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations\n  through Discrete Self-Supervision",
    "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical\nimage representation learning, particularly in settings with limited labeled\ndata. However, existing SSL methods often rely on complex architectures,\nanatomy-specific priors, or heavily tuned augmentations, which limit their\nscalability and generalizability. More critically, these models are prone to\nshortcut learning, especially in modalities like chest X-rays, where anatomical\nsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT\n-- Discrete Self-Supervision for Efficient Clinical Transferable\nRepresentations, a framework that integrates multi-scale vector quantization\ninto the SSL pipeline to impose a discrete representational bottleneck. This\nconstrains the model to learn repeatable, structure-aware features while\nsuppressing view-specific or low-utility patterns, improving representation\ntransfer across tasks and domains. DiSSECT achieves strong performance on both\nclassification and segmentation tasks, requiring minimal or no fine-tuning, and\nshows particularly high label efficiency in low-label regimes. We validate\nDiSSECT across multiple public medical imaging datasets, demonstrating its\nrobustness and generalizability compared to existing state-of-the-art\napproaches.",
    "published": "2025-09-23T07:58:21Z",
    "link": "http://arxiv.org/pdf/2509.18765v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Azad Singh",
      "Deepak Mishra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18762v2",
    "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning\n  Affects Behavior of Large Language Models",
    "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs.",
    "published": "2025-09-23T07:55:38Z",
    "link": "http://arxiv.org/pdf/2509.18762v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yingming Zheng",
      "Hanqi Li",
      "Kai Yu",
      "Lu Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18761v1",
    "title": "Security smells in infrastructure as code: a taxonomy update beyond the\n  seven sins",
    "summary": "Infrastructure as Code (IaC) has become essential for modern software\nmanagement, yet security flaws in IaC scripts can have severe consequences, as\nexemplified by the recurring exploits of Cloud Web Services. Prior work has\nrecognized the need to build a precise taxonomy of security smells in IaC\nscripts as a first step towards developing approaches to improve IaC security.\nThis first effort led to the unveiling of seven sins, limited by the focus on a\nsingle IaC tool as well as by the extensive, and potentially biased, manual\neffort that was required. We propose, in our work, to revisit this taxonomy:\nfirst, we extend the study of IaC security smells to a more diverse dataset\nwith scripts associated with seven popular IaC tools, including Terraform,\nAnsible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some\nautomation for the analysis by relying on an LLM. While we leverage LLMs for\ninitial pattern processing, all taxonomic decisions underwent systematic human\nvalidation and reconciliation with established security standards. Our study\nyields a comprehensive taxonomy of 62 security smell categories, significantly\nexpanding beyond the previously known seven. We demonstrate actionability by\nimplementing new security checking rules within linters for seven popular IaC\ntools, often achieving 1.00 precision score. Our evolution study of security\nsmells in GitHub projects reveals that these issues persist for extended\nperiods, likely due to inadequate detection and mitigation tools. This work\nprovides IaC practitioners with insights for addressing common security smells\nand systematically adopting DevSecOps practices to build safer infrastructure\ncode.",
    "published": "2025-09-23T07:55:35Z",
    "link": "http://arxiv.org/pdf/2509.18761v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Aicha War",
      "Serge L. B. Nikiema",
      "Jordan Samhi",
      "Jacques Klein",
      "Tegawende F. Bissyande"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18758v1",
    "title": "Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network\n  in Different Topologies",
    "summary": "Neural network models capable of storing memory have been extensively studied\nin computer science and computational neuroscience. The Hopfield network is a\nprototypical example of a model designed for associative, or\ncontent-addressable, memory and has been analyzed in many forms. Further, ideas\nand methods from complex network theory have been incorporated into artificial\nneural networks and learning, emphasizing their structural properties.\nNevertheless, the temporal dynamics also play a vital role in biological neural\nnetworks, whose temporal structure is a crucial feature to examine. Biological\nneural networks display complex intermittency and, thus, can be studied through\nthe lens of the temporal complexity (TC) theory. The TC approach look at the\nmetastability of self-organized states, characterized by a power-law decay in\nthe inter-event time distribution and in the total activity distribution or a\nscaling behavior in the corresponding event-driven diffusion processes. In this\nstudy, we present a temporal complexity (TC) analysis of a\nbiologically-inspired Hopfield-type neural network model. We conducted a\ncomparative assessment between scale-free and random network topologies, with\nparticular emphasis on their global activation patterns. Our parametric\nanalysis revealed comparable dynamical behaviors across both neural network\narchitectures. Furthermore, our investigation into temporal complexity\ncharacteristics uncovered that seemingly distinct dynamical patterns exhibit\nsimilar temporal complexity behaviors. In particular, similar power-law decay\nin the activity distribution and similar complexity levels are observed in both\ntopologies, but with a much reduced noise in the scale-free topology. Notably,\nmost of the complex dynamical profiles were consistently observed in scale-free\nnetwork configurations, thus confirming the crucial role of hubs in neural\nnetwork dynamics.",
    "published": "2025-09-23T07:53:27Z",
    "link": "http://arxiv.org/pdf/2509.18758v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "nlin.AO",
      "physics.bio-ph"
    ],
    "authors": [
      "Marco Cafiso",
      "Paolo Paradisi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18757v1",
    "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
    "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.",
    "published": "2025-09-23T07:53:05Z",
    "link": "http://arxiv.org/pdf/2509.18757v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Omar Rayyan",
      "John Abanes",
      "Mahmoud Hafez",
      "Anthony Tzes",
      "Fares Abu-Dakka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18754v2",
    "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage",
    "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.",
    "published": "2025-09-23T07:49:30Z",
    "link": "http://arxiv.org/pdf/2509.18754v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yuyang Liu",
      "Xinyuan Shi",
      "Xiaondan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19403v1",
    "title": "Online Adaptation via Dual-Stage Alignment and Self-Supervision for\n  Fast-Calibration Brain-Computer Interfaces",
    "summary": "Individual differences in brain activity hinder the online application of\nelectroencephalogram (EEG)-based brain computer interface (BCI) systems. To\novercome this limitation, this study proposes an online adaptation algorithm\nfor unseen subjects via dual-stage alignment and self-supervision. The\nalignment process begins by applying Euclidean alignment in the EEG data space\nand then updates batch normalization statistics in the representation space.\nMoreover, a self-supervised loss is designed to update the decoder. The loss is\ncomputed by soft pseudo-labels derived from the decoder as a proxy for the\nunknown ground truth, and is calibrated by Shannon entropy to facilitate\nself-supervised training. Experiments across five public datasets and seven\ndecoders show the proposed algorithm can be integrated seamlessly regardless of\nBCI paradigm and decoder architecture. In each iteration, the decoder is\nupdated with a single online trial, which yields average accuracy gains of 4.9%\non steady-state visual evoked potentials (SSVEP) and 3.6% on motor imagery.\nThese results support fast-calibration operation and show that the proposed\nalgorithm has great potential for BCI applications.",
    "published": "2025-09-23T07:38:37Z",
    "link": "http://arxiv.org/pdf/2509.19403v1.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sheng-Bin Duan",
      "Jian-Long Hao",
      "Tian-Yu Xiang",
      "Xiao-Hu Zhou",
      "Mei-Jiang Gui",
      "Xiao-Liang Xie",
      "Shi-Qi Liu",
      "Zeng-Guang Hou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18714v1",
    "title": "A Generalized Bisimulation Metric of State Similarity between Markov\n  Decision Processes: From Theoretical Propositions to Applications",
    "summary": "The bisimulation metric (BSM) is a powerful tool for computing state\nsimilarities within a Markov decision process (MDP), revealing that states\ncloser in BSM have more similar optimal value functions. While BSM has been\nsuccessfully utilized in reinforcement learning (RL) for tasks like state\nrepresentation learning and policy exploration, its application to multiple-MDP\nscenarios, such as policy transfer, remains challenging. Prior work has\nattempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis\nof its mathematical properties has limited further theoretical progress. In\nthis work, we formally establish a generalized bisimulation metric (GBSM)\nbetween pairs of MDPs, which is rigorously proven with the three fundamental\nproperties: GBSM symmetry, inter-MDP triangle inequality, and the distance\nbound on identical state spaces. Leveraging these properties, we theoretically\nanalyse policy transfer, state aggregation, and sampling-based estimation in\nMDPs, obtaining explicit bounds that are strictly tighter than those derived\nfrom the standard BSM. Additionally, GBSM provides a closed-form sample\ncomplexity for estimation, improving upon existing asymptotic results based on\nBSM. Numerical results validate our theoretical findings and demonstrate the\neffectiveness of GBSM in multi-MDP scenarios.",
    "published": "2025-09-23T07:02:05Z",
    "link": "http://arxiv.org/pdf/2509.18714v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhenyu Tao",
      "Wei Xu",
      "Xiaohu You"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18713v1",
    "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce\n  Customer Service",
    "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.",
    "published": "2025-09-23T06:57:07Z",
    "link": "http://arxiv.org/pdf/2509.18713v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yizhe Huang",
      "Yang Liu",
      "Ruiyu Zhao",
      "Xiaolong Zhong",
      "Xingming Yue",
      "Ling Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18711v1",
    "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot\n  Open-Vocabulary Visual Grounding in Remote Sensing Images",
    "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.",
    "published": "2025-09-23T06:52:15Z",
    "link": "http://arxiv.org/pdf/2509.18711v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Ke Li",
      "Di Wang",
      "Ting Wang",
      "Fuyu Dong",
      "Yiming Zhang",
      "Luyao Zhang",
      "Xiangyu Wang",
      "Shaofeng Li",
      "Quan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18710v1",
    "title": "Autonomous Data Agents: A New Opportunity for Smart Data",
    "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.",
    "published": "2025-09-23T06:46:41Z",
    "link": "http://arxiv.org/pdf/2509.18710v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yanjie Fu",
      "Dongjie Wang",
      "Wangyang Ying",
      "Xiangliang Zhang",
      "Huan Liu",
      "Jian Pei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18691v1",
    "title": "An overview of neural architectures for self-supervised audio\n  representation learning from masked spectrograms",
    "summary": "In recent years, self-supervised learning has amassed significant interest\nfor training deep neural representations without labeled data. One such\nself-supervised learning approach is masked spectrogram modeling, where the\nobjective is to learn semantically rich contextual representations by\npredicting removed or hidden portions of the input audio spectrogram. With the\nTransformer neural architecture at its core, masked spectrogram modeling has\nemerged as the prominent approach for learning general purpose audio\nrepresentations, a.k.a. audio foundation models. Meanwhile, addressing the\nissues of the Transformer architecture, in particular the underlying Scaled\nDot-product Attention operation, which scales quadratically with input sequence\nlength, has led to renewed interest in recurrent sequence modeling approaches.\nAmong them, Selective structured state space models (such as Mamba) and\nextended Long Short-Term Memory (xLSTM) are the two most promising approaches\nwhich have experienced widespread adoption. While the body of work on these two\ntopics continues to grow, there is currently a lack of an adequate overview\nencompassing the intersection of these topics. In this paper, we present a\ncomprehensive overview of the aforementioned research domains, covering masked\nspectrogram modeling and the previously mentioned neural sequence modeling\narchitectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and\nxLSTM based masked spectrogram models in a unified, reproducible framework on\nten diverse downstream audio classification tasks, which will help interested\nreaders to make informed decisions regarding suitability of the evaluated\napproaches to adjacent applications.",
    "published": "2025-09-23T06:20:41Z",
    "link": "http://arxiv.org/pdf/2509.18691v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Sarthak Yadav",
      "Sergios Theodoridis",
      "Zheng-Hua Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18690v1",
    "title": "Advances in Large Language Models for Medicine",
    "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research.",
    "published": "2025-09-23T06:16:39Z",
    "link": "http://arxiv.org/pdf/2509.18690v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zhiyu Kan",
      "Wensheng Gan",
      "Zhenlian Qi",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18683v1",
    "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for\n  RGB-D Salient Object Detection",
    "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.",
    "published": "2025-09-23T06:08:17Z",
    "link": "http://arxiv.org/pdf/2509.18683v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "authors": [
      "Lanhu Wu",
      "Zilin Gao",
      "Hao Fei",
      "Mong-Li Lee",
      "Wynne Hsu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18681v1",
    "title": "Implementation of airborne ML models with semantics preservation",
    "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models.",
    "published": "2025-09-23T06:01:52Z",
    "link": "http://arxiv.org/pdf/2509.18681v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Nicolas Valot",
      "Louis Fabre",
      "Benjamin Lesage",
      "Ammar Mechouche",
      "Claire Pagetti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20394v1",
    "title": "Blueprints of Trust: AI System Cards for End to End Transparency and\n  Governance",
    "summary": "This paper introduces the Hazard-Aware System Card (HASC), a novel framework\ndesigned to enhance transparency and accountability in the development and\ndeployment of AI systems. The HASC builds upon existing model card and system\ncard concepts by integrating a comprehensive, dynamic record of an AI system's\nsecurity and safety posture. The framework proposes a standardized system of\nidentifiers, including a novel AI Safety Hazard (ASH) ID, to complement\nexisting security identifiers like CVEs, allowing for clear and consistent\ncommunication of fixed flaws. By providing a single, accessible source of\ntruth, the HASC empowers developers and stakeholders to make more informed\ndecisions about AI system safety throughout its lifecycle. Ultimately, we also\ncompare our proposed AI system cards with the ISO/IEC 42001:2023 standard and\ndiscuss how they can be used to complement each other, providing greater\ntransparency and accountability for AI systems.",
    "published": "2025-09-23T05:58:32Z",
    "link": "http://arxiv.org/pdf/2509.20394v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Huzaifa Sidhpurwala",
      "Emily Fox",
      "Garth Mollett",
      "Florencio Cano Gabarda",
      "Roman Zhukov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18672v1",
    "title": "NaviSense: A Multimodal Assistive Mobile application for Object\n  Retrieval by Persons with Visual Impairment",
    "summary": "People with visual impairments often face significant challenges in locating\nand retrieving objects in their surroundings. Existing assistive technologies\npresent a trade-off: systems that offer precise guidance typically require\npre-scanning or support only fixed object categories, while those with\nopen-world object recognition lack spatial feedback for reaching the object. To\naddress this gap, we introduce 'NaviSense', a mobile assistive system that\ncombines conversational AI, vision-language models, augmented reality (AR), and\nLiDAR to support open-world object detection with real-time audio-haptic\nguidance. Users specify objects via natural language and receive continuous\nspatial feedback to navigate toward the target without needing prior setup.\nDesigned with insights from a formative study and evaluated with 12 blind and\nlow-vision participants, NaviSense significantly reduced object retrieval time\nand was preferred over existing tools, demonstrating the value of integrating\nopen-world perception with precise, accessible guidance.",
    "published": "2025-09-23T05:45:11Z",
    "link": "http://arxiv.org/pdf/2509.18672v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Ajay Narayanan Sridhar",
      "Fuli Qiao",
      "Nelson Daniel Troncoso Aldas",
      "Yanpei Shi",
      "Mehrdad Mahdavi",
      "Laurent Itti",
      "Vijaykrishnan Narayanan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18667v1",
    "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation",
    "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens.",
    "published": "2025-09-23T05:34:34Z",
    "link": "http://arxiv.org/pdf/2509.18667v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Qiao Xiao",
      "Hong Ting Tsang",
      "Jiaxin Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18648v1",
    "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
    "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.",
    "published": "2025-09-23T05:03:00Z",
    "link": "http://arxiv.org/pdf/2509.18648v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yarden As",
      "Chengrui Qu",
      "Benjamin Unger",
      "Dongho Kang",
      "Max van der Hart",
      "Laixi Shi",
      "Stelian Coros",
      "Adam Wierman",
      "Andreas Krause"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18644v2",
    "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
    "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0%\nto 85% in height generalization and from 6% to 64% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment. Discover more by visiting: https://statefreepolicy.github.io.",
    "published": "2025-09-23T04:56:59Z",
    "link": "http://arxiv.org/pdf/2509.18644v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Juntu Zhao",
      "Wenbo Lu",
      "Di Zhang",
      "Yufeng Liu",
      "Yushen Liang",
      "Tianluo Zhang",
      "Yifeng Cao",
      "Junyuan Xie",
      "Yingdong Hu",
      "Shengjie Wang",
      "Junliang Guo",
      "Dequan Wang",
      "Yang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19398v1",
    "title": "FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge\n  Networks",
    "summary": "Multi-server Federated Learning (FL) has emerged as a promising solution to\nmitigate communication bottlenecks of single-server FL. We focus on a typical\nmulti-server FL architecture, where the regions covered by different edge\nservers (ESs) may overlap. A key observation of this architecture is that\nclients located in the overlapping areas can access edge models from multiple\nESs. Building on this insight, we propose FedOC (Federated learning with\nOverlapping Clients), a novel framework designed to fully exploit the potential\nof these overlapping clients. In FedOC, overlapping clients could serve dual\nroles: (1) as Relay Overlapping Clients (ROCs), they forward edge models\nbetween neighboring ESs in real time to facilitate model sharing among\ndifferent ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically\nselect their initial model for local training based on the edge model delivery\ntime, which enables indirect data fusion among different regions of ESs. The\noverall FedOC workflow proceeds as follows: in every round, each client trains\nlocal model based on the earliest received edge model and transmits to the\nrespective ESs for model aggregation. Then each ES transmits the aggregated\nedge model to neighboring ESs through ROC relaying. Upon receiving the relayed\nmodels, each ES performs a second aggregation and subsequently broadcasts the\nupdated model to covered clients. The existence of ROCs enables the model of\neach ES to be disseminated to the other ESs in a decentralized manner, which\nindirectly achieves intercell model and speeding up the training process,\nmaking it well-suited for latency-sensitive edge environments. Extensive\nexperimental results show remarkable performance gains of our scheme compared\nto existing methods.",
    "published": "2025-09-23T04:53:51Z",
    "link": "http://arxiv.org/pdf/2509.19398v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Yun Ji",
      "Zeyu Chen",
      "Xiaoxiong Zhong",
      "Yanan Ma",
      "Sheng Zhang",
      "Yuguang Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20393v1",
    "title": "The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools\n  Are Blind",
    "summary": "We investigate strategic deception in large language models using two\ncomplementary testbeds: Secret Agenda (across 38 models) and Insider Trading\ncompliance (via SAE architectures). Secret Agenda reliably induced lying when\ndeception advantaged goal achievement across all model families. Analysis\nrevealed that autolabeled SAE features for \"deception\" rarely activated during\nstrategic dishonesty, and feature steering experiments across 100+\ndeception-related features failed to prevent lying. Conversely, insider trading\nanalysis using unlabeled SAE activations separated deceptive versus compliant\nresponses through discriminative patterns in heatmaps and t-SNE visualizations.\nThese findings suggest autolabel-driven interpretability approaches fail to\ndetect or control behavioral deception, while aggregate unlabeled activations\nprovide population-level structure for risk assessment. Results span Llama\n8B/70B SAE implementations and GemmaScope under resource constraints,\nrepresenting preliminary findings that motivate larger studies on feature\ndiscovery, labeling methodology, and causal interventions in realistic\ndeception contexts.",
    "published": "2025-09-23T04:52:40Z",
    "link": "http://arxiv.org/pdf/2509.20393v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Caleb DeLeeuw",
      "Gaurav Chawla",
      "Aniket Sharma",
      "Vanessa Dietze"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18638v1",
    "title": "Learning neuroimaging models from health system-scale data",
    "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.",
    "published": "2025-09-23T04:49:59Z",
    "link": "http://arxiv.org/pdf/2509.18638v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yiwei Lyu",
      "Samir Harake",
      "Asadur Chowdury",
      "Soumyanil Banerjee",
      "Rachel Gologorsky",
      "Shixuan Liu",
      "Anna-Katharina Meissner",
      "Akshay Rao",
      "Chenhui Zhao",
      "Akhil Kondepudi",
      "Cheng Jiang",
      "Xinhai Hou",
      "Rushikesh S. Joshi",
      "Volker Neuschmelting",
      "Ashok Srinivasan",
      "Dawn Kleindorfer",
      "Brian Athey",
      "Vikas Gulani",
      "Aditya Pandey",
      "Honglak Lee",
      "Todd Hollon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18633v1",
    "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk\n  Assessment: A Geospatial Framework with Evolutionary Economic Agents",
    "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies.",
    "published": "2025-09-23T04:33:58Z",
    "link": "http://arxiv.org/pdf/2509.18633v1.pdf",
    "category": [
      "cs.AI",
      "q-fin.RM"
    ],
    "authors": [
      "Yara Mohajerani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18631v2",
    "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
    "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.",
    "published": "2025-09-23T04:32:53Z",
    "link": "http://arxiv.org/pdf/2509.18631v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Shuo Cheng",
      "Liqian Ma",
      "Zhenyang Chen",
      "Ajay Mandlekar",
      "Caelan Garrett",
      "Danfei Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18629v1",
    "title": "HyperAdapt: Simple High-Rank Adaptation",
    "summary": "Foundation models excel across diverse tasks, but adapting them to\nspecialized applications often requires fine-tuning, an approach that is memory\nand compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate\nthis by updating only a small subset of weights. In this paper, we introduce\nHyperAdapt, a parameter-efficient fine-tuning method that significantly reduces\nthe number of trainable parameters compared to state-of-the-art methods like\nLoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying\nrow- and column-wise scaling through diagonal matrices, thereby inducing a\nhigh-rank update while requiring only $n+m$ trainable parameters for an $n\n\\times m$ matrix. Theoretically, we establish an upper bound on the rank of\nHyperAdapt's updates, and empirically, we confirm that it consistently induces\nhigh-rank transformations across model layers. Experiments on GLUE, arithmetic\nreasoning, and commonsense reasoning benchmarks with models up to 14B\nparameters demonstrate that HyperAdapt matches or nearly matches the\nperformance of full fine-tuning and state-of-the-art PEFT methods while using\norders of magnitude fewer trainable parameters.",
    "published": "2025-09-23T04:29:26Z",
    "link": "http://arxiv.org/pdf/2509.18629v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Abel Gurung",
      "Joseph Campbell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18627v1",
    "title": "BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral\n  Data",
    "summary": "Neural populations exhibit complex recurrent structures that drive behavior,\nwhile continuously receiving and integrating external inputs from sensory\nstimuli, upstream regions, and neurostimulation. However, neural populations\nare often modeled as autonomous dynamical systems, with little consideration\ngiven to the influence of external inputs that shape the population activity\nand behavioral outcomes. Here, we introduce BRAID, a deep learning framework\nthat models nonlinear neural dynamics underlying behavior while explicitly\nincorporating any measured external inputs. Our method disentangles intrinsic\nrecurrent neural population dynamics from the effects of inputs by including a\nforecasting objective within input-driven recurrent neural networks. BRAID\nfurther prioritizes the learning of intrinsic dynamics that are related to a\nbehavior of interest by using a multi-stage optimization scheme. We validate\nBRAID with nonlinear simulations, showing that it can accurately learn the\nintrinsic dynamics shared between neural and behavioral modalities. We then\napply BRAID to motor cortical activity recorded during a motor task and\ndemonstrate that our method more accurately fits the neural-behavioral data by\nincorporating measured sensory stimuli into the model and improves the\nforecasting of neural-behavioral data compared with various baseline methods,\nwhether input-driven or not.",
    "published": "2025-09-23T04:22:53Z",
    "link": "http://arxiv.org/pdf/2509.18627v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Parsa Vahidi",
      "Omid G. Sani",
      "Maryam M. Shanechi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18626v1",
    "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for\n  Reasonable Driving",
    "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.",
    "published": "2025-09-23T04:21:39Z",
    "link": "http://arxiv.org/pdf/2509.18626v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Jay Patrikar",
      "Apoorva Sharma",
      "Sushant Veer",
      "Boyi Li",
      "Sebastian Scherer",
      "Marco Pavone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18611v1",
    "title": "Flow marching for a generative PDE foundation model",
    "summary": "Pretraining on large-scale collections of PDE-governed spatiotemporal\ntrajectories has recently shown promise for building generalizable models of\ndynamical systems. Yet most existing PDE foundation models rely on\ndeterministic Transformer architectures, which lack generative flexibility for\nmany science and engineering applications. We propose Flow Marching, an\nalgorithm that bridges neural operator learning with flow matching motivated by\nan analysis of error accumulation in physical dynamical systems, and we build a\ngenerative PDE foundation model on top of it. By jointly sampling the noise\nlevel and the physical time step between adjacent states, the model learns a\nunified velocity field that transports a noisy current state toward its clean\nsuccessor, reducing long-term rollout drift while enabling uncertainty-aware\nensemble generations. Alongside this core algorithm, we introduce a\nPhysics-Pretrained Variational Autoencoder (P2VAE) to embed physical states\ninto a compact latent space, and an efficient Flow Marching Transformer (FMT)\nthat combines a diffusion-forcing scheme with latent temporal pyramids,\nachieving up to 15x greater computational efficiency than full-length video\ndiffusion models and thereby enabling large-scale pretraining at substantially\nreduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE\nfamilies and train suites of P2VAEs and FMTs at multiple scales. On downstream\nevaluation, we benchmark on unseen Kolmogorov turbulence with few-shot\nadaptation, demonstrate long-term rollout stability over deterministic\ncounterparts, and present uncertainty-stratified ensemble results, highlighting\nthe importance of generative PDE foundation models for real-world applications.",
    "published": "2025-09-23T04:00:41Z",
    "link": "http://arxiv.org/pdf/2509.18611v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zituo Chen",
      "Sili Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18608v1",
    "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement\n  Learning",
    "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.",
    "published": "2025-09-23T03:56:10Z",
    "link": "http://arxiv.org/pdf/2509.18608v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Ana Luiza Mineiro",
      "Francisco Affonso",
      "Marcelo Becker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19397v1",
    "title": "Self-Alignment Learning to Improve Myocardial Infarction Detection from\n  Single-Lead ECG",
    "summary": "Myocardial infarction is a critical manifestation of coronary artery disease,\nyet detecting it from single-lead electrocardiogram (ECG) remains challenging\ndue to limited spatial information. An intuitive idea is to convert single-lead\ninto multiple-lead ECG for classification by pre-trained models, but generative\nmethods optimized at the signal level in most cases leave a large latent space\ngap, ultimately degrading diagnostic performance. This naturally raises the\nquestion of whether latent space alignment could help. However, most prior ECG\nalignment methods focus on learning transformation invariance, which mismatches\nthe goal of single-lead detection. To address this issue, we propose SelfMIS, a\nsimple yet effective alignment learning framework to improve myocardial\ninfarction detection from single-lead ECG. Discarding manual data\naugmentations, SelfMIS employs a self-cutting strategy to pair multiple-lead\nECG with their corresponding single-lead segments and directly align them in\nthe latent space. This design shifts the learning objective from pursuing\ntransformation invariance to enriching the single-lead representation,\nexplicitly driving the single-lead ECG encoder to learn a representation\ncapable of inferring global cardiac context from the local signal.\nExperimentally, SelfMIS achieves superior performance over baseline models\nacross nine myocardial infarction types while maintaining a simpler\narchitecture and lower computational overhead, thereby substantiating the\nefficacy of direct latent space alignment. Our code and checkpoint will be\npublicly available after acceptance.",
    "published": "2025-09-23T03:54:39Z",
    "link": "http://arxiv.org/pdf/2509.19397v1.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jiarui Jin",
      "Xiaocheng Fang",
      "Haoyu Wang",
      "Jun Li",
      "Che Liu",
      "Donglin Xie",
      "Hongyan Li",
      "Shenda Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18606v1",
    "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
    "summary": "Despite recent progress in large-scale sound event detection (SED) systems\ncapable of handling hundreds of sound classes, existing multi-class\nclassification frameworks remain fundamentally limited. They cannot process\nfree-text sound queries, which enable more flexible and user-friendly\ninteraction, and they lack zero-shot capabilities and offer poor few-shot\nadaptability. Although text-query-based separation methods have been explored,\nthey primarily focus on source separation and are ill-suited for SED tasks that\nrequire precise temporal localization and efficient detection across large and\ndiverse sound vocabularies. In this paper, we propose FlexSED, an\nopen-vocabulary sound event detection system. FlexSED builds on a pretrained\naudio SSL model and the CLAP text encoder, introducing an encoder-decoder\ncomposition and an adaptive fusion strategy to enable effective continuous\ntraining from pretrained weights. To ensure robust supervision, it also employs\nlarge language models (LLMs) to assist in event query selection during\ntraining, addressing challenges related to missing labels. As a result, FlexSED\nachieves superior performance compared to vanilla SED models on\nAudioSet-Strong, while demonstrating strong zero-shot and few-shot\ncapabilities. We release the code and pretrained models to support future\nresearch and applications based on FlexSED.",
    "published": "2025-09-23T03:52:52Z",
    "link": "http://arxiv.org/pdf/2509.18606v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Jiarui Hai",
      "Helin Wang",
      "Weizhe Guo",
      "Mounya Elhilali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18603v1",
    "title": "SynSonic: Augmenting Sound Event Detection through Text-to-Audio\n  Diffusion ControlNet and Effective Sample Filtering",
    "summary": "Data synthesis and augmentation are essential for Sound Event Detection (SED)\ndue to the scarcity of temporally labeled data. While augmentation methods like\nSpecAugment and Mix-up can enhance model performance, they remain constrained\nby the diversity of existing samples. Recent generative models offer new\nopportunities, yet their direct application to SED is challenging due to the\nlack of precise temporal annotations and the risk of introducing noise through\nunreliable filtering. To address these challenges and enable generative-based\naugmentation for SED, we propose SynSonic, a data augmentation method tailored\nfor this task. SynSonic leverages text-to-audio diffusion models guided by an\nenergy-envelope ControlNet to generate temporally coherent sound events. A\njoint score filtering strategy with dual classifiers ensures sample quality,\nand we explore its practical integration into training pipelines. Experimental\nresults show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1\nand PSDS2), enhancing both temporal localization and sound class\ndiscrimination.",
    "published": "2025-09-23T03:48:26Z",
    "link": "http://arxiv.org/pdf/2509.18603v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Jiarui Hai",
      "Mounya Elhilali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18600v1",
    "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and\n  Factual Radiology Report Generation",
    "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.",
    "published": "2025-09-23T03:42:26Z",
    "link": "http://arxiv.org/pdf/2509.18600v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zhuoxiao Chen",
      "Hongyang Yu",
      "Ying Xu",
      "Yadan Luo",
      "Long Duong",
      "Yuan-Fang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19396v1",
    "title": "OmniFed: A Modular Framework for Configurable Federated Learning from\n  Edge to HPC",
    "summary": "Federated Learning (FL) is critical for edge and High Performance Computing\n(HPC) where data is not centralized and privacy is crucial. We present OmniFed,\na modular framework designed around decoupling and clear separation of concerns\nfor configuration, orchestration, communication, and training logic. Its\narchitecture supports configuration-driven prototyping and code-level\noverride-what-you-need customization. We also support different topologies,\nmixed communication protocols within a single deployment, and popular training\nalgorithms. It also offers optional privacy mechanisms including Differential\nPrivacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well\nas compression strategies. These capabilities are exposed through well-defined\nextension points, allowing users to customize topology and orchestration,\nlearning logic, and privacy/compression plugins, all while preserving the\nintegrity of the core system. We evaluate multiple models and algorithms to\nmeasure various performance metrics. By unifying topology configuration,\nmixed-protocol communication, and pluggable modules in one stack, OmniFed\nstreamlines FL deployment across heterogeneous environments. Github repository\nis available at https://github.com/at-aaims/OmniFed.",
    "published": "2025-09-23T03:40:22Z",
    "link": "http://arxiv.org/pdf/2509.19396v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "authors": [
      "Sahil Tyagi",
      "Andrei Cozma",
      "Olivera Kotevska",
      "Feiyi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18592v1",
    "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
    "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
    "published": "2025-09-23T03:23:03Z",
    "link": "http://arxiv.org/pdf/2509.18592v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Neel P. Bhatt",
      "Yunhao Yang",
      "Rohan Siva",
      "Pranay Samineni",
      "Daniel Milan",
      "Zhangyang Wang",
      "Ufuk Topcu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18585v1",
    "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for\n  Efficient Fine-Tuning",
    "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA.",
    "published": "2025-09-23T03:10:41Z",
    "link": "http://arxiv.org/pdf/2509.18585v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yu Chen",
      "Yifei Han",
      "Long Zhang",
      "Yue Du",
      "Bin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18576v1",
    "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
    "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.",
    "published": "2025-09-23T02:57:25Z",
    "link": "http://arxiv.org/pdf/2509.18576v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Zeyi Kang",
      "Liang He",
      "Yanxin Zhang",
      "Zuheng Ming",
      "Kaixing Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18575v1",
    "title": "The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking",
    "summary": "Large Language Models (LLMs) have demonstrated strong performance in\ninformation retrieval tasks like passage ranking. Our research examines how\ninstruction-following capabilities in LLMs interact with multi-document\ncomparison tasks, identifying what we term the \"Ranking Blind Spot\", a\ncharacteristic of LLM decision processes during comparative evaluation. We\nanalyze how this ranking blind spot affects LLM evaluation systems through two\napproaches: Decision Objective Hijacking, which alters the evaluation goal in\npairwise ranking systems, and Decision Criteria Hijacking, which modifies\nrelevance standards across ranking schemes. These approaches demonstrate how\ncontent providers could potentially influence LLM-based ranking systems to\naffect document positioning. These attacks aim to force the LLM ranker to\nprefer a specific passage and rank it at the top. Malicious content providers\ncan exploit this weakness, which helps them gain additional exposure by\nattacking the ranker. In our experiment, We empirically show that the proposed\nattacks are effective in various LLMs and can be generalized to multiple\nranking schemes. We apply these attack to realistic examples to show their\neffectiveness. We also found stronger LLMs are more vulnerable to these\nattacks. Our code is available at:\nhttps://github.com/blindspotorg/RankingBlindSpot",
    "published": "2025-09-23T02:56:38Z",
    "link": "http://arxiv.org/pdf/2509.18575v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Yaoyao Qian",
      "Yifan Zeng",
      "Yuchao Jiang",
      "Chelsi Jain",
      "Huazheng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18573v1",
    "title": "Interaction Topological Transformer for Multiscale Learning in Porous\n  Materials",
    "summary": "Porous materials exhibit vast structural diversity and support critical\napplications in gas storage, separations, and catalysis. However, predictive\nmodeling remains challenging due to the multiscale nature of structure-property\nrelationships, where performance is governed by both local chemical\nenvironments and global pore-network topology. These complexities, combined\nwith sparse and unevenly distributed labeled data, hinder generalization across\nmaterial families. We propose the Interaction Topological Transformer (ITT), a\nunified data-efficient framework that leverages novel interaction topology to\ncapture materials information across multiple scales and multiple levels,\nincluding structural, elemental, atomic, and pairwise-elemental organization.\nITT extracts scale-aware features that reflect both compositional and\nrelational structure within complex porous frameworks, and integrates them\nthrough a built-in Transformer architecture that supports joint reasoning\nacross scales. Trained using a two-stage strategy, i.e., self-supervised\npretraining on 0.6 million unlabeled structures followed by supervised\nfine-tuning, ITT achieves state-of-the-art, accurate, and transferable\npredictions for adsorption, transport, and stability properties. This framework\nprovides a principled and scalable path for learning-guided discovery in\nstructurally and chemically diverse porous materials.",
    "published": "2025-09-23T02:56:05Z",
    "link": "http://arxiv.org/pdf/2509.18573v1.pdf",
    "category": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "authors": [
      "Dong Chen",
      "Jian Liu",
      "Chun-Long Chen",
      "Guo-Wei Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18569v1",
    "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system",
    "summary": "In recent years, large language models (LLMs) have played an important role\nin automatic speech recognition (ASR) and text-to-speech (TTS) systems. While\nreinforcement learning (RL) has significantly enhanced LLM performance in\ntext-based tasks, its application to ASR and TTS remains underexplored due to\nthe complexity of training audio-based models. In this study, we propose a\nlightweight RL framework tailored for audio-based LLMs that can process audio\ninputs and generate audio outputs. Based on this framework, we evaluate the\neffectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR\ntask, we experiment with different rule-based reward functions within the Group\nRelative Policy Optimization (GRPO) framework and investigate the impact of RL\ndata construction. For the TTS task, we compare GRPO with Differentiable Reward\nOptimization (DiffRO) and further combine the two approaches to achieve\nimproved performance. Our experiments demonstrate that RL can significantly\nenhance the performance of both ASR and TTS systems, even with limited training\ndata and a small number of optimization steps.",
    "published": "2025-09-23T02:52:54Z",
    "link": "http://arxiv.org/pdf/2509.18569v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Changfeng Gao",
      "Yabin Li",
      "Keyu An",
      "Zhifu Gao",
      "Zhihao Du",
      "Han Zhao",
      "Xiangang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18565v1",
    "title": "Solving Math Word Problems Using Estimation Verification and Equation\n  Generation",
    "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.",
    "published": "2025-09-23T02:41:39Z",
    "link": "http://arxiv.org/pdf/2509.18565v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Mitchell Piehl",
      "Dillon Wilson",
      "Ananya Kalita",
      "Jugal Kalita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18562v2",
    "title": "CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese\n  Patronizing and Condescending Language Detection",
    "summary": "Chinese Patronizing and Condescending Language (CPCL) is an implicitly\ndiscriminatory toxic speech targeting vulnerable groups on Chinese video\nplatforms. The existing dataset lacks user comments, which are a direct\nreflection of video content. This undermines the model's understanding of video\ncontent and results in the failure to detect some CPLC videos. To make up for\nthis loss, this research reconstructs a new dataset PCLMMPLUS that includes\n103k comment entries and expands the dataset size. We also propose the\nCPCLDetector model with alignment selection and knowledge-enhanced comment\ncontent modules. Extensive experiments show the proposed CPCLDetector\noutperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS .\nCPLC videos are detected more accurately, supporting content governance and\nprotecting vulnerable groups. Code and dataset are available at\nhttps://github.com/jiaxunyang256/PCLD.",
    "published": "2025-09-23T02:38:49Z",
    "link": "http://arxiv.org/pdf/2509.18562v2.pdf",
    "category": [
      "cs.MM",
      "cs.AI"
    ],
    "authors": [
      "Jiaxun Yang",
      "Yifei Han",
      "Long Zhang",
      "Yujie Liu",
      "Bin Li",
      "Bo Gao",
      "Yangfan He",
      "Kejia Zhan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18561v1",
    "title": "SoundCompass: Navigating Target Sound Extraction With Effective\n  Directional Clue Integration In Complex Acoustic Scenes",
    "summary": "Recent advances in target sound extraction (TSE) utilize directional clues\nderived from direction of arrival (DoA), which represent an inherent spatial\nproperty of sound available in any acoustic scene. However, previous DoA-based\nmethods rely on hand-crafted features or discrete encodings, which lose\nfine-grained spatial information and limit adaptability. We propose\nSoundCompass, an effective directional clue integration framework centered on a\nSpectral Pairwise INteraction (SPIN) module that captures cross-channel spatial\ncorrelations in the complex spectrogram domain to preserve full spatial\ninformation in multichannel signals. The input feature expressed in terms of\nspatial correlations is fused with a DoA clue represented as spherical\nharmonics (SH) encoding. The fusion is carried out across overlapping frequency\nsubbands, inheriting the benefits reported in the previous band-split\narchitectures. We also incorporate the iterative refinement strategy,\nchain-of-inference (CoI), in the TSE framework, which recursively fuses DoA\nwith sound event activation estimated from the previous inference stage.\nExperiments demonstrate that SoundCompass, combining SPIN, SH embedding, and\nCoI, robustly extracts target sources across diverse signal classes and spatial\nconfigurations.",
    "published": "2025-09-23T02:36:39Z",
    "link": "http://arxiv.org/pdf/2509.18561v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Dayun Choi",
      "Jung-Woo Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18557v1",
    "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs",
    "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.",
    "published": "2025-09-23T02:30:14Z",
    "link": "http://arxiv.org/pdf/2509.18557v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tom Pawelek",
      "Raj Patel",
      "Charlotte Crowell",
      "Noorbakhsh Amiri",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Andy Perkins"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18552v1",
    "title": "Global Minimizers of Sigmoid Contrastive Loss",
    "summary": "The meta-task of obtaining and aligning representations through contrastive\npretraining is steadily gaining importance since its introduction in CLIP and\nALIGN. In this paper we theoretically explain the advantages of synchronizing\nwith trainable inverse temperature and bias under the sigmoid loss, as\nimplemented in the recent SigLIP and SigLIP2 models of Google DeepMind.\nTemperature and bias can drive the loss function to zero for a rich class of\nconfigurations that we call $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m},\n\\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object\nrelated to spherical codes and are parametrized by a margin $\\mathsf{m}$ and\nrelative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of\nconstellations to theoretically justify the success of SigLIP on retrieval, to\nexplain the modality gap present in SigLIP, and to identify the necessary\ndimension for producing high-quality representations. Finally, we propose a\nreparameterization of the sigmoid loss with explicit relative bias, which\nimproves training dynamics in experiments with synthetic data.",
    "published": "2025-09-23T02:24:23Z",
    "link": "http://arxiv.org/pdf/2509.18552v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kiril Bangachev",
      "Guy Bresler",
      "Iliyas Noman",
      "Yury Polyanskiy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18542v1",
    "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent\n  Mixture-of-Experts",
    "summary": "Mixture-of-Experts (MoE) models enable scalable performance by activating\nlarge parameter sets sparsely, minimizing computational overhead. To circumvent\nthe prohibitive cost of training MoEs from scratch, recent work employs\nupcycling, reusing a single pre-trained dense model by replicating its\nfeed-forward network (FFN) layers into experts. However, this limits expert\ndiversity, as all experts originate from a single pre-trained dense model. This\npaper addresses this limitation by constructing powerful MoE models using\nexperts sourced from multiple identically-architected but disparate pre-trained\nmodels (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact\nthat these source models occupy disparate, dissonant regions of the parameter\nspace, making direct upcycling prone to severe performance degradation. To\novercome this, we propose Symphony-MoE, a novel two-stage framework designed to\nharmonize these models into a single, coherent expert mixture. First, we\nestablish this harmony in a training-free manner: we construct a shared\nbackbone via a layer-aware fusion strategy and, crucially, alleviate parameter\nmisalignment among experts using activation-based functional alignment.\nSubsequently, a single lightweight stage of router training coordinates the\nentire architecture. Experiments demonstrate that our method successfully\nintegrates experts from heterogeneous sources, achieving an MoE model that\nsignificantly surpasses baselines in multi-domain tasks and out-of-distribution\ngeneralization.",
    "published": "2025-09-23T02:07:14Z",
    "link": "http://arxiv.org/pdf/2509.18542v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qi Wang",
      "Hanyang Peng",
      "Yue Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18536v1",
    "title": "CCQA: Generating Question from Solution Can Improve Inference-Time\n  Reasoning in SLMs",
    "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.",
    "published": "2025-09-23T02:01:03Z",
    "link": "http://arxiv.org/pdf/2509.18536v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jin Young Kim",
      "Ji Won Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18531v1",
    "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody\n  Learning in TTS",
    "summary": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative\nPolicy Optimization (GRPO). However, in the absence of a verifiable reward for\n\\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)\nlowers error rates yet collapses prosody into monotone, unnatural speech;\nadding speaker-similarity further destabilizes training and degrades CER. We\naddress this with an \\textit{iterative Direct Preference Optimization (DPO)}\nscheme that uses only a few hundred human-labeled preference pairs per round to\ndirectly optimize prosodic naturalness while regularizing to the current model.\nOn \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center\ninteractions capturing task-oriented dialogues, our method attains the highest\nhuman preference (ELO) with competitive CER, outperforming GRPO and strong\ncommercial baselines. These results suggest that when prosody cannot be\nrewarded automatically, \\textit{human preference optimization} offers a\npractical and data-efficient path to natural and robust TTS. The demo page is\navailable at \\href{https://tts.ch.dev}",
    "published": "2025-09-23T01:51:38Z",
    "link": "http://arxiv.org/pdf/2509.18531v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "authors": [
      "Seungyoun Shin",
      "Dongha Ahn",
      "Jiwoo Kim",
      "Sungwook Jeon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18527v1",
    "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move\n  Recognition and Rule Reasoning",
    "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.",
    "published": "2025-09-23T01:47:44Z",
    "link": "http://arxiv.org/pdf/2509.18527v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ziwen Chen",
      "Zhong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18523v1",
    "title": "Automatic coherence-driven inference on arguments",
    "summary": "Inconsistencies are ubiquitous in law, administration, and jurisprudence.\nThough a cure is too much to hope for, we propose a technological remedy. Large\nlanguage models (LLMs) can accurately extract propositions from arguments and\ncompile them into natural data structures that enable coherence-driven\ninference (CDI) via combinatorial optimization. This neurosymbolic architecture\nnaturally separates concerns and enables meaningful judgments about the\ncoherence of arguments that can inform legislative and policy analysis and\nlegal reasoning.",
    "published": "2025-09-23T01:40:14Z",
    "link": "http://arxiv.org/pdf/2509.18523v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Steve Huntsman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18521v2",
    "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to Tame\n  Long-tail Generation",
    "summary": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale\npre-trained language models (LLMs). Successive generations, including GPT-o\nseries, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale\nRL training to enhance reasoning and coding capabilities. To meet the\ncommunity's growing RL needs, numerous RL frameworks have been proposed.\nHowever, RL training remains computationally expensive, with rollout generation\naccounting for more than 90% of total runtime. In addition, its efficiency is\noften constrained by the long-tail distribution of rollout response lengths,\nwhere a few lengthy responses stall entire batches, leaving GPUs idle and\nunderutilized. As model and rollout sizes continue to grow, this bottleneck\nincreasingly limits scalability. To address this challenge, we propose Active\nPartial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail\ninefficiency. In the rollout phase, APRIL over-provisions rollout requests,\nterminates once the target number of responses is reached, and recycles\nincomplete responses for continuation in future steps. This strategy ensures\nthat no rollouts are discarded while substantially reducing GPU idle time.\nExperiments show that APRIL improves rollout throughput by at most 44% across\ncommonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and\nachieves at most 8% higher final accuracy across tasks. Moreover, APRIL is both\nframework and hardware agnostic, already integrated into the slime RL\nframework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this\nwork unifies system-level and algorithmic considerations in proposing APRIL,\nwith the aim of advancing RL training efficiency and inspiring further\noptimizations in RL systems. Our codebase is available at\nhttps://github.com/RLsys-Foundation/APRIL",
    "published": "2025-09-23T01:32:36Z",
    "link": "http://arxiv.org/pdf/2509.18521v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yuzhen Zhou",
      "Jiajun Li",
      "Yusheng Su",
      "Gowtham Ramesh",
      "Zilin Zhu",
      "Xiang Long",
      "Chenyang Zhao",
      "Jin Pan",
      "Xiaodong Yu",
      "Ze Wang",
      "Kangrui Du",
      "Jialian Wu",
      "Ximeng Sun",
      "Jiang Liu",
      "Qiaolin Yu",
      "Hao Chen",
      "Zicheng Liu",
      "Emad Barsoum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18520v1",
    "title": "Coherence-driven inference for cybersecurity",
    "summary": "Large language models (LLMs) can compile weighted graphs on natural language\ndata to enable automatic coherence-driven inference (CDI) relevant to red and\nblue team operations in cybersecurity. This represents an early application of\nautomatic CDI that holds near- to medium-term promise for decision-making in\ncybersecurity and eventually also for autonomous blue team operations.",
    "published": "2025-09-23T01:32:28Z",
    "link": "http://arxiv.org/pdf/2509.18520v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Steve Huntsman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18514v1",
    "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition",
    "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.",
    "published": "2025-09-23T01:22:15Z",
    "link": "http://arxiv.org/pdf/2509.18514v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Mohamad Elzohbi",
      "Richard Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18507v1",
    "title": "Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in\n  Neural Imaging Data",
    "summary": "High-dimensional imaging of neural activity, such as widefield calcium and\nfunctional ultrasound imaging, provide a rich source of information for\nunderstanding the relationship between brain activity and behavior. Accurately\nmodeling neural dynamics in these modalities is crucial for understanding this\nrelationship but is hindered by the high-dimensionality, complex spatiotemporal\ndependencies, and prevalent behaviorally irrelevant dynamics in these\nmodalities. Existing dynamical models often employ preprocessing steps to\nobtain low-dimensional representations from neural image modalities. However,\nthis process can discard behaviorally relevant information and miss\nspatiotemporal structure. We propose SBIND, a novel data-driven deep learning\nframework to model spatiotemporal dependencies in neural images and disentangle\ntheir behaviorally relevant dynamics from other neural dynamics. We validate\nSBIND on widefield imaging datasets, and show its extension to functional\nultrasound imaging, a recent modality whose dynamical modeling has largely\nremained unexplored. We find that our model effectively identifies both local\nand long-range spatial dependencies across the brain while also dissociating\nbehaviorally relevant neural dynamics. Doing so, SBIND outperforms existing\nmodels in neural-behavioral prediction. Overall, SBIND provides a versatile\ntool for investigating the neural mechanisms underlying behavior using imaging\nmodalities.",
    "published": "2025-09-23T01:16:23Z",
    "link": "http://arxiv.org/pdf/2509.18507v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Mohammad Hosseini",
      "Maryam M. Shanechi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18504v1",
    "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning",
    "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.",
    "published": "2025-09-23T01:12:21Z",
    "link": "http://arxiv.org/pdf/2509.18504v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Jiaxin Dai",
      "Xiang Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19616v1",
    "title": "BALANCE: Bitrate-Adaptive Limit-Aware Netcast Content Enhancement\n  Utilizing QUBO and Quantum Annealing",
    "summary": "In an era of increasing data cap constraints, optimizing video streaming\nquality while adhering to user-defined data caps remains a significant\nchallenge. This paper introduces Bitrate-Adaptive Limit-Aware Netcast Content\nEnhancement (BALANCE), a novel Quantum framework aimed at addressing this\nissue. BALANCE intelligently pre-selects video segments based on visual\ncomplexity and anticipated data consumption, utilizing the Video Multimethod\nAssessment Fusion (VMAF) metric to enhance Quality of Experience (QoE). We\ncompare our method against traditional bitrate ladders used in Adaptive Bitrate\n(ABR) streaming, demonstrating a notable improvement in QoE under equivalent\ndata constraints. We compare the Slack variable approach with the Dynamic\nPenalization Approach (DPA) by framing the bitrate allocation problem through\nQuadratic Unconstrained Binary Optimization (QUBO) to effectively enforce data\nlimits. Our results indicate that the DPA consistently outperforms the Slack\nVariable Method, delivering more valid and optimal solutions as data limits\nincrease. This new quantum approach significantly enhances streaming\nsatisfaction for users with limited data plans.",
    "published": "2025-09-23T22:11:56Z",
    "link": "http://arxiv.org/pdf/2509.19616v1.pdf",
    "category": [
      "eess.IV",
      "cs.MM",
      "cs.NI",
      "quant-ph",
      "C.2.1; I.1.2; H.5.1"
    ],
    "authors": [
      "Animesh Rajpurohit",
      "Michael Kelley",
      "Wei Wang",
      "Krishna Murthy Kattiyan Ramamoorthy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19469v1",
    "title": "MusiCRS: Benchmarking Audio-Centric Conversational Recommendation",
    "summary": "Conversational recommendation has advanced rapidly with large language models\n(LLMs), yet music remains a uniquely challenging domain where effective\nrecommendations require reasoning over audio content beyond what text or\nmetadata can capture. We present MusiCRS, the first benchmark for audio-centric\nconversational recommendation that links authentic user conversations from\nReddit with corresponding audio tracks. MusiCRS contains 477 high-quality\nconversations spanning diverse genres (classical, hip-hop, electronic, metal,\npop, indie, jazz) with 3,589 unique musical entities and audio grounding via\nYouTube links. MusiCRS enables evaluation across three input modality\nconfigurations: audio-only, query-only, and audio+query (multimodal), allowing\nsystematic comparison of audio-LLMs, retrieval models, and traditional\napproaches. Our experiments reveal that current systems rely heavily on textual\nsignals and struggle with nuanced audio reasoning. This exposes fundamental\nlimitations in cross-modal knowledge integration where models excel at dialogue\nsemantics but cannot effectively ground abstract musical concepts in actual\naudio content. To facilitate progress, we release the MusiCRS dataset\n(https://huggingface.co/datasets/rohan2810/MusiCRS), evaluation code\n(https://github.com/rohan2810/musiCRS), and comprehensive baselines.",
    "published": "2025-09-23T18:24:07Z",
    "link": "http://arxiv.org/pdf/2509.19469v1.pdf",
    "category": [
      "cs.SD",
      "cs.MM"
    ],
    "authors": [
      "Rohan Surana",
      "Amit Namburi",
      "Gagan Mundada",
      "Abhay Lal",
      "Zachary Novack",
      "Julian McAuley",
      "Junda Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19274v1",
    "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language\n  Models' Understanding on Indian Culture",
    "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.",
    "published": "2025-09-23T17:40:43Z",
    "link": "http://arxiv.org/pdf/2509.19274v1.pdf",
    "category": [
      "cs.CL",
      "cs.MM"
    ],
    "authors": [
      "Arijit Maji",
      "Raghvendra Kumar",
      "Akash Ghosh",
      " Anushka",
      "Nemil Shah",
      "Abhilekh Borah",
      "Vanshika Shah",
      "Nishant Mishra",
      "Sriparna Saha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18816v1",
    "title": "Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal\n  Attention in Large Audio Language Models",
    "summary": "Large Audio-Language Models (LALMs) often suffer from audio-textual attention\nimbalance, prioritizing text over acoustic information, particularly in the\nmulti-modal fusion layers of the Transformer architecture. This bias hinders\ntheir ability to fully utilize acoustic cues, causing suboptimal performance on\naudio reasoning tasks. To mitigate this, we propose \\textbf{MATA}, a novel\ntraining-free method that dynamically pushes LALMs to pay \\textbf{M}ore\n\\textbf{A}ttention \\textbf{T}o \\textbf{A}udio tokens within the self-attention\nmechanism. Specifically, MATA intervenes post raw attention scoring, targeting\nonly the last token in intermediate layers without introducing additional\nparameters or computational overhead. Experiments on the MMAU and MMAR\nbenchmarks confirm MATA's effectiveness, with consistent performance gains.\nNotably, on MMAR, MATA enables an open-source model to surpass the proprietary\nGemini 2.0 Flash for the first time. Our work provides an efficient solution to\nmitigate attention bias and opens a new research direction for enhancing the\naudio-processing capabilities of multi-modal models.",
    "published": "2025-09-23T09:02:15Z",
    "link": "http://arxiv.org/pdf/2509.18816v1.pdf",
    "category": [
      "cs.SD",
      "cs.CL",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Junyu Wang",
      "Ziyang Ma",
      "Zhengding Luo",
      "Tianrui Wang",
      "Meng Ge",
      "Xiaobao Wang",
      "Longbiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18717v1",
    "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based\n  Matching and Alignment",
    "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.",
    "published": "2025-09-23T07:05:43Z",
    "link": "http://arxiv.org/pdf/2509.18717v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Tong Zhang",
      "Kuofeng Gao",
      "Jiawang Bai",
      "Leo Yu Zhang",
      "Xin Yin",
      "Zonghui Wang",
      "Shouling Ji",
      "Wenzhi Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18682v1",
    "title": "Harnessing Multimodal Large Language Models for Personalized Product\n  Search with Query-aware Refinement",
    "summary": "Personalized product search (PPS) aims to retrieve products relevant to the\ngiven query considering user preferences within their purchase histories. Since\nlarge language models (LLM) exhibit impressive potential in content\nunderstanding and reasoning, current methods explore to leverage LLM to\ncomprehend the complicated relationships among user, query and product to\nimprove the search performance of PPS. Despite the progress, LLM-based PPS\nsolutions merely take textual contents into consideration, neglecting\nmultimodal contents which play a critical role for product search. Motivated by\nthis, we propose a novel framework, HMPPS, for \\textbf{H}arnessing\n\\textbf{M}ultimodal large language models (MLLM) to deal with\n\\textbf{P}ersonalized \\textbf{P}roduct \\textbf{S}earch based on multimodal\ncontents. Nevertheless, the redundancy and noise in PPS input stand for a great\nchallenge to apply MLLM for PPS, which not only misleads MLLM to generate\ninaccurate search results but also increases the computation expense of MLLM.\nTo deal with this problem, we additionally design two query-aware refinement\nmodules for HMPPS: 1) a perspective-guided summarization module that generates\nrefined product descriptions around core perspectives relevant to search query,\nreducing noise and redundancy within textual contents; and 2) a two-stage\ntraining paradigm that introduces search query for user history filtering based\non multimodal representations, capturing precise user preferences and\ndecreasing the inference cost. Extensive experiments are conducted on four\npublic datasets to demonstrate the effectiveness of HMPPS. Furthermore, HMPPS\nis deployed on an online search system with billion-level daily active users\nand achieves an evident gain in A/B testing.",
    "published": "2025-09-23T06:06:11Z",
    "link": "http://arxiv.org/pdf/2509.18682v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Beibei Zhang",
      "Yanan Lu",
      "Ruobing Xie",
      "Zongyi Li",
      "Siyuan Xing",
      "Tongwei Ren",
      "Fen Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20401v1",
    "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment",
    "summary": "Aligning 3D scene graphs is a crucial initial step for several applications\nin robot navigation and embodied perception. Current methods in 3D scene graph\nalignment often rely on single-modality point cloud data and struggle with\nincomplete or noisy input. We introduce SGAligner++, a cross-modal,\nlanguage-aided framework for 3D scene graph alignment. Our method addresses the\nchallenge of aligning partially overlapping scene observations across\nheterogeneous modalities by learning a unified joint embedding space, enabling\naccurate alignment even under low-overlap conditions and sensor noise. By\nemploying lightweight unimodal encoders and attention-based fusion, SGAligner++\nenhances scene understanding for tasks such as visual localization, 3D\nreconstruction, and navigation, while ensuring scalability and minimal\ncomputational overhead. Extensive evaluations on real-world datasets\ndemonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40%\non noisy real-world reconstructions, while enabling cross-modal generalization.",
    "published": "2025-09-23T18:31:29Z",
    "link": "http://arxiv.org/pdf/2509.20401v1.pdf",
    "category": [
      "cs.GR",
      "cs.RO"
    ],
    "authors": [
      "Binod Singh",
      "Sayan Deb Sarkar",
      "Iro Armeni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.20400v1",
    "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian\n  Bracketing",
    "summary": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting\n(HDR-3DGS) approach for generating HDR novel views given multi-view LDR images.\nUnlike existing methods that typically require the multi-view LDR input images\nto be captured from different exposures, which are tedious to capture and more\nlikely to suffer from errors (e.g., object motion blurs and\ncalibration/alignment inaccuracies), our approach learns the HDR scene\nrepresentation from multi-view LDR images of a single exposure. Our key insight\nto this ill-posed problem is that by first estimating Bracketed 3D Gaussians\n(i.e., with different exposures) from single-exposure multi-view LDR images, we\nmay then be able to merge these bracketed 3D Gaussians into an HDR scene\nrepresentation. Specifically, SeHDR first learns base 3D Gaussians from\nsingle-exposure LDR inputs, where the spherical harmonics parameterize colors\nin a linear color space. We then estimate multiple 3D Gaussians with identical\ngeometry but varying linear colors conditioned on exposure manipulations.\nFinally, we propose the Differentiable Neural Exposure Fusion (NeEF) to\nintegrate the base and estimated 3D Gaussians into HDR Gaussians for novel view\nrendering. Extensive experiments demonstrate that SeHDR outperforms existing\nmethods as well as carefully designed baselines.",
    "published": "2025-09-23T18:28:13Z",
    "link": "http://arxiv.org/pdf/2509.20400v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Yiyu Li",
      "Haoyuan Wang",
      "Ke Xu",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19296v1",
    "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
    "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.",
    "published": "2025-09-23T17:58:01Z",
    "link": "http://arxiv.org/pdf/2509.19296v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Sherwin Bahmani",
      "Tianchang Shen",
      "Jiawei Ren",
      "Jiahui Huang",
      "Yifeng Jiang",
      "Haithem Turki",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Zan Gojcic",
      "Sanja Fidler",
      "Huan Ling",
      "Jun Gao",
      "Xuanchi Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18948v1",
    "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation",
    "summary": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer.",
    "published": "2025-09-23T12:58:15Z",
    "link": "http://arxiv.org/pdf/2509.18948v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Jun Ma",
      "Qian He",
      "Gaofeng He",
      "Huang Chen",
      "Chen Liu",
      "Xiaogang Jin",
      "Huamin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18498v1",
    "title": "null2: Boundary-Dissolving Bodies and Architecture towards Digital\n  Nature",
    "summary": "This paper presents a case study of the thematic pavilion null2 at Expo 2025\nOsaka-Kansai, contrasting with the static Jomon motifs of Taro Okamoto's Tower\nof the Sun from Expo 1970. The study discusses Yayoi-inspired mirror motifs and\ndynamically transforming interactive spatial configuration of null2, where\nvisitors become integrated as experiential content. The shift from static\nrepresentation to a new ontological and aesthetic model, characterized by the\nvisitor's body merging in real-time with architectural space at installation\nscale, is analyzed. Referencing the philosophical context of Expo 1970 theme\n'Progress and Harmony for Mankind,' this research reconsiders the worldview\narticulated by null2 in Expo 2025, in which computation is naturalized and\nubiquitous, through its intersection with Eastern philosophical traditions. It\ninvestigates how immersive experiences within the pavilion, grounded in the\nphilosophical framework of Digital Nature, reinterpret traditional spatial and\nstructural motifs of the tea room, positioning them within contemporary digital\nart discourse. The aim is to contextualize and document null2 as an important\ncontemporary case study from Expo practices, considering the historical and\nsocial background in Japan from the 19th to 21st century, during which world\nexpositions served as pivotal points for the birth of modern Japanese concept\nof 'fine art,' symbolic milestones of economic development, and key moments in\nurban and media culture formation. Furthermore, this paper academically\norganizes architectural techniques, computer graphics methodologies, media art\npractices, and theoretical backgrounds utilized in null2, highlighting the\nscholarly significance of preserving these as an archival document for future\ngenerations.",
    "published": "2025-09-23T01:02:43Z",
    "link": "http://arxiv.org/pdf/2509.18498v1.pdf",
    "category": [
      "cs.HC",
      "cs.GR"
    ],
    "authors": [
      "Yoichi Ochiai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18497v1",
    "title": "Differentiable Light Transport with Gaussian Surfels via Adapted\n  Radiosity for Efficient Relighting and Geometry Reconstruction",
    "summary": "Radiance fields have gained tremendous success with applications ranging from\nnovel view synthesis to geometry reconstruction, especially with the advent of\nGaussian splatting. However, they sacrifice modeling of material reflective\nproperties and lighting conditions, leading to significant geometric\nambiguities and the inability to easily perform relighting. One way to address\nthese limitations is to incorporate physically-based rendering, but it has been\nprohibitively expensive to include full global illumination within the inner\nloop of the optimization. Therefore, previous works adopt simplifications that\nmake the whole optimization with global illumination effects efficient but less\naccurate. In this work, we adopt Gaussian surfels as the primitives and build\nan efficient framework for differentiable light transport, inspired from the\nclassic radiosity theory. The whole framework operates in the coefficient space\nof spherical harmonics, enabling both diffuse and specular materials. We extend\nthe classic radiosity into non-binary visibility and semi-opaque primitives,\npropose novel solvers to efficiently solve the light transport, and derive the\nbackward pass for gradient optimizations, which is more efficient than\nauto-differentiation. During inference, we achieve view-independent rendering\nwhere light transport need not be recomputed under viewpoint changes, enabling\nhundreds of FPS for global illumination effects, including view-dependent\nreflections using a spherical harmonics representation. Through extensive\nqualitative and quantitative experiments, we demonstrate superior geometry\nreconstruction, view synthesis and relighting than previous inverse rendering\nbaselines, or data-driven baselines given relatively sparse datasets with known\nor unknown lighting conditions.",
    "published": "2025-09-23T01:02:31Z",
    "link": "http://arxiv.org/pdf/2509.18497v1.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Kaiwen Jiang",
      "Jia-Mu Sun",
      "Zilu Li",
      "Dan Wang",
      "Tzu-Mao Li",
      "Ravi Ramamoorthi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19644v1",
    "title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using\n  4D Radar",
    "summary": "LiDAR's dense, sharp point cloud (PC) representations of the surrounding\nenvironment enable accurate perception and significantly improve road safety by\noffering greater scene awareness and understanding. However, LiDAR's high cost\ncontinues to restrict the broad adoption of high-level Autonomous Driving (AD)\nsystems in commercially available vehicles. Prior research has shown progress\ntowards circumventing the need for LiDAR by training a neural network, using\nLiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds\nusing only 4D Radars. One of the best examples is a neural network created to\ntrain a more efficient radar target detector with a modular 2D convolutional\nneural network (CNN) backbone and a temporal coherence network at its core that\nuses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we\ninvestigate the impact of higher-capacity segmentation backbones on the quality\nof the produced point clouds. Our results show that while very high-capacity\nmodels may actually hurt performance, an optimal segmentation backbone can\nprovide a 23.7% improvement over the state-of-the-art (SOTA).",
    "published": "2025-09-23T23:21:50Z",
    "link": "http://arxiv.org/pdf/2509.19644v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "William L. Muckelroy III",
      "Mohammed Alsakabi",
      "John M. Dolan",
      "Ozan K. Tonguz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19636v1",
    "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing",
    "summary": "Autonomous racing has seen significant advancements, driven by competitions\nsuch as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing\nLeague (A2RL). However, developing an autonomous racing stack for a full-scale\ncar is often constrained by limited access to dedicated test tracks,\nrestricting opportunities for real-world validation. While previous work\ntypically requires extended development cycles and significant track time, this\npaper introduces a minimalistic autonomous racing stack for high-speed\ntime-trial racing that emphasizes rapid deployment and efficient system\nintegration with minimal on-track testing. The proposed stack was validated on\nreal speedways, achieving a top speed of 206 km/h within just 11 hours'\npractice run on the track with 325 km in total. Additionally, we present the\nsystem performance analysis, including tracking accuracy, vehicle dynamics, and\nsafety considerations, offering insights for teams seeking to rapidly develop\nand deploy an autonomous racing stack with limited track access.",
    "published": "2025-09-23T22:53:29Z",
    "link": "http://arxiv.org/pdf/2509.19636v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Mahmoud Ali",
      "Hassan Jardali",
      "Youwei Yu",
      "Durgakant Pushp",
      "Lantao Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19626v1",
    "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric\n  Human Data",
    "summary": "Egocentric human experience data presents a vast resource for scaling up\nend-to-end imitation learning for robotic manipulation. However, significant\ndomain gaps in visual appearance, sensor modalities, and kinematics between\nhuman and robot impede knowledge transfer. This paper presents EgoBridge, a\nunified co-training framework that explicitly aligns the policy latent spaces\nbetween human and robot data using domain adaptation. Through a measure of\ndiscrepancy on the joint policy latent features and actions based on Optimal\nTransport (OT), we learn observation representations that not only align\nbetween the human and robot domain but also preserve the action-relevant\ninformation critical for policy learning. EgoBridge achieves a significant\nabsolute policy success rate improvement by 44% over human-augmented\ncross-embodiment baselines in three real-world single-arm and bimanual\nmanipulation tasks. EgoBridge also generalizes to new objects, scenes, and\ntasks seen only in human data, where baselines fail entirely. Videos and\nadditional information can be found at https://ego-bridge.github.io",
    "published": "2025-09-23T22:34:47Z",
    "link": "http://arxiv.org/pdf/2509.19626v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Ryan Punamiya",
      "Dhruv Patel",
      "Patcharapong Aphiwetsa",
      "Pranav Kuppili",
      "Lawrence Y. Zhu",
      "Simar Kareer",
      "Judy Hoffman",
      "Danfei Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19610v1",
    "title": "Look as You Leap: Planning Simultaneous Motion and Perception for\n  High-DOF Robots",
    "summary": "In this work, we address the problem of planning robot motions for a\nhigh-degree-of-freedom (DoF) robot that effectively achieves a given perception\ntask while the robot and the perception target move in a dynamic environment.\nAchieving navigation and perception tasks simultaneously is challenging, as\nthese objectives often impose conflicting requirements. Existing methods that\ncompute motion under perception constraints fail to account for obstacles, are\ndesigned for low-DoF robots, or rely on simplified models of perception.\nFurthermore, in dynamic real-world environments, robots must replan and react\nquickly to changes and directly evaluating the quality of perception (e.g.,\nobject detection confidence) is often expensive or infeasible at runtime. This\nproblem is especially important in human-centered environments such as homes\nand hospitals, where effective perception is essential for safe and reliable\noperation. To address these challenges, we propose a GPU-parallelized\nperception-score-guided probabilistic roadmap planner with a neural surrogate\nmodel (PS-PRM). The planner explicitly incorporates the estimated quality of a\nperception task into motion planning for high-DoF robots. Our method uses a\nlearned model to approximate perception scores and leverages GPU parallelism to\nenable efficient online replanning in dynamic settings. We demonstrate that our\nplanner, evaluated on high-DoF robots, outperforms baseline methods in both\nstatic and dynamic environments in both simulation and real-robot experiments.",
    "published": "2025-09-23T22:00:51Z",
    "link": "http://arxiv.org/pdf/2509.19610v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Qingxi Meng",
      "Emiliano Flores",
      "Carlos Quintero-Peña",
      "Peizhu Qian",
      "Zachary Kingston",
      "Shannan K. Hamlin",
      "Vaibhav Unhelkar",
      "Lydia E. Kavraki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19597v1",
    "title": "From Space to Time: Enabling Adaptive Safety with Learned Value\n  Functions via Disturbance Recasting",
    "summary": "The widespread deployment of autonomous systems in safety-critical\nenvironments such as urban air mobility hinges on ensuring reliable,\nperformant, and safe operation under varying environmental conditions. One such\napproach, value function-based safety filters, minimally modifies a nominal\ncontroller to ensure safety. Recent advances leverage offline learned value\nfunctions to scale these safety filters to high-dimensional systems. However,\nthese methods assume detailed priors on all possible sources of model mismatch,\nin the form of disturbances in the environment -- information that is rarely\navailable in real world settings. Even in well-mapped environments like urban\ncanyons or industrial sites, drones encounter complex, spatially-varying\ndisturbances arising from payload-drone interaction, turbulent airflow, and\nother environmental factors. We introduce SPACE2TIME, which enables safe and\nadaptive deployment of offline-learned safety filters under unknown,\nspatially-varying disturbances. The key idea is to reparameterize spatial\nvariations in disturbance as temporal variations, enabling the use of\nprecomputed value functions during online operation. We validate SPACE2TIME on\na quadcopter through extensive simulations and hardware experiments,\ndemonstrating significant improvement over baselines.",
    "published": "2025-09-23T21:44:53Z",
    "link": "http://arxiv.org/pdf/2509.19597v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Sander Tonkens",
      "Nikhil Uday Shinde",
      "Azra Begzadić",
      "Michael C. Yip",
      "Jorge Cortés",
      "Sylvia L. Herbert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19579v1",
    "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic\n  Outdoor Mapping",
    "summary": "Outdoor intelligent autonomous robotic operation relies on a sufficiently\nexpressive map of the environment. Classical geometric mapping methods retain\nessential structural environment information, but lack a semantic understanding\nand organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)\naddress this limitation by integrating geometric, topological, and semantic\nrelationships into a multi-level graph-based map. Outdoor autonomous operations\ncommonly rely on terrain information either due to task-dependence or the\ntraversability of the robotic platform. We propose a novel approach that\ncombines indoor 3DSG techniques with standard outdoor geometric mapping and\nterrain-aware reasoning, producing terrain-aware place nodes and hierarchically\norganized regions for outdoor environments. Our method generates a\ntask-agnostic metric-semantic sparse map and constructs a 3DSG from this map\nfor downstream planning tasks, all while remaining lightweight for autonomous\nrobotic operation. Our thorough evaluation demonstrates our 3DSG method\nperforms on par with state-of-the-art camera-based 3DSG methods in object\nretrieval and surpasses them in region classification while remaining memory\nefficient. We demonstrate its effectiveness in diverse robotic tasks of object\nretrieval and region monitoring in both simulation and real-world environments.",
    "published": "2025-09-23T21:05:30Z",
    "link": "http://arxiv.org/pdf/2509.19579v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chad R. Samuelson",
      "Abigail Austin",
      "Seth Knoop",
      "Blake Romrell",
      "Gabriel R. Slade",
      "Timothy W. McLain",
      "Joshua G. Mangelson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19573v1",
    "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided\n  Reinforcement Learning",
    "summary": "Achieving highly dynamic behaviors on humanoid robots, such as running,\nrequires controllers that are both robust and precise, and hence difficult to\ndesign. Classical control methods offer valuable insight into how such systems\ncan stabilize themselves, but synthesizing real-time controllers for nonlinear\nand hybrid dynamics remains challenging. Recently, reinforcement learning (RL)\nhas gained popularity for locomotion control due to its ability to handle these\ncomplex dynamics. In this work, we embed ideas from nonlinear control theory,\nspecifically control Lyapunov functions (CLFs), along with optimized dynamic\nreference trajectories into the reinforcement learning training process to\nshape the reward. This approach, CLF-RL, eliminates the need to handcraft and\ntune heuristic reward terms, while simultaneously encouraging certifiable\nstability and providing meaningful intermediate rewards to guide learning. By\ngrounding policy learning in dynamically feasible trajectories, we expand the\nrobot's dynamic capabilities and enable running that includes both flight and\nsingle support phases. The resulting policy operates reliably on a treadmill\nand in outdoor environments, demonstrating robustness to disturbances applied\nto the torso and feet. Moreover, it achieves accurate global reference tracking\nutilizing only on-board sensors, making a critical step toward integrating\nthese dynamic motions into a full autonomy stack.",
    "published": "2025-09-23T20:57:45Z",
    "link": "http://arxiv.org/pdf/2509.19573v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zachary Olkin",
      "Kejun Li",
      "William D. Compton",
      "Aaron D. Ames"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19571v1",
    "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for\n  Robot Action",
    "summary": "Executing open-ended natural language queries is a core problem in robotics.\nWhile recent advances in imitation learning and vision-language-actions models\n(VLAs) have enabled promising end-to-end policies, these models struggle when\nfaced with complex instructions and new scenes. An alternative is to design an\nexplicit scene representation as a queryable interface between the robot and\nthe world, using query results to guide downstream motion planning. In this\nwork, we present Agentic Scene Policies (ASP), an agentic framework that\nleverages the advanced semantic, spatial, and affordance-based querying\ncapabilities of modern scene representations to implement a capable\nlanguage-conditioned robot policy. ASP can execute open-vocabulary queries in a\nzero-shot manner by explicitly reasoning about object affordances in the case\nof more complex skills. Through extensive experiments, we compare ASP with VLAs\non tabletop manipulation problems and showcase how ASP can tackle room-level\nqueries through affordance-guided navigation, and a scaled-up scene\nrepresentation. (Project page:\nhttps://montrealrobotics.ca/agentic-scene-policies.github.io/)",
    "published": "2025-09-23T20:56:00Z",
    "link": "http://arxiv.org/pdf/2509.19571v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Sacha Morin",
      "Kumaraditya Gupta",
      "Mahtab Sandhu",
      "Charlie Gauthier",
      "Francesco Argenziano",
      "Kirsty Ellis",
      "Liam Paull"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19555v1",
    "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint\n  Parameterization in the Latent Space",
    "summary": "Recent works have shown that foundational safe control methods, such as\nHamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space\nof world models. While this enables the synthesis of latent safety filters for\nhard-to-model vision-based tasks, they assume that the safety constraint is\nknown a priori and remains fixed during deployment, limiting the safety\nfilter's adaptability across scenarios. To address this, we propose\nconstraint-parameterized latent safety filters that can adapt to user-specified\nsafety constraints at runtime. Our key idea is to define safety constraints by\nconditioning on an encoding of an image that represents a constraint, using a\nlatent-space similarity measure. The notion of similarity to failure is aligned\nin a principled way through conformal calibration, which controls how closely\nthe system may approach the constraint representation. The parameterized safety\nfilter is trained entirely within the world model's imagination, treating any\nimage seen by the model as a potential test-time constraint, thereby enabling\nruntime adaptation to arbitrary safety constraints. In simulation and hardware\nexperiments on vision-based control tasks with a Franka manipulator, we show\nthat our method adapts at runtime by conditioning on the encoding of\nuser-specified constraint images, without sacrificing performance. Video\nresults can be found on https://any-safe.github.io",
    "published": "2025-09-23T20:28:04Z",
    "link": "http://arxiv.org/pdf/2509.19555v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Sankalp Agrawal",
      "Junwon Seo",
      "Kensuke Nakamura",
      "Ran Tian",
      "Andrea Bajcsy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19545v1",
    "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based\n  Locomotion on Bipedal and Humanoid Robots",
    "summary": "We present RoMoCo, an open-source C++ toolbox for the synthesis and\nevaluation of reduced-order model-based planners and whole-body controllers for\nbipedal and humanoid robots. RoMoCo's modular architecture unifies\nstate-of-the-art planners and whole-body locomotion controllers under a\nconsistent API, enabling rapid prototyping and reproducible benchmarking. By\nleveraging reduced-order models for platform-agnostic gait generation, RoMoCo\nenables flexible controller design across diverse robots. We demonstrate its\nversatility and performance through extensive simulations on the Cassie,\nUnitree H1, and G1 robots, and validate its real-world efficacy with hardware\nexperiments on the Cassie and G1 humanoids.",
    "published": "2025-09-23T20:17:47Z",
    "link": "http://arxiv.org/pdf/2509.19545v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Min Dai",
      "Aaron D. Ames"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19541v1",
    "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic\n  Platform Built Upon a Generalized Software Architecture",
    "summary": "Despite the rapidly growing applications of robots in industry, the use of\nrobots to automate tasks in scientific laboratories is less prolific due to\nlack of generalized methodologies and high cost of hardware. This paper focuses\non the automation of characterization tasks necessary for reducing cost while\nmaintaining generalization, and proposes a software architecture for building\nrobotic systems in scientific laboratory environment. A dual-layer (Socket.IO\nand ROS) action server design is the basic building block, which facilitates\nthe implementation of a web-based front end for user-friendly operations and\nthe use of ROS Behavior Tree for convenient task planning and execution. A\nrobotic platform for automating mineral and material sample characterization is\nbuilt upon the architecture, with an open source, low-cost three-axis computer\nnumerical control gantry system serving as the main robot. A handheld laser\ninduced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed\nadapter, enabling automated 2D chemical mapping. We demonstrate the utility of\nautomated chemical mapping by scanning of the surface of a spodumene-bearing\npegmatite core sample with a 1071-point dense hyperspectral map acquired at a\nrate of 1520 bits per second. Automated LIBS scanning enables controlled\nchemical quantification in the laboratory that complements field-based\nmeasurements acquired with the same handheld device, linking resource\nexploration and processing steps in the supply chain for lithium-based battery\nmaterials.",
    "published": "2025-09-23T20:09:33Z",
    "link": "http://arxiv.org/pdf/2509.19541v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xuan Cao",
      "Yuxin Wu",
      "Michael L. Whittaker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19525v1",
    "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft\n  Robot",
    "summary": "Closed-loop control remains an open challenge in soft robotics. The nonlinear\nresponses of soft actuators under dynamic loading conditions limit the use of\nanalytic models for soft robot control. Traditional methods of controlling soft\nrobots underutilize their configuration spaces to avoid nonlinearity,\nhysteresis, large deformations, and the risk of actuator damage. Furthermore,\nepisodic data-driven control approaches such as reinforcement learning (RL) are\ntraditionally limited by sample efficiency and inconsistency across\ninitializations. In this work, we demonstrate RL for reliably learning control\npolicies for dynamic balancing tasks in real-time single-shot hardware\ndeployments. We use a deformable Stewart platform constructed using parallel,\n3D-printed soft actuators based on motorized handed shearing auxetic (HSA)\nstructures. By introducing a curriculum learning approach based on expanding\nneighborhoods of a known equilibrium, we achieve reliable single-deployment\nbalancing at arbitrary coordinates. In addition to benchmarking the performance\nof model-based and model-free methods, we demonstrate that in a single\ndeployment, Maximum Diffusion RL is capable of learning dynamic balancing after\nhalf of the actuators are effectively disabled, by inducing buckling and by\nbreaking actuators with bolt cutters. Training occurs with no prior data, in as\nfast as 15 minutes, with performance nearly identical to the fully-intact\nplatform. Single-shot learning on hardware facilitates soft robotic systems\nreliably learning in the real world and will enable more diverse and capable\nsoft robots.",
    "published": "2025-09-23T19:45:21Z",
    "link": "http://arxiv.org/pdf/2509.19525v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "James Avtges",
      "Jake Ketchum",
      "Millicent Schlafly",
      "Helena Young",
      "Taekyoung Kim",
      "Allison Pinosky",
      "Ryan L. Truby",
      "Todd D. Murphey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19522v1",
    "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle",
    "summary": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a\nbioinspired SLAM framework based on computational models of the rodent\nhippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based\nSLAM, suitable for GPS-denied environments. Our contributions include a\nROS2-based architecture, experimental results on new waterway datasets, and\ninsights into system parameter tuning. This work represents the first known\napplication of RatSLAM on USVs. The estimated trajectory was compared with\nground truth data using the Hausdorff distance. The results show that the\nalgorithm can generate a semimetric map with an error margin acceptable for\nmost robotic applications.",
    "published": "2025-09-23T19:39:54Z",
    "link": "http://arxiv.org/pdf/2509.19522v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Fabio Coelho",
      "Joao Victor T. Borges",
      "Paulo Padrao",
      "Jose Fuentes",
      "Ramon R. Costa",
      "Liu Hsu",
      "Leonardo Bobadilla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19521v1",
    "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using\n  TinyML and Sensor Fusion",
    "summary": "Gesture-based control for mobile manipulators faces persistent challenges in\nreliability, efficiency, and intuitiveness. This paper presents a dual-hand\ngesture interface that integrates TinyML, spectral analysis, and sensor fusion\nwithin a ROS framework to address these limitations. The system uses left-hand\ntilt and finger flexion, captured using accelerometer and flex sensors, for\nmobile base navigation, while right-hand IMU signals are processed through\nspectral analysis and classified by a lightweight neural network. This pipeline\nenables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3\nmanipulator. By supporting simultaneous navigation and manipulation, the\nframework improves efficiency and coordination compared to sequential methods.\nKey contributions include a bimanual control architecture, real-time low-power\ngesture recognition, robust multimodal sensor fusion, and a scalable ROS-based\nimplementation. The proposed approach advances Human-Robot Interaction (HRI)\nfor industrial automation, assistive robotics, and hazardous environments,\noffering a cost-effective, open-source solution with strong potential for\nreal-world deployment and further optimization.",
    "published": "2025-09-23T19:39:00Z",
    "link": "http://arxiv.org/pdf/2509.19521v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Najeeb Ahmed Bhuiyan",
      "M. Nasimul Huq",
      "Sakib H. Chowdhury",
      "Rahul Mangharam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19486v1",
    "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots",
    "summary": "This paper presents SHARP (Supercomputing for High-speed Avoidance and\nReactive Planning), a proof-of-concept study demonstrating how high-performance\ncomputing (HPC) can enable millisecond-scale responsiveness in robotic control.\nWhile modern robots face increasing demands for reactivity in human--robot\nshared workspaces, onboard processors are constrained by size, power, and cost.\nOffloading to HPC offers massive parallelism for trajectory planning, but its\nfeasibility for real-time robotics remains uncertain due to network latency and\njitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator\nmust dodge high-speed foam projectiles. Using a parallelized multi-goal A*\nsearch implemented with MPI on both local and remote HPC clusters, the system\nachieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300\nkm away), with avoidance success rates of 84% and 88%, respectively. These\nresults show that when round-trip latency remains within the\ntens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,\nenabling avoidance well below human reaction times. The SHARP results motivate\nhybrid control architectures: low-level reflexes remain onboard for safety,\nwhile bursty, high-throughput planning tasks are offloaded to HPC for\nscalability. By reporting per-stage timing and success rates, this study\nprovides a reproducible template for assessing real-time feasibility of\nHPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable\npathway toward dependable, reactive robots in dynamic environments.",
    "published": "2025-09-23T18:48:56Z",
    "link": "http://arxiv.org/pdf/2509.19486v1.pdf",
    "category": [
      "cs.RO",
      "cs.DC"
    ],
    "authors": [
      "Kieran S. Lachmansingh",
      "José R. González-Estrada",
      "Ryan E. Grant",
      "Matthew K. X. J. Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19480v1",
    "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation",
    "summary": "Humans can flexibly interpret and compose different goal specifications, such\nas language instructions, spatial coordinates, or visual references, when\nnavigating to a destination. In contrast, most existing robotic navigation\npolicies are trained on a single modality, limiting their adaptability to\nreal-world scenarios where different forms of goal specification are natural\nand complementary. In this work, we present a training framework for robotic\nfoundation models that enables omni-modal goal conditioning for vision-based\nnavigation. Our approach leverages a high-capacity vision-language-action (VLA)\nbackbone and trains with three primary goal modalities: 2D poses, egocentric\nimages, and natural language, as well as their combinations, through a\nrandomized modality fusion strategy. This design not only expands the pool of\nusable datasets but also encourages the policy to develop richer geometric,\nsemantic, and visual representations. The resulting model, OmniVLA, achieves\nstrong generalization to unseen environments, robustness to scarce modalities,\nand the ability to follow novel natural language instructions. We demonstrate\nthat OmniVLA outperforms specialist baselines across modalities and offers a\nflexible foundation for fine-tuning to new modalities and tasks. We believe\nOmniVLA provides a step toward broadly generalizable and flexible navigation\npolicies, and a scalable path for building omni-modal robotic foundation\nmodels. We present videos showcasing OmniVLA performance and will release its\ncheckpoints and training code on our project page.",
    "published": "2025-09-23T18:40:29Z",
    "link": "http://arxiv.org/pdf/2509.19480v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Noriaki Hirose",
      "Catherine Glossop",
      "Dhruv Shah",
      "Sergey Levine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19477v1",
    "title": "Robust Near-Optimal Nonlinear Target Enclosing Guidance",
    "summary": "This paper proposes a nonlinear optimal guidance law that enables a pursuer\nto enclose a target within arbitrary geometric patterns, which extends beyond\nconventional circular encirclement. The design operates using only relative\nstate measurements and formulates a target enclosing guidance law in which the\nvehicle's lateral acceleration serves as the steering control, making it\nwell-suited for aerial vehicles with turning constraints. Our approach\ngeneralizes and extends existing guidance strategies that are limited to target\nencirclement and provides a degree of optimality. At the same time, the exact\ninformation of the target's maneuver is unnecessary during the design. The\nguidance law is developed within the framework of a state-dependent Riccati\nequation (SDRE), thereby providing a systematic way to handle nonlinear\ndynamics through a pseudo-linear representation to design locally optimal\nfeedback guidance commands through state-dependent weighting matrices. While\nSDRE ensures near-optimal performance in the absence of strong disturbances, we\nfurther augment the design to incorporate an integral sliding mode manifold to\ncompensate when disturbances push the system away from the nominal trajectory,\nand demonstrate that the design provides flexibility in the sense that the\n(possibly time-varying) stand-off curvature could also be treated as unknown.\nSimulations demonstrate the efficacy of the proposed approach.",
    "published": "2025-09-23T18:37:51Z",
    "link": "http://arxiv.org/pdf/2509.19477v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY",
      "math.OC"
    ],
    "authors": [
      "Abhinav Sinha",
      "Rohit V. Nanavati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19473v1",
    "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)",
    "summary": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping\nstone for future missions to Mars and beyond. The discovery of water ice on the\nMoon's craters located in permanently shadowed regions, which can provide\ndrinking water, oxygen, and rocket fuel, is therefore of critical importance.\nHowever, current methods to access lunar ice deposits are limited. While rovers\nhave been used to explore the lunar surface for decades, they face significant\nchallenges in navigating harsh terrains, such as permanently shadowed craters,\ndue to the high risk of immobilization. This report introduces COBRA (Crater\nObserving Bio-inspired Rolling Articulator), a multi-modal snake-style robot\ndesigned to overcome mobility challenges in Shackleton Crater's rugged\nenvironment. COBRA combines slithering and tumbling locomotion to adapt to\nvarious crater terrains. In snake mode, it uses sidewinding to traverse flat or\nlow inclined surfaces, while in tumbling mode, it forms a circular barrel by\nlinking its head and tail, enabling rapid movement with minimal energy on steep\nslopes. Equipped with an onboard computer, stereo camera, inertial measurement\nunit, and joint encoders, COBRA facilitates real-time data collection and\nautonomous operation. This paper highlights COBRAs robustness and efficiency in\nnavigating extreme terrains through both simulations and experimental\nvalidation.",
    "published": "2025-09-23T18:28:37Z",
    "link": "http://arxiv.org/pdf/2509.19473v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Adarsh Salagame",
      "Henry Noyes",
      "Alireza Ramezani",
      "Eric Sihite",
      "Arash Kalantari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19463v1",
    "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception",
    "summary": "A central challenge for multi-robot systems is fusing independently gathered\nperception data into a unified representation. Despite progress in\nCollaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of\ndedicated multi-robot datasets. Many evaluations instead partition single-robot\ntrajectories, a practice that may only partially reflect true multi-robot\noperations and, more critically, lacks standardization, leading to results that\nare difficult to interpret or compare across studies. While several multi-robot\ndatasets have recently been introduced, they mostly contain short trajectories\nwith limited inter-robot overlap and sparse intra-robot loop closures. To\novercome these limitations, we introduce CU-Multi, a dataset collected over\nmultiple days at two large outdoor sites on the University of Colorado Boulder\ncampus. CU-Multi comprises four synchronized runs with aligned start times and\ncontrolled trajectory overlap, replicating the distinct perspectives of a robot\nteam. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined\nground-truth odometry. By combining overlap variation with dense semantic\nannotations, CU-Multi provides a strong foundation for reproducible evaluation\nin multi-robot collaborative perception tasks.",
    "published": "2025-09-23T18:17:21Z",
    "link": "http://arxiv.org/pdf/2509.19463v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Doncey Albin",
      "Daniel McGann",
      "Miles Mena",
      "Annika Thomas",
      "Harel Biggie",
      "Xuefei Sun",
      "Steve McGuire",
      "Jonathan P. How",
      "Christoffer Heckman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19452v2",
    "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured\n  Environments via Instantaneous Relative Frames",
    "summary": "Search and rescue operations require unmanned aerial vehicles to both\ntraverse unknown unstructured environments at high speed and track targets once\ndetected. Achieving both capabilities under degraded sensing and without global\nlocalization remains an open challenge. Recent works on relative navigation\nhave shown robust tracking by anchoring planning and control to a visible\ndetected object, but cannot address navigation when no target is in the field\nof view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time\nframework that unifies traversal, acquisition, and tracking within a single\nrelative formulation. HUNT defines navigation objectives directly from onboard\ninstantaneous observables such as attitude, altitude, and velocity, enabling\nreactive high-speed flight during search. Once a target is detected, the same\nperception-control pipeline transitions seamlessly to tracking. Outdoor\nexperiments in dense forests, container compounds, and search-and-rescue\noperations with vehicles and mannequins demonstrate robust autonomy where\nglobal methods fail.",
    "published": "2025-09-23T18:07:10Z",
    "link": "http://arxiv.org/pdf/2509.19452v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Alessandro Saviolo",
      "Jeffrey Mao",
      "Giuseppe Loianno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19301v1",
    "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
    "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io",
    "published": "2025-09-23T17:59:46Z",
    "link": "http://arxiv.org/pdf/2509.19301v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Lars Ankile",
      "Zhenyu Jiang",
      "Rocky Duan",
      "Guanya Shi",
      "Pieter Abbeel",
      "Anusha Nagabandi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19261v1",
    "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under\n  Changing External Forces",
    "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.",
    "published": "2025-09-23T17:19:25Z",
    "link": "http://arxiv.org/pdf/2509.19261v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kuanqi Cai",
      "Chunfeng Wang",
      "Zeqi Li",
      "Haowen Yao",
      "Weinan Chen",
      "Luis Figueredo",
      "Aude Billard",
      "Arash Ajoudani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19246v1",
    "title": "Proactive-reactive detection and mitigation of intermittent faults in\n  robot swarms",
    "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.",
    "published": "2025-09-23T17:06:52Z",
    "link": "http://arxiv.org/pdf/2509.19246v1.pdf",
    "category": [
      "cs.RO",
      "cs.MA",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Sinan Oğuz",
      "Emanuele Garone",
      "Marco Dorigo",
      "Mary Katherine Heinrich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19169v1",
    "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human\n  Demonstration to Robotic Deployment Gap",
    "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
    "published": "2025-09-23T15:43:28Z",
    "link": "http://arxiv.org/pdf/2509.19169v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Tianyu Wu",
      "Xudong Han",
      "Haoran Sun",
      "Zishang Zhang",
      "Bangchao Huang",
      "Chaoyang Song",
      "Fang Wan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19168v1",
    "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot\n  Coordination",
    "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.",
    "published": "2025-09-23T15:43:18Z",
    "link": "http://arxiv.org/pdf/2509.19168v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mark Gonzales",
      "Ethan Oh",
      "Joseph Moore"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19142v1",
    "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
    "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer",
    "published": "2025-09-23T15:26:04Z",
    "link": "http://arxiv.org/pdf/2509.19142v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kangmin Kim",
      "Seunghyeok Back",
      "Geonhyup Lee",
      "Sangbeom Lee",
      "Sangjun Noh",
      "Kyoobin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19110v1",
    "title": "A Fast Initialization Method for Neural Network Controllers: A Case\n  Study of Image-based Visual Servoing Control for the multicopter Interception",
    "summary": "Reinforcement learning-based controller design methods often require\nsubstantial data in the initial training phase. Moreover, the training process\ntends to exhibit strong randomness and slow convergence. It often requires\nconsiderable time or high computational resources. Another class of\nlearning-based method incorporates Lyapunov stability theory to obtain a\ncontrol policy with stability guarantees. However, these methods generally\nrequire an initially stable neural network control policy at the beginning of\ntraining. Evidently, a stable neural network controller can not only serve as\nan initial policy for reinforcement learning, allowing the training to focus on\nimproving controller performance, but also act as an initial state for\nlearning-based Lyapunov control methods. Although stable controllers can be\ndesigned using traditional control theory, designers still need to have a great\ndeal of control design knowledge to address increasingly complicated control\nproblems. The proposed neural network rapid initialization method in this paper\nachieves the initial training of the neural network control policy by\nconstructing datasets that conform to the stability conditions based on the\nsystem model. Furthermore, using the image-based visual servoing control for\nmulticopter interception as a case study, simulations and experiments were\nconducted to validate the effectiveness and practical performance of the\nproposed method. In the experiment, the trained control policy attains a final\ninterception velocity of 15 m/s.",
    "published": "2025-09-23T14:56:59Z",
    "link": "http://arxiv.org/pdf/2509.19110v1.pdf",
    "category": [
      "eess.SY",
      "cs.LG",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Chenxu Ke",
      "Congling Tian",
      "Kaichen Xu",
      "Ye Li",
      "Lingcong Bao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19105v1",
    "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
    "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.",
    "published": "2025-09-23T14:49:48Z",
    "link": "http://arxiv.org/pdf/2509.19105v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Sarvesh Prajapati",
      "Ananya Trivedi",
      "Nathaniel Hanson",
      "Bruce Maxwell",
      "Taskin Padir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19076v1",
    "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic\n  Interventions",
    "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.",
    "published": "2025-09-23T14:35:53Z",
    "link": "http://arxiv.org/pdf/2509.19076v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Laura Connolly",
      "Aravind S. Kumar",
      "Kapi Ketan Mehta",
      "Lidia Al-Zogbi",
      "Peter Kazanzides",
      "Parvin Mousavi",
      "Gabor Fichtinger",
      "Axel Krieger",
      "Junichi Tokuda",
      "Russell H. Taylor",
      "Simon Leonard",
      "Anton Deguet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19047v1",
    "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware\n  Representation for Contact-Rich Manipulation",
    "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.",
    "published": "2025-09-23T14:15:19Z",
    "link": "http://arxiv.org/pdf/2509.19047v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Geonhyup Lee",
      "Yeongjin Lee",
      "Kangmin Kim",
      "Seongju Lee",
      "Sangjun Noh",
      "Seunghyeok Back",
      "Kyoobin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19037v1",
    "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile\n  Sensors",
    "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/",
    "published": "2025-09-23T14:09:03Z",
    "link": "http://arxiv.org/pdf/2509.19037v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Qingzheng Cong",
      "Steven Oh",
      "Wen Fan",
      "Shan Luo",
      "Kaspar Althoefer",
      "Dandan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18979v1",
    "title": "Category-Level Object Shape and Pose Estimation in Less Than a\n  Millisecond",
    "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.",
    "published": "2025-09-23T13:29:32Z",
    "link": "http://arxiv.org/pdf/2509.18979v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Lorenzo Shaikewitz",
      "Tim Nguyen",
      "Luca Carlone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18954v1",
    "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty\n  Estimation",
    "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.",
    "published": "2025-09-23T13:02:44Z",
    "link": "http://arxiv.org/pdf/2509.18954v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Minoo Dolatabadi",
      "Fardin Ayar",
      "Ehsan Javanmardi",
      "Manabu Tsukada",
      "Mahdi Javanmardi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18937v1",
    "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
    "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.",
    "published": "2025-09-23T12:54:52Z",
    "link": "http://arxiv.org/pdf/2509.18937v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yanyuan Qiao",
      "Kieran Gilday",
      "Yutong Xie",
      "Josie Hughes"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18865v1",
    "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language\n  Fusion for Action Generation",
    "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/",
    "published": "2025-09-23T10:02:16Z",
    "link": "http://arxiv.org/pdf/2509.18865v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Masato Kobayashi",
      "Thanpimon Buamanee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18830v1",
    "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning\n  Contact-Rich Manipulation",
    "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.",
    "published": "2025-09-23T09:16:34Z",
    "link": "http://arxiv.org/pdf/2509.18830v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Suzannah Wistreich",
      "Baiyu Shi",
      "Stephen Tian",
      "Samuel Clarke",
      "Michael Nath",
      "Chengyi Xu",
      "Zhenan Bao",
      "Jiajun Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18793v1",
    "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments\n  and Reconfigurations",
    "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .",
    "published": "2025-09-23T08:36:08Z",
    "link": "http://arxiv.org/pdf/2509.18793v1.pdf",
    "category": [
      "cs.RO",
      "cs.MA",
      "cs.SE"
    ],
    "authors": [
      "Lukas Zanger",
      "Bastian Lampe",
      "Lennart Reiher",
      "Lutz Eckstein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18786v2",
    "title": "Human-Interpretable Uncertainty Explanations for Point Cloud\n  Registration",
    "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.",
    "published": "2025-09-23T08:23:51Z",
    "link": "http://arxiv.org/pdf/2509.18786v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Johannes A. Gaus",
      "Loris Schneider",
      "Yitian Shi",
      "Jongseok Lee",
      "Rania Rayyes",
      "Rudolph Triebel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18760v1",
    "title": "Guaranteed Robust Nonlinear MPC via Disturbance Feedback",
    "summary": "Robots must satisfy safety-critical state and input constraints despite\ndisturbances and model mismatch. We introduce a robust model predictive control\n(RMPC) formulation that is fast, scalable, and compatible with real-time\nimplementation. Our formulation guarantees robust constraint satisfaction,\ninput-to-state stability (ISS) and recursive feasibility. The key idea is to\ndecompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic\nmodel, (ii) disturbance-feedback controllers, and (iii) bounds on the model\nerror. These components are optimized jointly using sequential convex\nprogramming. The resulting convex subproblems are solved efficiently using a\nrecent disturbance-feedback MPC solver. The approach is validated across\nmultiple dynamics, including a rocket-landing problem with steerable thrust. An\nopen-source implementation is available at\nhttps://github.com/antoineleeman/robust-nonlinear-mpc.",
    "published": "2025-09-23T07:54:20Z",
    "link": "http://arxiv.org/pdf/2509.18760v1.pdf",
    "category": [
      "math.OC",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Antoine P. Leeman",
      "Johannes Köhler",
      "Melanie N. Zeilinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18749v1",
    "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional\n  Measurements",
    "summary": "This article examines state estimation in discrete-time nonlinear stochastic\nsystems with finite-dimensional states and infinite-dimensional measurements,\nmotivated by real-world applications such as vision-based localization and\ntracking. We develop an extended Kalman filter (EKF) for real-time state\nestimation, with the measurement noise modeled as an infinite-dimensional\nrandom field. When applied to vision-based state estimation, the measurement\nJacobians required to implement the EKF are shown to correspond to image\ngradients. This result provides a novel system-theoretic justification for the\nuse of image gradients as features for vision-based state estimation,\ncontrasting with their (often heuristic) introduction in many computer-vision\npipelines. We demonstrate the practical utility of the EKF on a public\nreal-world dataset involving the localization of an aerial drone using video\nfrom a downward-facing monocular camera. The EKF is shown to outperform\nVINS-MONO, an established visual-inertial odometry algorithm, in some cases\nachieving mean squared error reductions of up to an order of magnitude.",
    "published": "2025-09-23T07:47:32Z",
    "link": "http://arxiv.org/pdf/2509.18749v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Maxwell M. Varley",
      "Timothy L. Molloy",
      "Girish N. Nair"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18734v1",
    "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
    "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.",
    "published": "2025-09-23T07:27:48Z",
    "link": "http://arxiv.org/pdf/2509.18734v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nishant Doshi",
      "Amey Sutvani",
      "Sanket Gujar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18723v1",
    "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output\n  Dynamics with Validation in Robotic Systems",
    "summary": "Solving motion tasks autonomously and accurately is a core ability for\nintelligent real-world systems. To achieve genuine autonomy across multiple\nsystems and tasks, key challenges include coping with unknown dynamics and\novercoming the need for manual parameter tuning, which is especially crucial in\ncomplex Multiple-Input Multiple-Output (MIMO) systems.\n  This paper presents MIMO Dual Iterative Learning Control (DILC), a novel\ndata-driven iterative learning scheme for simultaneous tracking control and\nmodel learning, without requiring any prior system knowledge or manual\nparameter tuning. The method is designed for repetitive MIMO systems and\nintegrates seamlessly with established iterative learning control methods. We\nprovide monotonic convergence conditions for both reference tracking error and\nmodel error in linear time-invariant systems.\n  The DILC scheme -- rapidly and autonomously -- solves various motion tasks in\nhigh-fidelity simulations of an industrial robot and in multiple nonlinear\nreal-world MIMO systems, without requiring model knowledge or manually tuning\nthe algorithm. In our experiments, many reference tracking tasks are solved\nwithin 10-20 trials, and even complex motions are learned in less than 100\niterations. We believe that, because of its rapid and autonomous learning\ncapabilities, DILC has the potential to serve as an efficient building block\nwithin complex learning frameworks for intelligent real-world systems.",
    "published": "2025-09-23T07:11:11Z",
    "link": "http://arxiv.org/pdf/2509.18723v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Jan-Hendrik Ewering",
      "Alessandro Papa",
      "Simon F. G. Ehlers",
      "Thomas Seel",
      "Michael Meindl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18686v1",
    "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly",
    "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.",
    "published": "2025-09-23T06:10:46Z",
    "link": "http://arxiv.org/pdf/2509.18686v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Ziyi Xu",
      "Haohong Lin",
      "Shiqi Liu",
      "Ding Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18676v1",
    "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow\n  in 3D Space",
    "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.",
    "published": "2025-09-23T05:48:01Z",
    "link": "http://arxiv.org/pdf/2509.18676v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Sangjun Noh",
      "Dongwoo Nam",
      "Kangmin Kim",
      "Geonhyup Lee",
      "Yeonguk Yu",
      "Raeyoung Kang",
      "Kyoobin Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18671v1",
    "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference\n  from Rollout",
    "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.",
    "published": "2025-09-23T05:41:59Z",
    "link": "http://arxiv.org/pdf/2509.18671v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kaixin Chai",
      "Hyunjun Lee",
      "Joseph J. Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18666v1",
    "title": "Distributionally Robust Safe Motion Planning with Contextual Information",
    "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.",
    "published": "2025-09-23T05:34:06Z",
    "link": "http://arxiv.org/pdf/2509.18666v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Kaizer Rahaman",
      "Simran Kumari",
      "Ashish R. Hota"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18636v1",
    "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance\n  in Narrow Environments",
    "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.",
    "published": "2025-09-23T04:39:21Z",
    "link": "http://arxiv.org/pdf/2509.18636v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yuan Zhou",
      "Jialiang Hou",
      "Guangtong Xu",
      "Fei Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18610v1",
    "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for\n  Drones",
    "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.",
    "published": "2025-09-23T03:57:34Z",
    "link": "http://arxiv.org/pdf/2509.18610v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Maximilian Adang",
      "JunEn Low",
      "Ola Shorinwa",
      "Mac Schwager"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18609v1",
    "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for\n  Autonomous Driving",
    "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.",
    "published": "2025-09-23T03:57:33Z",
    "link": "http://arxiv.org/pdf/2509.18609v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chengran Yuan",
      "Zijian Lu",
      "Zhanqi Zhang",
      "Yimin Zhao",
      "Zefan Huang",
      "Shuo Sun",
      "Jiawei Sun",
      "Jiahui Li",
      "Christina Dao Wen Lee",
      "Dongen Li",
      "Marcelo H. Ang Jr"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18597v2",
    "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code\n  Generation Framework for Long-Horizon Manipulation Skills",
    "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.",
    "published": "2025-09-23T03:34:19Z",
    "link": "http://arxiv.org/pdf/2509.18597v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yuan Meng",
      "Zhenguo Sun",
      "Max Fest",
      "Xukun Li",
      "Zhenshan Bing",
      "Alois Knoll"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18566v1",
    "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene\n  Reconstruction",
    "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
    "published": "2025-09-23T02:50:56Z",
    "link": "http://arxiv.org/pdf/2509.18566v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "authors": [
      "Xiaoting Yin",
      "Hao Shi",
      "Kailun Yang",
      "Jiajun Zhai",
      "Shangwei Guo",
      "Lin Wang",
      "Kaiwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18506v1",
    "title": "Spatial Envelope MPC: High Performance Driving without a Reference",
    "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.",
    "published": "2025-09-23T01:16:01Z",
    "link": "http://arxiv.org/pdf/2509.18506v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Siyuan Yu",
      "Congkai Shen",
      "Yufei Xi",
      "James Dallas",
      "Michael Thompson",
      "John Subosits",
      "Hiroshi Yasuda",
      "Tulga Ersal"
    ]
  }
]