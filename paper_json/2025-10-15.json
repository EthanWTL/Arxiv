[
  {
    "id": "http://arxiv.org/abs/2510.10300v3",
    "title": "The Algorithmic Regulator",
    "summary": "The regulator theorem states that, under certain conditions, any optimal\ncontroller must embody a model of the system it regulates, grounding the idea\nthat controllers embed, explicitly or implicitly, internal models of the\ncontrolled. This principle underpins neuroscience and predictive brain theories\nlike the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However,\nthe theorem is only proven in limited settings. Here, we treat the\ndeterministic, closed, coupled world-regulator system $(W,R)$ as a single\nself-delimiting program $p$ via a constant-size wrapper that produces the world\noutput string~$x$ fed to the regulator. We analyze regulation from the\nviewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to\nbe a \\emph{good algorithmic regulator} if it \\emph{reduces} the algorithmic\ncomplexity of the readout relative to a null (unregulated) baseline\n$\\varnothing$, i.e., \\[ \\Delta = K\\big(O_{W,\\varnothing}\\big) -\nK\\big(O_{W,R}\\big) > 0. \\] We then prove that the larger $\\Delta$ is, the more\nworld-regulator pairs with high mutual algorithmic information are favored.\nMore precisely, a complexity gap $\\Delta > 0$ yields \\[ \\Pr\\big((W,R)\\mid\nx\\big) \\le C\\,2^{\\,M(W{:}R)}\\,2^{-\\Delta}, \\] making low $M(W{:}R)$\nexponentially unlikely as $\\Delta$ grows. This is an AIT version of the idea\nthat ``the regulator contains a model of the world.'' The framework is\ndistribution-free, applies to individual sequences, and complements the\nInternal Model Principle. Beyond this necessity claim, the same coding-theorem\ncalculus singles out a \\emph{canonical scalar objective} and implicates a\n\\emph{planner}. On the realized episode, a regulator behaves \\emph{as if} it\nminimized the conditional description length of the readout.",
    "published": "2025-10-11T17:54:08Z",
    "updated": "2025-10-15T10:23:52Z",
    "link": "http://arxiv.org/pdf/2510.10300v3.pdf",
    "category": [
      "cs.CC",
      "cs.AI",
      "cs.IT",
      "cs.SY",
      "eess.SY",
      "math.IT",
      "q-bio.NC"
    ],
    "authors": [
      "Giulio Ruffini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12563v2",
    "title": "HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic\n  Puzzle Games",
    "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance on\ncomplex tasks, including logical puzzle games that require deriving solutions\nsatisfying all constraints. However, whether they can flexibly apply\nappropriate rules to varying conditions, particularly when faced with\nnon-canonical game variants, remains an open question. Existing corpora focus\non popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats\nand memorization of solution patterns, which can mask deficiencies in\nunderstanding novel rules or adapting strategies to new variants. To address\nthis, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles\nacross 10 games, designed to test the robustness of LRMs on the \"long-tail\" of\nlogical games. HardcoreLogic systematically transforms canonical puzzles\nthrough three dimensions: Increased Complexity (IC), Uncommon Elements (UE),\nand Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.\nEvaluations on a diverse set of LRMs reveal significant performance drops, even\nfor models achieving top scores on existing benchmarks, indicating heavy\nreliance on memorized stereotypes. While increased complexity is the dominant\nsource of difficulty, models also struggle with subtle rule variations that do\nnot necessarily increase puzzle difficulty. Our systematic error analysis on\nsolvable and unsolvable puzzles further highlights gaps in genuine reasoning.\nOverall, HardcoreLogic exposes the limitations of current LRMs and establishes\na benchmark for advancing high-level logical reasoning.",
    "published": "2025-10-14T14:23:24Z",
    "updated": "2025-10-15T10:31:28Z",
    "link": "http://arxiv.org/pdf/2510.12563v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jingcong Liang",
      "Shijun Wan",
      "Xuehai Wu",
      "Yitong Li",
      "Qianglong Chen",
      "Duyu Tang",
      "Siyuan Wang",
      "Zhongyu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12364v2",
    "title": "(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm",
    "summary": "Recent advancements in generative artificial intelligence (GenAI),\nparticularly large language models, have introduced new possibilities for\nsoftware development practices. In our paper we investigate the emerging Vibe\nCoding (VC) paradigm that emphasizes intuitive, affect-driven, and\nimprovisational interactions between developers and AI systems. Building upon\nthe discourse of End-User Development (EUD), we explore how VC diverges from\nconventional programming approaches such as those supported by tools like\nGitHub Copilot. Through five semi-structured interview sessions with ten\nexperienced software practitioners, we identify five thematic dimensions:\ncreativity, sustainability, the future of programming, collaboration, and\ncriticism. Our analysis conceptualizes VC within the metaphor of co-drifting,\ncontrasting it with the prevalent co-piloting perspective of AI-assisted\ndevelopment. We argue that VC reconfigures the developers role, blurring\nboundaries between professional and non-developers. While VC enables novel\nforms of expression and rapid prototyping, it also introduces challenges\nregarding reproducibility, scalability, and inclusivity. We propose that VC\nrepresents a meaningful shift in programming culture, warranting further\ninvestigation within human-computer interaction (HCI) and software engineering\nresearch.",
    "published": "2025-10-14T10:25:56Z",
    "updated": "2025-10-15T14:43:26Z",
    "link": "http://arxiv.org/pdf/2510.12364v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI",
      "cs.HC",
      "D.2.3"
    ],
    "authors": [
      "Kevin Krings",
      "Nino S. Bohn",
      "Thomas Ludwig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12269v2",
    "title": "Tensor Logic: The Language of AI",
    "summary": "Progress in AI is hindered by the lack of a programming language with all the\nrequisite features. Libraries like PyTorch and TensorFlow provide automatic\ndifferentiation and efficient GPU implementation, but are additions to Python,\nwhich was never intended for AI. Their lack of support for automated reasoning\nand knowledge acquisition has led to a long and costly series of hacky attempts\nto tack them on. On the other hand, AI languages like LISP an Prolog lack\nscalability and support for learning. This paper proposes tensor logic, a\nlanguage that solves these problems by unifying neural and symbolic AI at a\nfundamental level. The sole construct in tensor logic is the tensor equation,\nbased on the observation that logical rules and Einstein summation are\nessentially the same operation, and all else can be reduced to them. I show how\nto elegantly implement key forms of neural, symbolic and statistical AI in\ntensor logic, including transformers, formal reasoning, kernel machines and\ngraphical models. Most importantly, tensor logic makes new directions possible,\nsuch as sound reasoning in embedding space. This combines the scalability and\nlearnability of neural networks with the reliability and transparency of\nsymbolic reasoning, and is potentially a basis for the wider adoption of AI.",
    "published": "2025-10-14T08:24:08Z",
    "updated": "2025-10-15T01:45:27Z",
    "link": "http://arxiv.org/pdf/2510.12269v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.PL",
      "stat.ML",
      "I.2.3; I.2.4; I.2.5; I.2.6; I.5.1"
    ],
    "authors": [
      "Pedro Domingos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10412v4",
    "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate\n  Time Series",
    "summary": "Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://github.com/blacksnail789521/Time-IMM, and the benchmark library can be\naccessed at https://github.com/blacksnail789521/IMM-TSF. Project page:\nhttps://blacksnail789521.github.io/time-imm-project-page/",
    "published": "2025-06-12T07:07:22Z",
    "updated": "2025-10-15T08:31:07Z",
    "link": "http://arxiv.org/pdf/2506.10412v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Ching Chang",
      "Jeehyun Hwang",
      "Yidan Shi",
      "Haixin Wang",
      "Wen-Chih Peng",
      "Tien-Fu Chen",
      "Wei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01656v3",
    "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM\n  reasoning",
    "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing\nthem with average advantage baselines. This shift is largely pragmatic:\nconventional value functions are computationally expensive to train at LLM\nscale and often fail under sparse rewards and long reasoning horizons. We\nrevisit this bottleneck from an architectural perspective and introduce\nAsymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable\nframework that restores the critics role while remaining efficient in\nlarge-model settings. AsyPPO employs a set of lightweight mini-critics, each\ntrained on disjoint prompt shards. This design encourages diversity while\npreserving calibration, reducing value-estimation bias. Beyond robust\nestimation, AsyPPO leverages inter-critic uncertainty to refine the policy\nupdate: (i) masking advantages in states where critics agree and gradients add\nlittle learning signal, and (ii) filtering high-divergence states from entropy\nregularization, suppressing spurious exploration. After training on open-source\ndata with only 5,000 samples, AsyPPO consistently improves learning stability\nand performance across multiple benchmarks over strong baselines, such as GRPO,\nachieving performance gains of more than six percent on Qwen3-4b-Base and about\nthree percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without\nadditional tricks. These results highlight the importance of architectural\ninnovations for scalable, efficient algorithms.",
    "published": "2025-10-02T04:24:27Z",
    "updated": "2025-10-15T08:36:52Z",
    "link": "http://arxiv.org/pdf/2510.01656v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiashun Liu",
      "Johan Obando-Ceron",
      "Han Lu",
      "Yancheng He",
      "Weixun Wang",
      "Wenbo Su",
      "Bo Zheng",
      "Pablo Samuel Castro",
      "Aaron Courville",
      "Ling Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12047v2",
    "title": "Do Large Language Models Respect Contracts? Evaluating and Enforcing\n  Contract-Adherence in Code Generation",
    "summary": "Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,\nprimarily evaluate large language models (LLMs) with pass@k on functional\ncorrectness using well-formed inputs. However, they ignore a crucial aspect of\nreal-world software: adherence to contracts-the preconditions and validity\nconstraints that dictate how ill-formed inputs must be rejected. This critical\noversight means that existing benchmarks fail to measure, and models\nconsequently fail to generate, truly robust and reliable code snippets. We\nintroduce PACT, a program assessment and contract-adherence evaluation\nframework, to bridge this gap. PACT is the first framework designed to\nsystematically evaluate and enhance contract-adherence in LLM-generated code\nsnippets alongside functional correctness. PACT's contributions are threefold:\nFirst, it provides a comprehensive test-suite corpus focused on contract\nviolations, extending HumanEval+ and MBPP+. Second, it enables a systematic\nanalysis of code generation under varied prompting conditions. This analysis\ndemonstrates that augmenting prompts with contract-violating test cases\nsignificantly enhance a model's ability to respect contracts compared to using\ncontract description alone. Finally, it introduces novel metrics to rigorously\nquantify contract adherence in both test generation and code generation. By\nrevealing critical errors that conventional benchmarks overlook, PACT provides\nthe rigorous and interpretable metrics to evaluate the robustness of\nLLM-generated code snippets in both functionality and contract-adherence. Our\ncode and data are available at https://github.com/suhanmen/PACT.",
    "published": "2025-10-14T01:12:37Z",
    "updated": "2025-10-15T02:21:54Z",
    "link": "http://arxiv.org/pdf/2510.12047v2.pdf",
    "category": [
      "cs.AI",
      "cs.SE",
      "68T01",
      "I.2.7"
    ],
    "authors": [
      "Soohan Lim",
      "Joonghyuk Hahn",
      "Hyunwoo Park",
      "Sang-Ki Ko",
      "Yo-Sub Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11955v2",
    "title": "Y-shaped Generative Flows",
    "summary": "Modern continuous-time generative models often induce V-shaped transport:\neach sample travels independently along nearly straight trajectories from prior\nto data, overlooking shared structure. We introduce Y-shaped generative flows,\nwhich move probability mass together along shared pathways before branching to\ntarget-specific endpoints. Our formulation is based on novel velocity-powered\nobjective with a sublinear exponent (between zero and one). this concave\ndependence rewards joint and fast mass movement. Practically, we instantiate\nthe idea in a scalable neural ODE training objective. On synthetic, image, and\nbiology datasets, Y-flows recover hierarchy-aware structure, improve\ndistributional metrics over strong flow-based baselines, and reach targets with\nfewer integration steps.",
    "published": "2025-10-13T21:33:37Z",
    "updated": "2025-10-15T10:40:14Z",
    "link": "http://arxiv.org/pdf/2510.11955v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Arip Asadulaev",
      "Semyon Semenov",
      "Abduragim Shtanchaev",
      "Eric Moulines",
      "Fakhri Karray",
      "Martin Takac"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06443v3",
    "title": "Superior Molecular Representations from Intermediate Encoder Layers",
    "summary": "Pretrained molecular encoders have become indispensable in computational\nchemistry for tasks such as property prediction and molecular generation.\nHowever, the standard practice of relying solely on final-layer embeddings for\ndownstream tasks may discard valuable information. In this work, we first\nanalyze the information flow in five diverse molecular encoders and find that\nintermediate layers retain more general-purpose features, whereas the\nfinal-layer specializes and compresses information. We then perform an\nempirical layer-wise evaluation across 22 property prediction tasks. We find\nthat using frozen embeddings from optimal intermediate layers improves\ndownstream performance by an average of 5.4%, up to 28.6%, compared to the\nfinal-layer. Furthermore, finetuning encoders truncated at intermediate depths\nachieves even greater average improvements of 8.5%, with increases as high as\n40.8%, obtaining new state-of-the-art results on several benchmarks. These\nfindings highlight the importance of exploring the full representational depth\nof molecular encoders to achieve substantial performance improvements and\ncomputational efficiency. The code will be made publicly available.",
    "published": "2025-06-06T18:03:51Z",
    "updated": "2025-10-15T01:55:53Z",
    "link": "http://arxiv.org/pdf/2506.06443v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "authors": [
      "Luis Pinto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10645v2",
    "title": "Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse\n  Ensemble of Reaction Scorers",
    "summary": "Retrosynthesis is one of the domains transformed by the rise of generative\nmodels, and it is one where the problem of nonsensical or erroneous outputs\n(hallucinations) is particularly insidious: reliable assessment of synthetic\nplans is time-consuming, with automatic methods lacking. In this work, we\npresent RetroTrim, a retrosynthesis system that successfully avoids nonsensical\nplans on a set of challenging drug-like targets. Compared to common baselines\nin the field, our system is not only the sole method that succeeds in filtering\nout hallucinated reactions, but it also results in the highest number of\nhigh-quality paths overall. The key insight behind RetroTrim is the combination\nof diverse reaction scoring strategies, based on machine learning models and\nexisting chemical databases. We show that our scoring strategies capture\ndifferent classes of hallucinations by analyzing them on a dataset of labeled\nretrosynthetic intermediates. This approach formed the basis of our winning\nsolution to the Standard Industries \\$1 million Retrosynthesis Challenge. To\nmeasure the performance of retrosynthesis systems, we propose a novel\nevaluation protocol for reactions and synthetic paths based on a structured\nreview by expert chemists. Using this protocol, we compare systems on a set of\n32 novel targets, curated to reflect recent trends in drug structures. While\nthe insights behind our methodology are broadly applicable to retrosynthesis,\nour focus is on targets in the drug-like domain. By releasing our benchmark\ntargets and the details of our evaluation protocol, we hope to inspire further\nresearch into reliable retrosynthesis.",
    "published": "2025-10-12T14:56:34Z",
    "updated": "2025-10-15T15:58:16Z",
    "link": "http://arxiv.org/pdf/2510.10645v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Michal Sadowski",
      "Tadija Radusinović",
      "Maria Wyrzykowska",
      "Lukasz Sztukiewicz",
      "Jan Rzymkowski",
      "Paweł Włodarczyk-Pruszyński",
      "Mikołaj Sacha",
      "Piotr Kozakowski",
      "Ruard van Workum",
      "Stanislaw Kamil Jastrzebski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23144v3",
    "title": "Coordination Requires Simplification: Thermodynamic Bounds on\n  Multi-Objective Compromise in Natural and Artificial Intelligence",
    "summary": "Information-processing systems that coordinate multiple agents and objectives\nface fundamental thermodynamic constraints. We show that solutions with maximum\nutility to act as coordination focal points have a much higher selection\npressure for being findable across agents rather than accuracy. We derive that\nthe information-theoretic minimum description length of coordination protocols\nto precision $\\varepsilon$ scales as $L(P)\\geq NK\\log_2 K+N^2d^2\\log\n(1/\\varepsilon)$ for $N$ agents with $d$ potentially conflicting objectives and\ninternal model complexity $K$. This scaling forces progressive simplification,\nwith coordination dynamics changing the environment itself and shifting\noptimization across hierarchical levels. Moving from established focal points\nrequires re-coordination, creating persistent metastable states and hysteresis\nuntil significant environmental shifts trigger phase transitions through\nspontaneous symmetry breaking. We operationally define coordination temperature\nto predict critical phenomena and estimate coordination work costs, identifying\nmeasurable signatures across systems from neural networks to restaurant bills\nto bureaucracies. Extending the topological version of Arrow's theorem on the\nimpossibility of consistent preference aggregation, we find it recursively\nbinds whenever preferences are combined. This potentially explains the\nindefinite cycling in multi-objective gradient descent and alignment faking in\nLarge Language Models trained with reinforcement learning with human feedback.\nWe term this framework Thermodynamic Coordination Theory (TCT), which\ndemonstrates that coordination requires radical information loss.",
    "published": "2025-09-27T06:16:56Z",
    "updated": "2025-10-14T20:38:12Z",
    "link": "http://arxiv.org/pdf/2509.23144v3.pdf",
    "category": [
      "cs.AI",
      "cond-mat.stat-mech",
      "cs.MA",
      "nlin.AO",
      "physics.soc-ph"
    ],
    "authors": [
      "Atma Anand"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10125v2",
    "title": "Ctrl-World: A Controllable Generative World Model for Robot Manipulation",
    "summary": "Generalist robot policies can now perform a wide range of manipulation\nskills, but evaluating and improving their ability with unfamiliar objects and\ninstructions remains a significant challenge. Rigorous evaluation requires a\nlarge number of real-world rollouts, while systematic improvement demands\nadditional corrective data with expert labels. Both of these processes are\nslow, costly, and difficult to scale. World models offer a promising, scalable\nalternative by enabling policies to rollout within imagination space. However,\na key challenge is building a controllable world model that can handle\nmulti-step interactions with generalist robot policies. This requires a world\nmodel compatible with modern generalist policies by supporting multi-view\nprediction, fine-grained action control, and consistent long-horizon\ninteractions, which is not achieved by previous works. In this paper, we make a\nstep forward by introducing a controllable multi-view world model that can be\nused to evaluate and improve the instruction-following ability of generalist\nrobot policies. Our model maintains long-horizon consistency with a\npose-conditioned memory retrieval mechanism and achieves precise action control\nthrough frame-level action conditioning. Trained on the DROID dataset (95k\ntrajectories, 564 scenes), our model generates spatially and temporally\nconsistent trajectories under novel scenarios and new camera placements for\nover 20 seconds. We show that our method can accurately rank policy performance\nwithout real-world robot rollouts. Moreover, by synthesizing successful\ntrajectories in imagination and using them for supervised fine-tuning, our\napproach can improve policy success by 44.7\\%.",
    "published": "2025-10-11T09:13:10Z",
    "updated": "2025-10-15T00:46:49Z",
    "link": "http://arxiv.org/pdf/2510.10125v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yanjiang Guo",
      "Lucy Xiaoyang Shi",
      "Jianyu Chen",
      "Chelsea Finn"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.08891v4",
    "title": "Reliable Decision Making via Calibration Oriented Retrieval Augmented\n  Generation",
    "summary": "Recently, Large Language Models (LLMs) have been increasingly used to support\nvarious decision-making tasks, assisting humans in making informed decisions.\nHowever, when LLMs confidently provide incorrect information, it can lead\nhumans to make suboptimal decisions. To prevent LLMs from generating incorrect\ninformation on topics they are unsure of and to improve the accuracy of\ngenerated content, prior works have proposed Retrieval Augmented Generation\n(RAG), where external documents are referenced to generate responses. However,\nprevious RAG methods focus only on retrieving documents most relevant to the\ninput query, without specifically aiming to ensure that the human user's\ndecisions are well-calibrated. To address this limitation, we propose a novel\nretrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),\nwhich ensures that decisions informed by RAG are well-calibrated. Then we\nempirically validate that CalibRAG improves calibration performance as well as\naccuracy, compared to other baselines across various datasets.",
    "published": "2024-10-28T06:41:05Z",
    "updated": "2025-10-15T13:04:09Z",
    "link": "http://arxiv.org/pdf/2411.08891v4.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Chaeyun Jang",
      "Deukhwan Cho",
      "Seanie Lee",
      "Hyungi Lee",
      "Juho Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12621v2",
    "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
    "summary": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation.",
    "published": "2025-10-14T15:20:06Z",
    "updated": "2025-10-15T06:42:22Z",
    "link": "http://arxiv.org/pdf/2510.12621v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Iñaki Lacunza",
      "Javier Garcia Gilabert",
      "Francesca De Luca Fornaciari",
      "Javier Aula-Blasco",
      "Aitor Gonzalez-Agirre",
      "Maite Melero",
      "Marta Villegas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15969v3",
    "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
    "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
    "published": "2025-06-19T02:25:04Z",
    "updated": "2025-10-15T01:55:31Z",
    "link": "http://arxiv.org/pdf/2506.15969v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Haoyue Zhang",
      "Hualei Zhang",
      "Xiaosong Ma",
      "Jie Zhang",
      "Song Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12041v2",
    "title": "Improving Text-to-Image Generation with Input-Side Inference-Time\n  Scaling",
    "summary": "Recent advances in text-to-image (T2I) generation have achieved impressive\nresults, yet existing models often struggle with simple or underspecified\nprompts, leading to suboptimal image-text alignment, aesthetics, and quality.\nWe propose a prompt rewriting framework that leverages large language models\n(LLMs) to refine user inputs before feeding them into T2I backbones. Our\napproach introduces a carefully designed reward system and an iterative direct\npreference optimization (DPO) training pipeline, enabling the rewriter to\nenhance prompts without requiring supervised fine-tuning data. We evaluate our\nmethod across diverse T2I models and benchmarks. Results show that our prompt\nrewriter consistently improves image-text alignment, visual quality, and\naesthetics, outperforming strong baselines. Furthermore, we demonstrate strong\ntransferability by showing that a prompt rewriter trained on one T2I backbone\ngeneralizes effectively to others without needing to be retrained. We also\nsystematically study scalability, evaluating how performance gains scale with\nthe capacity of the large LLM used as the rewriter. These findings highlight\nthat prompt rewriting is an effective, scalable, and practical model-agnostic\nstrategy for improving T2I systems. We plan to release the code and trained\nprompt rewriters soon.",
    "published": "2025-10-14T00:51:39Z",
    "updated": "2025-10-15T03:43:21Z",
    "link": "http://arxiv.org/pdf/2510.12041v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ruibo Chen",
      "Jiacheng Pan",
      "Heng Huang",
      "Zhenheng Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.12470v3",
    "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
    "summary": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet\ntheir reliance on structured step-by-step processing reveals a critical\nlimitation. In contrast, human cognition fluidly adapts between intuitive,\nheuristic (System 1) and analytical, deliberative (System 2) reasoning\ndepending on the context. This difference between human cognitive flexibility\nand LLMs' reliance on a single reasoning style raises a critical question:\nwhile human fast heuristic reasoning evolved for its efficiency and\nadaptability, is a uniform reasoning approach truly optimal for LLMs, or does\nits inflexibility make them brittle and unreliable when faced with tasks\ndemanding more agile, intuitive responses? To answer these questions, we\nexplicitly align LLMs to these reasoning styles by curating a dataset with\nvalid System 1 and System 2 answers, and evaluate their performance across\nreasoning benchmarks. Our results reveal an accuracy-efficiency trade-off:\nSystem 2-aligned models excel in arithmetic and symbolic reasoning, while\nSystem 1-aligned models perform better in commonsense reasoning tasks. To\nanalyze the reasoning spectrum, we interpolated between the two extremes by\nvarying the proportion of alignment data, which resulted in a monotonic change\nin accuracy. A mechanistic analysis of model responses shows that System 1\nmodels employ more definitive outputs, whereas System 2 models demonstrate\ngreater uncertainty. Building on these findings, we further combine System 1-\nand System 2-aligned models based on the entropy of their generations, without\nadditional training, and obtain a dynamic model that outperforms across nearly\nall benchmarks. This work challenges the assumption that step-by-step reasoning\nis always optimal and highlights the need for adapting reasoning strategies\nbased on task demands.",
    "published": "2025-02-18T02:58:37Z",
    "updated": "2025-10-15T17:36:19Z",
    "link": "http://arxiv.org/pdf/2502.12470v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Alireza S. Ziabari",
      "Nona Ghazizadeh",
      "Zhivar Sourati",
      "Farzan Karimi-Malekabadi",
      "Payam Piray",
      "Morteza Dehghani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11052v2",
    "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by\n  Refining Belief States",
    "summary": "Autoregressive (AR) models remain the standard for natural language\ngeneration but still suffer from high latency due to strictly sequential\ndecoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,\nmitigate this by generating in parallel, yet they suffer from two core\nlimitations: information loss, as predictive distributions for non-finalized\ntokens are discarded at each step, and premature commitment, where local\ndecisions are made without sufficient global coordination. We introduce Latent\nRefinement Decoding (LRD), a two-stage framework with Latent Refinement and a\nPredictive Feedback Loop. The first stage maintains masked positions as\ndistributional mixtures of predicted tokens and the mask embedding, allowing\nthe model to establish more globally consistent beliefs. The second stage\nprogressively finalizes confident tokens while retaining uncertain ones for\niterative feedback. KL-divergence dynamics provide a principled and reliable\ncriterion for convergence and early stopping. Experiments across coding\n(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that\nLRD improves accuracy while delivering speedups of up to 10.6x, making it a\nstrong and versatile alternative for parallel sequence generation.",
    "published": "2025-10-13T06:38:13Z",
    "updated": "2025-10-15T10:33:35Z",
    "link": "http://arxiv.org/pdf/2510.11052v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Qinglin Zhu",
      "Yizhen Yao",
      "Runcong Zhao",
      "Yanzheng Xiang",
      "Amrutha Saseendran",
      "Chen Jin",
      "Philip Teare",
      "Bin Liang",
      "Yulan He",
      "Lin Gui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09558v2",
    "title": "AutoPR: Let's Automate Your Academic Promotion!",
    "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
    "published": "2025-10-10T17:08:36Z",
    "updated": "2025-10-15T15:32:50Z",
    "link": "http://arxiv.org/pdf/2510.09558v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Qiguang Chen",
      "Zheng Yan",
      "Mingda Yang",
      "Libo Qin",
      "Yixin Yuan",
      "Hanjing Li",
      "Jinhao Liu",
      "Yiyan Ji",
      "Dengyun Peng",
      "Jiannan Guan",
      "Mengkang Hu",
      "Yantao Du",
      "Wanxiang Che"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.01328v4",
    "title": "Can Large Language Models Master Complex Card Games?",
    "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can achieve a certain level of proficiency in multiple complex\ncard games simultaneously, with performance augmentation for games with similar\nrules and conflicts for dissimilar ones, and (3) LLMs experience a decline in\ngeneral capabilities when mastering complex games, but this decline can be\nmitigated by integrating a certain amount of general instruction data. The\nevaluation results demonstrate strong learning ability and versatility of LLMs.\nThe code is available at https://github.com/THUDM/LLM4CardGame",
    "published": "2025-09-01T10:11:56Z",
    "updated": "2025-10-15T02:38:52Z",
    "link": "http://arxiv.org/pdf/2509.01328v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Wei Wang",
      "Fuqing Bie",
      "Junzhe Chen",
      "Dan Zhang",
      "Shiyu Huang",
      "Evgeny Kharlamov",
      "Jie Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08163v3",
    "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable\n  Code",
    "summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking''\nproblem, generating unnecessarily long reasoning on simple tasks. Some\nstrategies have been proposed to mitigate this issue, such as length penalties\nor routing mechanisms, but they are typically heuristic and task-specific,\nlacking a general framework for adaptive reasoning. In this paper, we present\nARM2, a unified model that adaptively balances reasoning performance and\nefficiency across multiple formats through a reinforcement learning framework\naugmented with length-aware optimization. Beyond conventional natural language\ninference, ARM2 integrates vision understanding, extending its applicability to\nmultimodal. Moreover, ARM2 integrates executable code into reasoning, enabling\nsubstantial reductions in token cost while preserving task performance compared\nto long CoT. Experiments demonstrate that ARM2 achieves performance on par with\ntraditional reasoning models trained with GRPO, while reducing token usage by\nover 70% on average. We further conduct extensive analyses to validate the\neffectiveness of ARM2 and the soundness of its design.",
    "published": "2025-10-09T12:49:34Z",
    "updated": "2025-10-14T18:35:24Z",
    "link": "http://arxiv.org/pdf/2510.08163v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jian Xie",
      "Zhendong Chu",
      "Aoxiao Zhong",
      "Kai Zhang",
      "Mingzhe Han",
      "Xing Fan",
      "Jialie Shen",
      "Qingsong Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09710v2",
    "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework\n  for Trustworthy RAG",
    "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) with external knowledge but are vulnerable to corpus poisoning and\ncontamination attacks, which can compromise output integrity. Existing defenses\noften apply aggressive filtering, leading to unnecessary loss of valuable\ninformation and reduced reliability in generation. To address this problem, we\npropose a two-stage semantic filtering and conflict-free framework for\ntrustworthy RAG. In the first stage, we perform a joint filter with semantic\nand cluster-based filtering which is guided by the Entity-intent-relation\nextractor (EIRE). EIRE extracts entities, latent objectives, and entity\nrelations from both the user query and filtered documents, scores their\nsemantic relevance, and selectively adds valuable documents into the clean\nretrieval database. In the second stage, we proposed an EIRE-guided\nconflict-aware filtering module, which analyzes semantic consistency between\nthe query, candidate answers, and retrieved knowledge before final answer\ngeneration, filtering out internal and external contradictions that could\nmislead the model. Through this two-stage process, SeCon-RAG effectively\npreserves useful knowledge while mitigating conflict contamination, achieving\nsignificant improvements in both generation robustness and output\ntrustworthiness. Extensive experiments across various LLMs and datasets\ndemonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art\ndefense methods.",
    "published": "2025-10-10T03:44:29Z",
    "updated": "2025-10-15T07:05:25Z",
    "link": "http://arxiv.org/pdf/2510.09710v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Xiaonan Si",
      "Meilin Zhu",
      "Simeng Qin",
      "Lijia Yu",
      "Lijun Zhang",
      "Shuaitong Liu",
      "Xinfeng Li",
      "Ranjie Duan",
      "Yang Liu",
      "Xiaojun Jia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08567v2",
    "title": "MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning",
    "summary": "Vision language models (VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge with\na vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a VLM\ncontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIX on it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,\nMATRIX consistently surpasses both open- and closed-source VLMs, demonstrating\nscalable and effective multimodal tool use. Our data and code is avaliable at\nhttps://github.com/mbzuai-oryx/MATRIX.",
    "published": "2025-10-09T17:59:54Z",
    "updated": "2025-10-15T05:17:07Z",
    "link": "http://arxiv.org/pdf/2510.08567v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Tajamul Ashraf",
      "Umair Nawaz",
      "Abdelrahman M. Shaker",
      "Rao Anwer",
      "Philip Torr",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12709v2",
    "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
    "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the\nDouyin feed rank model, the match features produced by SAIL-Embedding yield a\n+0.1% AUC gain.",
    "published": "2025-10-14T16:43:22Z",
    "updated": "2025-10-15T03:46:29Z",
    "link": "http://arxiv.org/pdf/2510.12709v2.pdf",
    "category": [
      "cs.IR",
      "cs.CV"
    ],
    "authors": [
      "Lin Lin",
      "Jiefeng Long",
      "Zhihe Wan",
      "Yuchi Wang",
      "Dingkang Yang",
      "Shuang Yang",
      "Yueyang Yao",
      "Xu Chen",
      "Zirui Guo",
      "Shengqiang Li",
      "Weiran Li",
      "Hanyu Li",
      "Yaling Mou",
      "Yan Qiu",
      "Haiyang Yu",
      "Xiao Liang",
      "Hongsheng Li",
      "Chao Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12687v2",
    "title": "EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for\n  Open-Set Domain Generalization under Noisy Labels",
    "summary": "Open-Set Domain Generalization (OSDG) aims to enable deep learning models to\nrecognize unseen categories in new domains, which is crucial for real-world\napplications. Label noise hinders open-set domain generalization by corrupting\nsource-domain knowledge, making it harder to recognize known classes and reject\nunseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL)\nusing hyperbolic prototype-guided meta-learning, they struggle to bridge domain\ngaps, especially with limited clean labeled data. In this paper, we propose\nEvidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first\nintroduce an unsupervised two-stage evidential loss clustering method to\npromote label reliability awareness. Then, we propose a residual flow matching\nmechanism that models structured domain- and category-conditioned residuals,\nenabling diverse and uncertainty-aware transfer paths beyond\ninterpolation-based augmentation. During this meta-learning process, the model\nis optimized such that the update direction on the clean set maximizes the loss\ndecrease on the noisy set, using pseudo labels derived from the most confident\npredicted class for supervision. Experimental results show that EReLiFM\noutperforms existing methods on OSDG-NL, achieving state-of-the-art\nperformance. The source code is available at\nhttps://github.com/KPeng9510/ERELIFM.",
    "published": "2025-10-14T16:23:11Z",
    "updated": "2025-10-15T02:20:38Z",
    "link": "http://arxiv.org/pdf/2510.12687v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Kunyu Peng",
      "Di Wen",
      "Kailun Yang",
      "Jia Fu",
      "Yufan Chen",
      "Ruiping Liu",
      "Jiamin Wu",
      "Junwei Zheng",
      "M. Saquib Sarfraz",
      "Luc Van Gool",
      "Danda Pani Paudel",
      "Rainer Stiefelhagen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12679v2",
    "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
    "summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient\ncollaborative perception mechanisms for diverse operational scenarios. Current\nBird's Eye View (BEV)-based approaches exhibit two main limitations:\nbounding-box representations fail to capture complete semantic and geometric\ninformation of the scene, and their performance significantly degrades when\nencountering undefined or occluded objects. To address these limitations, we\npropose a novel multi-UAV collaborative occupancy prediction framework. Our\nframework effectively preserves 3D spatial structures and semantics through\nintegrating a Spatial-Aware Feature Encoder and Cross-Agent Feature\nIntegration. To enhance efficiency, we further introduce Altitude-Aware Feature\nReduction to compactly represent scene information, along with a Dual-Mask\nPerceptual Guidance mechanism to adaptively select features and reduce\ncommunication overhead. Due to the absence of suitable benchmark datasets, we\nextend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and\nUAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results\ndemonstrate that our method achieves state-of-the-art accuracy, significantly\noutperforming existing collaborative methods while reducing communication\noverhead to only a fraction of previous approaches.",
    "published": "2025-10-14T16:17:42Z",
    "updated": "2025-10-15T01:11:34Z",
    "link": "http://arxiv.org/pdf/2510.12679v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zefu Lin",
      "Wenbo Chen",
      "Xiaojuan Jin",
      "Yuran Yang",
      "Lue Fan",
      "Yixin Zhang",
      "Yufeng Zhang",
      "Zhaoxiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26165v3",
    "title": "Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal\n  Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances in visual understanding tasks. However, their capacity to comprehend\nhuman-centric scenes has rarely been explored, primarily due to the absence of\ncomprehensive evaluation benchmarks that take into account both the\nhuman-oriented granular level and higher-dimensional causal reasoning ability.\nSuch high-quality evaluation benchmarks face tough obstacles, given the\nphysical complexity of the human body and the difficulty of annotating granular\nstructures. In this paper, we propose Human-MME, a curated benchmark designed\nto provide a more holistic evaluation of MLLMs in human-centric scene\nunderstanding. Compared with other existing benchmarks, our work provides three\nkey features: 1. Diversity in human scene, spanning 4 primary visual domains\nwith 15 secondary domains and 43 sub-fields to ensure broad scenario coverage.\n2. Progressive and diverse evaluation dimensions, evaluating the human-based\nactivities progressively from the human-oriented granular perception to the\nhigher-dimensional reasoning, consisting of eight dimensions with 19,945\nreal-world image question pairs and an evaluation suite. 3. High-quality\nannotations with rich data paradigms, constructing the automated annotation\npipeline and human-annotation platform, supporting rigorous manual labeling to\nfacilitate precise and reliable model assessment. Our benchmark extends the\nsingle-target understanding to the multi-person and multi-image mutual\nunderstanding by constructing the choice, short-answer, grounding, ranking and\njudgment question components, and complex questions of their combination. The\nextensive experiments on 17 state-of-the-art MLLMs effectively expose the\nlimitations and guide future MLLMs research toward better human-centric image\nunderstanding. All data and code are available at\nhttps://github.com/Yuan-Hou/Human-MME.",
    "published": "2025-09-30T12:20:57Z",
    "updated": "2025-10-15T16:53:06Z",
    "link": "http://arxiv.org/pdf/2509.26165v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuansen Liu",
      "Haiming Tang",
      "Jinlong Peng",
      "Jiangning Zhang",
      "Xiaozhong Ji",
      "Qingdong He",
      "Wenbin Wu",
      "Donghao Luo",
      "Zhenye Gan",
      "Junwei Zhu",
      "Yunhang Shen",
      "Chaoyou Fu",
      "Chengjie Wang",
      "Xiaobin Hu",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.00731v3",
    "title": "Robust Real-Time Endoscopic Stereo Matching under Fuzzy Tissue\n  Boundaries",
    "summary": "Real-time acquisition of accurate scene depth is essential for automated\nrobotic minimally invasive surgery. Stereo matching with binocular endoscopy\ncan provide this depth information. However, existing stereo matching methods,\ndesigned primarily for natural images, often struggle with endoscopic images\ndue to fuzzy tissue boundaries and typically fail to meet real-time\nrequirements for high-resolution endoscopic image inputs. To address these\nchallenges, we propose \\textbf{RRESM}, a real-time stereo matching method\ntailored for endoscopic images. Our approach integrates a 3D Mamba Coordinate\nAttention module that enhances cost aggregation through position-sensitive\nattention maps and long-range spatial dependency modeling via the Mamba block,\ngenerating a robust cost volume without substantial computational overhead.\nAdditionally, we introduce a High-Frequency Disparity Optimization module that\nrefines disparity predictions near tissue boundaries by amplifying\nhigh-frequency details in the wavelet domain. Evaluations on the SCARED and\nSERV-CT datasets demonstrate state-of-the-art matching accuracy with a\nreal-time inference speed of 42 FPS. The code is available at\nhttps://github.com/Sonne-Ding/RRESM.",
    "published": "2025-03-02T05:06:52Z",
    "updated": "2025-10-15T01:27:34Z",
    "link": "http://arxiv.org/pdf/2503.00731v3.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Yang Ding",
      "Can Han",
      "Sijia Du",
      "Yaqi Wang",
      "Dahong Qian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12126v2",
    "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source\n  Suites",
    "summary": "Generalist visual captioning goes beyond a simple appearance description\ntask, but requires integrating a series of visual cues into a caption and\nhandling various visual domains. In this task, current open-source models\npresent a large performance gap with commercial ones, which limits various\napplications such as data synthesis. To bridge the gap, this paper proposes\nCapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for\nthe first time that, by capitalizing on open-source models, it is possible to\nachieve caption quality on par with GPT-4.1 in various domains with an 89.5%\nreduction in costs. By leveraging CapFlow as the data synthesizer, we produce\nhigh-quality visual captions from image and video domains at scale, and obtain\na generalist visual captioner via fine-tuning, namely MetaCaptioner. Through\nextensive experiments, we show that MetaCaptioner not only achieves comparable\ncaptioning capabilities with commercial models but also reaches top-tier\nmultimodal performance in the open-source community. We hope CapFlow and\nMetaCaptioner can benefit future multimodal research by providing a strong and\ncost-effective visual captioning solution.",
    "published": "2025-10-14T04:03:25Z",
    "updated": "2025-10-15T15:13:47Z",
    "link": "http://arxiv.org/pdf/2510.12126v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhenxin Lei",
      "Zhangwei Gao",
      "Changyao Tian",
      "Erfei Cui",
      "Guanzhou Chen",
      "Danni Yang",
      "Yuchen Duan",
      "Zhaokai Wang",
      "Wenhao Li",
      "Weiyun Wang",
      "Xiangyu Zhao",
      "Jiayi Ji",
      "Yu Qiao",
      "Wenhai Wang",
      "Gen Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10518v3",
    "title": "VR-Thinker: Boosting Video Reward Models through Thinking-with-Image\n  Reasoning",
    "summary": "Recent advancements in multimodal reward models (RMs) have substantially\nimproved post-training for visual generative models. However, current RMs face\ninherent limitations: (1) visual inputs consume large context budgets, forcing\nfewer frames and causing loss of fine-grained details; and (2) all visual\ninformation is packed into the initial prompt, exacerbating hallucination and\nforgetting during chain-of-thought reasoning. To overcome these issues, we\nintroduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework\nthat equips the RM with visual reasoning operations (e.g., select frame) and a\nconfigurable visual memory window. This allows the RM to actively acquire and\nupdate visual evidence within context limits, improving reasoning fidelity and\nreliability. We activate visual reasoning via a reinforcement fine-tuning\npipeline: (i) Cold Start with curated visual chain-of-thought data to distill\nbasic reasoning skills and operation formatting; (ii) select samples whose\nper-dimension and overall judgments are all correct, then conduct Rejection\nsampling Fine-Tuning on these high-quality traces to further enhance reasoning;\nand (iii) apply Group Relative Policy Optimization (GRPO) to strengthen\nreasoning. Our approach delivers state-of-the-art accuracy among open-source\nmodels on video preference benchmarks, especially for longer videos: a 7B\nVR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%\non MJ-Bench-Video. These results validate the effectiveness and promise of\nthinking-with-image multimodal reward modeling.",
    "published": "2025-10-12T09:29:50Z",
    "updated": "2025-10-15T03:25:47Z",
    "link": "http://arxiv.org/pdf/2510.10518v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qunzhong Wang",
      "Jie Liu",
      "Jiajun Liang",
      "Yilei Jiang",
      "Yuanxing Zhang",
      "Jinyuan Chen",
      "Yaozhi Zheng",
      "Xintao Wang",
      "Pengfei Wan",
      "Xiangyu Yue",
      "Jiaheng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11354v3",
    "title": "Software System for Low-Cost, GUI-Based Microscopy Segmentation:\n  Algorithmic Implementation",
    "summary": "This article presents a novel microscopy image analysis framework designed\nfor low-budget labs equipped with a standard CPU desktop. The Python-based\nprogram enables cytometric analysis of live, unstained cells in culture through\nan advanced computer vision and machine learning pipeline. Crucially, the\nframework operates on label-free data, requiring no manually annotated training\ndata or training phase. It is accessible via a user-friendly, cross-platform\nGUI that requires no programming skills, while also providing a scripting\ninterface for programmatic control and integration by developers. The\nend-to-end workflow performs semantic and instance segmentation, feature\nextraction, analysis, evaluation, and automated report generation. Its modular\narchitecture supports easy maintenance and flexible integration while\nsupporting both single-image and batch processing. Validated on several\nunstained cell types from the public dataset of livecells, the framework\ndemonstrates superior accuracy and reproducibility compared to contemporary\ntools like Cellpose and StarDist. Its competitive segmentation speed on a\nCPU-based platform highlights its significant potential for basic research and\nclinical application-particularly in cell transplantation for personalised\nmedicine and muscle regeneration therapies. The access to the application is\navailable for reproducibility.",
    "published": "2025-09-14T17:12:17Z",
    "updated": "2025-10-15T13:41:49Z",
    "link": "http://arxiv.org/pdf/2509.11354v3.pdf",
    "category": [
      "q-bio.QM",
      "cs.CV",
      "eess.IV",
      "q-bio.CB"
    ],
    "authors": [
      "Surajit Das",
      "Pavel Zun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11296v2",
    "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During\n  Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
    "summary": "Recent approaches for vision-language models (VLMs) have shown remarkable\nsuccess in achieving fast downstream adaptation. When applied to real-world\ndownstream tasks, VLMs inevitably encounter both the in-distribution (ID) data\nand out-of-distribution (OOD) data. The OOD datasets often include both\ncovariate shifts (e.g., known classes with changes in image styles) and\nsemantic shifts (e.g., test-time unseen classes). This highlights the\nimportance of improving VLMs' generalization ability to covariate-shifted OOD\ndata, while effectively detecting open-set semantic-shifted OOD classes. In\nthis paper, inspired by the substantial energy change observed in closed-set\ndata when re-aligning vision-language modalities (specifically by directly\nreducing the maximum cosine similarity to a low value), we introduce a novel\nOOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the\nvanilla energy-based OOD score and provides a more reliable approach for OOD\ndetection. Furthermore, {\\Delta}Energy can simultaneously improve OOD\ngeneralization under covariate shifts, which is achieved by lower-bound\nmaximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to\nnot only enhance OOD detection but also yields a domain-consistent Hessian,\nwhich serves as a strong indicator for OOD generalization. Based on this\nfinding, we developed a unified fine-tuning framework that allows for improving\nVLMs' robustness in both OOD generalization and OOD detection. Extensive\nexperiments on challenging OOD detection and generalization benchmarks\ndemonstrate the superiority of our method, outperforming recent approaches by\n10% to 25% in AUROC.",
    "published": "2025-10-13T11:36:58Z",
    "updated": "2025-10-15T05:52:23Z",
    "link": "http://arxiv.org/pdf/2510.11296v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Lin Zhu",
      "Yifeng Yang",
      "Xinbing Wang",
      "Qinying Gu",
      "Nanyang Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07810v3",
    "title": "FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion\n  Attention Network for Robust Micro-expression Recognition",
    "summary": "Facial micro-expressions, characterized by their subtle and brief nature, are\nvaluable indicators of genuine emotions. Despite their significance in\npsychology, security, and behavioral analysis, micro-expression recognition\nremains challenging due to the difficulty of capturing subtle facial movements.\nOptical flow has been widely employed as an input modality for this task due to\nits effectiveness. However, most existing methods compute optical flow only\nbetween the onset and apex frames, thereby overlooking essential motion\ninformation in the apex-to-offset phase. To address this limitation, we first\nintroduce a comprehensive motion representation, termed Magnitude-Modulated\nCombined Optical Flow (MM-COF), which integrates motion dynamics from both\nmicro-expression phases into a unified descriptor suitable for direct use in\nrecognition networks. Building upon this principle, we then propose FMANet, a\nnovel end-to-end neural network architecture that internalizes the dual-phase\nanalysis and magnitude modulation into learnable modules. This allows the\nnetwork to adaptively fuse motion cues and focus on salient facial regions for\nclassification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM\ndatasets, widely recognized as standard benchmarks, demonstrate that our\nproposed MM-COF representation and FMANet outperforms existing methods,\nunderscoring the potential of a learnable, dual-phase framework in advancing\nmicro-expression recognition.",
    "published": "2025-10-09T05:36:40Z",
    "updated": "2025-10-15T14:28:10Z",
    "link": "http://arxiv.org/pdf/2510.07810v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Luu Tu Nguyen",
      "Vu Tram Anh Khuong",
      "Thi Bich Phuong Man",
      "Thi Duyen Ngo",
      "Thanh Ha Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04483v3",
    "title": "TBStar-Edit: From Image Editing Pattern Shifting to Consistency\n  Enhancement",
    "summary": "Recent advances in image generation and editing technologies have enabled\nstate-of-the-art models to achieve impressive results in general domains.\nHowever, when applied to e-commerce scenarios, these general models often\nencounter consistency limitations. To address this challenge, we introduce\nTBStar-Edit, an new image editing model tailored for the e-commerce domain.\nThrough rigorous data engineering, model architecture design and training\nstrategy, TBStar-Edit achieves precise and high-fidelity image editing while\nmaintaining the integrity of product appearance and layout. Specifically, for\ndata engineering, we establish a comprehensive data construction pipeline,\nencompassing data collection, construction, filtering, and augmentation, to\nacquire high-quality, instruction-following, and strongly consistent editing\ndata to support model training. For model architecture design, we design a\nhierarchical model framework consisting of a base model, pattern shifting\nmodules, and consistency enhancement modules. For model training, we adopt a\ntwo-stage training strategy to enhance the consistency preservation: first\nstage for editing pattern shifting, and second stage for consistency\nenhancement. Each stage involves training different modules with separate\ndatasets. Finally, we conduct extensive evaluations of TBStar-Edit on a\nself-proposed e-commerce benchmark, and the results demonstrate that\nTBStar-Edit outperforms existing general-domain editing models in both\nobjective metrics (VIE Score) and subjective user preference.",
    "published": "2025-10-06T04:46:42Z",
    "updated": "2025-10-15T14:07:17Z",
    "link": "http://arxiv.org/pdf/2510.04483v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Fang",
      "Zechao Zhan",
      "Weixin Feng",
      "Ziwei Huang",
      "Xubin Li",
      "Tiezheng Ge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08979v2",
    "title": "Uncolorable Examples: Preventing Unauthorized AI Colorization via\n  Perception-Aware Chroma-Restrictive Perturbation",
    "summary": "AI-based colorization has shown remarkable capability in generating realistic\ncolor images from grayscale inputs. However, it poses risks of copyright\ninfringement -- for example, the unauthorized colorization and resale of\nmonochrome manga and films. Despite these concerns, no effective method\ncurrently exists to prevent such misuse. To address this, we introduce the\nfirst defensive paradigm, Uncolorable Examples, which embed imperceptible\nperturbations into grayscale images to invalidate unauthorized colorization. To\nensure real-world applicability, we establish four criteria: effectiveness,\nimperceptibility, transferability, and robustness. Our method, Perception-Aware\nChroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that\nmeet these four criteria by optimizing imperceptible perturbations with a\nLaplacian filter to preserve perceptual quality, and applying diverse input\ntransformations during optimization to enhance transferability across models\nand robustness against common post-processing (e.g., compression). Experiments\non ImageNet and Danbooru datasets demonstrate that PAChroma effectively\ndegrades colorization quality while maintaining the visual appearance. This\nwork marks the first step toward protecting visual content from illegitimate AI\ncolorization, paving the way for copyright-aware defenses in generative media.",
    "published": "2025-10-10T03:46:17Z",
    "updated": "2025-10-15T06:52:47Z",
    "link": "http://arxiv.org/pdf/2510.08979v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yuki Nii",
      "Futa Waseda",
      "Ching-Chun Chang",
      "Isao Echizen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12328v2",
    "title": "Leveraging Teleconnections with Physics-Informed Graph Attention\n  Networks for Long-Range Extreme Rainfall Forecasting in Thailand",
    "summary": "Accurate rainfall forecasting, particularly for extreme events, remains a\nsignificant challenge in climatology and the Earth system. This paper presents\nnovel physics-informed Graph Neural Networks (GNNs) combined with extreme-value\nanalysis techniques to improve gauge-station rainfall predictions across\nThailand. The model leverages a graph-structured representation of gauge\nstations to capture complex spatiotemporal patterns, and it offers\nexplainability through teleconnections. We preprocess relevant climate indices\nthat potentially influence regional rainfall. The proposed Graph Attention\nNetwork with Long Short-Term Memory (Attention-LSTM) applies the attention\nmechanism using initial edge features derived from simple\norographic-precipitation physics formulation. The embeddings are subsequently\nprocessed by LSTM layers. To address extremes, we perform Peak-Over-Threshold\n(POT) mapping using the novel Spatial Season-aware Generalized Pareto\nDistribution (GPD) method, which overcomes limitations of traditional\nmachine-learning models. Experiments demonstrate that our method outperforms\nwell-established baselines across most regions, including areas prone to\nextremes, and remains strongly competitive with the state of the art. Compared\nwith the operational forecasting system SEAS5, our real-world application\nimproves extreme-event prediction and offers a practical enhancement to produce\nfine-resolution maps that support decision-making in long-term water\nmanagement.",
    "published": "2025-10-14T09:34:14Z",
    "updated": "2025-10-15T08:26:05Z",
    "link": "http://arxiv.org/pdf/2510.12328v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kiattikun Chobtham",
      "Kanoksri Sarinnapakorn",
      "Kritanai Torsri",
      "Prattana Deeprasertkul",
      "Jirawan Kamma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18185v3",
    "title": "BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals",
    "summary": "Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural\nactivity non-invasively by capturing electromagnetic fields generated by\ndendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit\ndistinct signal patterns, further complicated by variations in sensor\nconfigurations across modalities and recording devices. Existing approaches\ntypically rely on separate, modality- and dataset-specific models, which limits\nthe performance and cross-domain scalability. This paper proposes BrainOmni,\nthe first brain foundation model that generalises across heterogeneous EEG and\nMEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the\nfirst tokenizer that quantises spatiotemporal brain activity into discrete\nrepresentations. Central to BrainTokenizer is a novel Sensor Encoder that\nencodes sensor properties such as spatial layout, orientation, and type,\nenabling compatibility across devices and modalities. Building upon the\ndiscrete representations, BrainOmni learns unified semantic embeddings of brain\nsignals by self-supervised pretraining. To the best of our knowledge, it is the\nfirst foundation model to support both EEG and MEG signals, as well as the\nfirst to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG\nand 656 hours of MEG data are curated and standardised from publicly available\nsources for pretraining. Experiments show that BrainOmni outperforms both\nexisting foundation models and state-of-the-art task-specific models on a range\nof downstream tasks. It also demonstrates strong generalisation to unseen EEG\nand MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training\nyields consistent improvements across both modalities. Code and checkpoints are\npublicly available at https://github.com/OpenTSLab/BrainOmni.",
    "published": "2025-05-18T14:07:14Z",
    "updated": "2025-10-15T04:51:35Z",
    "link": "http://arxiv.org/pdf/2505.18185v3.pdf",
    "category": [
      "eess.SP",
      "cs.LG"
    ],
    "authors": [
      "Qinfan Xiao",
      "Ziyun Cui",
      "Chi Zhang",
      "Siqi Chen",
      "Wen Wu",
      "Andrew Thwaites",
      "Alexandra Woolgar",
      "Bowen Zhou",
      "Chao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12026v2",
    "title": "Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature\n  Learning",
    "summary": "Mamba, a recently proposed linear-time sequence model, has attracted\nsignificant attention for its computational efficiency and strong empirical\nperformance. However, a rigorous theoretical understanding of its underlying\nmechanisms remains limited. In this work, we provide a theoretical analysis of\nMamba's in-context learning (ICL) capability by focusing on tasks defined by\nlow-dimensional nonlinear target functions. Specifically, we study in-context\nlearning of a single-index model $y \\approx g_*(\\langle \\boldsymbol{\\beta},\n\\boldsymbol{x} \\rangle)$, which depends on only a single relevant direction\n$\\boldsymbol{\\beta}$, referred to as feature. We prove that Mamba, pretrained\nby gradient-based methods, can achieve efficient ICL via test-time feature\nlearning, extracting the relevant direction directly from context examples.\nConsequently, we establish a test-time sample complexity that improves upon\nlinear Transformers -- analyzed to behave like kernel methods -- and is\ncomparable to nonlinear Transformers, which have been shown to surpass the\nCorrelational Statistical Query (CSQ) lower bound and achieve near\ninformation-theoretically optimal rate in previous works. Our analysis reveals\nthe crucial role of the nonlinear gating mechanism in Mamba for feature\nextraction, highlighting it as the fundamental driver behind Mamba's ability to\nachieve both computational efficiency and high performance.",
    "published": "2025-10-14T00:21:20Z",
    "updated": "2025-10-15T01:49:35Z",
    "link": "http://arxiv.org/pdf/2510.12026v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Junsoo Oh",
      "Wei Huang",
      "Taiji Suzuki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18530v3",
    "title": "Re-uploading quantum data: A universal function approximator for quantum\n  inputs",
    "summary": "Quantum data re-uploading has proved powerful for classical inputs, where\nrepeatedly encoding features into a small circuit yields universal function\napproximation. Extending this idea to quantum inputs remains underexplored, as\nthe information contained in a quantum state is not directly accessible in\nclassical form. We propose and analyze a quantum data re-uploading architecture\nin which a qubit interacts sequentially with fresh copies of an arbitrary input\nstate. The circuit can approximate any bounded continuous function using only\none ancilla qubit and single-qubit measurements. By alternating entangling\nunitaries with mid-circuit resets of the input register, the architecture\nrealizes a discrete cascade of completely positive and trace-preserving maps,\nanalogous to collision models in open quantum system dynamics. Our framework\nprovides a qubit-efficient and expressive approach to designing quantum machine\nlearning models that operate directly on quantum data.",
    "published": "2025-09-23T01:50:37Z",
    "updated": "2025-10-15T00:45:25Z",
    "link": "http://arxiv.org/pdf/2509.18530v3.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Hyunho Cha",
      "Daniel K. Park",
      "Jungwoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08141v3",
    "title": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in\n  Reinforcement Fine-tuning",
    "summary": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers.",
    "published": "2025-10-09T12:24:08Z",
    "updated": "2025-10-15T13:54:25Z",
    "link": "http://arxiv.org/pdf/2510.08141v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chen Wang",
      "Zhaochun Li",
      "Jionghao Bai",
      "Yuzhi Zhang",
      "Shisheng Cui",
      "Zhou Zhao",
      "Yue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19330v2",
    "title": "LibEMER: A novel benchmark and algorithms library for EEG-based\n  Multimodal Emotion Recognition",
    "summary": "EEG-based multimodal emotion recognition(EMER) has gained significant\nattention and witnessed notable advancements, the inherent complexity of human\nneural systems has motivated substantial efforts toward multimodal approaches.\nHowever, this field currently suffers from three critical limitations: (i) the\nabsence of open-source implementations. (ii) the lack of standardized and\ntransparent benchmarks for fair performance analysis. (iii) in-depth discussion\nregarding main challenges and promising research directions is a notable\nscarcity. To address these challenges, we introduce LibEMER, a unified\nevaluation framework that provides fully reproducible PyTorch implementations\nof curated deep learning methods alongside standardized protocols for data\npreprocessing, model realization, and experimental setups. This framework\nenables unbiased performance assessment on three widely-used public datasets\nacross two learning tasks. The open-source library is publicly accessible at:\nhttps://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384",
    "published": "2025-09-14T03:50:07Z",
    "updated": "2025-10-15T06:46:18Z",
    "link": "http://arxiv.org/pdf/2509.19330v2.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Zejun Liu",
      "Yunshan Chen",
      "Chengxi Xie",
      "Yugui Xie",
      "Huan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.04247v3",
    "title": "I$^3$-MRec: Invariant Learning with Information Bottleneck for\n  Incomplete Modality Recommendation",
    "summary": "Multimodal recommender systems (MRS) improve recommendation performance by\nintegrating complementary semantic information from multiple modalities.\nHowever, the assumption of complete multimodality rarely holds in practice due\nto missing images and incomplete descriptions, hindering model robustness and\ngeneralization. To address these challenges, we introduce a novel method called\n\\textbf{I$^3$-MRec}, which uses \\textbf{I}nvariant learning with\n\\textbf{I}nformation bottleneck principle for \\textbf{I}ncomplete\n\\textbf{M}odality \\textbf{Rec}ommendation. To achieve robust performance in\nmissing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)\ncross-modal preference invariance, ensuring consistent user preference modeling\nacross varying modality environments, and (ii) compact yet effective multimodal\nrepresentation, as modality information becomes unreliable in such scenarios,\nreducing the dependence on modality-specific information is particularly\nimportant. By treating each modality as a distinct semantic environment,\nI$^3$-MRec employs invariant risk minimization (IRM) to learn\npreference-oriented representations. In parallel, a missing-aware fusion module\nis developed to explicitly simulate modality-missing scenarios. Built upon the\nInformation Bottleneck (IB) principle, the module aims to preserve essential\nuser preference signals across these scenarios while effectively compressing\nmodality-specific information. Extensive experiments conducted on three\nreal-world datasets demonstrate that I$^3$-MRec consistently outperforms\nexisting state-of-the-art MRS methods across various modality-missing\nscenarios, highlighting its effectiveness and robustness in practical\napplications.",
    "published": "2025-08-06T09:29:50Z",
    "updated": "2025-10-15T11:10:53Z",
    "link": "http://arxiv.org/pdf/2508.04247v3.pdf",
    "category": [
      "cs.IR",
      "cs.MM",
      "H.3.3; H.5.1"
    ],
    "authors": [
      "Huilin Chen",
      "Miaomiao Cai",
      "Fan Liu",
      "Zhiyong Cheng",
      "Richang Hong",
      "Meng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09945v2",
    "title": "ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual\n  Event Localization",
    "summary": "Dense audio-visual event localization (DAVE) aims to identify event\ncategories and locate the temporal boundaries in untrimmed videos. Most studies\nonly employ event-related semantic constraints on the final outputs, lacking\ncross-modal semantic bridging in intermediate layers. This causes modality\nsemantic gap for further fusion, making it difficult to distinguish between\nevent-related content and irrelevant background content. Moreover, they rarely\nconsider the correlations between events, which limits the model to infer\nconcurrent events among complex scenarios. In this paper, we incorporate\nmulti-stage semantic guidance and multi-event relationship modeling, which\nrespectively enable hierarchical semantic understanding of audio-visual events\nand adaptive extraction of event dependencies, thereby better focusing on\nevent-related information. Specifically, our eventaware semantic guided network\n(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of\ndependency experts (MoDE) module. ESI applys multi-stage semantic guidance to\nexplicitly constrain the model in learning semantic information through\nmulti-modal early fusion and several classification loss functions, ensuring\nhierarchical understanding of event-related content. MoDE promotes the\nextraction of multi-event dependencies through multiple serial mixture of\nexperts with adaptive weight allocation. Extensive experiments demonstrate that\nour method significantly surpasses the state-of-the-art methods, while greatly\nreducing parameters and computational load. Our code will be released on\nhttps://github.com/uchiha99999/ESG-Net.",
    "published": "2025-07-14T05:42:00Z",
    "updated": "2025-10-15T03:36:50Z",
    "link": "http://arxiv.org/pdf/2507.09945v2.pdf",
    "category": [
      "cs.MM",
      "cs.CV"
    ],
    "authors": [
      "Huilai Li",
      "Yonghao Dang",
      "Ying Xing",
      "Yiming Wang",
      "Jianqin Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15298v2",
    "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and\n  Visual Question Answering",
    "summary": "Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.",
    "published": "2025-06-18T09:29:51Z",
    "updated": "2025-10-15T14:45:59Z",
    "link": "http://arxiv.org/pdf/2506.15298v2.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Xinqi Fan",
      "Jingting Li",
      "John See",
      "Moi Hoon Yap",
      "Wen-Huang Cheng",
      "Xiaobai Li",
      "Xiaopeng Hong",
      "Su-Jing Wang",
      "Adrian K. Davision"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12410v3",
    "title": "MTIL: Encoding Full History with Mamba for Temporal Imitation Learning",
    "summary": "Standard imitation learning (IL) methods have achieved considerable success\nin robotics, yet often rely on the Markov assumption, which falters in\nlong-horizon tasks where history is crucial for resolving perceptual ambiguity.\nThis limitation stems not only from a conceptual gap but also from a\nfundamental computational barrier: prevailing architectures like Transformers\nare often constrained by quadratic complexity, rendering the processing of\nlong, high-dimensional observation sequences infeasible. To overcome this dual\nchallenge, we introduce Mamba Temporal Imitation Learning (MTIL). Our approach\nrepresents a new paradigm for robotic learning, which we frame as a practical\nsynthesis of World Model and Dynamical System concepts. By leveraging the\nlinear-time recurrent dynamics of State Space Models (SSMs), MTIL learns an\nimplicit, action-oriented world model that efficiently encodes the entire\ntrajectory history into a compressed, evolving state. This allows the policy to\nbe conditioned on a comprehensive temporal context, transcending the confines\nof Markovian approaches. Through extensive experiments on simulated benchmarks\n(ACT, Robomimic, LIBERO) and on challenging real-world tasks, MTIL demonstrates\nsuperior performance against SOTA methods like ACT and Diffusion Policy,\nparticularly in resolving long-term temporal ambiguities. Our findings not only\naffirm the necessity of full temporal context but also validate MTIL as a\npowerful and a computationally feasible approach for learning long-horizon,\nnon-Markovian behaviors from high-dimensional observations.",
    "published": "2025-05-18T13:22:34Z",
    "updated": "2025-10-15T01:42:48Z",
    "link": "http://arxiv.org/pdf/2505.12410v3.pdf",
    "category": [
      "cs.RO",
      "I.2.9"
    ],
    "authors": [
      "Yulin Zhou",
      "Yuankai Lin",
      "Fanzhe Peng",
      "Jiahui Chen",
      "Kaiji Huang",
      "Hua Yang",
      "Zhouping Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12169v2",
    "title": "Hybrid Terrain-Aware Path Planning: Integrating VD-RRT* Exploration and\n  VD-D* Lite Repair",
    "summary": "Autonomous ground vehicles operating off-road must plan curvature-feasible\npaths while accounting for spatially varying soil strength and slope hazards in\nreal time. We present a continuous state--cost metric that combines a Bekker\npressure--sinkage model with elevation-derived slope and attitude penalties.\nThe resulting terrain cost field is analytic, bounded, and monotonic in soil\nmodulus and slope, ensuring well-posed discretization and stable updates under\nsensor noise. This metric is evaluated on a lattice with exact steering\nprimitives: Dubins and Reeds--Shepp motions for differential drive and\ntime-parameterized bicycle arcs for Ackermann steering. Global exploration is\nperformed using Vehicle-Dynamics RRT\\(^{*}\\), while local repair is managed by\nVehicle-Dynamics D\\(^{*}\\) Lite, enabling millisecond-scale replanning without\nheuristic smoothing. By separating the terrain--vehicle model from the planner,\nthe framework provides a reusable basis for deterministic, sampling-based, or\nlearning-driven planning in deformable terrain. Hardware trials on an off-road\nplatform demonstrate real-time navigation across soft soil and slope\ntransitions, supporting reliable autonomy in unstructured environments.",
    "published": "2025-10-14T05:54:46Z",
    "updated": "2025-10-15T06:58:23Z",
    "link": "http://arxiv.org/pdf/2510.12169v2.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Akshay Naik",
      "William R. Norris",
      "Dustin Nottage",
      "Ahmet Soylemezoglu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02167v3",
    "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle\n  Batteries Utilizing Product-Process-Resource Asset Network",
    "summary": "In a circular economy, products in their end-of-life phase should be either\nremanufactured or recycled. Both of these processes are crucial for\nsustainability and environmental conservation. However, manufacturers\nfrequently do not support these processes enough in terms of not sharing\nrelevant data about the products nor their (re-)manufacturing processes. This\npaper proposes to accompany each product with a digital twin technology,\nspecifically the Product Digital Twin (PDT), which can carry information for\nfacilitating and optimizing production and remanufacturing processes. This\npaper introduces a knowledge representation called Bi-Flow\nProduct-Process-Resource Asset Network (Bi-PAN). Bi-PAN extends a well-proven\nProduct-Process-Resource Asset Network (PAN) paradigm by integrating both\nassembly and disassembly workflows into a single information model. Such\nnetworks enable capturing relevant relationships across products, production\nresources, manufacturing processes, and specific production operations that\nhave to be done in the manufacturing phase of a product. The proposed approach\nis demonstrated in a use-case of disassembling electric vehicle (EV) batteries.\nBy utilizing PDTs with Bi-PAN knowledge models, challenges associated with\ndisassembling of EV batteries can be solved flexibly and efficiently for\nvarious battery types, enhancing the sustainability of the EV battery\nlife-cycle management.",
    "published": "2025-10-02T16:14:24Z",
    "updated": "2025-10-15T10:05:46Z",
    "link": "http://arxiv.org/pdf/2510.02167v3.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Sara Strakosova",
      "Petr Novak",
      "Petr Kadera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10912v2",
    "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance\n  Heatmaps for Spatial Grounding in Robotic Tasks",
    "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning\ninto discrete points, making them brittle to perceptual noise and semantic\nambiguity. To address this challenge, we propose RoboMAP, a framework that\nrepresents spatial targets as continuous, adaptive affordance heatmaps. This\ndense representation captures the uncertainty in spatial grounding and provides\nricher information for downstream policies, thereby significantly enhancing\ntask success and interpretability. RoboMAP surpasses the previous\nstate-of-the-art on a majority of grounding benchmarks with up to a 50x speed\nimprovement, and achieves an 82\\% success rate in real-world manipulation.\nAcross extensive simulated and physical experiments, it demonstrates robust\nperformance and shows strong zero-shot generalization to navigation. More\ndetails and videos can be found at https://robo-map.github.io.",
    "published": "2025-10-13T02:14:30Z",
    "updated": "2025-10-15T13:06:39Z",
    "link": "http://arxiv.org/pdf/2510.10912v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Xinyu Shao",
      "Yanzhe Tang",
      "Pengwei Xie",
      "Kaiwen Zhou",
      "Yuzheng Zhuang",
      "Xingyue Quan",
      "Jianye Hao",
      "Long Zeng",
      "Xiu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07869v3",
    "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General\n  Underwater Robots",
    "summary": "Underwater environments present unique challenges for robotic operation,\nincluding complex hydrodynamics, limited visibility, and constrained\ncommunication. Although data-driven approaches have advanced embodied\nintelligence in terrestrial robots and enabled task-specific autonomous\nunderwater robots, developing underwater intelligence capable of autonomously\nperforming multiple tasks remains highly challenging, as large-scale,\nhigh-quality underwater datasets are still scarce. To address these\nlimitations, we introduce USIM, a simulation-based multi-task\nVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over\n561K frames from 1,852 trajectories, totaling approximately 15.6 hours of\nBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from\nvisual navigation to mobile manipulation. Building upon this dataset, we\npropose U0, a VLA model for general underwater robots, which integrates\nbinocular vision and other sensor modalities through multimodal fusion, and\nfurther incorporates a convolution-attention-based perception focus enhancement\nmodule (CAP) to improve spatial understanding and mobile manipulation. Across\ntasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,\nthe framework achieves a success rate of 80%, while in challenging mobile\nmanipulation tasks, it reduces the distance to the target by 21.2% compared\nwith baseline methods, demonstrating its effectiveness. USIM and U0 show that\nVLA models can be effectively applied to underwater robotic applications,\nproviding a foundation for scalable dataset construction, improved task\nautonomy, and the practical realization of intelligent general underwater\nrobots.",
    "published": "2025-10-09T07:19:29Z",
    "updated": "2025-10-15T08:39:24Z",
    "link": "http://arxiv.org/pdf/2510.07869v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Junwen Gu",
      "Zhiheng Wu",
      "Pengxuan Si",
      "Shuang Qiu",
      "Yukai Feng",
      "Luoyang Sun",
      "Laien Luo",
      "Lianyi Yu",
      "Jian Wang",
      "Zhengxing Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07882v2",
    "title": "Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid\n  Robots",
    "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nthe ability to serve as high-level planners, enabling robots to follow complex\nhuman instructions. However, their effectiveness, especially in long-horizon\ntasks involving dual-arm humanoid robots, remains limited. This limitation\narises from two main challenges: (i) the absence of simulation platforms that\nsystematically support task evaluation and data collection for humanoid robots,\nand (ii) the insufficient embodiment awareness of current MLLMs, which hinders\nreasoning about dual-arm selection logic and body positions during planning. To\naddress these issues, we present DualTHOR, a new dual-arm humanoid simulator,\nwith continuous transition and a contingency mechanism. Building on this\nplatform, we propose Proprio-MLLM, a model that enhances embodiment awareness\nby incorporating proprioceptive information with motion-based position\nembedding and a cross-spatial encoder. Experiments show that, while existing\nMLLMs struggle in this environment, Proprio-MLLM achieves an average\nimprovement of 19.75% in planning performance. Our work provides both an\nessential simulation platform and an effective model to advance embodied\nintelligence in humanoid robotics. The code is available at\nhttps://anonymous.4open.science/r/DualTHOR-5F3B.",
    "published": "2025-10-09T07:35:12Z",
    "updated": "2025-10-15T13:29:29Z",
    "link": "http://arxiv.org/pdf/2510.07882v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Boyu Li",
      "Siyuan He",
      "Hang Xu",
      "Haoqi Yuan",
      "Xinrun Xu",
      "Yu Zang",
      "Liwei Hu",
      "Junpeng Yue",
      "Zhenxiong Jiang",
      "Pengbo Hu",
      "Börje F. Karlsson",
      "Yehui Tang",
      "Zongqing Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06207v2",
    "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern\n  Coding Model",
    "summary": "Recent advances in control robot methods, from end-to-end\nvision-language-action frameworks to modular systems with predefined\nprimitives, have advanced robots' ability to follow natural language\ninstructions. Nonetheless, many approaches still struggle to scale to diverse\nenvironments, as they often rely on large annotated datasets and offer limited\ninterpretability.In this work, we introduce EmbodiedCoder, a training-free\nframework for open-world mobile robot manipulation that leverages coding models\nto directly generate executable robot trajectories. By grounding high-level\ninstructions in code, EmbodiedCoder enables flexible object geometry\nparameterization and manipulation trajectory synthesis without additional data\ncollection or fine-tuning.This coding-based paradigm provides a transparent and\ngeneralizable way to connect perception with manipulation. Experiments on real\nmobile robots show that EmbodiedCoder achieves robust performance across\ndiverse long-term tasks and generalizes effectively to novel objects and\nenvironments.Our results demonstrate an interpretable approach for bridging\nhigh-level reasoning and low-level control, moving beyond fixed primitives\ntoward versatile robot intelligence. See the project page at:\nhttps://embodiedcoder.github.io/EmbodiedCoder/",
    "published": "2025-10-07T17:58:02Z",
    "updated": "2025-10-15T01:16:18Z",
    "link": "http://arxiv.org/pdf/2510.06207v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Zefu Lin",
      "Rongxu Cui",
      "Chen Hanning",
      "Xiangyu Wang",
      "Junjia Xu",
      "Xiaojuan Jin",
      "Chen Wenbo",
      "Hui Zhou",
      "Lue Fan",
      "Wenling Li",
      "Zhaoxiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01675v2",
    "title": "Geometric Backstepping Control of Omnidirectional Tiltrotors\n  Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances",
    "summary": "This work presents a geometric backstepping controller for a variable-tilt\nomnidirectional multirotor that explicitly accounts for both servo and rotor\ndynamics. Considering actuator dynamics is essential for more effective and\nreliable operation, particularly during aggressive flight maneuvers or recovery\nfrom sudden disturbances. While prior studies have investigated actuator-aware\ncontrol for conventional and fixed-tilt multirotors, these approaches rely on\nlinear relationships between actuator input and wrench, which cannot capture\nthe nonlinearities induced by variable tilt angles. In this work, we exploit\nthe cascade structure between the rigid-body dynamics of the multirotor and its\nnonlinear actuator dynamics to design the proposed backstepping controller and\nestablish exponential stability of the overall system. Furthermore, we reveal\nparametric uncertainty in the actuator model through experiments, and we\ndemonstrate that the proposed controller remains robust against such\nuncertainty. The controller was compared against a baseline that does not\naccount for actuator dynamics across three experimental scenarios: fast\ntranslational tracking, rapid rotational tracking, and recovery from sudden\ndisturbance. The proposed method consistently achieved better tracking\nperformance, and notably, while the baseline diverged and crashed during the\nfastest translational trajectory tracking and the recovery experiment, the\nproposed controller maintained stability and successfully completed the tasks,\nthereby demonstrating its effectiveness.",
    "published": "2025-10-02T05:00:24Z",
    "updated": "2025-10-15T10:31:55Z",
    "link": "http://arxiv.org/pdf/2510.01675v2.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Jaewoo Lee",
      "Dongjae Lee",
      "Jinwoo Lee",
      "Hyungyu Lee",
      "Yeonjoon Kim",
      "H. Jin Kim"
    ]
  }
]