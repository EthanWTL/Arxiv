[
  {
    "id": "http://arxiv.org/abs/2510.11718v1",
    "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
    "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
    "published": "2025-10-13T17:59:55Z",
    "updated": "2025-10-13T17:59:55Z",
    "link": "http://arxiv.org/pdf/2510.11718v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Chengqi Duan",
      "Kaiyue Sun",
      "Rongyao Fang",
      "Manyuan Zhang",
      "Yan Feng",
      "Ying Luo",
      "Yufang Liu",
      "Ke Wang",
      "Peng Pei",
      "Xunliang Cai",
      "Hongsheng Li",
      "Yi Ma",
      "Xihui Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20705v2",
    "title": "On Convolutions, Intrinsic Dimension, and Diffusion Models",
    "summary": "The manifold hypothesis asserts that data of interest in high-dimensional\nambient spaces, such as image data, lies on unknown low-dimensional\nsubmanifolds. Diffusion models (DMs) -- which operate by convolving data with\nprogressively larger amounts of Gaussian noise and then learning to revert this\nprocess -- have risen to prominence as the most performant generative models,\nand are known to be able to learn distributions with low-dimensional support.\nFor a given datum in one of these submanifolds, we should thus intuitively\nexpect DMs to have implicitly learned its corresponding local intrinsic\ndimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari\net al. (2024b) recently showed that this is indeed the case by linking this LID\nto the rate of change of the log marginal densities of the DM with respect to\nthe amount of added noise, resulting in an LID estimator known as FLIPD. LID\nestimators such as FLIPD have a plethora of uses, among others they quantify\nthe complexity of a given datum, and can be used to detect outliers,\nadversarial examples and AI-generated text. FLIPD achieves state-of-the-art\nperformance at LID estimation, yet its theoretical underpinnings are incomplete\nsince Kamkari et al. (2024b) only proved its correctness under the highly\nunrealistic assumption of affine submanifolds. In this work we bridge this gap\nby formally proving the correctness of FLIPD under realistic assumptions.\nAdditionally, we show that an analogous result holds when Gaussian convolutions\nare replaced with uniform ones, and discuss the relevance of this result.",
    "published": "2025-06-25T18:00:00Z",
    "updated": "2025-10-13T17:59:41Z",
    "link": "http://arxiv.org/pdf/2506.20705v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Kin Kwan Leung",
      "Rasa Hosseinzadeh",
      "Gabriel Loaiza-Ganem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11709v1",
    "title": "Adversarial Attacks Leverage Interference Between Features in\n  Superposition",
    "summary": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.",
    "published": "2025-10-13T17:59:02Z",
    "updated": "2025-10-13T17:59:02Z",
    "link": "http://arxiv.org/pdf/2510.11709v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Edward Stevinson",
      "Lucas Prieto",
      "Melih Barsbey",
      "Tolga Birdal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11694v1",
    "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine\n  Learning Engineering",
    "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.",
    "published": "2025-10-13T17:54:02Z",
    "updated": "2025-10-13T17:54:02Z",
    "link": "http://arxiv.org/pdf/2510.11694v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Arjun Sahney",
      "Ram Gorthi",
      "Cezary Łastowski",
      "Javier Vega"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11693v1",
    "title": "Scaling Language-Centric Omnimodal Representation Learning",
    "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
    "published": "2025-10-13T17:53:52Z",
    "updated": "2025-10-13T17:53:52Z",
    "link": "http://arxiv.org/pdf/2510.11693v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Chenghao Xiao",
      "Hou Pong Chan",
      "Hao Zhang",
      "Weiwen Xu",
      "Mahani Aljunied",
      "Yu Rong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11689v1",
    "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for\n  Uncertainty-Aware Sim-to-Real Manipulation",
    "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .",
    "published": "2025-10-13T17:51:23Z",
    "updated": "2025-10-13T17:51:23Z",
    "link": "http://arxiv.org/pdf/2510.11689v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Maggie Wang",
      "Stephen Tian",
      "Aiden Swann",
      "Ola Shorinwa",
      "Jiajun Wu",
      "Mac Schwager"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11688v1",
    "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities",
    "summary": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models.",
    "published": "2025-10-13T17:50:25Z",
    "updated": "2025-10-13T17:50:25Z",
    "link": "http://arxiv.org/pdf/2510.11688v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Zicheng Liu",
      "Lige Huang",
      "Jie Zhang",
      "Dongrui Liu",
      "Yuan Tian",
      "Jing Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11686v1",
    "title": "Representation-Based Exploration for Language Models: From Test-Time to\n  Post-Training",
    "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.",
    "published": "2025-10-13T17:49:05Z",
    "updated": "2025-10-13T17:49:05Z",
    "link": "http://arxiv.org/pdf/2510.11686v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jens Tuyls",
      "Dylan J. Foster",
      "Akshay Krishnamurthy",
      "Jordan T. Ash"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11683v1",
    "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models",
    "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.",
    "published": "2025-10-13T17:47:50Z",
    "updated": "2025-10-13T17:47:50Z",
    "link": "http://arxiv.org/pdf/2510.11683v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Nianyi Lin",
      "Jiajie Zhang",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11682v1",
    "title": "Ego-Vision World Model for Humanoid Contact Planning",
    "summary": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/",
    "published": "2025-10-13T17:47:39Z",
    "updated": "2025-10-13T17:47:39Z",
    "link": "http://arxiv.org/pdf/2510.11682v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Hang Liu",
      "Yuman Gao",
      "Sangli Teng",
      "Yufeng Chi",
      "Yakun Sophia Shao",
      "Zhongyu Li",
      "Maani Ghaffari",
      "Koushil Sreenath"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11676v1",
    "title": "Accelerated stochastic first-order method for convex optimization under\n  heavy-tailed noise",
    "summary": "We study convex composite optimization problems, where the objective function\nis given by the sum of a prox-friendly function and a convex function whose\nsubgradients are estimated under heavy-tailed noise. Existing work often\nemploys gradient clipping or normalization techniques in stochastic first-order\nmethods to address heavy-tailed noise. In this paper, we demonstrate that a\nvanilla stochastic algorithm -- without additional modifications such as\nclipping or normalization -- can achieve optimal complexity for these problems.\nIn particular, we establish that an accelerated stochastic proximal subgradient\nmethod achieves a first-order oracle complexity that is universally optimal for\nsmooth, weakly smooth, and nonsmooth convex optimization, as well as for\nstochastic convex optimization under heavy-tailed noise. Numerical experiments\nare further provided to validate our theoretical results.",
    "published": "2025-10-13T17:45:05Z",
    "updated": "2025-10-13T17:45:05Z",
    "link": "http://arxiv.org/pdf/2510.11676v1.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "49M05, 49M37, 90C25, 90C30"
    ],
    "authors": [
      "Chuan He",
      "Zhaosong Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11675v1",
    "title": "FACE: Faithful Automatic Concept Extraction",
    "summary": "Interpreting deep neural networks through concept-based explanations offers a\nbridge between low-level features and high-level human-understandable\nsemantics. However, existing automatic concept discovery methods often fail to\nalign these extracted concepts with the model's true decision-making process,\nthereby compromising explanation faithfulness. In this work, we propose FACE\n(Faithful Automatic Concept Extraction), a novel framework that augments\nNon-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence\nregularization term to ensure alignment between the model's original and\nconcept-based predictions. Unlike prior methods that operate solely on encoder\nactivations, FACE incorporates classifier supervision during concept learning,\nenforcing predictive consistency and enabling faithful explanations. We provide\ntheoretical guarantees showing that minimizing the KL divergence bounds the\ndeviation in predictive distributions, thereby promoting faithful local\nlinearity in the learned concept space. Systematic evaluations on ImageNet,\nCOCO, and CelebA datasets demonstrate that FACE outperforms existing methods\nacross faithfulness and sparsity metrics.",
    "published": "2025-10-13T17:44:45Z",
    "updated": "2025-10-13T17:44:45Z",
    "link": "http://arxiv.org/pdf/2510.11675v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Dipkamal Bhusal",
      "Michael Clifford",
      "Sara Rampazzi",
      "Nidhi Rastogi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11661v1",
    "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
    "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
    "published": "2025-10-13T17:35:23Z",
    "updated": "2025-10-13T17:35:23Z",
    "link": "http://arxiv.org/pdf/2510.11661v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shijie Xia",
      "Yuhan Sun",
      "Pengfei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11660v1",
    "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
    "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.",
    "published": "2025-10-13T17:34:48Z",
    "updated": "2025-10-13T17:34:48Z",
    "link": "http://arxiv.org/pdf/2510.11660v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yi Yang",
      "Kefan Gu",
      "Yuqing Wen",
      "Hebei Li",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Xudong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13563v3",
    "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient\n  Delta Compression",
    "summary": "With the rise of the fine-tuned-pretrained paradigm, storing numerous\nfine-tuned models for multi-tasking creates significant storage overhead. Delta\ncompression alleviates this by storing only the pretrained model and the highly\ncompressed delta weights (the differences between fine-tuned and pretrained\nmodel weights). However, existing methods fail to maintain both high\ncompression and performance, and often rely on data. To address these\nchallenges, we propose UltraDelta, the first data-free delta compression\npipeline that achieves both ultra-high compression and strong performance.\nUltraDelta is designed to minimize redundancy, maximize information, and\nstabilize performance across inter-layer, intra-layer, and global dimensions,\nusing three key components: (1) Variance-Based Mixed Sparsity Allocation\nassigns sparsity based on variance, giving lower sparsity to high-variance\nlayers to preserve inter-layer information. (2) Distribution-Aware Compression\napplies uniform quantization and then groups parameters by value, followed by\ngroup-wise pruning, to better preserve intra-layer distribution. (3)\nTrace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a\nglobal rescaling factor, improving model stability under higher compression.\nExtensive experiments across (a) large language models (fine-tuned on LLaMA-2\n7B and 13B) with up to 50x compression, (b) general NLP models (RoBERTa-base,\nT5-base) with up to 224x compression, (c) vision models (ViT-B/32, ViT-L/14)\nwith up to 132x compression, and (d) multi-modal models (BEiT-3) with 18x\ncompression, demonstrate that UltraDelta consistently outperforms existing\nmethods, especially under ultra-high compression. Code is available at\nhttps://github.com/xiaohuiwang000/UltraDelta.",
    "published": "2025-05-19T10:37:22Z",
    "updated": "2025-10-13T17:33:00Z",
    "link": "http://arxiv.org/pdf/2505.13563v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Xiaohui Wang",
      "Peng Ye",
      "Chenyu Huang",
      "Shenghe Zheng",
      "Bo Zhang",
      "Lei Bai",
      "Wanli Ouyang",
      "Tao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11654v1",
    "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection",
    "summary": "Financial markets face growing threats from misinformation that can trigger\nbillions in losses in minutes. Most existing approaches lack transparency in\ntheir decision-making and provide limited attribution to credible sources. We\nintroduce FinVet, a novel multi-agent framework that integrates two\nRetrieval-Augmented Generation (RAG) pipelines with external fact-checking\nthrough a confidence-weighted voting mechanism. FinVet employs adaptive\nthree-tier processing that dynamically adjusts verification strategies based on\nretrieval confidence, from direct metadata extraction to hybrid reasoning to\nfull model-based analysis. Unlike existing methods, FinVet provides\nevidence-backed verdicts, source attribution, confidence scores, and explicit\nuncertainty flags when evidence is insufficient. Experimental evaluation on the\nFinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a\n10.4% improvement over the best individual pipeline (fact-check pipeline) and\n37% improvement over standalone RAG approaches.",
    "published": "2025-10-13T17:31:49Z",
    "updated": "2025-10-13T17:31:49Z",
    "link": "http://arxiv.org/pdf/2510.11654v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Daniel Berhane Araya",
      "Duoduo Liao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11653v1",
    "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
    "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
    "published": "2025-10-13T17:30:54Z",
    "updated": "2025-10-13T17:30:54Z",
    "link": "http://arxiv.org/pdf/2510.11653v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Prasanna Mayilvahanan",
      "Ricardo Dominguez-Olmedo",
      "Thaddäus Wiedemer",
      "Wieland Brendel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02356v2",
    "title": "Measuring Physical-World Privacy Awareness of Large Language Models: An\n  Evaluation Benchmark",
    "summary": "The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment. Codes and datasets will be available at\nhttps://github.com/Graph-COM/EAPrivacy.",
    "published": "2025-09-27T23:39:56Z",
    "updated": "2025-10-13T17:24:22Z",
    "link": "http://arxiv.org/pdf/2510.02356v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Xinjie Shen",
      "Mufei Li",
      "Pan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23703v4",
    "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability",
    "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an\nend-to-end framework designed to incorporate the FL expert into NL math\nproblem-solving. To bridge the NL and FL input format gap, we propose the NL-FL\nProblem Alignment method, which reformulates the Question-Answering (QA)\nproblems in NL as existence theorems in FL. Subsequently, the Mixed Problem\nInput technique we provide enables the FL reasoner to handle both QA and\nexistence problems concurrently. Lastly, we mitigate the NL and FL output\nformat gap in reasoning through an LLM-based Answer Extraction mechanism.\nComprehensive experiments demonstrate that the NFL-HR framework achieves\n**89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by **4.60%** and **4.82%**,\nrespectively. Notably, some problems resolved by our framework remain unsolved\nby the NL baseline model even under a larger number of trials.",
    "published": "2025-05-29T17:39:30Z",
    "updated": "2025-10-13T17:20:33Z",
    "link": "http://arxiv.org/pdf/2505.23703v4.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Ruida Wang",
      "Yuxin Li",
      "Yi R. Fung",
      "Tong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11632v1",
    "title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object\n  Detection",
    "summary": "Recent studies in 3D object detection for autonomous vehicles aim to enrich\nfeatures through the utilization of multi-modal setups or the extraction of\nlocal patterns within LiDAR point clouds. However, multi-modal methods face\nsignificant challenges in feature alignment, and gaining features locally can\nbe oversimplified for complex 3D object detection tasks. In this paper, we\npropose a novel model, NV3D, which utilizes local features acquired from voxel\nneighbors, as normal vectors computed per voxel basis using K-nearest neighbors\n(KNN) and principal component analysis (PCA). This informative feature enables\nNV3D to determine the relationship between the surface and pertinent target\nentities, including cars, pedestrians, or cyclists. During the normal vector\nextraction process, NV3D offers two distinct sampling strategies: normal vector\ndensity-based sampling and FOV-aware bin-based sampling, allowing elimination\nof up to 55% of data while maintaining performance. In addition, we applied\nelement-wise attention fusion, which accepts voxel features as the query and\nvalue and normal vector features as the key, similar to the attention\nmechanism. Our method is trained on the KITTI dataset and has demonstrated\nsuperior performance in car and cyclist detection owing to their spatial\nshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%\nmean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%\nand 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in\ncar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of\nvoxels being filtered out.",
    "published": "2025-10-13T17:13:06Z",
    "updated": "2025-10-13T17:13:06Z",
    "link": "http://arxiv.org/pdf/2510.11632v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.9; I.2.10; I.4.8; I.4.10; I.5.1; I.5.4"
    ],
    "authors": [
      "Krittin Chaowakarn",
      "Paramin Sangwongngam",
      "Nang Htet Htet Aung",
      "Chalie Charoenlarpnopparut"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11631v1",
    "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
    "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
    "published": "2025-10-13T17:12:02Z",
    "updated": "2025-10-13T17:12:02Z",
    "link": "http://arxiv.org/pdf/2510.11631v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "authors": [
      "Tobias Preintner",
      "Weixuan Yuan",
      "Adrian König",
      "Thomas Bäck",
      "Elena Raponi",
      "Niki van Stein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.22578v2",
    "title": "The Hidden Link Between RLHF and Contrastive Learning",
    "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be interpreted as methods that performing\ncontrastive learning based on the positive and negative samples derived from\nbase model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). Such paradigm further illuminates why RLHF\nmay not intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on the perspective, we replace the\nDV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.",
    "published": "2025-06-27T18:51:25Z",
    "updated": "2025-10-13T17:12:00Z",
    "link": "http://arxiv.org/pdf/2506.22578v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Xufei Lv",
      "Kehai Chen",
      "Haoyuan Sun",
      "Xuefeng Bai",
      "Min Zhang",
      "Houde Liu",
      "Kehai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11616v1",
    "title": "Attention Factors for Statistical Arbitrage",
    "summary": "Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a framework to jointly identify similar assets through\nfactors, identify mispricing and form a trading policy that maximizes\nrisk-adjusted performance after trading costs. Our Attention Factors are\nconditional latent factors that are the most useful for arbitrage trading. They\nare learned from firm characteristic embeddings that allow for complex\ninteractions. We identify time-series signals from the residual portfolios of\nour factors with a general sequence model. Estimating factors and the arbitrage\ntrading strategy jointly is crucial to maximize profitability after trading\ncosts. In a comprehensive empirical study we show that our Attention Factor\nmodel achieves an out-of-sample Sharpe ratio above 4 on the largest U.S.\nequities over a 24-year period. Our one-step solution yields an unprecedented\nSharpe ratio of 2.3 net of transaction costs. We show that weak factors are\nimportant for arbitrage trading.",
    "published": "2025-10-13T16:56:30Z",
    "updated": "2025-10-13T16:56:30Z",
    "link": "http://arxiv.org/pdf/2510.11616v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP",
      "I.2.0"
    ],
    "authors": [
      "Elliot L. Epstein",
      "Rose Wang",
      "Jaewon Choi",
      "Markus Pelger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11615v1",
    "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
    "summary": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.",
    "published": "2025-10-13T16:55:07Z",
    "updated": "2025-10-13T16:55:07Z",
    "link": "http://arxiv.org/pdf/2510.11615v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Xurong Xie",
      "Zhucun Xue",
      "Jiafu Wu",
      "Jian Li",
      "Yabiao Wang",
      "Xiaobin Hu",
      "Yong Liu",
      "Jiangning Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19343v3",
    "title": "Part-of-speech tagging for Nagamese Language using CRF",
    "summary": "This paper investigates part-of-speech tagging, an important task in Natural\nLanguage Processing (NLP) for the Nagamese language. The Nagamese language,\na.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily\nas a means of communication in trade between the Nagas and people from Assam in\nnortheast India. A substantial amount of work in part-of-speech-tagging has\nbeen done for resource-rich languages like English, Hindi, etc. However, no\nwork has been done in the Nagamese language. To the best of our knowledge, this\nis the first attempt at part-of-speech tagging for the Nagamese Language. The\naim of this work is to identify the part-of-speech for a given sentence in the\nNagamese language. An annotated corpus of 16,112 tokens is created and applied\nmachine learning technique known as Conditional Random Fields (CRF). Using CRF,\nan overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score\nof 85% is achieved.\n  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.",
    "published": "2025-09-16T12:59:55Z",
    "updated": "2025-10-13T16:54:53Z",
    "link": "http://arxiv.org/pdf/2509.19343v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Alovi N Shohe",
      "Chonglio Khiamungam",
      "Teisovi Angami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11608v1",
    "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
    "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.",
    "published": "2025-10-13T16:47:07Z",
    "updated": "2025-10-13T16:47:07Z",
    "link": "http://arxiv.org/pdf/2510.11608v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shiqi Zhang",
      "Xinbei Ma",
      "Yunqing Xu",
      "Zouying Cao",
      "Pengrui Lu",
      "Haobo Yuan",
      "Tiancheng Shen",
      "Zhuosheng Zhang",
      "Hai Zhao",
      "Ming-Hsuan Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11604v1",
    "title": "Explainability, risk modeling, and segmentation based customer churn\n  analytics for personalized retention in e-commerce",
    "summary": "In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.",
    "published": "2025-10-13T16:44:24Z",
    "updated": "2025-10-13T16:44:24Z",
    "link": "http://arxiv.org/pdf/2510.11604v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Sanjula De Alwis",
      "Indrajith Ekanayake"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.05396v2",
    "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent\n  Debate",
    "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. Prior work has primarily focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time - even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. We perform\nadditional experiments investigating various potential contributing factors to\nthese harmful shifts - including sycophancy, social conformity, and model and\ntask type. These results highlight important failure modes in the exchange of\nreasons during multi-agent debate, suggesting that naive applications of debate\nmay cause performance degradation when agents are neither incentivised nor\nadequately equipped to resist persuasive but incorrect reasoning.",
    "published": "2025-09-05T13:47:38Z",
    "updated": "2025-10-13T16:40:01Z",
    "link": "http://arxiv.org/pdf/2509.05396v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "authors": [
      "Andrea Wynn",
      "Harsh Satija",
      "Gillian Hadfield"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11599v1",
    "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific\n  and Interpretable Scientific Domain Mapping",
    "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating\nmultifaceted embeddings of scientific abstracts, evaluated in the domains of\ninvasion biology and medicine. These embeddings capture distinct, individually\nspecifiable aspects in isolation, thus enabling fine-grained and controllable\nsimilarity assessments as well as adaptive, user-driven visualizations of\nscientific domains. Our approach relies on an unsupervised procedure that\nproduces aspect-specific summarizing sentences and trains embedding models to\nmap semantically related summaries to nearby positions in the embedding space.\nWe then distill these aspect-specific embedding capabilities into a unified\nembedding model that directly predicts multiple aspect embeddings from a\nscientific abstract in a single, efficient forward pass. In addition, we\nintroduce an embedding decoding pipeline that decodes embeddings back into\nnatural language descriptions of their associated aspects. Notably, we show\nthat this decoding remains effective even for unoccupied regions in\nlow-dimensional visualizations, thus offering vastly improved interpretability\nin user-centric settings.",
    "published": "2025-10-13T16:38:20Z",
    "updated": "2025-10-13T16:38:20Z",
    "link": "http://arxiv.org/pdf/2510.11599v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Marc Brinner",
      "Sina Zarrieß"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11595v1",
    "title": "Reproducibility: The New Frontier in AI Governance",
    "summary": "AI policymakers are responsible for delivering effective governance\nmechanisms that can provide safe, aligned and trustworthy AI development.\nHowever, the information environment offered to policymakers is characterised\nby an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and\ncreating deep uncertainty and divides on which risks should be prioritised from\na governance perspective. We posit that the current publication speeds in AI\ncombined with the lack of strong scientific standards, via weak reproducibility\nprotocols, effectively erodes the power of policymakers to enact meaningful\npolicy and governance protocols. Our paper outlines how AI research could adopt\nstricter reproducibility guidelines to assist governance endeavours and improve\nconsensus on the AI risk landscape. We evaluate the forthcoming reproducibility\ncrisis within AI research through the lens of crises in other scientific\ndomains; providing a commentary on how adopting preregistration, increased\nstatistical power and negative result publication reproducibility protocols can\nenable effective AI governance. While we maintain that AI governance must be\nreactive due to AI's significant societal implications we argue that\npolicymakers and governments must consider reproducibility protocols as a core\ntool in the governance arsenal and demand higher standards for AI research.\nCode to replicate data and figures:\nhttps://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance",
    "published": "2025-10-13T16:34:25Z",
    "updated": "2025-10-13T16:34:25Z",
    "link": "http://arxiv.org/pdf/2510.11595v1.pdf",
    "category": [
      "cs.AI",
      "cs.GL"
    ],
    "authors": [
      "Israel Mason-Williams",
      "Gabryel Mason-Williams"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11593v1",
    "title": "Hierarchical Qubit-Merging Transformer for Quantum Error Correction",
    "summary": "For reliable large-scale quantum computation, a quantum error correction\n(QEC) scheme must effectively resolve physical errors to protect logical\ninformation. Leveraging recent advances in deep learning, neural network-based\ndecoders have emerged as a promising approach to enhance the reliability of\nQEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and\ngeneral decoding framework that explicitly leverages the structural graph of\nstabilizer codes to learn error correlations across multiple scales. Our\narchitecture first computes attention locally on structurally related groups of\nstabilizers and then systematically merges these qubit-centric representations\nto build a global view of the error syndrome. The proposed HQMT achieves\nsubstantially lower logical error rates for surface codes by integrating a\ndedicated qubit-merging layer within the transformer architecture. Across\nvarious code distances, HQMT significantly outperforms previous neural\nnetwork-based QEC decoders as well as a powerful belief propagation with\nordered statistics decoding (BP+OSD) baseline. This hierarchical approach\nprovides a scalable and effective framework for surface code decoding,\nadvancing the realization of reliable quantum computing.",
    "published": "2025-10-13T16:31:46Z",
    "updated": "2025-10-13T16:31:46Z",
    "link": "http://arxiv.org/pdf/2510.11593v1.pdf",
    "category": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Seong-Joon Park",
      "Hee-Youl Kwak",
      "Yongjune Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11588v1",
    "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents",
    "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.",
    "published": "2025-10-13T16:30:07Z",
    "updated": "2025-10-13T16:30:07Z",
    "link": "http://arxiv.org/pdf/2510.11588v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jiateng Liu",
      "Zhenhailong Wang",
      "Xiaojiang Huang",
      "Yingjie Li",
      "Xing Fan",
      "Xiang Li",
      "Chenlei Guo",
      "Ruhi Sarikaya",
      "Heng Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20333v2",
    "title": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A\n  Unified Information-Geometric Framework",
    "summary": "We present Multi-Scale Manifold Alignment(MSMA), an information-geometric\nframework that decomposes LLM representations into local, intermediate, and\nglobal manifolds and learns cross-scale mappings that preserve geometry and\ninformation. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent\nhierarchical patterns and find that MSMA improves alignment metrics under\nmultiple estimators (e.g., relative KL reduction and MI gains with statistical\nsignificance across seeds). Controlled interventions at different scales yield\ndistinct and architecture-dependent effects on lexical diversity, sentence\nstructure, and discourse coherence. While our theoretical analysis relies on\nidealized assumptions, the empirical results suggest that multi-objective\nalignment offers a practical lens for analyzing cross-scale information flow\nand guiding representation-level control.",
    "published": "2025-05-24T10:25:58Z",
    "updated": "2025-10-13T16:09:37Z",
    "link": "http://arxiv.org/pdf/2505.20333v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.04310v3",
    "title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in\n  Multi-Turn Price Negotiation",
    "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
    "published": "2025-09-04T15:23:58Z",
    "updated": "2025-10-13T16:04:56Z",
    "link": "http://arxiv.org/pdf/2509.04310v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yunbo Long",
      "Liming Xu",
      "Lukas Beckenbauer",
      "Yuhan Liu",
      "Alexandra Brintrup"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11560v1",
    "title": "Characterizing Web Search in The Age of Generative AI",
    "summary": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI.",
    "published": "2025-10-13T16:04:03Z",
    "updated": "2025-10-13T16:04:03Z",
    "link": "http://arxiv.org/pdf/2510.11560v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Elisabeth Kirsten",
      "Jost Grosse Perdekamp",
      "Mihir Upadhyay",
      "Krishna P. Gummadi",
      "Muhammad Bilal Zafar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07772v2",
    "title": "An approach for systematic decomposition of complex llm tasks",
    "summary": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point).",
    "published": "2025-10-09T04:24:47Z",
    "updated": "2025-10-13T16:03:13Z",
    "link": "http://arxiv.org/pdf/2510.07772v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tianle Zhou",
      "Jiakai Xu",
      "Guanhong Liu",
      "Jiaxiang Liu",
      "Haonan Wang",
      "Eugene Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11558v1",
    "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative\n  Study of Market Leading Agentic AI Products",
    "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.",
    "published": "2025-10-13T16:00:34Z",
    "updated": "2025-10-13T16:00:34Z",
    "link": "http://arxiv.org/pdf/2510.11558v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Komal Gupta",
      "Aditya Shrivastava"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20340v2",
    "title": "Empirical Investigation of Latent Representational Dynamics in Large\n  Language Models: A Manifold Evolution Perspective",
    "summary": "This paper introduces the Dynamical Manifold Evolution Theory (DMET), a\nconceptual framework that models large language model (LLM) generation as a\ncontinuous trajectory evolving on a low-dimensional semantic manifold. The\ntheory characterizes latent dynamics through three interpretable metrics-state\ncontinuity ($C$), attractor compactness ($Q$), and topological persistence\n($P$)-which jointly capture the smoothness, stability, and structure of\nrepresentation evolution. Empirical analyses across multiple Transformer\narchitectures reveal consistent links between these latent dynamics and text\nquality: smoother trajectories correspond to greater fluency, and richer\ntopological organization correlates with enhanced coherence. Different models\nexhibit distinct dynamical regimes, reflecting diverse strategies of semantic\norganization in latent space. Moreover, decoding parameters such as temperature\nand top-$p$ shape these trajectories in predictable ways, defining a balanced\nregion that harmonizes fluency and creativity. As a phenomenological rather\nthan first-principles framework, DMET provides a unified and testable\nperspective for interpreting, monitoring, and guiding LLM behavior, offering\nnew insights into the interplay between internal representation dynamics and\nexternal text generation quality.",
    "published": "2025-05-24T14:17:50Z",
    "updated": "2025-10-13T15:56:25Z",
    "link": "http://arxiv.org/pdf/2505.20340v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14641v2",
    "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot",
    "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.",
    "published": "2025-06-17T15:39:33Z",
    "updated": "2025-10-13T15:55:14Z",
    "link": "http://arxiv.org/pdf/2506.14641v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Xiang Cheng",
      "Chengyan Pan",
      "Minjun Zhao",
      "Deyang Li",
      "Fangchao Liu",
      "Xinyu Zhang",
      "Xiao Zhang",
      "Yong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.06577v2",
    "title": "The Minimal Search Space for Conditional Causal Bandits",
    "summary": "Causal knowledge can be used to support decision-making problems. This has\nbeen recognized in the causal bandits literature, where a causal (multi-armed)\nbandit is characterized by a causal graphical model and a target variable. The\narms are then interventions on the causal model, and rewards are samples of the\ntarget variable. Causal bandits were originally studied with a focus on hard\ninterventions. We focus instead on cases where the arms are conditional\ninterventions, which more accurately model many real-world decision-making\nproblems by allowing the value of the intervened variable to be chosen based on\nthe observed values of other variables. This paper presents a graphical\ncharacterization of the minimal set of nodes guaranteed to contain the optimal\nconditional intervention, which maximizes the expected reward. We then propose\nan efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify\nthis minimal set of nodes. We prove that the graphical characterization and the\nproposed algorithm are correct. Finally, we empirically demonstrate that our\nalgorithm significantly prunes the search space and substantially accelerates\nconvergence rates when integrated into standard multi-armed bandit algorithms.",
    "published": "2025-02-10T15:45:18Z",
    "updated": "2025-10-13T15:52:22Z",
    "link": "http://arxiv.org/pdf/2502.06577v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Francisco N. F. Q. Simoes",
      "Itai Feigenbaum",
      "Mehdi Dastani",
      "Thijs van Ommen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.04192v2",
    "title": "ViDRiP-LLaVA: A Dataset and Benchmark for Diagnostic Reasoning from\n  Pathology Videos",
    "summary": "We present ViDRiP-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios,\nincluding single patch images, automatically segmented pathology video clips,\nand manually segmented pathology videos. This integration closely mirrors the\nnatural diagnostic process of pathologists. By generating detailed histological\ndescriptions and culminating in a definitive sign-out diagnosis, ViDRiP-LLaVA\nbridges visual narratives with diagnostic reasoning. Central to our approach is\nthe ViDRiP-Instruct dataset, comprising 4278 video and diagnosis-specific\nchain-of-thought instructional pairs sourced from educational histopathology\nvideos on YouTube. Although high-quality data is critical for enhancing\ndiagnostic reasoning, its creation is time-intensive and limited in volume. To\novercome this challenge, we transfer knowledge from existing single-image\ninstruction datasets to train on weakly annotated, keyframe-extracted clips,\nfollowed by fine-tuning on manually segmented videos. ViDRiP-LLaVA establishes\na new benchmark in pathology video analysis and offers a promising foundation\nfor future AI systems that support clinical decision-making through integrated\nvisual and diagnostic reasoning. Our code, data, and model are publicly\navailable at: https://github.com/QuIIL/ViDRiP-LLaVA.",
    "published": "2025-05-07T07:41:19Z",
    "updated": "2025-10-13T15:50:05Z",
    "link": "http://arxiv.org/pdf/2505.04192v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Trinh T. L. Vuong",
      "Jin Tae Kwak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11541v1",
    "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method\n  for Retrieval Augmented Generation",
    "summary": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN.",
    "published": "2025-10-13T15:41:15Z",
    "updated": "2025-10-13T15:41:15Z",
    "link": "http://arxiv.org/pdf/2510.11541v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yuchen Yan",
      "Zhihua Liu",
      "Hao Wang",
      "Weiming Li",
      "Xiaoshuai Hao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02300v3",
    "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
    "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
    "published": "2025-10-02T17:59:06Z",
    "updated": "2025-10-13T15:39:31Z",
    "link": "http://arxiv.org/pdf/2510.02300v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Runqian Wang",
      "Yilun Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11536v1",
    "title": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding\n  Interactions with LLMs",
    "summary": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview.",
    "published": "2025-10-13T15:39:08Z",
    "updated": "2025-10-13T15:39:08Z",
    "link": "http://arxiv.org/pdf/2510.11536v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Manaal Basha",
      "Aimeê M. Ribeiro",
      "Jeena Javahar",
      "Cleidson R. B. de Souza",
      "Gema Rodríguez-Pérez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11535v1",
    "title": "A Flexible Multi-Agent Deep Reinforcement Learning Framework for Dynamic\n  Routing and Scheduling of Latency-Critical Services",
    "summary": "Timely delivery of delay-sensitive information over dynamic, heterogeneous\nnetworks is increasingly essential for a range of interactive applications,\nsuch as industrial automation, self-driving vehicles, and augmented reality.\nHowever, most existing network control solutions target only average delay\nperformance, falling short of providing strict End-to-End (E2E) peak latency\nguarantees. This paper addresses the challenge of reliably delivering packets\nwithin application-imposed deadlines by leveraging recent advancements in\nMulti-Agent Deep Reinforcement Learning (MA-DRL). After introducing the\nDelay-Constrained Maximum-Throughput (DCMT) dynamic network control problem,\nand highlighting the limitations of current solutions, we present a novel\nMA-DRL network control framework that leverages a centralized routing and\ndistributed scheduling architecture. The proposed framework leverages critical\nnetworking domain knowledge for the design of effective MA-DRL strategies based\non the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where\ncentralized routing and distributed scheduling agents dynamically assign paths\nand schedule packet transmissions according to packet lifetimes, thereby\nmaximizing on-time packet delivery. The generality of the proposed framework\nallows integrating both data-driven \\blue{Deep Reinforcement Learning (DRL)}\nagents and traditional rule-based policies in order to strike the right balance\nbetween performance and learning complexity. Our results confirm the\nsuperiority of the proposed framework with respect to traditional stochastic\noptimization-based approaches and provide key insights into the role and\ninterplay between data-driven DRL agents and new rule-based policies for both\nefficient and high-performance control of latency-critical services.",
    "published": "2025-10-13T15:38:10Z",
    "updated": "2025-10-13T15:38:10Z",
    "link": "http://arxiv.org/pdf/2510.11535v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Vincenzo Norman Vitale",
      "Antonia Maria Tulino",
      "Andreas F. Molisch",
      "Jaime Llorca"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13837v3",
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential.",
    "published": "2025-04-18T17:59:56Z",
    "updated": "2025-10-13T15:37:46Z",
    "link": "http://arxiv.org/pdf/2504.13837v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Yang Yue",
      "Zhiqi Chen",
      "Rui Lu",
      "Andrew Zhao",
      "Zhaokai Wang",
      "Yang Yue",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18339v2",
    "title": "Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and\n  Symbolic Discovery of Nonlinear Dynamics",
    "summary": "Understanding and modeling nonlinear dynamical systems is a fundamental\nchallenge across science and engineering. Deep learning has shown remarkable\npotential for capturing complex system behavior, yet achieving models that are\nboth accurate and physically interpretable remains difficult. To address this,\nwe propose Structured Kolmogorov-Arnold Neural ODEs (SKANODEs), a framework\nthat integrates structured state-space modeling with Kolmogorov-Arnold Networks\n(KANs). Within a Neural ODE architecture, SKANODE employs a fully trainable KAN\nas a universal function approximator to perform virtual sensing, recovering\nlatent states that correspond to interpretable physical quantities such as\ndisplacements and velocities. Leveraging KAN's symbolic regression capability,\nSKANODE then extracts compact, interpretable expressions for the system's\ngoverning dynamics. Extensive experiments on simulated and real-world systems\ndemonstrate that SKANODE achieves superior predictive accuracy, discovers\nphysics-consistent dynamics, and reveals complex nonlinear behavior. Notably,\nit identifies hysteretic behavior in an F-16 aircraft and recovers a concise\nsymbolic equation describing this phenomenon. SKANODE thus enables\ninterpretable, data-driven discovery of physically grounded models for complex\nnonlinear dynamical systems.",
    "published": "2025-06-23T06:42:43Z",
    "updated": "2025-10-13T15:37:03Z",
    "link": "http://arxiv.org/pdf/2506.18339v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.SC",
      "nlin.CD",
      "physics.data-an"
    ],
    "authors": [
      "Wei Liu",
      "Kiran Bacsa",
      "Loon Ching Tang",
      "Eleni Chatzi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23135v3",
    "title": "Trust Region Reward Optimization and Proximal Inverse Reward\n  Optimization Algorithm",
    "summary": "Inverse Reinforcement Learning (IRL) learns a reward function to explain\nexpert demonstrations. Modern IRL methods often use the adversarial (minimax)\nformulation that alternates between reward and policy optimization, which often\nlead to unstable training. Recent non-adversarial IRL approaches improve\nstability by jointly learning reward and policy via energy-based formulations\nbut lack formal guarantees. This work bridges this gap. We first present a\nunified view showing canonical non-adversarial methods explicitly or implicitly\nmaximize the likelihood of expert behavior, which is equivalent to minimizing\nthe expected return gap. This insight leads to our main contribution: Trust\nRegion Reward Optimization (TRRO), a framework that guarantees monotonic\nimprovement in this likelihood via a Minorization-Maximization process. We\ninstantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical\nand stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to\nthe stability guarantees of Trust Region Policy Optimization (TRPO) in forward\nRL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward\nrecovery, policy imitation with high sample efficiency on MuJoCo and\nGym-Robotics benchmarks and a real-world animal behavior modeling task.",
    "published": "2025-09-27T05:36:13Z",
    "updated": "2025-10-13T15:33:42Z",
    "link": "http://arxiv.org/pdf/2509.23135v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yang Chen",
      "Menglin Zou",
      "Jiaqi Zhang",
      "Yitan Zhang",
      "Junyi Yang",
      "Gael Gendron",
      "Libo Zhang",
      "Jiamou Liu",
      "Michael J. Witbrock"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08338v2",
    "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings",
    "summary": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability.",
    "published": "2025-10-09T15:24:48Z",
    "updated": "2025-10-13T15:22:47Z",
    "link": "http://arxiv.org/pdf/2510.08338v2.pdf",
    "category": [
      "cs.AI",
      "I.2.7; J.4"
    ],
    "authors": [
      "Benjamin F. Maier",
      "Ulf Aslak",
      "Luca Fiaschi",
      "Nina Rismal",
      "Kemble Fletcher",
      "Christian C. Luhmann",
      "Robbie Dow",
      "Kli Pappas",
      "Thomas V. Wiecki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11516v1",
    "title": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns\n  During Programming Tasks",
    "summary": "The use of AI code-generation tools is becoming increasingly common, making\nit important to understand how software developers are adopting these tools. In\nthis study, we investigate how developers engage with Amazon's CodeWhisperer,\nan LLM-based code-generation tool. We conducted two user studies with two\ngroups of 10 participants each, interacting with CodeWhisperer - the first to\nunderstand which interactions were critical to capture and the second to\ncollect low-level interaction data using a custom telemetry plugin. Our\nmixed-methods analysis identified four behavioral patterns: 1) incremental code\nrefinement, 2) explicit instruction using natural language comments, 3)\nbaseline structuring with model suggestions, and 4) integrative use with\nexternal sources. We provide a comprehensive analysis of these patterns .",
    "published": "2025-10-13T15:22:12Z",
    "updated": "2025-10-13T15:22:12Z",
    "link": "http://arxiv.org/pdf/2510.11516v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Jeena Javahar",
      "Tanya Budhrani",
      "Manaal Basha",
      "Cleidson R. B. de Souza",
      "Ivan Beschastnikh",
      "Gema Rodriguez-Perez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05835v3",
    "title": "Contrastive Representation Distillation via Multi-Scale Feature\n  Decoupling",
    "summary": "Knowledge distillation enhances the performance of compact student networks\nby transferring knowledge from more powerful teacher networks without\nintroducing additional parameters. In the feature space, local regions within\nan individual global feature encode distinct yet interdependent semantic\ninformation. Previous feature-based distillation methods mainly emphasize\nglobal feature alignment while neglecting the decoupling of local regions\nwithin an individual global feature, which often results in semantic confusion\nand suboptimal performance. Moreover, conventional contrastive representation\ndistillation suffers from low efficiency due to its reliance on a large memory\nbuffer to store feature samples. To address these limitations, this work\nproposes MSDCRD, a model-agnostic distillation framework that systematically\ndecouples global features into multi-scale local features and leverages the\nresulting semantically rich feature samples with tailored sample-wise and\nfeature-wise contrastive losses. This design enables efficient distillation\nusing only a single batch, eliminating the dependence on external memory.\nExtensive experiments demonstrate that MSDCRD achieves superior performance not\nonly in homogeneous teacher-student settings but also in heterogeneous\narchitectures where feature discrepancies are more pronounced, highlighting its\nstrong generalization capability.",
    "published": "2025-02-09T10:03:18Z",
    "updated": "2025-10-13T15:21:44Z",
    "link": "http://arxiv.org/pdf/2502.05835v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Cuipeng Wang",
      "Haipeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11512v1",
    "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
    "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
    "published": "2025-10-13T15:19:07Z",
    "updated": "2025-10-13T15:19:07Z",
    "link": "http://arxiv.org/pdf/2510.11512v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jianhao Yuan",
      "Fabio Pizzati",
      "Francesco Pinto",
      "Lars Kunze",
      "Ivan Laptev",
      "Paul Newman",
      "Philip Torr",
      "Daniele De Martini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16204v2",
    "title": "Multi-Functional RIS-Enabled in SAGIN for IoT: A Hybrid Deep\n  Reinforcement Learning Approach with Compressed Twin-Models",
    "summary": "A space-air-ground integrated network (SAGIN) for Internet of Things (IoT)\nnetwork architecture is investigated, empowered by multi-functional\nreconfigurable intelligent surfaces (MF-RIS) capable of simultaneously\nreflecting, amplifying, and harvesting wireless energy. The MF-RIS plays a\npivotal role in addressing the energy shortages of low-Earth orbit (LEO)\nsatellites operating in the shadowed regions, while accounting for both\ncommunication and computing energy consumption across the SAGIN nodes. To\nmaximize the long-term energy efficiency (EE) of IoT devices, we formulate a\njoint optimization problem over the MF-RIS parameters, including signal\namplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, IoT device association, and computing\ncapability. The formulated problem is highly non-convex and non-linear and\ncontains mixed discrete-continuous parameters. To tackle this, we conceive a\ncompressed hybrid twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture in IoT network achieves superior EE performance due\nto its complementary coverage, offering notable advantages over either\nstandalone satellite, aerial, or ground-only deployments.",
    "published": "2025-07-22T03:40:56Z",
    "updated": "2025-10-13T15:17:47Z",
    "link": "http://arxiv.org/pdf/2507.16204v2.pdf",
    "category": [
      "cs.AI",
      "eess.SP"
    ],
    "authors": [
      "Li-Hsiang Shen",
      "Jyun-Jhe Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11507v1",
    "title": "Automatic Music Sample Identification with Multi-Track Contrastive\n  Learning",
    "summary": "Sampling, the technique of reusing pieces of existing audio tracks to create\nnew music content, is a very common practice in modern music production. In\nthis paper, we tackle the challenging task of automatic sample identification,\nthat is, detecting such sampled content and retrieving the material from which\nit originates. To do so, we adopt a self-supervised learning approach that\nleverages a multi-track dataset to create positive pairs of artificial mixes,\nand design a novel contrastive learning objective. We show that such method\nsignificantly outperforms previous state-of-the-art baselines, that is robust\nto various genres, and that scales well when increasing the number of noise\nsongs in the reference database. In addition, we extensively analyze the\ncontribution of the different components of our training pipeline and\nhighlight, in particular, the need for high-quality separated stems for this\ntask.",
    "published": "2025-10-13T15:17:08Z",
    "updated": "2025-10-13T15:17:08Z",
    "link": "http://arxiv.org/pdf/2510.11507v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Alain Riou",
      "Joan Serrà",
      "Yuki Mitsufuji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11503v1",
    "title": "People use fast, flat goal-directed simulation to reason about novel\n  problems",
    "summary": "Games have long been a microcosm for studying planning and reasoning in both\nnatural and artificial intelligence, especially with a focus on expert-level or\neven super-human play. But real life also pushes human intelligence along a\ndifferent frontier, requiring people to flexibly navigate decision-making\nproblems that they have never thought about before. Here, we use novice\ngameplay to study how people make decisions and form judgments in new problem\nsettings. We show that people are systematic and adaptively rational in how\nthey play a game for the first time, or evaluate a game (e.g., how fair or how\nfun it is likely to be) before they have played it even once. We explain these\ncapacities via a computational cognitive model that we call the \"Intuitive\nGamer\". The model is based on mechanisms of fast and flat (depth-limited)\ngoal-directed probabilistic simulation--analogous to those used in Monte Carlo\ntree-search models of expert game-play, but scaled down to use very few\nstochastic samples, simple goal heuristics for evaluating actions, and no deep\nsearch. In a series of large-scale behavioral studies with over 1000\nparticipants and 121 two-player strategic board games (almost all novel to our\nparticipants), our model quantitatively captures human judgments and decisions\nvarying the amount and kind of experience people have with a game--from no\nexperience at all (\"just thinking\"), to a single round of play, to indirect\nexperience watching another person and predicting how they should play--and\ndoes so significantly better than much more compute-intensive expert-level\nmodels. More broadly, our work offers new insights into how people rapidly\nevaluate, act, and make suggestions when encountering novel problems, and could\ninform the design of more flexible and human-like AI systems that can determine\nnot just how to solve new tasks, but whether a task is worth thinking about at\nall.",
    "published": "2025-10-13T15:12:08Z",
    "updated": "2025-10-13T15:12:08Z",
    "link": "http://arxiv.org/pdf/2510.11503v1.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.GT"
    ],
    "authors": [
      "Katherine M. Collins",
      "Cedegao E. Zhang",
      "Lionel Wong",
      "Mauricio Barba da Costa",
      "Graham Todd",
      "Adrian Weller",
      "Samuel J. Cheyette",
      "Thomas L. Griffiths",
      "Joshua B. Tenenbaum"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11499v1",
    "title": "Offline Reinforcement Learning with Generative Trajectory Policies",
    "summary": "Generative models have emerged as a powerful class of policies for offline\nreinforcement learning (RL) due to their ability to capture complex,\nmulti-modal behaviors. However, existing methods face a stark trade-off: slow,\niterative models like diffusion policies are computationally expensive, while\nfast, single-step models like consistency policies often suffer from degraded\nperformance. In this paper, we demonstrate that it is possible to bridge this\ngap. The key to moving beyond the limitations of individual methods, we argue,\nlies in a unifying perspective that views modern generative models, including\ndiffusion, flow matching, and consistency models, as specific instances of\nlearning a continuous-time generative trajectory governed by an Ordinary\nDifferential Equation (ODE). This principled foundation provides a clearer\ndesign space for generative policies in RL and allows us to propose Generative\nTrajectory Policies (GTPs), a new and more general policy paradigm that learns\nthe entire solution map of the underlying ODE. To make this paradigm practical\nfor offline RL, we further introduce two key theoretically principled\nadaptations. Empirical results demonstrate that GTP achieves state-of-the-art\nperformance on D4RL benchmarks - it significantly outperforms prior generative\npolicies, achieving perfect scores on several notoriously hard AntMaze tasks.",
    "published": "2025-10-13T15:06:28Z",
    "updated": "2025-10-13T15:06:28Z",
    "link": "http://arxiv.org/pdf/2510.11499v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xinsong Feng",
      "Leshu Tang",
      "Chenan Wang",
      "Haipeng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11496v1",
    "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
    "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoR",
    "published": "2025-10-13T15:04:38Z",
    "updated": "2025-10-13T15:04:38Z",
    "link": "http://arxiv.org/pdf/2510.11496v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhiwei Jin",
      "Xiaohui Song",
      "Nan Wang",
      "Yafei Liu",
      "Chao Li",
      "Xin Li",
      "Ruichen Wang",
      "Zhihao Li",
      "Qi Qi",
      "Long Cheng",
      "Dongze Hao",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Haobo Ji",
      "Jian Ma",
      "Zhitong Zheng",
      "Zhenyi Lin",
      "Haolin Deng",
      "Xin Zou",
      "Xiaojie Yin",
      "Ruilin Wang",
      "Liankai Cai",
      "Haijing Liu",
      "Yuqing Qiu",
      "Ke Chen",
      "Zixian Li",
      "Chi Xie",
      "Huafei Li",
      "Chenxing Li",
      "Chuangchuang Wang",
      "Kai Tang",
      "Zhiguang Zhu",
      "Kai Tang",
      "Wenmei Gao",
      "Rui Wang",
      "Jun Wu",
      "Chao Liu",
      "Qin Xie",
      "Chen Chen",
      "Haonan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11482v1",
    "title": "Investigating Large Language Models' Linguistic Abilities for Text\n  Preprocessing",
    "summary": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
    "published": "2025-10-13T14:53:44Z",
    "updated": "2025-10-13T14:53:44Z",
    "link": "http://arxiv.org/pdf/2510.11482v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Marco Braga",
      "Gian Carlo Milanese",
      "Gabriella Pasi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11474v1",
    "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical\n  Multi-Agent Reinforcement Learning",
    "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios.",
    "published": "2025-10-13T14:44:51Z",
    "updated": "2025-10-13T14:44:51Z",
    "link": "http://arxiv.org/pdf/2510.11474v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Ardian Selmonaj",
      "Giacomo Del Rio",
      "Adrian Schneider",
      "Alessandro Antonucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.18361v4",
    "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile\n  Processing in the Rodent Brain",
    "summary": "Tactile sensing remains far less understood in neuroscience and less\neffective in artificial systems compared to more mature modalities such as\nvision and language. We bridge these gaps by introducing a novel\nEncoder-Attender-Decoder (EAD) framework to systematically explore the space of\ntask-optimized temporal neural networks trained on realistic tactile input\nsequences from a customized rodent whisker-array simulator. We identify\nconvolutional recurrent neural networks (ConvRNNs) as superior encoders to\npurely feedforward and state-space architectures for tactile categorization.\nCrucially, these ConvRNN-encoder-based EAD models achieve neural\nrepresentations closely matching rodent somatosensory cortex, saturating the\nexplainable neural variability and revealing a clear linear relationship\nbetween supervised categorization performance and neural alignment.\nFurthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained\nwith tactile-specific augmentations, match supervised neural fits, serving as\nan ethologically-relevant, label-free proxy.\n  For neuroscience, our findings highlight nonlinear recurrent processing as\nimportant for general-purpose tactile representations in somatosensory cortex,\nproviding the first quantitative characterization of the underlying inductive\nbiases in this system. For embodied AI, our results emphasize the importance of\nrecurrent EAD architectures to handle realistic tactile inputs, along with\ntailored self-supervised learning methods for achieving robust tactile\nperception with the same type of sensors animals use to sense in unstructured\nenvironments.",
    "published": "2025-05-23T20:40:28Z",
    "updated": "2025-10-13T14:44:11Z",
    "link": "http://arxiv.org/pdf/2505.18361v4.pdf",
    "category": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Trinity Chung",
      "Yuchen Shen",
      "Nathan C. L. Kong",
      "Aran Nayebi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11471v1",
    "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned\n  Optimizers",
    "summary": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation.",
    "published": "2025-10-13T14:40:47Z",
    "updated": "2025-10-13T14:40:47Z",
    "link": "http://arxiv.org/pdf/2510.11471v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sarthak Mittal",
      "Divyat Mahajan",
      "Guillaume Lajoie",
      "Mohammad Pezeshki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11462v1",
    "title": "Unifying Deductive and Abductive Reasoning in Knowledge Graphs with\n  Masked Diffusion Model",
    "summary": "Deductive and abductive reasoning are two critical paradigms for analyzing\nknowledge graphs, enabling applications from financial query answering to\nscientific discovery. Deductive reasoning on knowledge graphs usually involves\nretrieving entities that satisfy a complex logical query, while abductive\nreasoning generates plausible logical hypotheses from observations. Despite\ntheir clear synergistic potential, where deduction can validate hypotheses and\nabduction can uncover deeper logical patterns, existing methods address them in\nisolation. To bridge this gap, we propose DARK, a unified framework for\nDeductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion\nmodel capable of capturing the bidirectional relationship between queries and\nconclusions, DARK has two key innovations. First, to better leverage deduction\nfor hypothesis refinement during abductive reasoning, we introduce a\nself-reflective denoising process that iteratively generates and validates\ncandidate hypotheses against the observed conclusion. Second, to discover\nricher logical associations, we propose a logic-exploration reinforcement\nlearning approach that simultaneously masks queries and conclusions, enabling\nthe model to explore novel reasoning compositions. Extensive experiments on\nmultiple benchmark knowledge graphs show that DARK achieves state-of-the-art\nperformance on both deductive and abductive reasoning tasks, demonstrating the\nsignificant benefits of our unified approach.",
    "published": "2025-10-13T14:34:57Z",
    "updated": "2025-10-13T14:34:57Z",
    "link": "http://arxiv.org/pdf/2510.11462v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yisen Gao",
      "Jiaxin Bai",
      "Yi Huang",
      "Xingcheng Fu",
      "Qingyun Sun",
      "Yangqiu Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11457v1",
    "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning\n  Process for LLM Optimization",
    "summary": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.",
    "published": "2025-10-13T14:29:15Z",
    "updated": "2025-10-13T14:29:15Z",
    "link": "http://arxiv.org/pdf/2510.11457v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Beining Wang",
      "Weihang Su",
      "Hongtao Tian",
      "Tao Yang",
      "Yujia Zhou",
      "Ting Yao",
      "Qingyao Ai",
      "Yiqun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16043v2",
    "title": "Beyond Rate Coding: Surrogate Gradients Enable Spike Timing Learning in\n  Spiking Neural Networks",
    "summary": "We investigate the extent to which Spiking Neural Networks (SNNs) trained\nwith Surrogate Gradient Descent (Surrogate GD), with and without delay\nlearning, can learn from precise spike timing beyond firing rates. We first\ndesign synthetic tasks isolating intra-neuron inter-spike intervals and\ncross-neuron synchrony under matched spike counts. On more complex spike-based\nspeech recognition datasets (Spiking Heidelberg Digits (SHD) and Spiking Speech\nCommands (SSC), we construct variants where spike count information is\neliminated and only timing information remains, and show that Surrogate\nGD-trained SNNs are able to perform significantly above chance whereas purely\nrate-based models perform at chance level. We further evaluate robustness under\nbiologically inspired perturbations -- including Gaussian jitter per spike or\nper-neuron, and spike deletion -- revealing consistent but\nperturbation-specific degradation. Networks show a sharp performance drop when\nspike sequences are reversed in time, with a larger drop in performance from\nSNNs trained with delays, indicating that these networks are more human-like in\nterms of behaviour. To facilitate further studies of temporal coding, we have\nreleased our modified SHD and SSC datasets.",
    "published": "2025-07-21T20:19:19Z",
    "updated": "2025-10-13T14:26:15Z",
    "link": "http://arxiv.org/pdf/2507.16043v2.pdf",
    "category": [
      "cs.NE",
      "cs.AI"
    ],
    "authors": [
      "Ziqiao Yu",
      "Pengfei Sun",
      "Dan F. M. Goodman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11454v1",
    "title": "Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented\n  Reasoning",
    "summary": "Recent advancements in large multimodal models (LMMs) have shown strong\ncapabilities in audio understanding. However, most systems rely solely on\nend-to-end reasoning, limiting interpretability and accuracy for tasks that\nrequire structured knowledge or specialized signal analysis. In this work, we\npresent Audio-Maestro -- a tool-augmented audio reasoning framework that\nenables audio-language models to autonomously call external tools and integrate\ntheir timestamped outputs into the reasoning process. This design allows the\nmodel to analyze, transform, and interpret audio signals through specialized\ntools rather than relying solely on end-to-end inference. Experiments show that\nAudio-Maestro consistently improves general audio reasoning performance:\nGemini-2.5-flash's average accuracy on MMAU-Test rises from 67.4% to 72.1%,\nDeSTA-2.5 from 58.3% to 62.8%, and GPT-4o from 60.8% to 63.9%. To our\nknowledge, Audio-Maestro is the first framework to integrate structured tool\noutput into the large audio language model reasoning process.",
    "published": "2025-10-13T14:25:34Z",
    "updated": "2025-10-13T14:25:34Z",
    "link": "http://arxiv.org/pdf/2510.11454v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Kuan-Yi Lee",
      "Tsung-En Lin",
      "Hung-Yi Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11442v1",
    "title": "Reconstructing 12-Lead ECG from 3-Lead ECG using Variational Autoencoder\n  to Improve Cardiac Disease Detection of Wearable ECG Devices",
    "summary": "Twelve-lead electrocardiograms (ECGs) are the clinical gold standard for\ncardiac diagnosis, providing comprehensive spatial coverage of the heart\nnecessary to detect conditions such as myocardial infarction (MI). However,\ntheir lack of portability limits continuous and large-scale use. Three-lead ECG\nsystems are widely used in wearable devices due to their simplicity and\nmobility, but they often fail to capture pathologies in unmeasured regions. To\naddress this, we propose WearECG, a Variational Autoencoder (VAE) method that\nreconstructs twelve-lead ECGs from three leads: II, V1, and V5. Our model\nincludes architectural improvements to better capture temporal and spatial\ndependencies in ECG signals. We evaluate generation quality using MSE, MAE, and\nFrechet Inception Distance (FID), and assess clinical validity via a Turing\ntest with expert cardiologists. To further validate diagnostic utility, we\nfine-tune ECGFounder, a large-scale pretrained ECG model, on a multi-label\nclassification task involving over 40 cardiac conditions, including six\ndifferent myocardial infarction locations, using both real and generated\nsignals. Experiments on the MIMIC dataset show that our method produces\nphysiologically realistic and diagnostically informative signals, with robust\nperformance in downstream tasks. This work demonstrates the potential of\ngenerative modeling for ECG reconstruction and its implications for scalable,\nlow-cost cardiac screening.",
    "published": "2025-10-13T14:14:37Z",
    "updated": "2025-10-13T14:14:37Z",
    "link": "http://arxiv.org/pdf/2510.11442v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "68T05",
      "I.2.6; I.2.7"
    ],
    "authors": [
      "Xinyan Guan",
      "Yongfan Lai",
      "Jiarui Jin",
      "Jun Li",
      "Haoyu Wang",
      "Qinghao Zhao",
      "Deyun Zhang",
      "Shijia Geng",
      "Shenda Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.01450v6",
    "title": "Investigating Memory in RL with POPGym Arcade",
    "summary": "How should we analyze memory in deep RL? We introduce mathematical tools for\nfairly analyzing policies under partial observability and revealing how agents\nuse memory to make decisions. To utilize these tools, we present POPGym Arcade,\na collection of Atari-inspired, hardware-accelerated, pixel-based environments\nsharing a single observation and action space. Each environment provides fully\nand partially observable variants, enabling counterfactual studies on\nobservability. We find that controlled studies are necessary for fair\ncomparisons, and identify a pathology where value functions smear credit over\nirrelevant history. With this pathology, we demonstrate how out-of-distribution\nscenarios can contaminate memory, perturbing the policy far into the future,\nwith implications for sim-to-real transfer and offline RL.",
    "published": "2025-03-03T11:59:03Z",
    "updated": "2025-10-13T13:58:21Z",
    "link": "http://arxiv.org/pdf/2503.01450v6.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Zekang Wang",
      "Zhe He",
      "Borong Zhang",
      "Edan Toledo",
      "Steven Morad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08792v2",
    "title": "Auction Design using Value Prediction with Hallucinations",
    "summary": "We investigate a Bayesian mechanism design problem where a seller seeks to\nmaximize revenue by selling an indivisible good to one of n buyers,\nincorporating potentially unreliable predictions (signals) of buyers' private\nvalues derived from a machine learning model. We propose a framework where\nthese signals are sometimes reflective of buyers' true valuations but other\ntimes are hallucinations, which are uncorrelated with the buyers' true\nvaluations. Our main contribution is a characterization of the optimal auction\nunder this framework. Our characterization establishes a near-decomposition of\nhow to treat types above and below the signal. For the one buyer case, the\nseller's optimal strategy is to post one of three fairly intuitive prices\ndepending on the signal, which we call the \"ignore\", \"follow\" and \"cap\"\nactions.",
    "published": "2025-02-12T21:08:28Z",
    "updated": "2025-10-13T13:49:32Z",
    "link": "http://arxiv.org/pdf/2502.08792v2.pdf",
    "category": [
      "cs.GT",
      "cs.AI"
    ],
    "authors": [
      "Ilan Lobel",
      "Humberto Moreira",
      "Omar Mouchtaki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11407v1",
    "title": "KnowRL: Teaching Language Models to Know What They Know",
    "summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models.",
    "published": "2025-10-13T13:47:14Z",
    "updated": "2025-10-13T13:47:14Z",
    "link": "http://arxiv.org/pdf/2510.11407v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sahil Kale",
      "Devendra Singh Dhami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23468v2",
    "title": "Multi-Modal Manipulation via Multi-Modal Policy Consensus",
    "summary": "Effectively integrating diverse sensory modalities is crucial for robotic\nmanipulation. However, the typical approach of feature concatenation is often\nsuboptimal: dominant modalities such as vision can overwhelm sparse but\ncritical signals like touch in contact-rich tasks, and monolithic architectures\ncannot flexibly incorporate new or missing modalities without retraining. Our\nmethod factorizes the policy into a set of diffusion models, each specialized\nfor a single representation (e.g., vision or touch), and employs a router\nnetwork that learns consensus weights to adaptively combine their\ncontributions, enabling incremental of new representations. We evaluate our\napproach on simulated manipulation tasks in {RLBench}, as well as real-world\ntasks such as occluded object picking, in-hand spoon reorientation, and puzzle\ninsertion, where it significantly outperforms feature-concatenation baselines\non scenarios requiring multimodal reasoning. Our policy further demonstrates\nrobustness to physical perturbations and sensor corruption. We further conduct\nperturbation-based importance analysis, which reveals adaptive shifts between\nmodalities.",
    "published": "2025-09-27T19:43:04Z",
    "updated": "2025-10-13T13:46:32Z",
    "link": "http://arxiv.org/pdf/2509.23468v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Haonan Chen",
      "Jiaming Xu",
      "Hongyu Chen",
      "Kaiwen Hong",
      "Binghao Huang",
      "Chaoqi Liu",
      "Jiayuan Mao",
      "Yunzhu Li",
      "Yilun Du",
      "Katherine Driggs-Campbell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11398v1",
    "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
    "summary": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat.",
    "published": "2025-10-13T13:41:27Z",
    "updated": "2025-10-13T13:41:27Z",
    "link": "http://arxiv.org/pdf/2510.11398v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Sean Oesch",
      "Jack Hutchins",
      "Luke Koch",
      "Kevin Kurian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11391v1",
    "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
    "summary": "Recent advances in agentic workflows have enabled the automation of tasks\nsuch as professional document generation. However, they primarily focus on\ntextual quality, neglecting visual structure and style, which are crucial for\nreadability and engagement. This gap arises mainly from the absence of suitable\nreward models to guide agentic workflows toward producing documents with\nstronger structural and stylistic quality. To address this, we propose\nDocReward, a document reward model that evaluates documents based on their\nstructure and style. We construct a multi-domain dataset DocPair of 117K paired\ndocuments, covering 32 domains and 267 document types, each including a high-\nand low-professionalism document with identical content but different structure\nand style. This enables the model to evaluate professionalism comprehensively,\nand in a textual-quality-agnostic way. DocReward is trained using the\nBradley-Terry loss to score documents, penalizing predictions that contradict\nthe annotated ranking. To assess the performance of reward models, we create a\ntest dataset containing document bundles ranked by well-educated human\nevaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6\nand 19.4 percentage points, respectively, demonstrating its superiority over\nbaselines. In an extrinsic evaluation of document generation, DocReward\nachieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%\nwin rate, demonstrating its utility in guiding generation agents toward\nproducing human-preferred documents.",
    "published": "2025-10-13T13:36:32Z",
    "updated": "2025-10-13T13:36:32Z",
    "link": "http://arxiv.org/pdf/2510.11391v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Junpeng Liu",
      "Yuzhong Zhao",
      "Bowen Cao",
      "Jiayu Ding",
      "Yilin Jia",
      "Tengchao Lv",
      "Yupan Huang",
      "Shaohan Huang",
      "Nan Yang",
      "Li Dong",
      "Lei Cui",
      "Tao Ge",
      "Xun Wang",
      "Huitian Jiao",
      "Sun Mao",
      "FNU Kartik",
      "Si-Qing Chen",
      "Wai Lam",
      "Furu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11390v1",
    "title": "Medical Interpretability and Knowledge Maps of Large Language Models",
    "summary": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied.",
    "published": "2025-10-13T13:34:05Z",
    "updated": "2025-10-13T13:34:05Z",
    "link": "http://arxiv.org/pdf/2510.11390v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Razvan Marinescu",
      "Victoria-Elisabeth Gruber",
      "Diego Fajardo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09459v2",
    "title": "Failure Prediction at Runtime for Generative Robot Policies",
    "summary": "Imitation learning (IL) with generative models, such as diffusion and flow\nmatching, has enabled robots to perform complex, long-horizon tasks. However,\ndistribution shifts from unseen environments or compounding action errors can\nstill cause unpredictable and unsafe behavior, leading to task failure. Early\nfailure prediction during runtime is therefore essential for deploying robots\nin human-centered and safety-critical environments. We propose FIPER, a general\nframework for Failure Prediction at Runtime for generative IL policies that\ndoes not require failure data. FIPER identifies two key indicators of impending\nfailure: (i) out-of-distribution (OOD) observations detected via random network\ndistillation in the policy's embedding space, and (ii) high uncertainty in\ngenerated actions measured by a novel action-chunk entropy score. Both failure\nprediction scores are calibrated using a small set of successful rollouts via\nconformal prediction. A failure alarm is triggered when both indicators,\naggregated over short time windows, exceed their thresholds. We evaluate FIPER\nacross five simulation and real-world environments involving diverse failure\nmodes. Our results demonstrate that FIPER better distinguishes actual failures\nfrom benign OOD situations and predicts failures more accurately and earlier\nthan existing methods. We thus consider this work an important step towards\nmore interpretable and safer generative robot policies. Code, data and videos\nare available at https://tum-lsy.github.io/fiper_website.",
    "published": "2025-10-10T15:09:27Z",
    "updated": "2025-10-13T13:29:31Z",
    "link": "http://arxiv.org/pdf/2510.09459v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2.8; I.2.9; I.2.10"
    ],
    "authors": [
      "Ralf Römer",
      "Adrian Kobras",
      "Luca Worbis",
      "Angela P. Schoellig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11380v1",
    "title": "AI-Driven anemia diagnosis: A review of advanced models and techniques",
    "summary": "Anemia, a condition marked by insufficient levels of red blood cells or\nhemoglobin, remains a widespread health issue affecting millions of individuals\nglobally. Accurate and timely diagnosis is essential for effective management\nand treatment of anemia. In recent years, there has been a growing interest in\nthe use of artificial intelligence techniques, i.e., machine learning (ML) and\ndeep learning (DL) for the detection, classification, and diagnosis of anemia.\nThis paper provides a systematic review of the recent advancements in this\nfield, with a focus on various models applied to anemia detection. The review\nalso compares these models based on several performance metrics, including\naccuracy, sensitivity, specificity, and precision. By analyzing these metrics,\nthe paper evaluates the strengths and limitation of discussed models in\ndetecting and classifying anemia, emphasizing the importance of addressing\nthese factors to improve diagnostic accuracy.",
    "published": "2025-10-13T13:22:45Z",
    "updated": "2025-10-13T13:22:45Z",
    "link": "http://arxiv.org/pdf/2510.11380v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Abdullah Al Mahmud",
      "Prangon Chowdhury",
      "Mohammed Borhan Uddin",
      "Khaled Eabne Delowar",
      "Tausifur Rahman Talha",
      "Bijoy Dewanjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11372v1",
    "title": "Early Detection and Reduction of Memorisation for Domain Adaptation and\n  Instruction Tuning",
    "summary": "Although large language models excel across many tasks, they can memorise\ntraining data and thereby expose private or copyrighted text. Most defences\ntarget the pre-training stage, leaving memorisation during fine-tuning,\nespecially for domain adaptation and instruction tuning, poorly understood. We\nfine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on\ncommon evaluation datasets and track verbatim memorisation throughout training.\nWe find that memorisation increases dramatically in the first few epochs, often\nsignificantly before either validation perplexity or evaluation performance is\noptimised. We use a simple but effective n-gram memorisation score which\nreliably precedes verbatim memorisation; using it as an early-stopping\ncriterion mitigates memorisation with minimal performance loss. Further, we\nintroduce an n-gram-aware loss regulariser and show that it reduces\nmemorisation across all model families tested by up to 40% while minimising\nevaluation performance trade-offs when compared to an existing memorisation\nmitigation strategy. These results yield practical, scalable insights into\nmemorisation dynamics during language model fine-tuning.",
    "published": "2025-10-13T13:12:46Z",
    "updated": "2025-10-13T13:12:46Z",
    "link": "http://arxiv.org/pdf/2510.11372v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Dean L. Slack",
      "Noura Al Moubayed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11370v1",
    "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers",
    "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.",
    "published": "2025-10-13T13:11:27Z",
    "updated": "2025-10-13T13:11:27Z",
    "link": "http://arxiv.org/pdf/2510.11370v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wenhan Ma",
      "Hailin Zhang",
      "Liang Zhao",
      "Yifan Song",
      "Yudong Wang",
      "Zhifang Sui",
      "Fuli Luo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25123v2",
    "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones",
    "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
    "published": "2025-09-29T17:44:27Z",
    "updated": "2025-10-13T13:03:40Z",
    "link": "http://arxiv.org/pdf/2509.25123v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Lifan Yuan",
      "Weize Chen",
      "Yuchen Zhang",
      "Ganqu Cui",
      "Hanbin Wang",
      "Ziming You",
      "Ning Ding",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Hao Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.00934v3",
    "title": "GRAM: Spatial general-purpose audio representation models for real-world\n  applications",
    "summary": "Although audio foundations models have seen great progress on a wide variety\nof tasks, their application in real-world acoustic environments with\nreverberation and noise has been less successful. Moreover, as audio foundation\nmodels are typically trained on dry, single-channel audio clips, the inherent\nspatial nature of real-world sound scenes is overlooked and tasks involving\nsound localization ruled out. To address these limitations, we propose GRAM: a\nGeneral-purpose Real-world Audio Model utilizing a multi-channel masked\nauto-encoder approach to efficiently learn spatial audio representations from\nhigh-quality simulated real-world scenes. To evaluate the performance of GRAM\nand other audio foundation models in real-world sound scenes, we release\nNat-HEAR: A naturalistic version of the HEAR benchmark suite comprising a\nsimulated real-world version, as well as two new sound localization tasks. We\nshow that the performance of GRAM surpasses all state-of-the-art\nself-supervised audio foundation models and speech models on both HEAR and\nNat-HEAR, while using only a fraction of the training data. GRAM also showcases\nstate-of-the-art localization performance, surpassing even supervised sound\nlocalization approaches, and can be flexibly applied either to a two-channel,\nbinaural sound format or a four-channel, Ambisonics format. Validating GRAM's\nperformance on real-world sound recordings demonstrates robust transfer to\nreal-world scenes. Taken together, GRAM presents a significant advancement\ntowards robust, spatial audio foundation models for real-world applications.",
    "published": "2025-06-01T09:56:33Z",
    "updated": "2025-10-13T13:02:08Z",
    "link": "http://arxiv.org/pdf/2506.00934v3.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Goksenin Yuksel",
      "Marcel van Gerven",
      "Kiki van der Heijden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26605v2",
    "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based\n  Reinforcement Learning",
    "summary": "Deploying reinforcement learning (RL) in robotics, industry, and health care\nis blocked by two obstacles: the difficulty of specifying accurate rewards and\nthe risk of unsafe, data-hungry exploration. We address this by proposing a\ntwo-stage framework that first learns a safe initial policy from a reward-free\ndataset of expert demonstrations, then fine-tunes it online using\npreference-based human feedback. We provide the first principled analysis of\nthis offline-to-online approach and introduce BRIDGE, a unified algorithm that\nintegrates both signals via an uncertainty-weighted objective. We derive regret\nbounds that shrink with the number of offline demonstrations, explicitly\nconnecting the quantity of offline data to online sample efficiency. We\nvalidate BRIDGE in discrete and continuous control MuJoCo environments, showing\nit achieves lower regret than both standalone behavioral cloning and online\npreference-based RL. Our work establishes a theoretical foundation for\ndesigning more sample-efficient interactive agents.",
    "published": "2025-09-30T17:50:19Z",
    "updated": "2025-10-13T13:00:55Z",
    "link": "http://arxiv.org/pdf/2509.26605v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Maël Macuglia",
      "Paul Friedrich",
      "Giorgia Ramponi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11358v1",
    "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries.",
    "published": "2025-10-13T12:57:45Z",
    "updated": "2025-10-13T12:57:45Z",
    "link": "http://arxiv.org/pdf/2510.11358v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Jiaming Zhang",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xueqi Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11354v1",
    "title": "Understanding the Generalization of Stochastic Gradient Adam in Learning\n  Neural Networks",
    "summary": "Adam is a popular and widely used adaptive gradient method in deep learning,\nwhich has also received tremendous focus in theoretical research. However, most\nexisting theoretical work primarily analyzes its full-batch version, which\ndiffers fundamentally from the stochastic variant used in practice. Unlike SGD,\nstochastic Adam does not converge to its full-batch counterpart even with\ninfinitesimal learning rates. We present the first theoretical characterization\nof how batch size affects Adam's generalization, analyzing two-layer\nover-parameterized CNNs on image data. Our results reveal that while both Adam\nand AdamW with proper weight decay $\\lambda$ converge to poor test error\nsolutions, their mini-batch variants can achieve near-zero test error. We\nfurther prove Adam has a strictly smaller effective weight decay bound than\nAdamW, theoretically explaining why Adam requires more sensitive $\\lambda$\ntuning. Extensive experiments validate our findings, demonstrating the critical\nrole of batch size and weight decay in Adam's generalization performance.",
    "published": "2025-10-13T12:48:22Z",
    "updated": "2025-10-13T12:48:22Z",
    "link": "http://arxiv.org/pdf/2510.11354v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Xuan Tang",
      "Han Zhang",
      "Yuan Cao",
      "Difan Zou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11347v1",
    "title": "Multi-View Graph Feature Propagation for Privacy Preservation and\n  Feature Sparsity",
    "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features.",
    "published": "2025-10-13T12:42:00Z",
    "updated": "2025-10-13T12:42:00Z",
    "link": "http://arxiv.org/pdf/2510.11347v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Etzion Harari",
      "Moshe Unger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11346v1",
    "title": "Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image\n  Generation",
    "summary": "Generative Models are a valuable tool for the controlled creation of\nhigh-quality image data. Controlled diffusion models like the ControlNet have\nallowed the creation of labeled distributions. Such synthetic datasets can\naugment the original training distribution when discriminative models, like\nsemantic segmentation, are trained. However, this augmentation effect is\nlimited since ControlNets tend to reproduce the original training distribution.\n  This work introduces a method to utilize data from unlabeled domains to train\nControlNets by introducing the concept of uncertainty into the control\nmechanism. The uncertainty indicates that a given image was not part of the\ntraining distribution of a downstream task, e.g., segmentation. Thus, two types\nof control are engaged in the final network: an uncertainty control from an\nunlabeled dataset and a semantic control from the labeled dataset. The\nresulting ControlNet allows us to create annotated data with high uncertainty\nfrom the target domain, i.e., synthetic data from the unlabeled distribution\nwith labels. In our scenario, we consider retinal OCTs, where typically\nhigh-quality Spectralis images are available with given ground truth\nsegmentations, enabling the training of segmentation networks. The recent\ndevelopment in Home-OCT devices, however, yields retinal OCTs with lower\nquality and a large domain shift, such that out-of-the-pocket segmentation\nnetworks cannot be applied for this type of data. Synthesizing annotated images\nfrom the Home-OCT domain using the proposed approach closes this gap and leads\nto significantly improved segmentation results without adding any further\nsupervision. The advantage of uncertainty-guidance becomes obvious when\ncompared to style transfer: it enables arbitrary domain shifts without any\nstrict learning of an image style. This is also demonstrated in a traffic scene\nexperiment.",
    "published": "2025-10-13T12:41:28Z",
    "updated": "2025-10-13T12:41:28Z",
    "link": "http://arxiv.org/pdf/2510.11346v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Joshua Niemeijer",
      "Jan Ehrhardt",
      "Heinz Handels",
      "Hristina Uzunova"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11345v1",
    "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with\n  Asynchrony",
    "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training.",
    "published": "2025-10-13T12:41:27Z",
    "updated": "2025-10-13T12:41:27Z",
    "link": "http://arxiv.org/pdf/2510.11345v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Han Lu",
      "Zichen Liu",
      "Shaopan Xiong",
      "Yancheng He",
      "Wei Gao",
      "Yanan Wu",
      "Weixun Wang",
      "Jiashun Liu",
      "Yang Li",
      "Haizhou Zhao",
      "Ju Huang",
      "Siran Yang",
      "Xiaoyang Li",
      "Yijia Luo",
      "Zihe Liu",
      "Ling Pan",
      "Junchi Yan",
      "Wei Wang",
      "Wenbo Su",
      "Jiamang Wang",
      "Lin Qu",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.21776v2",
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
    "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose WebThinker, a deep research agent that empowers LRMs to\nautonomously search the web, navigate among web pages, and draft reports during\nthe reasoning process. WebThinker integrates a Deep Web Explorer module,\nenabling LRMs to dynamically search, navigate, and extract information from the\nweb when encountering knowledge gaps. It also employs an Autonomous\nThink-Search-and-Draft strategy, allowing the model to seamlessly interleave\nreasoning, information gathering, and report writing in real time. To further\nenhance research tool utilization, we introduce an RL-based training strategy\nvia iterative online Direct Preference Optimization (DPO). Extensive\nexperiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and\nscientific report generation tasks (Glaive) demonstrate that WebThinker\nsignificantly outperforms existing methods and strong proprietary systems. Our\napproach enhances LRM reliability and applicability in complex scenarios,\npaving the way for more capable and versatile deep research systems. The code\nis available at https://github.com/RUC-NLPIR/WebThinker.",
    "published": "2025-04-30T16:25:25Z",
    "updated": "2025-10-13T12:40:15Z",
    "link": "http://arxiv.org/pdf/2504.21776v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Xiaoxi Li",
      "Jiajie Jin",
      "Guanting Dong",
      "Hongjin Qian",
      "Yongkang Wu",
      "Ji-Rong Wen",
      "Yutao Zhu",
      "Zhicheng Dou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11339v1",
    "title": "Event-Aware Prompt Learning for Dynamic Graphs",
    "summary": "Real-world graph typically evolve via a series of events, modeling dynamic\ninteractions between objects across various domains. For dynamic graph\nlearning, dynamic graph neural networks (DGNNs) have emerged as popular\nsolutions. Recently, prompt learning methods have been explored on dynamic\ngraphs. However, existing methods generally focus on capturing the relationship\nbetween nodes and time, while overlooking the impact of historical events. In\nthis paper, we propose EVP, an event-aware dynamic graph prompt learning\nframework that can serve as a plug-in to existing methods, enhancing their\nability to leverage historical events knowledge. First, we extract a series of\nhistorical events for each node and introduce an event adaptation mechanism to\nalign the fine-grained characteristics of these events with downstream tasks.\nSecond, we propose an event aggregation mechanism to effectively integrate\nhistorical knowledge into node representations. Finally, we conduct extensive\nexperiments on four public datasets to evaluate and analyze EVP.",
    "published": "2025-10-13T12:37:53Z",
    "updated": "2025-10-13T12:37:53Z",
    "link": "http://arxiv.org/pdf/2510.11339v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xingtong Yu",
      "Ruijuan Liang",
      "Xinming Zhang",
      "Yuan Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11330v1",
    "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the\n  Audio-Text Modality Gap",
    "summary": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link",
    "published": "2025-10-13T12:25:33Z",
    "updated": "2025-10-13T12:25:33Z",
    "link": "http://arxiv.org/pdf/2510.11330v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "KiHyun Nam",
      "Jongmin Choi",
      "Hyeongkeun Lee",
      "Jungwoo Heo",
      "Joon Son Chung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11328v1",
    "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
    "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.",
    "published": "2025-10-13T12:24:24Z",
    "updated": "2025-10-13T12:24:24Z",
    "link": "http://arxiv.org/pdf/2510.11328v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chenxi Wang",
      "Yixuan Zhang",
      "Ruiji Yu",
      "Yufei Zheng",
      "Lang Gao",
      "Zirui Song",
      "Zixiang Xu",
      "Gus Xia",
      "Huishuai Zhang",
      "Dongyan Zhao",
      "Xiuying Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05195v2",
    "title": "Train-before-Test Harmonizes Language Model Rankings",
    "summary": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. In this paper, we take a different\nperspective on model comparison: instead of relying on out-of-the-box\nperformance via direct evaluation, we compare model potential by providing each\nmodel with identical benchmark-specific fine-tuning before evaluation. We call\nthis approach train-before-test. Our primary contribution is a comprehensive\nempirical evaluation of model potential across 24 benchmarks and 61 models.\nFirst, we demonstrate that model potential rankings obtained through\ntrain-before-test exhibit remarkable consistency across all benchmarks. Whereas\ntraditional rankings demonstrate little external validity under direct\nevaluation, they enjoy a significant degree of external validity when applying\ntrain-before-test: model potential rankings transfer gracefully from one\nbenchmark to another. Second, train-before-test restores the connection between\nperplexity and downstream task performance, lost under direct evaluation.\nRemarkably, even pre-finetuning perplexity of a base model predicts\npost-finetuning downstream performance, suggesting that ranking consistency\nreflects inherent model potential rather than fine-tuning artifacts. Finally,\ntrain-before-test reduces the model-score matrix to essentially rank one,\nindicating that model potential is dominated by one latent factor, uncovered by\ntrain-before-test. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.",
    "published": "2025-07-07T16:54:18Z",
    "updated": "2025-10-13T12:21:52Z",
    "link": "http://arxiv.org/pdf/2507.05195v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Guanhua Zhang",
      "Ricardo Dominguez-Olmedo",
      "Moritz Hardt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.18995v2",
    "title": "SWIFT: Semantic Watermarking for Image Forgery Thwarting",
    "summary": "This paper proposes a novel approach towards image authentication and\ntampering detection by using watermarking as a communication channel for\nsemantic information. We modify the HiDDeN deep-learning watermarking\narchitecture to embed and extract high-dimensional real vectors representing\nimage captions. Our method improves significantly robustness on both malign and\nbenign edits. We also introduce a local confidence metric correlated with\nMessage Recovery Rate, enhancing the method's practical applicability. This\napproach bridges the gap between traditional watermarking and passive forensic\nmethods, offering a robust solution for image integrity verification.",
    "published": "2024-07-26T09:50:13Z",
    "updated": "2025-10-13T12:17:33Z",
    "link": "http://arxiv.org/pdf/2407.18995v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Gautier Evennou",
      "Vivien Chappelier",
      "Ewa Kijak",
      "Teddy Furon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01271v2",
    "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model\n  Unlearning",
    "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.",
    "published": "2025-07-02T01:13:08Z",
    "updated": "2025-10-13T12:16:25Z",
    "link": "http://arxiv.org/pdf/2507.01271v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Tatsuki Kawakami",
      "Kazuki Egashira",
      "Atsuyuki Miyai",
      "Go Irie",
      "Kiyoharu Aizawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06443v2",
    "title": "Superior Molecular Representations from Intermediate Encoder Layers",
    "summary": "Pretrained molecular encoders have become indispensable in computational\nchemistry for tasks such as property prediction and molecular generation.\nHowever, the standard practice of relying solely on final-layer embeddings for\ndownstream tasks may discard valuable information. In this work, we first\nanalyze the information flow in five diverse molecular encoders and find that\nintermediate layers retain more general-purpose features, whereas the\nfinal-layer specializes and compresses information. We then perform an\nempirical layer-wise evaluation across 22 property prediction tasks. We find\nthat using frozen embeddings from optimal intermediate layers improves\ndownstream performance by an average of 5.4%, up to 28.6%, compared to the\nfinal-layer. Furthermore, finetuning encoders truncated at intermediate depths\nachieves even greater average improvements of 8.5%, with increases as high as\n40.8%, obtaining new state-of-the-art results on several benchmarks. These\nfindings highlight the importance of exploring the full representational depth\nof molecular encoders to achieve substantial performance improvements and\ncomputational efficiency. The code will be made publicly available.",
    "published": "2025-06-06T18:03:51Z",
    "updated": "2025-10-13T12:11:30Z",
    "link": "http://arxiv.org/pdf/2506.06443v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "authors": [
      "Luis Pinto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11313v1",
    "title": "Automated Skill Decomposition Meets Expert Ontologies: Bridging the\n  Granularity Gap with LLMs",
    "summary": "This paper investigates automated skill decomposition using Large Language\nModels (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.\nOur framework standardizes the pipeline from prompting and generation to\nnormalization and alignment with ontology nodes. To evaluate outputs, we\nintroduce two metrics: a semantic F1-score that uses optimal embedding-based\nmatching to assess content accuracy, and a hierarchy-aware F1-score that\ncredits structurally correct placements to assess granularity. We conduct\nexperiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing\ntwo prompting strategies: zero-shot and leakage-safe few-shot with exemplars.\nAcross diverse LLMs, zero-shot offers a strong baseline, while few-shot\nconsistently stabilizes phrasing and granularity and improves hierarchy-aware\nalignment. A latency analysis further shows that exemplar-guided prompts are\ncompetitive - and sometimes faster - than unguided zero-shot due to more\nschema-compliant completions. Together, the framework, benchmark, and metrics\nprovide a reproducible foundation for developing ontology-faithful skill\ndecomposition systems.",
    "published": "2025-10-13T12:03:06Z",
    "updated": "2025-10-13T12:03:06Z",
    "link": "http://arxiv.org/pdf/2510.11313v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Le Ngoc Luyen",
      "Marie-Hélène Abel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08649v2",
    "title": "Formalizing Style in Personal Narratives",
    "summary": "Personal narratives are stories authors construct to make meaning of their\nexperiences. Style, the distinctive way authors use language to express\nthemselves, is fundamental to how these narratives convey subjective\nexperiences. Yet there is a lack of a formal framework for systematically\nanalyzing these stylistic choices. We present a novel approach that formalizes\nstyle in personal narratives as patterns in the linguistic choices authors make\nwhen communicating subjective experiences. Our framework integrates three\ndomains: functional linguistics establishes language as a system of meaningful\nchoices, computer science provides methods for automatically extracting and\nanalyzing sequential patterns, and these patterns are linked to psychological\nobservations. Using language models, we automatically extract linguistic\nfeatures such as processes, participants, and circumstances. We apply our\nframework to hundreds of dream narratives, including a case study on a war\nveteran with post-traumatic stress disorder. Analysis of his narratives\nuncovers distinctive patterns, particularly how verbal processes dominate over\nmental ones, illustrating the relationship between linguistic choices and\npsychological states.",
    "published": "2025-10-09T06:48:06Z",
    "updated": "2025-10-13T11:58:35Z",
    "link": "http://arxiv.org/pdf/2510.08649v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Gustave Cortal",
      "Alain Finkel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11307v1",
    "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient\n  Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks",
    "summary": "Current approaches to embodied AI tend to learn policies from expert\ndemonstrations. However, without a mechanism to evaluate the quality of\ndemonstrated actions, they are limited to learning from optimal behaviour, or\nthey risk replicating errors and inefficiencies. While reinforcement learning\noffers one alternative, the associated exploration typically results in\nsacrificing data efficiency. This work explores how agents trained with\nimitation learning can learn robust representations from both optimal and\nsuboptimal demonstrations when given access to constructive language feedback\nas a means to contextualise different modes of behaviour. We directly provide\nlanguage feedback embeddings as part of the input sequence into a\nTransformer-based policy, and optionally complement the traditional next action\nprediction objective with auxiliary self-supervised learning objectives for\nfeedback prediction. We test our approach on a range of embodied\nVision-and-Language tasks in our custom BabyAI-XGen environment and show\nsignificant improvements in agents' compositional generalisation abilities and\nrobustness, suggesting that our data-efficient method allows models to\nsuccessfully convert suboptimal behaviour into learning opportunities. Overall,\nour results suggest that language feedback is a competitive and intuitive\nalternative to intermediate scalar rewards for language-specified embodied\ntasks.",
    "published": "2025-10-13T11:55:21Z",
    "updated": "2025-10-13T11:55:21Z",
    "link": "http://arxiv.org/pdf/2510.11307v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Sabrina McCallum",
      "Amit Parekh",
      "Alessandro Suglia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11302v1",
    "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object\n  Detection in the Era of Vision-Language Models",
    "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.",
    "published": "2025-10-13T11:48:48Z",
    "updated": "2025-10-13T11:48:48Z",
    "link": "http://arxiv.org/pdf/2510.11302v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Samer Al-Hamadani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11300v1",
    "title": "Beyond touch-based HMI: Control your machines in natural language by\n  utilizing large language models and OPC UA",
    "summary": "This paper proposes an agent-based approach toward a more natural interface\nbetween humans and machines. Large language models equipped with tools and the\ncommunication standard OPC UA are utilized to control machines in natural\nlanguage. Instead of touch interaction, which is currently the state-of-the-art\nmedium for interaction in operations, the proposed approach enables operators\nto talk or text with machines. This allows commands such as 'Please decrease\nthe temperature by 20 % in machine 1 and set the motor speed to 5000 rpm in\nmachine 2.' The large language model receives the user input and selects one of\nthree predefined tools that connect to an OPC UA server and either change or\nread the value of a node. Afterwards, the result of the tool execution is\npassed back to the language model, which then provides a final response to the\nuser. The approach is universally designed and can therefore be applied to any\nmachine that supports the OPC UA standard. The large language model is neither\nfine-tuned nor requires training data, only the relevant machine credentials\nand a parameter dictionary are included within the system prompt. The approach\nis evaluated on a Siemens S7-1500 programmable logic controller with four\nmachine parameters in a case study of fifty synthetically generated commands on\nfive different models. The results demonstrate high success rate, with\nproprietary GPT 5 models achieving accuracies between 96.0 % and 98.0 %, and\nopen-weight models reaching up to 90.0 %. The proposed approach of this\nempirical study contributes to advancing natural interaction in industrial\nhuman-machine interfaces.",
    "published": "2025-10-13T11:46:47Z",
    "updated": "2025-10-13T11:46:47Z",
    "link": "http://arxiv.org/pdf/2510.11300v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Bernd Hofmann",
      "Sven Kreitlein",
      "Joerg Franke",
      "Patrick Bruendl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00743v3",
    "title": "Agentic large language models improve retrieval-based radiology question\n  answering",
    "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose radiology Retrieval\nand Reasoning (RaR), a multi-step retrieval and reasoning framework designed to\nimprove diagnostic accuracy, factual consistency, and clinical reliability of\nLLMs in radiology question answering. We evaluated 25 LLMs spanning diverse\narchitectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. To assess generalizability, we additionally tested on an\nunseen internal dataset of 65 real-world radiology board examination questions.\nRaR significantly improved mean diagnostic accuracy over zero-shot prompting\nand conventional online RAG. The greatest gains occurred in small-scale models,\nwhile very large models (>200B parameters) demonstrated minimal changes (<2%\nimprovement). Additionally, RaR retrieval reduced hallucinations (mean 9.4%)\nand retrieved clinically relevant context in 46% of cases, substantially aiding\nfactual grounding. Even clinically fine-tuned models showed gains from RaR\n(e.g., MedGemma-27B), indicating that retrieval remains beneficial despite\nembedded domain knowledge. These results highlight the potential of RaR to\nenhance factuality and diagnostic accuracy in radiology QA, warranting future\nstudies to validate their clinical utility. All datasets, code, and the full\nRaR framework are publicly available to support open research and clinical\ntranslation.",
    "published": "2025-08-01T16:18:52Z",
    "updated": "2025-10-13T11:42:11Z",
    "link": "http://arxiv.org/pdf/2508.00743v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sebastian Wind",
      "Jeta Sopa",
      "Daniel Truhn",
      "Mahshad Lotfinia",
      "Tri-Thien Nguyen",
      "Keno Bressem",
      "Lisa Adams",
      "Mirabela Rusu",
      "Harald Köstler",
      "Gerhard Wellein",
      "Andreas Maier",
      "Soroosh Tayebi Arasteh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11292v1",
    "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
    "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
    "published": "2025-10-13T11:28:30Z",
    "updated": "2025-10-13T11:28:30Z",
    "link": "http://arxiv.org/pdf/2510.11292v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wenbo Wu",
      "Qingyi Si",
      "Xiurui Pan",
      "Ye Wang",
      "Jie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11713v1",
    "title": "Are Large Reasoning Models Interruptible?",
    "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.",
    "published": "2025-10-13T17:59:35Z",
    "updated": "2025-10-13T17:59:35Z",
    "link": "http://arxiv.org/pdf/2510.11713v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Tsung-Han Wu",
      "Mihran Miroyan",
      "David M. Chan",
      "Trevor Darrell",
      "Narges Norouzi",
      "Joseph E. Gonzalez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11701v1",
    "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
    "published": "2025-10-13T17:57:15Z",
    "updated": "2025-10-13T17:57:15Z",
    "link": "http://arxiv.org/pdf/2510.11701v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhaochen Yu",
      "Ling Yang",
      "Jiaru Zou",
      "Shuicheng Yan",
      "Mengdi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11696v1",
    "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
    "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
    "published": "2025-10-13T17:55:09Z",
    "updated": "2025-10-13T17:55:09Z",
    "link": "http://arxiv.org/pdf/2510.11696v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Wei Huang",
      "Yi Ge",
      "Shuai Yang",
      "Yicheng Xiao",
      "Huizi Mao",
      "Yujun Lin",
      "Hanrong Ye",
      "Sifei Liu",
      "Ka Chun Cheung",
      "Hongxu Yin",
      "Yao Lu",
      "Xiaojuan Qi",
      "Song Han",
      "Yukang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11695v1",
    "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
    "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.",
    "published": "2025-10-13T17:54:09Z",
    "updated": "2025-10-13T17:54:09Z",
    "link": "http://arxiv.org/pdf/2510.11695v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lingfei Qian",
      "Xueqing Peng",
      "Yan Wang",
      "Vincent Jim Zhang",
      "Huan He",
      "Hanley Smith",
      "Yi Han",
      "Yueru He",
      "Haohang Li",
      "Yupeng Cao",
      "Yangyang Yu",
      "Alejandro Lopez-Lira",
      "Peng Lu",
      "Jian-Yun Nie",
      "Guojun Xiong",
      "Jimin Huang",
      "Sophia Ananiadou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11652v1",
    "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
    "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
    "published": "2025-10-13T17:30:36Z",
    "updated": "2025-10-13T17:30:36Z",
    "link": "http://arxiv.org/pdf/2510.11652v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xin Gui",
      "King Zhu",
      "JinCheng Ren",
      "Qianben Chen",
      "Zekun Moore Wang",
      "Yizhi LI",
      "Xinpeng Liu",
      "Xiaowan Li",
      "Wenli Ren",
      "Linyu Miao",
      "Tianrui Qin",
      "Ziqi Shu",
      "He Zhu",
      "Xiangru Tang",
      "Dingfeng Shi",
      "Jiaheng Liu",
      "Yuchen Eleanor Jiang",
      "Minghao Liu",
      "Ge Zhang",
      "Wangchunshu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11620v1",
    "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan\n  Aggregation",
    "summary": "Inference-time scaling enhances the reasoning ability of a language model\n(LM) by extending its chain-of-thought (CoT). However, existing approaches\ntypically generate the entire reasoning chain in a single forward pass, which\noften leads to CoT derailment, i.e., the reasoning trajectory drifting off\ncourse due to compounding errors. This problem is particularly severe for\nsmaller LMs with long CoTs due to their limited capacity. To address this, we\nanalyze raw long CoTs and uncover a reasoning hierarchy consisting of planning\nand execution steps. Our analysis reveals that most reasoning errors stem from\nincorrect planning. Motivated by this observation, we propose Multi-Path Plan\nAggregation (MPPA), a framework that augments single-pass reasoning with plan\nexploration and aggregation. Following a variable interval schedule based on\nthe token position, MPPA generates multiple candidate plans and aggregates them\ninto a refined planning step. To maintain efficiency, we adopt a minimal design\nin which the base LM serves as the primary policy, while a lightweight LoRA\nmodule implements the plan aggregation policy. We further observe that\noutcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K\ntokens). To overcome this, we introduce online Step-DPO, a process-level\npreference optimization scheme that leverages Twisted Sequential Monte Carlo\n(TSMC) to provide scalable stepwise supervision using small LMs. This yields\nmore efficient training, improved stability, and higher accuracy. Extensive\nexperiments on challenging math, science, and logical reasoning benchmarks\ndemonstrate that, with only 10% SFT data and 5% of preference pairs, our method\noutperforms both the DeepSeek-R1 distillation baseline and the outcome-reward\nRL baseline across multiple base models and tasks.",
    "published": "2025-10-13T17:02:41Z",
    "updated": "2025-10-13T17:02:41Z",
    "link": "http://arxiv.org/pdf/2510.11620v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Siheng Xiong",
      "Ali Payani",
      "Faramarz Fekri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11618v1",
    "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up\n  Long-Form Story Generation Using Large Language Models",
    "summary": "Human writers often begin their stories with an overarching mental scene,\nwhere they envision the interactions between characters and their environment.\nInspired by this creative process, we propose a novel approach to long-form\nstory generation, termed hybrid bottom-up long-form story generation, using\nmulti-agent simulations. In our method, agents interact within a dynamic\nsandbox environment, where their behaviors and interactions with one another\nand the environment generate emergent events. These events form the foundation\nfor the story, enabling organic character development and plot progression.\nUnlike traditional top-down approaches that impose rigid structures, our hybrid\nbottom-up approach allows for the natural unfolding of events, fostering more\nspontaneous and engaging storytelling. The system is capable of generating\nstories exceeding 10,000 words while maintaining coherence and consistency,\naddressing some of the key challenges faced by current story generation models.\nWe achieve state-of-the-art performance across several metrics. This approach\noffers a scalable and innovative solution for creating dynamic, immersive\nlong-form stories that evolve organically from agent-driven interactions.",
    "published": "2025-10-13T16:57:32Z",
    "updated": "2025-10-13T16:57:32Z",
    "link": "http://arxiv.org/pdf/2510.11618v1.pdf",
    "category": [
      "cs.CL",
      "cs.MA"
    ],
    "authors": [
      "Zehao Chen",
      "Rong Pan",
      "Haoran Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11602v1",
    "title": "Deconstructing Attention: Investigating Design Principles for Effective\n  Language Modeling",
    "summary": "The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.",
    "published": "2025-10-13T16:42:14Z",
    "updated": "2025-10-13T16:42:14Z",
    "link": "http://arxiv.org/pdf/2510.11602v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Huiyin Xue",
      "Nafise Sadat Moosavi",
      "Nikolaos Aletras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11598v1",
    "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language\n  Models",
    "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data.",
    "published": "2025-10-13T16:37:40Z",
    "updated": "2025-10-13T16:37:40Z",
    "link": "http://arxiv.org/pdf/2510.11598v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Bo Cheng",
      "Xu Wang",
      "Jinda Liu",
      "Yi Chang",
      "Yuan Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11592v1",
    "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural\n  Re-Ranking",
    "summary": "Current neural re-rankers often struggle with complex information needs and\nlong, content-rich documents. The fundamental issue is not computational--it is\nintelligent content selection: identifying what matters in lengthy,\nmulti-faceted texts. While humans naturally anchor their understanding around\nkey entities and concepts, neural models process text within rigid token\nwindows, treating all interactions as equally important and missing critical\nsemantic signals. We introduce REGENT, a neural re-ranking model that mimics\nhuman-like understanding by using entities as a \"semantic skeleton\" to guide\nattention. REGENT integrates relevance guidance directly into the attention\nmechanism, combining fine-grained lexical matching with high-level semantic\nreasoning. This relevance-guided attention enables the model to focus on\nconceptually important content while maintaining sensitivity to precise term\nmatches. REGENT achieves new state-of-the-art performance in three challenging\ndatasets, providing up to 108% improvement over BM25 and consistently\noutperforming strong baselines including ColBERT and RankVicuna. To our\nknowledge, this is the first work to successfully integrate entity semantics\ndirectly into neural attention, establishing a new paradigm for entity-aware\ninformation retrieval.",
    "published": "2025-10-13T16:31:42Z",
    "updated": "2025-10-13T16:31:42Z",
    "link": "http://arxiv.org/pdf/2510.11592v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Shubham Chatterjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11589v1",
    "title": "QDER: Query-Specific Document and Entity Representations for\n  Multi-Vector Document Re-Ranking",
    "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches\nleveraging knowledge graphs and multi-vector models capturing fine-grained\nsemantics. We introduce QDER, a neural re-ranking model that unifies these\napproaches by integrating knowledge graph semantics into a multi-vector model.\nQDER's key innovation lies in its modeling of query-document relationships:\nrather than computing similarity scores on aggregated embeddings, we maintain\nindividual token and entity representations throughout the ranking process,\nperforming aggregation only at the final scoring stage - an approach we call\n\"late aggregation.\" We first transform these fine-grained representations\nthrough learned attention patterns, then apply carefully chosen mathematical\noperations for precise matches. Experiments across five standard benchmarks\nshow that QDER achieves significant performance gains, with improvements of 36%\nin nDCG@20 over the strongest baseline on TREC Robust 2004 and similar\nimprovements on other datasets. QDER particularly excels on difficult queries,\nachieving an nDCG@20 of 0.70 where traditional approaches fail completely\n(nDCG@20 = 0.0), setting a foundation for future work in entity-aware\nretrieval.",
    "published": "2025-10-13T16:31:06Z",
    "updated": "2025-10-13T16:31:06Z",
    "link": "http://arxiv.org/pdf/2510.11589v1.pdf",
    "category": [
      "cs.IR",
      "cs.CL"
    ],
    "authors": [
      "Shubham Chatterjee",
      "Jeff Dalton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11586v1",
    "title": "Survey Response Generation: Generating Closed-Ended Survey Responses\n  In-Silico with Large Language Models",
    "summary": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods.",
    "published": "2025-10-13T16:29:19Z",
    "updated": "2025-10-13T16:29:19Z",
    "link": "http://arxiv.org/pdf/2510.11586v1.pdf",
    "category": [
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Georg Ahnert",
      "Anna-Carolina Haensch",
      "Barbara Plank",
      "Markus Strohmaier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11584v1",
    "title": "LLMAtKGE: Large Language Models as Explainable Attackers against\n  Knowledge Graph Embeddings",
    "summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations.",
    "published": "2025-10-13T16:29:17Z",
    "updated": "2025-10-13T16:29:17Z",
    "link": "http://arxiv.org/pdf/2510.11584v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ting Li",
      "Yang Yang",
      "Yipeng Yu",
      "Liang Yao",
      "Guoqing Chao",
      "Ruifeng Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11570v1",
    "title": "Bag of Tricks for Subverting Reasoning-based Safety Guardrails",
    "summary": "Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),\nsuch as deliberative alignment, have shown strong defense against jailbreak\nattacks. By leveraging LRMs' reasoning ability, these guardrails help the\nmodels to assess the safety of user inputs before generating final responses.\nThe powerful reasoning ability can analyze the intention of the input query and\nwill refuse to assist once it detects the harmful intent hidden by the\njailbreak methods. Such guardrails have shown a significant boost in defense,\nsuch as the near-perfect refusal rates on the open-source gpt-oss series.\nUnfortunately, we find that these powerful reasoning-based guardrails can be\nextremely vulnerable to subtle manipulation of the input prompts, and once\nhijacked, can lead to even more harmful results. Specifically, we first uncover\na surprisingly fragile aspect of these guardrails: simply adding a few template\ntokens to the input prompt can successfully bypass the seemingly powerful\nguardrails and lead to explicit and harmful responses. To explore further, we\nintroduce a bag of jailbreak methods that subvert the reasoning-based\nguardrails. Our attacks span white-, gray-, and black-box settings and range\nfrom effortless template manipulations to fully automated optimization. Along\nwith the potential for scalable implementation, these methods also achieve\nalarmingly high attack success rates (e.g., exceeding 90% across 5 different\nbenchmarks on gpt-oss series on both local host models and online API\nservices). Evaluations across various leading open-source LRMs confirm that\nthese vulnerabilities are systemic, underscoring the urgent need for stronger\nalignment techniques for open-sourced LRMs to prevent malicious misuse. Code is\nopen-sourced at https://chenxshuo.github.io/bag-of-tricks.",
    "published": "2025-10-13T16:16:44Z",
    "updated": "2025-10-13T16:16:44Z",
    "link": "http://arxiv.org/pdf/2510.11570v1.pdf",
    "category": [
      "cs.CR",
      "cs.CL"
    ],
    "authors": [
      "Shuo Chen",
      "Zhen Han",
      "Haokun Chen",
      "Bailan He",
      "Shengyun Si",
      "Jingpei Wu",
      "Philip Torr",
      "Volker Tresp",
      "Jindong Gu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.13142v2",
    "title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence",
    "summary": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We first propose a holistic taxonomy of spatial\ntasks that unifies existing benchmarks and a standardized protocol for the fair\nevaluation of state-of-the-art proprietary and open-source models across eight\nkey benchmarks, at a cost exceeding ten billion total tokens. Our empirical\nstudy then reveals that (1) GPT-5 demonstrates unprecedented strength in\nspatial intelligence (SI), yet (2) still falls short of human performance\nsignificantly across a broad spectrum of SI-tasks. Moreover, we (3) show that\nSI-tasks expose greater model capability deficiency than non-SI tasks, to the\nextent that (4) proprietary models do not exhibit a decisive advantage when\nfacing the most difficult ones. In addition, we conduct a qualitative\nevaluation across a diverse set of scenarios that are intuitive for humans, yet\nfail even the most advanced multimodal models.",
    "published": "2025-08-18T17:55:17Z",
    "updated": "2025-10-13T16:08:38Z",
    "link": "http://arxiv.org/pdf/2508.13142v2.pdf",
    "category": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "authors": [
      "Zhongang Cai",
      "Yubo Wang",
      "Qingping Sun",
      "Ruisi Wang",
      "Chenyang Gu",
      "Wanqi Yin",
      "Zhiqian Lin",
      "Zhitao Yang",
      "Chen Wei",
      "Xuanke Shi",
      "Kewang Deng",
      "Xiaoyang Han",
      "Zukai Chen",
      "Jiaqi Li",
      "Xiangyu Fan",
      "Hanming Deng",
      "Lewei Lu",
      "Bo Li",
      "Ziwei Liu",
      "Quan Wang",
      "Dahua Lin",
      "Lei Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11563v1",
    "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs",
    "summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting.",
    "published": "2025-10-13T16:06:14Z",
    "updated": "2025-10-13T16:06:14Z",
    "link": "http://arxiv.org/pdf/2510.11563v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shreya Havaldar",
      "Sunny Rai",
      "Young-Min Cho",
      "Lyle Ungar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11557v1",
    "title": "Invisible Languages of the LLM Universe",
    "summary": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits.",
    "published": "2025-10-13T16:00:15Z",
    "updated": "2025-10-13T16:00:15Z",
    "link": "http://arxiv.org/pdf/2510.11557v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Saurabh Khanna",
      "Xinxu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11545v1",
    "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
    "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
    "published": "2025-10-13T15:42:11Z",
    "updated": "2025-10-13T15:42:11Z",
    "link": "http://arxiv.org/pdf/2510.11545v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jiayu Ding",
      "Lei Cui",
      "Li Dong",
      "Nanning Zheng",
      "Furu Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11537v1",
    "title": "An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese\n  Token-Level Classification",
    "summary": "We propose a novel neural architecture named TextGraphFuseGAT, which\nintegrates a pretrained transformer encoder (PhoBERT) with Graph Attention\nNetworks for token-level classification tasks. The proposed model constructs a\nfully connected graph over the token embeddings produced by PhoBERT, enabling\nthe GAT layer to capture rich inter-token dependencies beyond those modeled by\nsequential context alone. To further enhance contextualization, a\nTransformer-style self-attention layer is applied on top of the graph-enhanced\nembeddings. The final token representations are passed through a classification\nhead to perform sequence labeling. We evaluate our approach on three Vietnamese\nbenchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19\ndomain, PhoDisfluency for speech disfluency detection, and VietMed-NER for\nmedical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER\ndataset, featuring 18 entity types collected from real-world medical speech\ntranscripts and annotated with the BIO tagging scheme. Its specialized\nvocabulary and domain-specific expressions make it a challenging benchmark for\ntoken-level classification models. Experimental results show that our method\nconsistently outperforms strong baselines, including transformer-only and\nhybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness\nof combining pretrained semantic features with graph-based relational modeling\nfor improved token classification across multiple domains.",
    "published": "2025-10-13T15:39:09Z",
    "updated": "2025-10-13T15:39:09Z",
    "link": "http://arxiv.org/pdf/2510.11537v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ba-Quang Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11529v1",
    "title": "Hallucination Detection via Internal States and Structured Reasoning\n  Consistency in Large Language Models",
    "summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet.",
    "published": "2025-10-13T15:31:21Z",
    "updated": "2025-10-13T15:31:21Z",
    "link": "http://arxiv.org/pdf/2510.11529v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yusheng Song",
      "Lirong Qiu",
      "Xi Zhang",
      "Zhihao Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11498v1",
    "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
    "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
    "published": "2025-10-13T15:05:50Z",
    "updated": "2025-10-13T15:05:50Z",
    "link": "http://arxiv.org/pdf/2510.11498v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Yuhang Li",
      "Chenchen Zhang",
      "Ruilin Lv",
      "Ao Liu",
      "Ken Deng",
      "Yuanxing Zhang",
      "Jiaheng Liu",
      "Wiggin Zhou",
      "Bo Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04641v2",
    "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A\n  Comprehensive Benchmark Study",
    "summary": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks.",
    "published": "2025-10-06T09:45:32Z",
    "updated": "2025-10-13T15:04:15Z",
    "link": "http://arxiv.org/pdf/2510.04641v2.pdf",
    "category": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Ayan Majumdar",
      "Feihao Chen",
      "Jinghui Li",
      "Xiaozhen Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.02502v3",
    "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
    "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
    "published": "2025-03-04T11:10:13Z",
    "updated": "2025-10-13T14:55:40Z",
    "link": "http://arxiv.org/pdf/2503.02502v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jianghao Chen",
      "Junhong Wu",
      "Yangyifan Xu",
      "Jiajun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.05605v4",
    "title": "Evolving LLMs' Self-Refinement Capability via Synergistic\n  Training-Inference Optimization",
    "summary": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH.",
    "published": "2025-02-08T15:21:55Z",
    "updated": "2025-10-13T14:24:10Z",
    "link": "http://arxiv.org/pdf/2502.05605v4.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Yongcheng Zeng",
      "Xinyu Cui",
      "Xuanfa Jin",
      "Qirui Mi",
      "Guoqing Liu",
      "Zexu Sun",
      "Mengyue Yang",
      "Dong Li",
      "Weiyu Ma",
      "Ning Yang",
      "Jian Zhao",
      "Jianye Hao",
      "Haifeng Zhang",
      "Jun Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24563v2",
    "title": "NeMo: Needle in a Montage for Video-Language Understanding",
    "summary": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench.",
    "published": "2025-09-29T10:16:05Z",
    "updated": "2025-10-13T14:23:19Z",
    "link": "http://arxiv.org/pdf/2509.24563v2.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Zi-Yuan Hu",
      "Shuo Liang",
      "Duo Zheng",
      "Yanyang Li",
      "Yeyao Tao",
      "Shijia Huang",
      "Wei Feng",
      "Jia Qin",
      "Jianguang Yu",
      "Jing Huang",
      "Meng Fang",
      "Yin Li",
      "Liwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11444v1",
    "title": "GenCNER: A Generative Framework for Continual Named Entity Recognition",
    "summary": "Traditional named entity recognition (NER) aims to identify text mentions\ninto pre-defined entity types. Continual Named Entity Recognition (CNER) is\nintroduced since entity categories are continuously increasing in various\nreal-world scenarios. However, existing continual learning (CL) methods for NER\nface challenges of catastrophic forgetting and semantic shift of non-entity\ntype. In this paper, we propose GenCNER, a simple but effective Generative\nframework for CNER to mitigate the above drawbacks. Specifically, we skillfully\nconvert the CNER task into sustained entity triplet sequence generation problem\nand utilize a powerful pre-trained seq2seq model to solve it. Additionally, we\ndesign a type-specific confidence-based pseudo labeling strategy along with\nknowledge distillation (KD) to preserve learned knowledge and alleviate the\nimpact of label noise at the triplet level. Experimental results on two\nbenchmark datasets show that our framework outperforms previous\nstate-of-the-art methods in multiple CNER settings, and achieves the smallest\ngap compared with non-CL results.",
    "published": "2025-10-13T14:15:31Z",
    "updated": "2025-10-13T14:15:31Z",
    "link": "http://arxiv.org/pdf/2510.11444v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yawen Yang",
      "Fukun Ma",
      "Shiao Meng",
      "Aiwei Liu",
      "Lijie Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11434v1",
    "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated\n  Content",
    "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.",
    "published": "2025-10-13T14:06:17Z",
    "updated": "2025-10-13T14:06:17Z",
    "link": "http://arxiv.org/pdf/2510.11434v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Dana Sotto Porat",
      "Ella Rabinovich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18208v2",
    "title": "On the Interplay between Musical Preferences and Personality through the\n  Lens of Language",
    "summary": "Music serves as a powerful reflection of individual identity, often aligning\nwith deeper psychological traits. Prior research has established correlations\nbetween musical preferences and personality, while separate studies have\ndemonstrated that personality is detectable through linguistic analysis. Our\nstudy bridges these two research domains by investigating whether individuals'\nmusical preferences leave traces in their spontaneous language through the lens\nof the Big Five personality traits (Openness, Conscientiousness, Extroversion,\nAgreeableness, and Neuroticism). Using a carefully curated dataset of over\n500,000 text samples from nearly 5,000 authors with reliably identified musical\npreferences, we build advanced models to assess personality characteristics.\nOur results reveal significant personality differences across fans of five\nmusical genres. We release resources for future research at the intersection of\ncomputational linguistics, music psychology and personality analysis.",
    "published": "2025-08-25T17:10:08Z",
    "updated": "2025-10-13T14:01:33Z",
    "link": "http://arxiv.org/pdf/2508.18208v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Eliran Shem-Tov",
      "Ella Rabinovich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11423v1",
    "title": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health\n  Misinformation",
    "summary": "Community Notes, the crowd-sourced misinformation governance system on X\n(formerly Twitter), enables users to flag misleading posts, attach contextual\nnotes, and vote on their helpfulness. However, our analysis of 30.8K\nhealth-related notes reveals significant latency, with a median delay of 17.6\nhours before the first note receives a helpfulness status. To improve\nresponsiveness during real-world misinformation surges, we propose CrowdNotes+,\na unified framework that leverages large language models (LLMs) to augment\nCommunity Notes for faster and more reliable health misinformation governance.\nCrowdNotes+ integrates two complementary modes: (1) evidence-grounded note\naugmentation and (2) utility-guided note automation, along with a hierarchical\nthree-step evaluation that progressively assesses relevance, correctness, and\nhelpfulness. We instantiate the framework through HealthNotes, a benchmark of\n1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness\njudge. Experiments on fifteen LLMs reveal an overlooked loophole in current\nhelpfulness evaluation, where stylistic fluency is mistaken for factual\naccuracy, and demonstrate that our hierarchical evaluation and LLM-augmented\ngeneration jointly enhance factual precision and evidence utility. These\nresults point toward a hybrid human-AI governance model that improves both the\nrigor and timeliness of crowd-sourced fact-checking.",
    "published": "2025-10-13T13:57:23Z",
    "updated": "2025-10-13T13:57:23Z",
    "link": "http://arxiv.org/pdf/2510.11423v1.pdf",
    "category": [
      "cs.SI",
      "cs.CL"
    ],
    "authors": [
      "Jiaying Wu",
      "Zihang Fu",
      "Haonan Wang",
      "Fanxiao Li",
      "Min-Yen Kan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11408v1",
    "title": "Valid Survey Simulations with Limited Human Data: The Roles of\n  Prompting, Fine-Tuning, and Rectification",
    "summary": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation.",
    "published": "2025-10-13T13:48:07Z",
    "updated": "2025-10-13T13:48:07Z",
    "link": "http://arxiv.org/pdf/2510.11408v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Stefan Krsteski",
      "Giuseppe Russo",
      "Serina Chang",
      "Robert West",
      "Kristina Gligorić"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.11919v2",
    "title": "Effectiveness of Counter-Speech against Abusive Content: A\n  Multidimensional Annotation and Classification Study",
    "summary": "Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in linguistics, communication and argumentation\nconcepts. Our framework defines six core dimensions - Clarity, Evidence,\nEmotional Appeal, Rebuttal, Audience Adaptation, and Fairness - which we use to\nannotate 4,214 CS instances from two benchmark datasets, resulting in a novel\nlinguistic resource released to the community. In addition, we propose two\nclassification strategies, multi-task and dependency-based, achieving strong\nresults (0.94 and 0.96 average F1 respectively on both expert- and user-written\nCS), outperforming standard baselines, and revealing strong interdependence\namong dimensions.",
    "published": "2025-06-13T16:11:04Z",
    "updated": "2025-10-13T13:41:38Z",
    "link": "http://arxiv.org/pdf/2506.11919v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Greta Damo",
      "Elena Cabrio",
      "Serena Villata"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11389v1",
    "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with\n  Human-Aligned Strategies",
    "summary": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction.",
    "published": "2025-10-13T13:33:30Z",
    "updated": "2025-10-13T13:33:30Z",
    "link": "http://arxiv.org/pdf/2510.11389v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zirui Song",
      "Yuan Huang",
      "Junchang Liu",
      "Haozhe Luo",
      "Chenxi Wang",
      "Lang Gao",
      "Zixiang Xu",
      "Mingfei Han",
      "Xiaojun Chang",
      "Xiuying Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11314v1",
    "title": "Template-Based Text-to-Image Alignment for Language Accessibility: A\n  Study on Visualizing Text Simplifications",
    "summary": "Individuals with intellectual disabilities often have difficulties in\ncomprehending complex texts. While many text-to-image models prioritize\naesthetics over accessibility, it is not clear how visual illustrations relate\nto text simplifications (TS) generated from them. This paper presents a\nstructured vision-language model (VLM) prompting framework for generating\naccessible images from simplified texts. We designed five prompt templates,\ni.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level\nDetail, and Grid Layout, each following distinct spatial arrangements while\nadhering to accessibility constraints such as object count limits, spatial\nseparation, and content restrictions. Using 400 sentence-level simplifications\nfrom four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and\nASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template\neffectiveness with CLIPScores, and Phase 2 involved human annotation of\ngenerated images across ten visual styles by four accessibility experts.\nResults show that the Basic Object Focus prompt template achieved the highest\nsemantic alignment, indicating that visual minimalism enhances language\naccessibility. Expert evaluation further identified Retro style as the most\naccessible and Wikipedia as the most effective data source. Inter-annotator\nagreement varied across dimensions, with Text Simplicity showing strong\nreliability and Image Quality proving more subjective. Overall, our framework\noffers practical guidelines for accessible content generation and underscores\nthe importance of structured prompting in AI-generated visual accessibility\ntools.",
    "published": "2025-10-13T12:03:36Z",
    "updated": "2025-10-13T12:03:36Z",
    "link": "http://arxiv.org/pdf/2510.11314v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Belkiss Souayed",
      "Sarah Ebling",
      "Yingqiang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15524v2",
    "title": "The Enemy from Within: A Study of Political Delegitimization Discourse\n  in Israeli Political Speech",
    "summary": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse.",
    "published": "2025-08-21T12:57:04Z",
    "updated": "2025-10-13T11:56:31Z",
    "link": "http://arxiv.org/pdf/2508.15524v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Naama Rivlin-Angert",
      "Guy Mor-Lan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11297v1",
    "title": "Are Large Language Models Effective Knowledge Graph Constructors?",
    "summary": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown\npromise in reducing hallucinations in large language models (LLMs). However,\nconstructing high-quality KGs remains difficult, requiring accurate information\nextraction and structured representations that support interpretability and\ndownstream utility. Existing LLM-based approaches often focus narrowly on\nentity and relation extraction, limiting coverage to sentence-level contexts or\nrelying on predefined schemas. We propose a hierarchical extraction framework\nthat organizes information at multiple levels, enabling the creation of\nsemantically rich and well-structured KGs. Using state-of-the-art LLMs, we\nextract and construct knowledge graphs and evaluate them comprehensively from\nboth structural and semantic perspectives. Our results highlight the strengths\nand shortcomings of current LLMs in KG construction and identify key challenges\nfor future work. To advance research in this area, we also release a curated\ndataset of LLM-generated KGs derived from research papers on children's mental\nwell-being. This resource aims to foster more transparent, reliable, and\nimpactful applications in high-stakes domains such as healthcare.",
    "published": "2025-10-13T11:37:48Z",
    "updated": "2025-10-13T11:37:48Z",
    "link": "http://arxiv.org/pdf/2510.11297v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ruirui Chen",
      "Weifeng Jiang",
      "Chengwei Qin",
      "Bo Xiong",
      "Fiona Liausvia",
      "Dongkyu Choi",
      "Boon Kiat Quek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11288v1",
    "title": "Emergent Misalignment via In-Context Learning: Narrow in-context\n  examples can produce broadly misaligned LLMs",
    "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM.",
    "published": "2025-10-13T11:23:56Z",
    "updated": "2025-10-13T11:23:56Z",
    "link": "http://arxiv.org/pdf/2510.11288v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nikita Afonin",
      "Nikita Andriyanov",
      "Nikhil Bageshpura",
      "Kyle Liu",
      "Kevin Zhu",
      "Sunishchal Dev",
      "Ashwinee Panda",
      "Alexander Panchenko",
      "Oleg Rogov",
      "Elena Tutubalina",
      "Mikhail Seleznyov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11278v1",
    "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models",
    "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability",
    "published": "2025-10-13T11:13:09Z",
    "updated": "2025-10-13T11:13:09Z",
    "link": "http://arxiv.org/pdf/2510.11278v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Gareth Seneque",
      "Lap-Hang Ho",
      "Nafise Erfanian Saeedi",
      "Jeffrey Molendijk",
      "Ariel Kupermann",
      "Tim Elson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11277v1",
    "title": "Towards Real-Time Fake News Detection under Evidence Scarcity",
    "summary": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE.",
    "published": "2025-10-13T11:11:46Z",
    "updated": "2025-10-13T11:11:46Z",
    "link": "http://arxiv.org/pdf/2510.11277v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Guangyu Wei",
      "Ke Han",
      "Yueming Lyu",
      "Yu Luo",
      "Yue Jiang",
      "Caifeng Shan",
      "Nicu Sebe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10320v3",
    "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
    "summary": "The progress of AI is bottlenecked by the quality of evaluation, making\npowerful LLM-as-a-Judge models a core solution. The efficacy of these judges\ndepends on their chain-of-thought reasoning, creating a critical need for\nmethods that can effectively optimize this reasoning process. In this work, we\nintroduce J1, a reinforcement learning framework for teaching LLM judges to\nthink before making decisions. Our core contribution lies in converting all\njudgment tasks for non-verifiable and verifiable prompts into a unified format\nwith verifiable rewards, enabling direct optimization of evaluation quality\nwhile mitigating positional bias. We then use RL to train thinking-judges at\nscales of 8B, 32B, and 70B and show that they obtain state-of-the-art\nperformance across multiple benchmarks. In particular, J1-Qwen-32B, our\nmultitasked pointwise and pairwise judge also outperforms o1-mini, o3, and a\nmuch larger 671B DeepSeek-R1 on some benchmarks, while only training on\nsynthetic data. Through comprehensive ablations of pairwise, pointwise, and\nmultitask J1 variants, we demonstrate the effectiveness of our approach across\nseed prompts, reward strategies, and training recipes. Qualitative analysis\nreveals that J1 develops systematic evaluation strategies, including dynamic\ncriteria generation, reference answer creation, iterative self-correction of\ninitial assessments, and feedback generation for low-quality responses.",
    "published": "2025-05-15T14:05:15Z",
    "updated": "2025-10-13T11:06:51Z",
    "link": "http://arxiv.org/pdf/2505.10320v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chenxi Whitehouse",
      "Tianlu Wang",
      "Ping Yu",
      "Xian Li",
      "Jason Weston",
      "Ilia Kulikov",
      "Swarnadeep Saha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.13959v3",
    "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent",
    "summary": "Drug discovery is a long, expensive, and complex process, relying heavily on\nhuman medicinal chemists, who can spend years searching the vast space of\npotential therapies. Recent advances in artificial intelligence for chemistry\nhave sought to expedite individual drug discovery tasks; however, there remains\na critical need for an intelligent agent that can navigate the drug discovery\nprocess. Towards this end, we introduce LIDDIA, an autonomous agent capable of\nintelligently navigating the drug discovery process in silico. By leveraging\nthe reasoning capabilities of large language models, LIDDIA serves as a\nlow-cost and highly-adaptable tool for autonomous drug discovery. We\ncomprehensively examine LIDDIA , demonstrating that (1) it can generate\nmolecules meeting key pharmaceutical criteria on over 70% of 30 clinically\nrelevant targets, (2) it intelligently balances exploration and exploitation in\nthe chemical space, and (3) it identifies one promising novel candidate on\nAR/NR3C4, a critical target for both prostate and breast cancers. Code and\ndataset are available at https://github.com/ninglab/LIDDiA",
    "published": "2025-02-19T18:56:12Z",
    "updated": "2025-10-13T10:56:21Z",
    "link": "http://arxiv.org/pdf/2502.13959v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Reza Averly",
      "Frazier N. Baker",
      "Ian A. Watson",
      "Xia Ning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00544v3",
    "title": "When Thinking Backfires: Mechanistic Insights Into Reasoning-Induced\n  Misalignment",
    "summary": "With the growing accessibility and wide adoption of large language models,\nconcerns about their safety and alignment with human values have become\nparamount. In this paper, we identify a concerning phenomenon:\nReasoning-Induced Misalignment (RIM), in which misalignment emerges when\nreasoning capabilities strengthened-particularly when specific types of\nreasoning patterns are introduced during inference or training. Beyond\nreporting this vulnerability, we provide the first mechanistic account of its\norigins. Through representation analysis, we discover that specific attention\nheads facilitate refusal by reducing their attention to CoT tokens, a mechanism\nthat modulates the model's rationalization process during inference. During\ntraining, we find significantly higher activation entanglement between\nreasoning and safety in safety-critical neurons than in control neurons,\nparticularly after fine-tuning with those identified reasoning patterns. This\nentanglement strongly correlates with catastrophic forgetting, providing a\nneuron-level explanation for RIM.",
    "published": "2025-08-30T16:04:54Z",
    "updated": "2025-10-13T10:53:43Z",
    "link": "http://arxiv.org/pdf/2509.00544v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hanqi Yan",
      "Hainiu Xu",
      "Siya Qi",
      "Shu Yang",
      "Yulan He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11254v1",
    "title": "Do Psychometric Tests Work for Large Language Models? Evaluation of\n  Tests on Sexism, Racism, and Morality",
    "summary": "Psychometric tests are increasingly used to assess psychological constructs\nin large language models (LLMs). However, it remains unclear whether these\ntests -- originally developed for humans -- yield meaningful results when\napplied to LLMs. In this study, we systematically evaluate the reliability and\nvalidity of human psychometric tests for three constructs: sexism, racism, and\nmorality. We find moderate reliability across multiple item and prompt\nvariations. Validity is evaluated through both convergent (i.e., testing\ntheory-based inter-test correlations) and ecological approaches (i.e., testing\nthe alignment between tests scores and behavior in real-world downstream\ntasks). Crucially, we find that psychometric test scores do not align, and in\nsome cases even negatively correlate with, model behavior in downstream tasks,\nindicating low ecological validity. Our results highlight that systematic\nevaluations of psychometric tests is essential before interpreting their\nscores. They also suggest that psychometric tests designed for humans cannot be\napplied directly to LLMs without adaptation.",
    "published": "2025-10-13T10:43:49Z",
    "updated": "2025-10-13T10:43:49Z",
    "link": "http://arxiv.org/pdf/2510.11254v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jana Jung",
      "Marlene Lutz",
      "Indira Sen",
      "Markus Strohmaier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08666v2",
    "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
    "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
    "published": "2025-10-09T16:19:42Z",
    "updated": "2025-10-13T10:39:59Z",
    "link": "http://arxiv.org/pdf/2510.08666v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuxin Ma",
      "Lun Du",
      "Lanning Wei",
      "Kun Chen",
      "Qian Xu",
      "Kangyu Wang",
      "Guofeng Feng",
      "Guoshan Lu",
      "Lin Liu",
      "Xiaojing Qi",
      "Xinyuan Zhang",
      "Zhen Tao",
      "Haibo Feng",
      "Ziyun Jiang",
      "Ying Xu",
      "Zenan Huang",
      "Yihong Zhuang",
      "Haokai Xu",
      "Jiaqi Hu",
      "Zhenzhong Lan",
      "Junbo Zhao",
      "Jianguo Li",
      "Da Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.19981v3",
    "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided\n  GFlowNets",
    "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4\\% absolute on SAT MATH). Furthermore,\nwe benchmark our PRM against existing open-source reward models, demonstrating\nsuperior alignment with reasoning quality and more consistent guidance for\ndownstream generation. Our work demonstrates the potential of PRM-guided,\nstep-level GFlowNets for developing more robust and versatile mathematical\nreasoning in LLMs.",
    "published": "2025-04-28T16:56:41Z",
    "updated": "2025-10-13T10:39:21Z",
    "link": "http://arxiv.org/pdf/2504.19981v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Adam Younsi",
      "Ahmed Attia",
      "Abdalgader Abubaker",
      "Mohamed El Amine Seddik",
      "Hakim Hacid",
      "Salem Lahlou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11238v1",
    "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
    "summary": "When AI agents retrieve and reason over external documents, adversaries can\nmanipulate the data they receive to subvert their behaviour. Previous research\nhas studied indirect prompt injection, where the attacker injects malicious\ninstructions. We argue that injection of instructions is not necessary to\nmanipulate agents - attackers could instead supply biased, misleading, or false\ninformation. We term this an attack by content. Existing defenses, which focus\non detecting hidden commands, are ineffective against attacks by content. To\ndefend themselves and their users, agents must critically evaluate retrieved\ninformation, corroborating claims with external evidence and evaluating source\ntrustworthiness. We argue that this is analogous to an existing NLP task,\nautomated fact-checking, which we propose to repurpose as a cognitive\nself-defense tool for agents.",
    "published": "2025-10-13T10:18:48Z",
    "updated": "2025-10-13T10:18:48Z",
    "link": "http://arxiv.org/pdf/2510.11238v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Michael Schlichtkrull"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11236v1",
    "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
    "published": "2025-10-13T10:17:21Z",
    "updated": "2025-10-13T10:17:21Z",
    "link": "http://arxiv.org/pdf/2510.11236v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Haoqi Yang",
      "Yao Yao",
      "Zuchao Li",
      "Baoyuan Qi",
      "Guoming Liu",
      "Hai Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11233v1",
    "title": "CNSocialDepress: A Chinese Social Media Dataset for Depression Risk\n  Detection and Structured Analysis",
    "summary": "Depression is a pressing global public health issue, yet publicly available\nChinese-language resources for risk detection remain scarce and are mostly\nlimited to binary classification. To address this limitation, we release\nCNSocialDepress, a benchmark dataset for depression risk detection from Chinese\nsocial media posts. The dataset contains 44,178 texts from 233 users, within\nwhich psychological experts annotated 10,306 depression-related segments.\nCNSocialDepress provides binary risk labels together with structured\nmulti-dimensional psychological attributes, enabling interpretable and\nfine-grained analysis of depressive signals. Experimental results demonstrate\nits utility across a wide range of NLP tasks, including structured\npsychological profiling and fine-tuning of large language models for depression\ndetection. Comprehensive evaluations highlight the dataset's effectiveness and\npractical value for depression risk identification and psychological analysis,\nthereby providing insights to mental health applications tailored for\nChinese-speaking populations.",
    "published": "2025-10-13T10:14:18Z",
    "updated": "2025-10-13T10:14:18Z",
    "link": "http://arxiv.org/pdf/2510.11233v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jinyuan Xu",
      "Tian Lan",
      "Xintao Yu",
      "Xue He",
      "Hezhi Zhang",
      "Ying Wang",
      "Pierre Magistry",
      "Mathieu Valette",
      "Lei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11225v1",
    "title": "A Theorem-Proving-Based Evaluation of Neural Semantic Parsing",
    "summary": "Graph-matching metrics such as Smatch are the de facto standard for\nevaluating neural semantic parsers, yet they capture surface overlap rather\nthan logical equivalence. We reassess evaluation by pairing graph-matching with\nautomated theorem proving. We compare two approaches to building parsers:\nsupervised fine-tuning (T5-Small/Base) and few-shot in-context learning\n(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs\nusing graph-matching, bidirectional entailment between source and target\nformulas with a first-order logic theorem prover, and well-formedness. Across\nsettings, we find that models performing well on graph-matching often fail to\nproduce logically equivalent formulas. Normalization reduces incidental target\nvariability, improves well-formedness, and strengthens logical adequacy. Error\nanalysis shows performance degrades with increasing formula complexity and with\ncoordination, prepositional phrases, and passive voice; the dominant failures\ninvolve variable binding and indexing, and predicate naming. These findings\nhighlight limits of graph-based metrics for reasoning-oriented applications and\nmotivate logic-sensitive evaluation and training objectives together with\nsimplified, normalized target representations. All code and data for our\nexperiments are publicly available.",
    "published": "2025-10-13T10:09:38Z",
    "updated": "2025-10-13T10:09:38Z",
    "link": "http://arxiv.org/pdf/2510.11225v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hayate Funakura",
      "Hyunsoo Kim",
      "Koji Mineshima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11222v1",
    "title": "Fairness Metric Design Exploration in Multi-Domain Moral Sentiment\n  Classification using Transformer-Based Models",
    "summary": "Ensuring fairness in natural language processing for moral sentiment\nclassification is challenging, particularly under cross-domain shifts where\ntransformer models are increasingly deployed. Using the Moral Foundations\nTwitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work\nevaluates BERT and DistilBERT in a multi-label setting with in-domain and\ncross-domain protocols. Aggregate performance can mask disparities: we observe\npronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by\n14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness\nviolations hidden by overall scores; notably, the authority label exhibits\nDemographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of\n0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency\n(MFC) metric, which quantifies the cross-domain stability of moral foundation\ndetection. MFC shows strong empirical validity, achieving a perfect negative\ncorrelation with Demographic Parity Difference (rho = -1.000, p < 0.001) while\nremaining independent of standard performance metrics. Across labels, loyalty\ndemonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC\n= 0.78). These findings establish MFC as a complementary, diagnosis-oriented\nmetric for fairness-aware evaluation of moral reasoning models, enabling more\nreliable deployment across heterogeneous linguistic contexts. .",
    "published": "2025-10-13T10:05:57Z",
    "updated": "2025-10-13T10:05:57Z",
    "link": "http://arxiv.org/pdf/2510.11222v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Battemuulen Naranbat",
      "Seyed Sahand Mohammadi Ziabari",
      "Yousuf Nasser Al Husaini",
      "Ali Mohammed Mansoor Alsahag"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11221v1",
    "title": "WebRouter: Query-specific Router via Variational Information Bottleneck\n  for Cost-sensitive Web Agent",
    "summary": "LLM-brained web agents offer powerful capabilities for web automation but\nface a critical cost-performance trade-off. The challenge is amplified by web\nagents' inherently complex prompts that include goals, action histories, and\nenvironmental states, leading to degraded LLM ensemble performance. To address\nthis, we introduce WebRouter, a novel query-specific router trained from an\ninformation-theoretic perspective. Our core contribution is a cost-aware\nVariational Information Bottleneck (ca-VIB) objective, which learns a\ncompressed representation of the input prompt while explicitly penalizing the\nexpected operational cost. Experiments on five real-world websites from the\nWebVoyager benchmark show that WebRouter reduces operational costs by a\nstriking 87.8\\% compared to a GPT-4o baseline, while incurring only a 3.8\\%\naccuracy drop.",
    "published": "2025-10-13T10:05:43Z",
    "updated": "2025-10-13T10:05:43Z",
    "link": "http://arxiv.org/pdf/2510.11221v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tao Li",
      "Jinlong Hu",
      "Yang Wang",
      "Junfeng Liu",
      "Xuejun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11218v1",
    "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and\n  Long-Form Answers",
    "summary": "Large language models (LLMs) can correctly answer \"When was Einstein born?\"\nyet fail to provide the same date when writing about Einstein's life revealing\na fundamental inconsistency in how models access factual knowledge across task\ncomplexities. While models display impressive accuracy on factual\nquestion-answering benchmarks, the reliability gap between simple and complex\nqueries remains poorly understood, eroding their trustworthiness. In this work,\nwe introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a\ncontrolled evaluation framework that compares LLMs' answers to the same factual\nquestions asked (a) in isolation (short) vs. (b) integrated into complex\nqueries (long). Looking at 16 LLMs across 600 queries, we find a systematic\nmisalignment of answers to the corresponding short and long queries. We further\nuncover position-dependent accuracy loss and momentum effects where consecutive\ncorrect or incorrect answers create self-reinforcing patterns. Through\nmechanistic analysis, we find that aligned facts activate overlapping model\ninternals, and that metrics based on mechanistic similarity can predict\nshort-long answer alignment with up to 78% accuracy. Our work establishes\nfactual consistency over query complexity as an important aspect of LLMs'\ntrustworthiness and challenges current evaluation practices, which implicitly\nassume that good performance for simple factual queries implies reliability in\nmore complex knowledge-seeking tasks too.",
    "published": "2025-10-13T10:00:58Z",
    "updated": "2025-10-13T10:00:58Z",
    "link": "http://arxiv.org/pdf/2510.11218v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Saad Obaid ul Islam",
      "Anne Lauscher",
      "Goran Glavaš"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11217v1",
    "title": "Domain-Specific Data Generation Framework for RAG Adaptation",
    "summary": "Retrieval-Augmented Generation (RAG) combines the language understanding and\nreasoning power of large language models (LLMs) with external retrieval to\nenable domain-grounded responses. Effectively adapting RAG systems to\ndomain-specific settings requires specialized, context-rich training data\nbeyond general-purpose question-answering. Here, we propose RAGen, a scalable\nand modular framework for generating domain-grounded question-answer-context\n(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces\nthese QAC triples by identifying key concepts in documents, generating diverse\nquestions guided by Bloom's Taxonomy-inspired principles, and pairing them with\nprecise answers extracted from relevant contexts. RAGen supports multiple RAG\nadaptation strategies, including the optimization of key components such as the\nLLM, retriever, and embedding model, etc. Its modular pipeline features\nsemantic chunking, hierarchical concept extraction, and multi-chunk retrieval,\nalong with the introduction of curated distractor contexts to promote robust\nreasoning. Designed for scalability, RAGen efficiently handles large and\nevolving document corpora without redundant processing, making it especially\nsuitable for dynamic evolving domains such as scientific research and\nenterprise knowledge bases.",
    "published": "2025-10-13T09:59:49Z",
    "updated": "2025-10-13T09:59:49Z",
    "link": "http://arxiv.org/pdf/2510.11217v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chris Xing Tian",
      "Weihao Xie",
      "Zhen Chen",
      "Zhengyuan Yi",
      "Hui Liu",
      "Haoliang Li",
      "Shiqi Wang",
      "Siwei Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11210v1",
    "title": "Discursive Circuits: How Do Language Models Understand Discourse\n  Relations?",
    "summary": "Which components in transformer language models are responsible for discourse\nunderstanding? We hypothesize that sparse computational graphs, termed as\ndiscursive circuits, control how models process discourse relations. Unlike\nsimpler tasks, discourse relations involve longer spans and complex reasoning.\nTo make circuit discovery feasible, we introduce a task called Completion under\nDiscourse Relation (CuDR), where a model completes a discourse given a\nspecified relation. To support this task, we construct a corpus of minimal\ncontrastive pairs tailored for activation patching in circuit discovery.\nExperiments show that sparse circuits ($\\approx 0.2\\%$ of a full GPT-2 model)\nrecover discourse understanding in the English PDTB-based CuDR task. These\ncircuits generalize well to unseen discourse frameworks such as RST and SDRT.\nFurther analysis shows lower layers capture linguistic features such as lexical\nsemantics and coreference, while upper layers encode discourse-level\nabstractions. Feature utility is consistent across frameworks (e.g.,\ncoreference supports Expansion-like relations).",
    "published": "2025-10-13T09:45:49Z",
    "updated": "2025-10-13T09:45:49Z",
    "link": "http://arxiv.org/pdf/2510.11210v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Yisong Miao",
      "Min-Yen Kan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05423v4",
    "title": "LiTransProQA: an LLM-based Literary Translation evaluation metric with\n  Professional Question Answering",
    "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics for literature prioritize\nmechanical accuracy over artistic expression and tend to overrate machine\ntranslation as being superior to human translation from experienced\nprofessionals. In the long run, this bias could result in an irreversible\ndecline in translation quality and cultural authenticity. In response to the\nurgent need for a specialized literary evaluation metric, we introduce\nLITRANSPROQA, a novel, reference-free, LLM-based question-answering framework\ndesigned for literary translation evaluation. LITRANSPROQA integrates humans in\nthe loop to incorporate insights from professional literary translators and\nresearchers, focusing on critical elements in literary quality assessment such\nas literary devices, cultural understanding, and authorial voice. Our extensive\nevaluation shows that while literary-finetuned XCOMET-XL yields marginal gains,\nLITRANSPROQA substantially outperforms current metrics, achieving up to 0.07\ngain in correlation and surpassing the best state-of-the-art metrics by over 15\npoints in adequacy assessments. Incorporating professional translator insights\nas weights further improves performance, highlighting the value of translator\ninputs. Notably, LITRANSPROQA reaches an adequacy performance comparable to\ntrained linguistic student evaluators, though it still falls behind experienced\nprofessional translators. LITRANSPROQA shows broad applicability to open-source\nmodels like LLaMA3.3-70b and Qwen2.5-32b, indicating its potential as an\naccessible and training-free tool for evaluating literary translations that\nrequire local processing due to copyright or ethical considerations.",
    "published": "2025-05-08T17:12:56Z",
    "updated": "2025-10-13T09:31:45Z",
    "link": "http://arxiv.org/pdf/2505.05423v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Ran Zhang",
      "Wei Zhao",
      "Lieve Macken",
      "Steffen Eger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11196v1",
    "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models\n  using Multimodal Perturbations",
    "summary": "Vision-language models (VLMs) often produce chain-of-thought (CoT)\nexplanations that sound plausible yet fail to reflect the underlying decision\nprocess, undermining trust in high-stakes clinical use. Existing evaluations\nrarely catch this misalignment, prioritizing answer accuracy or adherence to\nformats. We present a clinically grounded framework for chest X-ray visual\nquestion answering (VQA) that probes CoT faithfulness via controlled text and\nimage modifications across three axes: clinical fidelity, causal attribution,\nand confidence calibration. In a reader study (n=4), evaluator-radiologist\ncorrelations fall within the observed inter-radiologist range for all axes,\nwith strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate\nalignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone\n($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows\nthat answer accuracy and explanation quality are decoupled, acknowledging\ninjected cues does not ensure grounding, and text cues shift explanations more\nthan visual cues. While some open-source models match final answer accuracy,\nproprietary models score higher on attribution (25.0% vs. 1.4%) and often on\nfidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to\nevaluate beyond final answer accuracy.",
    "published": "2025-10-13T09:28:22Z",
    "updated": "2025-10-13T09:28:22Z",
    "link": "http://arxiv.org/pdf/2510.11196v1.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Johannes Moll",
      "Markus Graf",
      "Tristan Lemke",
      "Nicolas Lenhart",
      "Daniel Truhn",
      "Jean-Benoit Delbrouck",
      "Jiazhen Pan",
      "Daniel Rueckert",
      "Lisa C. Adams",
      "Keno K. Bressem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11184v1",
    "title": "Can Tool-Integrated Reinforcement Learning Generalize Across Diverse\n  Domains?",
    "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in reasoning and tool utilization. However, the generalization of\ntool-augmented reinforcement learning (RL) across diverse domains remains\nunderexplored. In this work, we investigate the cross-domain generalization of\nan LLM agent equipped with a code interpreter tool, which is exclusively\ntrained on mathematical problem-solving tasks. Despite the restricted training\ndomain, we evaluate the agent's performance across several distinct reasoning\ndomains. The results reveal that RL-based tool usage learned from mathematical\ntasks can be effectively transferred to complex tasks in other domains,\nenabling great task performance and high token efficiency. To facilitate this\ncross-domain transfer, we propose a Tool Generalization Reinforcement Learning\n(TGRL) framework designed to promote domain-agnostic learning and skill\nmigration, encompassing: (i) a standardized tool interface that abstracts\ndomain-specific nuances through consistent formatting and explicit termination,\nfostering transferable invocation patterns; (ii) a dual-component reward system\nthat decomposes rewards to incentivize generalizable behaviors like tool\nefficiency and reasoning abstraction, ensuring alignment and robustness across\ndomain shifts; and (iii) an XML-based prompt template that separates thinking,\ntool calls, and responses to encourage modular, domain-invariant planning and\ncoherent multi-turn interactions. Extensive experiments across diverse\nbenchmarks validate our approach, achieving state-of-the-art performance and\nhighlighting the cross-domain potential of Tool RL for LLM reasoning.",
    "published": "2025-10-13T09:19:13Z",
    "updated": "2025-10-13T09:19:13Z",
    "link": "http://arxiv.org/pdf/2510.11184v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Zhengyu Chen",
      "Jinluan Yang",
      "Teng Xiao",
      "Ruochen Zhou",
      "Luan Zhang",
      "Xiangyu Xi",
      "Xiaowei Shi",
      "Wei Wang",
      "Jinggang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11170v1",
    "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling",
    "summary": "With the rise of reasoning language models and test-time scaling methods as a\nparadigm for improving model performance, substantial computation is often\nrequired to generate multiple candidate sequences from the same prompt. This\nenables exploration of different reasoning paths toward the correct solution,\nhowever, allocates the same compute budget for each prompt. Grounded on the\nassumption that different prompts carry different degrees of complexity, and\nthus different computation needs, we propose EAGer, a training-free generation\nmethod that leverages model uncertainty through token-wise entropy distribution\nto reduce redundant computation and concurrently improve overall performance.\nEAGer allows branching to multiple reasoning paths only in the presence of\nhigh-entropy tokens, and then reallocates the saved compute budget to the\ninstances where exploration of alternative paths is most needed. We find that\nacross multiple open-source models on complex reasoning benchmarks such as AIME\n2025, EAGer can reallocate the budget without accessing target labels,\nachieving the best efficiency-performance trade-off in terms of reasoning\nlength and Pass@k. When target labels are accessible, EAGer generates up to 65%\nfewer tokens (hence saving compute) and achieves up to 37% improvement in\nPass@k compared to the Full Parallel Sampling.",
    "published": "2025-10-13T09:04:28Z",
    "updated": "2025-10-13T09:04:28Z",
    "link": "http://arxiv.org/pdf/2510.11170v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Daniel Scalena",
      "Leonidas Zotos",
      "Elisabetta Fersini",
      "Malvina Nissim",
      "Ahmet Üstün"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11168v1",
    "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large\n  Output Spaces",
    "summary": "Large output spaces, also referred to as Extreme multilabel classification\n(XMC), is a setting that arises, e.g., in large-scale tagging and\nproduct-to-product recommendation, and is characterized by the number of labels\nranging from hundreds of thousands to millions. This means that the linear\nclassification head, usually only a tiny fraction of the overall model, turns\ninto the main driver for compute and memory demand. Current state-of-the-art\nXMC methods predominantly rely on FP16-FP32 mixed-precision training, which we\nshow can be unstable, and inefficient in terms of memory usage and\ncomputational overhead. Meanwhile, existing low-precision methods typically\nretain higher precision for the classification layer. In this work, we propose\nELMO, a pure low-precision training framework for XMC models using BFloat16 and\nFloat8 data types. By leveraging Kahan summation and stochastic rounding, we\ndemonstrate that XMC models can be effectively trained entirely in Float8,\nwithout relying on single-precision master weights or tensor scaling.\nLow-precision training, combined with our proposed memory optimizations --\ngradient fusion and chunking -- enables significant reductions in GPU memory\nusage. For example, we train a 3-million-label XMC model with only 6.6 GiB of\nGPU memory, compared to the 39.7 GiB required by the optimized SOTA method,\nRenee without compromising accuracy.",
    "published": "2025-10-13T08:59:13Z",
    "updated": "2025-10-13T08:59:13Z",
    "link": "http://arxiv.org/pdf/2510.11168v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Jinbin Zhang",
      "Nasib Ullah",
      "Erik Schultheis",
      "Rohit Babbar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11167v1",
    "title": "Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks\n  for Low-Resource Iberian Languages",
    "summary": "Hate speech poses a serious threat to social cohesion and individual\nwell-being, particularly on social media, where it spreads rapidly. While\nresearch on hate speech detection has progressed, it remains largely focused on\nEnglish, resulting in limited resources and benchmarks for low-resource\nlanguages. Moreover, many of these languages have multiple linguistic\nvarieties, a factor often overlooked in current approaches. At the same time,\nlarge language models require substantial amounts of data to perform reliably,\na requirement that low-resource languages often cannot meet. In this work, we\naddress these gaps by compiling a meta-collection of hate speech datasets for\nEuropean Spanish, standardised with unified labels and metadata. This\ncollection is based on a systematic analysis and integration of existing\nresources, aiming to bridge the data gap and support more consistent and\nscalable hate speech detection. We extended this collection by translating it\ninto European Portuguese and into a Galician standard that is more convergent\nwith Spanish and another Galician variant that is more convergent with\nPortuguese, creating aligned multilingual corpora. Using these resources, we\nestablish new benchmarks for hate speech detection in Iberian languages. We\nevaluate state-of-the-art large language models in zero-shot, few-shot, and\nfine-tuning settings, providing baseline results for future research. Moreover,\nwe perform a cross-lingual analysis with our target languages. Our findings\nunderscore the importance of multilingual and variety-aware approaches in hate\nspeech detection and offer a foundation for improved benchmarking in\nunderrepresented European languages.",
    "published": "2025-10-13T08:58:02Z",
    "updated": "2025-10-13T08:58:02Z",
    "link": "http://arxiv.org/pdf/2510.11167v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Paloma Piot",
      "José Ramom Pichel Campos",
      "Javier Parapar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11160v1",
    "title": "One Size Does Not Fit All: Exploring Variable Thresholds for\n  Distance-Based Multi-Label Text Classification",
    "summary": "Distance-based unsupervised text classification is a method within text\nclassification that leverages the semantic similarity between a label and a\ntext to determine label relevance. This method provides numerous benefits,\nincluding fast inference and adaptability to expanding label sets, as opposed\nto zero-shot, few-shot, and fine-tuned neural networks that require re-training\nin such cases. In multi-label distance-based classification and information\nretrieval algorithms, thresholds are required to determine whether a text\ninstance is \"similar\" to a label or query. Similarity between a text and label\nis determined in a dense embedding space, usually generated by state-of-the-art\nsentence encoders. Multi-label classification complicates matters, as a text\ninstance can have multiple true labels, unlike in multi-class or binary\nclassification, where each instance is assigned only one label. We expand upon\nprevious literature on this underexplored topic by thoroughly examining and\nevaluating the ability of sentence encoders to perform distance-based\nclassification. First, we perform an exploratory study to verify whether the\nsemantic relationships between texts and labels vary across models, datasets,\nand label sets by conducting experiments on a diverse collection of realistic\nmulti-label text classification (MLTC) datasets. We find that similarity\ndistributions show statistically significant differences across models,\ndatasets and even label sets. We propose a novel method for optimizing\nlabel-specific thresholds using a validation set. Our label-specific\nthresholding method achieves an average improvement of 46% over normalized 0.5\nthresholding and outperforms uniform thresholding approaches from previous work\nby an average of 14%. Additionally, the method demonstrates strong performance\neven with limited labeled examples.",
    "published": "2025-10-13T08:52:14Z",
    "updated": "2025-10-13T08:52:14Z",
    "link": "http://arxiv.org/pdf/2510.11160v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jens Van Nooten",
      "Andriy Kosar",
      "Guy De Pauw",
      "Walter Daelemans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06552v3",
    "title": "References Indeed Matter? Reference-Free Preference Optimization for\n  Conversational Query Reformulation",
    "summary": "Conversational query reformulation (CQR) has become indispensable for\nimproving retrieval in dialogue-based applications. However, existing\napproaches typically rely on reference passages for optimization, which are\nimpractical to acquire in real-world scenarios. To address this limitation, we\nintroduce a novel reference-free preference optimization framework DualReform\nthat generates pseudo reference passages from commonly-encountered\nconversational datasets containing only queries and responses. DualReform\nattains this goal through two key innovations: (1) response-based inference,\nwhere responses serve as proxies to infer pseudo reference passages, and (2)\nresponse refinement via the dual-role of CQR, where a CQR model refines\nresponses based on the shared objectives between response refinement and CQR.\nDespite not relying on reference passages, DualReform achieves 96.9--99.1% of\nthe retrieval accuracy attainable only with reference passages and surpasses\nthe state-of-the-art method by up to 31.6%.",
    "published": "2025-05-10T07:43:23Z",
    "updated": "2025-10-13T08:52:07Z",
    "link": "http://arxiv.org/pdf/2505.06552v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Doyoung Kim",
      "Youngjun Lee",
      "Joeun Kim",
      "Jihwan Bang",
      "Hwanjun Song",
      "Susik Yoon",
      "Jae-Gil Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.20795v3",
    "title": "Test-Time Alignment for Large Language Models via Textual Model\n  Predictive Control",
    "summary": "Aligning Large Language Models (LLMs) with human preferences through\nfinetuning is resource-intensive, motivating lightweight alternatives at test\ntime. We address test-time alignment through the lens of sequential decision\nmaking, a perspective that reveals two fundamental challenges. When actions are\ndefined at the token level, as in guided decoding, alignment suffers from the\ncurse of horizon. Conversely, when actions are at the response level, as in\ntraditional iterative refinement, the curse of dimensionality emerges. To\nresolve this trade-off, we draw inspiration from Model Predictive Control (MPC)\nin control theory to propose Textual Model Predictive Control (TMPC), a novel\npredictive planning framework adapted for aligning LLMs at inference time. A\nkey limitation of standard MPC is its reliance on predefined, hard segment\nboundaries, which are often absent in text generation. TMPC overcomes this by\nintroducing two principles inspired by hierarchical reinforcement learning: (1)\nHindsight Subgoal Identification, where TMPC analyzes generation subgoals to\nretrospectively identify high-reward intermediate outputs as subgoals. This\nallows the framework to discover meaningful, task-specific planning steps\n(e.g., a sentence in machine translation or a bug fix in code generation.). (2)\nSubgoal-Conditioned Re-Generation, where these identified subgoals are used to\nguide subsequent planning iterations. By conditioning on these proven,\nhigh-quality subgoals, TMPC ensures stable improvement by building upon\npreviously validated successes. TMPC is evaluated on three tasks with distinct\nsegmentation properties: discourse-level translation, long-form response\ngeneration, and program synthesis. The results demonstrate that TMPC\nconsistently improves performance, highlighting the generality.",
    "published": "2025-02-28T07:24:33Z",
    "updated": "2025-10-13T08:51:05Z",
    "link": "http://arxiv.org/pdf/2502.20795v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kuang-Da Wang",
      "Teng-Ruei Chen",
      "Yu Heng Hung",
      "Guo-Xun Ko",
      "Shuoyang Ding",
      "Yueh-Hua Wu",
      "Yu-Chiang Frank Wang",
      "Chao-Han Huck Yang",
      "Wen-Chih Peng",
      "Ping-Chun Hsieh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11151v1",
    "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated\n  Code",
    "summary": "Large language Models (LLMs) have shown remarkable proficiency in code\ngeneration tasks across various programming languages. However, their outputs\noften contain subtle but critical vulnerabilities, posing significant risks\nwhen deployed in security-sensitive or mission-critical systems. This paper\nintroduces TypePilot, an agentic AI framework designed to enhance the security\nand robustness of LLM-generated code by leveraging strongly typed and\nverifiable languages, using Scala as a representative example. We evaluate the\neffectiveness of our approach in two settings: formal verification with the\nStainless framework and general-purpose secure code generation. Our experiments\nwith leading open-source LLMs reveal that while direct code generation often\nfails to enforce safety constraints, just as naive prompting for more secure\ncode, our type-focused agentic pipeline substantially mitigates input\nvalidation and injection vulnerabilities. The results demonstrate the potential\nof structured, type-guided LLM workflows to improve the SotA of the\ntrustworthiness of automated code generation in high-assurance domains.",
    "published": "2025-10-13T08:44:01Z",
    "updated": "2025-10-13T08:44:01Z",
    "link": "http://arxiv.org/pdf/2510.11151v1.pdf",
    "category": [
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Alexander Sternfeld",
      "Andrei Kucharavy",
      "Ljiljana Dolamic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.21708v3",
    "title": "On the Mathematical Relationship Between Layer Normalization and Dynamic\n  Activation Functions",
    "summary": "Layer normalization (LN) is an essential component of modern neural networks.\nWhile many alternative techniques have been proposed, none of them have\nsucceeded in replacing LN so far. The latest suggestion in this line of\nresearch is a dynamic activation function called Dynamic Tanh (DyT). Although\nit is empirically well-motivated and appealing from a practical point of view,\nit lacks a theoretical foundation. In this work, we shed light on the\nmathematical relationship between LN and dynamic activation functions. In\nparticular, we derive DyT from the LN variant RMSNorm, and show that a\nwell-defined decoupling in derivative space as well as an approximation are\nneeded to do so. By applying the same decoupling procedure directly in function\nspace, we are able to omit the approximation and obtain the exact element-wise\ncounterpart of RMSNorm, which we call Dynamic Inverse Square Root Unit\n(DyISRU). We demonstrate numerically that DyISRU reproduces the normalization\neffect on outliers more accurately than DyT does.",
    "published": "2025-03-27T17:20:44Z",
    "updated": "2025-10-13T08:43:46Z",
    "link": "http://arxiv.org/pdf/2503.21708v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Felix Stollenwerk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11144v1",
    "title": "$How^{2}$: How to learn from procedural How-to questions",
    "summary": "An agent facing a planning problem can use answers to how-to questions to\nreduce uncertainty and fill knowledge gaps, helping it solve both current and\nfuture tasks. However, their open ended nature, where valid answers to \"How do\nI X?\" range from executable actions to high-level descriptions of X's\nsub-goals, makes them challenging for AI agents to ask, and for AI experts to\nanswer, in ways that support efficient planning. We introduce $How^{2}$, a\nmemory agent framework that enables agents to ask how-to questions, store the\nanswers, and reuse them for lifelong learning in interactive environments. We\nevaluate our approach in Plancraft, a Minecraft crafting environment, where\nagents must complete an assembly task by manipulating inventory items. Using\nteacher models that answer at varying levels of abstraction, from executable\naction sequences to high-level subgoal descriptions, we show that lifelong\nlearning agents benefit most from answers that are abstracted and decoupled\nfrom the current state. $How^{2}$ offers a way for LLM-based agents to improve\ntheir planning capabilities over time by asking questions in interactive\nenvironments.",
    "published": "2025-10-13T08:35:20Z",
    "updated": "2025-10-13T08:35:20Z",
    "link": "http://arxiv.org/pdf/2510.11144v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Gautier Dagan",
      "Frank Keller",
      "Alex Lascarides"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08907v2",
    "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
    "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
    "published": "2025-10-10T01:42:14Z",
    "updated": "2025-10-13T08:26:21Z",
    "link": "http://arxiv.org/pdf/2510.08907v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xin Liu",
      "Runsong Zhao",
      "Pengcheng Huang",
      "Xinyu Liu",
      "Junyi Xiao",
      "Chunyang Xiao",
      "Tong Xiao",
      "Shengxiang Gao",
      "Zhengtao Yu",
      "Jingbo Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15291v2",
    "title": "Hallucinate at the Last in Long Response Generation: A Case Study on\n  Long Document Summarization",
    "summary": "Large Language Models (LLMs) have significantly advanced text generation\ncapabilities, including tasks like summarization, often producing coherent and\nfluent outputs. However, faithfulness to source material remains a significant\nchallenge due to the generation of hallucinations. While extensive research\nfocuses on detecting and reducing these inaccuracies, less attention has been\npaid to the positional distribution of hallucination within generated text,\nparticularly in long outputs. In this work, we investigate where hallucinations\noccur in LLM-based long response generation, using long document summarization\nas a key case study. Focusing on the challenging setting of long context-aware\nlong response generation, we find a consistent and concerning phenomenon:\nhallucinations tend to concentrate disproportionately in the latter parts of\nthe generated long response. To understand this bias, we explore potential\ncontributing factors related to the dynamics of attention and decoding over\nlong sequences. Furthermore, we investigate methods to mitigate this positional\nhallucination, aiming to improve faithfulness specifically in the concluding\nsegments of long outputs.",
    "published": "2025-05-21T09:22:11Z",
    "updated": "2025-10-13T08:24:08Z",
    "link": "http://arxiv.org/pdf/2505.15291v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Joonho Yang",
      "Seunghyun Yoon",
      "Hwan Chang",
      "Byeongjeong Kim",
      "Hwanhee Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10935v2",
    "title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key\n  Information from Documents",
    "summary": "In this paper, we introduce Spotlight, a novel paradigm for information\nextraction that produces concise, engaging narratives by highlighting the most\ncompelling aspects of a document. Unlike traditional summaries, which\nprioritize comprehensive coverage, spotlights selectively emphasize intriguing\ncontent to foster deeper reader engagement with the source material. We\nformally differentiate spotlights from related constructs and support our\nanalysis with a detailed benchmarking study using new datasets curated for this\nwork. To generate high-quality spotlights, we propose a two-stage approach:\nfine-tuning a large language model on our benchmark data, followed by alignment\nvia Direct Preference Optimization (DPO). Our comprehensive evaluation\ndemonstrates that the resulting model not only identifies key elements with\nprecision but also enhances readability and boosts the engagement value of the\noriginal document.",
    "published": "2025-09-13T18:18:37Z",
    "updated": "2025-10-13T08:23:41Z",
    "link": "http://arxiv.org/pdf/2509.10935v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ankan Mullick",
      "Sombit Bose",
      "Rounak Saha",
      "Ayan Kumar Bhowmick",
      "Aditya Vempaty",
      "Prasenjit Dey",
      "Ravi Kokku",
      "Pawan Goyal",
      "Niloy Ganguly"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12661v2",
    "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven\n  Prompt Optimization",
    "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.",
    "published": "2025-04-17T05:46:41Z",
    "updated": "2025-10-13T08:07:46Z",
    "link": "http://arxiv.org/pdf/2504.12661v2.pdf",
    "category": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Menglan Chen",
      "Xianghe Pang",
      "Jingjing Dong",
      "WenHao Wang",
      "Yaxin Du",
      "Siheng Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.10497v3",
    "title": "QUITO-X: A New Perspective on Context Compression from the Information\n  Bottleneck Theory",
    "summary": "Generative LLM have achieved remarkable success in various industrial\napplications, owing to their promising In-Context Learning capabilities.\nHowever, the issue of long context in complex tasks poses a significant barrier\nto their wider adoption, manifested in two main aspects: (i) The excessively\nlong context leads to high costs and inference delays. (ii) A substantial\namount of task-irrelevant information introduced by long contexts exacerbates\nthe \"lost in the middle\" problem. Existing methods compress context by removing\nredundant tokens using metrics such as self-information or PPL, which is\ninconsistent with the objective of retaining the most important tokens when\nconditioning on a given query. In this study, we introduce information\nbottleneck theory (IB) to model the problem, offering a novel perspective that\nthoroughly addresses the essential properties required for context compression.\nAdditionally, we propose a cross-attention-based approach to approximate mutual\ninformation in IB, which can be flexibly replaced with suitable alternatives in\ndifferent scenarios. Extensive experiments on four datasets demonstrate that\nour method achieves a 25% increase in compression rate compared to the\nstate-of-the-art, while maintaining question answering performance. In\nparticular, the context compressed by our method even outperform the full\ncontext in some cases.",
    "published": "2024-08-20T02:44:45Z",
    "updated": "2025-10-13T07:51:33Z",
    "link": "http://arxiv.org/pdf/2408.10497v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yihang Wang",
      "Xu Huang",
      "Bowen Tian",
      "Yueyang Su",
      "Lei Yu",
      "Huaming Liao",
      "Yixing Fan",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11104v1",
    "title": "Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference\n  Optimization",
    "summary": "Current approaches for strengthening LLM reasoning tend to introduce a\ntraining bias toward human-like reasoning trajectories. In step-wise preference\noptimization, in particular, dependence on human or higher-capacity model\nannotations for intermediate steps limits exploration of alternative,\nnon-human-like reasoning paths and thus constrains achievable performance.\nFurthermore, through a small-scale pilot study, we observed that in\napproximately 75% of cases, the model's first erroneous step occurs after the\nlowest-confidence point. This suggests that guiding the model at its\nlowest-confidence point before an error provides more accurate supervision than\nlocating the first explicit error. In this paper, we propose Confidence-Guided\nReasoning Path Preference Optimization (CGPO), a method that leverages a\nconfidence signal to identify points of maximal uncertainty in the model's\nreasoning process and applies self-generated, non-human-like reasoning-path\nguidance to mitigate trajectory drift. Our experiments span diverse models\napplied to both code and mathematical reasoning tasks. The results show that,\nwith the same amount of training data, our method using data generated by a\nsmall model can achieve better performance in most cases compared with\napproaches using data generated by a strong model or human-annotated.",
    "published": "2025-10-13T07:51:16Z",
    "updated": "2025-10-13T07:51:16Z",
    "link": "http://arxiv.org/pdf/2510.11104v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Junjie Lu",
      "Yuliang Liu",
      "Chaofeng Qu",
      "Wei Shen",
      "Zhouhan Lin",
      "Min Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11098v1",
    "title": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language\n  Model Conversational Agents",
    "summary": "Recent advances in large audio language models (LALMs) have greatly enhanced\nmultimodal conversational systems. However, existing benchmarks remain limited\n-- they are mainly English-centric, rely on synthetic speech, and lack\ncomprehensive, discriminative evaluation across multiple dimensions. To address\nthese gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality\nChinese benchmark built entirely on real human speech. VCB Bench evaluates\nLALMs from three complementary perspectives: instruction following (including\nspeech-level control beyond text commands), knowledge understanding (general\nknowledge, reasoning, and daily dialogue), and robustness (stability under\nperturbations in content, environment, and speaker traits). Experiments on\nrepresentative LALMs reveal notable performance gaps and highlight future\ndirections for improvement. VCB Bench provides a reproducible and fine-grained\nevaluation framework, offering standardized methodology and practical insights\nfor advancing Chinese voice conversational models.",
    "published": "2025-10-13T07:45:52Z",
    "updated": "2025-10-13T07:45:52Z",
    "link": "http://arxiv.org/pdf/2510.11098v1.pdf",
    "category": [
      "cs.SD",
      "cs.CL"
    ],
    "authors": [
      "Jiliang Hu",
      "Wenfu Wang",
      "Zuchao Li",
      "Chenxing Li",
      "Yiyang Zhao",
      "Hanzhao Li",
      "Liqiang Zhang",
      "Meng Yu",
      "Dong Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09479v2",
    "title": "Draw with Thought: Unleashing Multimodal Reasoning for Scientific\n  Diagram Generation",
    "summary": "Scientific diagrams are vital tools for communicating structured knowledge\nacross disciplines. However, they are often published as static raster images,\nlosing symbolic semantics and limiting reuse. While Multimodal Large Language\nModels (MLLMs) offer a pathway to bridging vision and structure, existing\nmethods lack semantic control and structural interpretability, especially on\ncomplex diagrams. We propose Draw with Thought (DwT), a training-free framework\nthat guides MLLMs to reconstruct diagrams into editable mxGraph XML code\nthrough cognitively-grounded Chain-of-Thought reasoning. DwT enables\ninterpretable and controllable outputs without model fine-tuning by dividing\nthe task into two stages: Coarse-to-Fine Planning, which handles perceptual\nstructuring and semantic specification, and Structure-Aware Code Generation,\nenhanced by format-guided refinement. To support evaluation, we release\nPlot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard\nXML annotations. Extensive experiments across eight MLLMs show that our\napproach yields high-fidelity, semantically aligned, and structurally valid\nreconstructions, with human evaluations confirming strong alignment in both\naccuracy and visual aesthetics, offering a scalable solution for converting\nstatic visuals into executable representations and advancing machine\nunderstanding of scientific graphics.",
    "published": "2025-04-13T08:22:09Z",
    "updated": "2025-10-13T07:29:31Z",
    "link": "http://arxiv.org/pdf/2504.09479v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zhiqing Cui",
      "Jiahao Yuan",
      "Hanqing Wang",
      "Yanshu Li",
      "Chenxu Du",
      "Zhenglong Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15692v3",
    "title": "TemplateRL: Structured Template-Guided Reinforcement Learning for LLM\n  Reasoning",
    "summary": "Reinforcement learning (RL) has emerged as an effective paradigm for\nenhancing model reasoning. However, existing RL methods like GRPO often rely on\nunstructured self-sampling to fit scalar rewards, often producing inefficient\nrollouts that fail to capture transferable problem-solving strategies. To\naddress these limitations, we propose **TemplateRL**, a structured\ntemplate-guided RL framework that augments policy optimization with explicit\ntemplate guidance. Our approach first constructs a problem-solving template\nlibrary via MCTS on a small seed set, then seamlessly integrates this\nhigh-level structured guidance into RL training. By guiding rollout generation\nto align with proven template structures, TemplateRL significantly improves\nhigh-quality trajectory hit rates while reducing ineffective exploration. This\nstructure-guided design steers the policy toward validated strategic patterns,\nstabilizing training dynamics, and enhancing RL sampling efficiency. Notably,\nthe explicit template library is interpretable, editable, and supports online\nupdates-enabling continuous updates during both training and inference.\nExtensive experiments demonstrate that TemplateRL outperforms GRPO by 99% on\nAIME and 41% on AMC, with superior stability on weak models and remarkable\ncross-domain generalization, highlighting its potential for broader tasks.",
    "published": "2025-05-21T16:06:10Z",
    "updated": "2025-10-13T07:21:10Z",
    "link": "http://arxiv.org/pdf/2505.15692v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jinyang Wu",
      "Chonghua Liao",
      "Mingkuan Feng",
      "Shuai Zhang",
      "Zhengqi Wen",
      "Haoran Luo",
      "Ling Yang",
      "Huazhe Xu",
      "Jianhua Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.09177v3",
    "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL",
    "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs. long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms baselines across\nmodel sizes and evaluation datasets, with the largest gains on the\nQwen3-8B-Base model.",
    "published": "2025-09-11T06:27:10Z",
    "updated": "2025-10-13T07:18:07Z",
    "link": "http://arxiv.org/pdf/2509.09177v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hanyi Mao",
      "Quanjia Xiao",
      "Lei Pang",
      "Haixiao Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10068v2",
    "title": "SaraCoder: Orchestrating Semantic and Structural Cues for\n  Resource-Optimized Repository-Level Code Completion",
    "summary": "Despite Retrieval-Augmented Generation improving code completion, traditional\nretrieval methods struggle with information redundancy and a lack of diversity\nwithin limited context windows. To solve this, we propose a resource-optimized\nretrieval augmentation method, SaraCoder. It maximizes information diversity\nand representativeness in a limited context window, significantly boosting the\naccuracy and reliability of repository-level code completion. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that SaraCoder outperforms existing baselines across\nmultiple programming languages and models. Our work proves that systematically\nrefining retrieval results across multiple dimensions provides a new paradigm\nfor building more accurate and resource-optimized repository-level code\ncompletion systems.",
    "published": "2025-08-13T11:56:05Z",
    "updated": "2025-10-13T07:16:49Z",
    "link": "http://arxiv.org/pdf/2508.10068v2.pdf",
    "category": [
      "cs.SE",
      "cs.CL",
      "cs.IR",
      "cs.PL"
    ],
    "authors": [
      "Xiaohan Chen",
      "Zhongying Pan",
      "Quan Feng",
      "Yu Tian",
      "Shuqun Yang",
      "Mengru Wang",
      "Lina Gong",
      "Yuxia Geng",
      "Piji Li",
      "Xiang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18673v2",
    "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in\n  Multimodal Chain-of-Thought via Prompt Curriculum",
    "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.",
    "published": "2025-08-26T04:32:15Z",
    "updated": "2025-10-13T07:02:18Z",
    "link": "http://arxiv.org/pdf/2508.18673v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "authors": [
      "Xinglong Yang",
      "Quan Feng",
      "Zhongying Pan",
      "Xiang Chen",
      "Yu Tian",
      "Wentong Li",
      "Shuofei Qiao",
      "Yuxia Geng",
      "Xingyu Zhao",
      "Sheng-Jun Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04573v3",
    "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
    "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.",
    "published": "2025-10-06T08:15:03Z",
    "updated": "2025-10-13T07:01:12Z",
    "link": "http://arxiv.org/pdf/2510.04573v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Haoqiang Kang",
      "Yizhe Zhang",
      "Nikki Lijing Kuang",
      "Nicklas Majamaki",
      "Navdeep Jaitly",
      "Yi-An Ma",
      "Lianhui Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.11789v3",
    "title": "Personality Editing for Language Models through Adjusting\n  Self-Referential Queries",
    "summary": "Large Language Models (LLMs) are integral to applications such as\nconversational agents and content creation, where precise control over a\nmodel's personality is essential for maintaining tone, consistency, and user\nengagement. However, prevailing prompt-based or fine-tuning approaches either\nlack robustness or demand large-scale training data, making them costly and\nimpractical. In this paper, we present PALETTE (Personality Adjustment by LLM\nSElf-TargeTed quEries), a novel method for personality editing in LLMs. Our\napproach introduces adjustment queries, where self-referential statements\ngrounded in psychological constructs are treated analogously to factual\nknowledge, enabling direct editing of personality-related responses. Unlike\nfine-tuning, PALETTE requires only 12 editing samples to achieve substantial\nimprovements in personality alignment across personality dimensions.\nExperimental results from both automatic and human evaluations demonstrate that\nour method enables more stable and well-balanced personality control in LLMs.",
    "published": "2025-02-17T13:28:14Z",
    "updated": "2025-10-13T06:53:27Z",
    "link": "http://arxiv.org/pdf/2502.11789v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Seojin Hwang",
      "Yumin Kim",
      "Byeongjeong Kim",
      "Donghoon Shin",
      "Hwanhee Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08933v2",
    "title": "Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning\n  Evaluation",
    "summary": "Language models have demonstrated remarkable performance on complex\nmulti-step reasoning tasks. However, their evaluation has been predominantly\nconfined to high-resource languages such as English. In this paper, we\nintroduce a manually translated Bangla multi-step reasoning dataset derived\nfrom the English Reveal dataset, featuring both binary and non-binary question\ntypes. We conduct a controlled evaluation of English-centric and Bangla-centric\nmultilingual small language models on the original dataset and our translated\nversion to compare their ability to exploit relevant reasoning steps to produce\ncorrect answers. Our results show that, in comparable settings, reasoning\ncontext is beneficial for more challenging non-binary questions, but models\nstruggle to employ relevant Bangla reasoning steps effectively. We conclude by\nexploring how reasoning steps contribute to models' predictions, highlighting\ndifferent trends across models and languages.",
    "published": "2025-08-12T13:34:10Z",
    "updated": "2025-10-13T06:43:47Z",
    "link": "http://arxiv.org/pdf/2508.08933v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Khondoker Ittehadul Islam",
      "Gabriele Sarti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11052v1",
    "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by\n  Refining Belief States",
    "summary": "Autoregressive (AR) models remain the standard for natural language\ngeneration but still suffer from high latency due to strictly sequential\ndecoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,\nmitigate this by generating in parallel, yet they suffer from two core\nlimitations: information loss, as predictive distributions for non-finalized\ntokens are discarded at each step, and premature commitment, where local\ndecisions are made without sufficient global coordination. We introduce Latent\nRefinement Decoding (LRD), a two-stage framework with Latent Refinement and a\nPredictive Feedback Loop. The first stage maintains masked positions as\ndistributional mixtures of predicted tokens and the mask embedding, allowing\nthe model to establish more globally consistent beliefs. The second stage\nprogressively finalizes confident tokens while retaining uncertain ones for\niterative feedback. KL-divergence dynamics provide a principled and reliable\ncriterion for convergence and early stopping. Experiments across coding\n(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that\nLRD improves accuracy while delivering speedups of up to 10.6x, making it a\nstrong and versatile alternative for parallel sequence generation.",
    "published": "2025-10-13T06:38:13Z",
    "updated": "2025-10-13T06:38:13Z",
    "link": "http://arxiv.org/pdf/2510.11052v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Qinglin Zhu",
      "Yizhen Yao",
      "Runcong Zhao",
      "Yanzheng Xiang",
      "Amrutha Saseendran",
      "Chen Jin",
      "Philip Alexander Teare",
      "Bin Liang",
      "Yulan He",
      "Lin Gui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05444v3",
    "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically\n  Distant Language Pairs",
    "summary": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most methods\nstill rely on direct IPA-based phonetic matching or employ LLMs without\nphonological guidance. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that performs IPA-based phonological\nadaptation and syllable-aware alignment to retrieve L1 keyword sequence and\nuses LLMs to generate verbal cues. We evaluate PhoniTale through automated\nmetrics and a short-term recall test with human participants, comparing its\noutput to human-written and prior automated mnemonics. Our findings show that\nPhoniTale consistently outperforms previous automated approaches and achieves\nquality comparable to human-written mnemonics.",
    "published": "2025-07-07T19:50:12Z",
    "updated": "2025-10-13T06:35:48Z",
    "link": "http://arxiv.org/pdf/2507.05444v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sana Kang",
      "Myeongseok Gwon",
      "Su Young Kwon",
      "Jaewook Lee",
      "Andrew Lan",
      "Bhiksha Raj",
      "Rita Singh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09426v2",
    "title": "BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning",
    "summary": "Human infants rapidly develop visual reasoning skills from minimal input,\nsuggesting that developmentally inspired pretraining could significantly\nenhance the efficiency of vision-language models (VLMs). Although recent\nefforts have leveraged infant-inspired datasets like SAYCam, existing\nevaluation benchmarks remain misaligned--they are either too simplistic,\nnarrowly scoped, or tailored for large-scale pretrained models. Additionally,\ntraining exclusively on infant data overlooks the broader, diverse input from\nwhich infants naturally learn. To address these limitations, we propose\nBabyVLM, a novel framework comprising comprehensive in-domain evaluation\nbenchmarks and a synthetic training dataset created via child-directed\ntransformations of existing datasets. We demonstrate that VLMs trained with our\nsynthetic dataset achieve superior performance on BabyVLM tasks compared to\nmodels trained solely on SAYCam or general-purpose data of the SAYCam size.\nBabyVLM thus provides a robust, developmentally aligned evaluation tool and\nillustrates how compact models trained on carefully curated data can generalize\neffectively, opening pathways toward data-efficient vision-language learning\nparadigms.",
    "published": "2025-04-13T04:17:12Z",
    "updated": "2025-10-13T06:33:45Z",
    "link": "http://arxiv.org/pdf/2504.09426v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shengao Wang",
      "Arjun Chandra",
      "Aoming Liu",
      "Venkatesh Saligrama",
      "Boqing Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24821v2",
    "title": "DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the\n  Initiation-Response-Evaluation Framework",
    "summary": "While cognitive diagnosis (CD) effectively assesses students' knowledge\nmastery from structured test data, applying it to real-world teacher-student\ndialogues presents two fundamental challenges. Traditional CD models lack a\nsuitable framework for handling dynamic, unstructured dialogues, and it's\ndifficult to accurately extract diagnostic semantics from lengthy dialogues. To\novercome these hurdles, we propose DiaCDM, an innovative model. We've adapted\nthe initiation-response-evaluation (IRE) framework from educational theory to\ndesign a diagnostic framework tailored for dialogue. We also developed a unique\ngraph-based encoding method that integrates teacher questions with relevant\nknowledge components to capture key information more precisely. To our\nknowledge, this is the first exploration of cognitive diagnosis in a dialogue\nsetting. Experiments on three real-world dialogue datasets confirm that DiaCDM\nnot only significantly improves diagnostic accuracy but also enhances the\nresults' interpretability, providing teachers with a powerful tool for\nassessing students' cognitive states. The code is available at\nhttps://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.",
    "published": "2025-09-29T14:09:04Z",
    "updated": "2025-10-13T06:30:07Z",
    "link": "http://arxiv.org/pdf/2509.24821v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Rui Jia",
      "Yuang Wei",
      "Ruijia Li",
      "Yuan-Hao Jiang",
      "Xinyu Xie",
      "Yaomin Shen",
      "Min Zhang",
      "Bo Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11040v1",
    "title": "Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned\n  Tasks and Benchmarks",
    "summary": "The rise of large language models (LLMs) has transformed healthcare by\noffering clinical guidance, yet their direct deployment to patients poses\nsafety risks due to limited domain expertise. To mitigate this, we propose\nrepositioning LLMs as clinical assistants that collaborate with experienced\nphysicians rather than interacting with patients directly. We conduct a\ntwo-stage inspiration-feedback survey to identify real-world needs in clinical\nworkflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese\nmedical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27\nspecialties. To evaluate model performance in doctor-facing applications, we\nintroduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74\nmulti-turn conversations). Experimental results with over ten popular LLMs\ndemonstrate that DoctorFLAN notably improves the performance of open-source\nLLMs in medical contexts, facilitating their alignment with physician workflows\nand complementing existing patient-oriented models. This work contributes a\nvaluable resource and framework for advancing doctor-centered medical LLM\ndevelopment",
    "published": "2025-10-13T06:18:27Z",
    "updated": "2025-10-13T06:18:27Z",
    "link": "http://arxiv.org/pdf/2510.11040v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Wenya Xie",
      "Qingying Xiao",
      "Yu Zheng",
      "Xidong Wang",
      "Junying Chen",
      "Ke Ji",
      "Anningzhe Gao",
      "Prayag Tiwari",
      "Xiang Wan",
      "Feng Jiang",
      "Benyou Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11031v1",
    "title": "LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems\n  for Language Models",
    "summary": "Joint logical-numerical reasoning remains a major challenge for language\nmodels, yet existing datasets rely on fixed rule sets and offer limited control\nover task complexity, constraining their generalizability for evaluation and\ntraining. We present LogiNumSynth, a flexible natural language problem\nsynthesizer that synthesizes tasks requiring proficiency in joint logical\nreasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,\narithmetic computation). LogiNumSynth supports fine-grained control over\nreasoning world richness, logical reasoning depth, and the complexity of\nnumerical computations, enabling flexible data synthesis across difficulty\nlevels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing\nfully controllable joint reasoning tasks over natural language; (2) Evaluation\n& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)\nTargeted Training -- using synthesized data to enhance LLMs' reasoning\nperformance. Experiments with multiple LLMs highlight persistent weaknesses in\nlogical-numerical reasoning, showing that LogiNumSynth can serve as both a\ndiagnostic tool and a source of targeted supervision for advancing integrated\nreasoning skills.",
    "published": "2025-10-13T06:01:02Z",
    "updated": "2025-10-13T06:01:02Z",
    "link": "http://arxiv.org/pdf/2510.11031v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yiwei Liu",
      "Yucheng Li",
      "Xiao Li",
      "Gong Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09116v2",
    "title": "DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel\n  Translation",
    "summary": "Large language models (LLMs) have substantially advanced machine translation\n(MT), yet their effectiveness in translating web novels remains unclear.\nExisting benchmarks rely on surface-level metrics that fail to capture the\ndistinctive traits of this genre. To address these gaps, we introduce DITING,\nthe first comprehensive evaluation framework for web novel translation,\nassessing narrative and cultural fidelity across six dimensions: idiom\ntranslation, lexical ambiguity, terminology localization, tense consistency,\nzero-pronoun resolution, and cultural safety, supported by over 18K\nexpert-annotated Chinese-English sentence pairs. We further propose AgentEval,\na reasoning-driven multi-agent evaluation framework that simulates expert\ndeliberation to assess translation quality beyond lexical overlap, achieving\nthe highest correlation with human judgments among seven tested automatic\nmetrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation\ndataset of 300 sentence pairs annotated with error labels and scalar quality\nscores. Comprehensive evaluation of fourteen open, closed, and commercial\nmodels reveals that Chinese-trained LLMs surpass larger foreign counterparts,\nand that DeepSeek-V3 delivers the most faithful and stylistically coherent\ntranslations. Our work establishes a new paradigm for exploring LLM-based web\nnovel translation and provides public resources to advance future research.",
    "published": "2025-10-10T08:10:10Z",
    "updated": "2025-10-13T05:51:38Z",
    "link": "http://arxiv.org/pdf/2510.09116v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Enze Zhang",
      "Jiaying Wang",
      "Mengxi Xiao",
      "Jifei Liu",
      "Ziyan Kuang",
      "Rui Dong",
      "Eric Dong",
      "Sophia Ananiadou",
      "Min Peng",
      "Qianqian Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01891v5",
    "title": "Training and Evaluating with Human Label Variation: An Empirical Study",
    "summary": "Human label variation (HLV) challenges the standard assumption that a\nlabelled instance has a single ground truth, instead embracing the natural\nvariation in human annotation to train and evaluate models. While various\ntraining methods and metrics for HLV have been proposed, it is still unclear\nwhich methods and metrics perform best in what settings. We propose new\nevaluation metrics for HLV leveraging fuzzy set theory. Since these new\nproposed metrics are differentiable, we then in turn experiment with employing\nthese metrics as training objectives. We conduct an extensive study over 6 HLV\ndatasets testing 14 training methods and 6 evaluation metrics. We find that\ntraining on either disaggregated annotations or soft labels performs best\nacross metrics, outperforming training using the proposed training objectives\nwith differentiable metrics. We also show that our proposed soft micro F1 score\nis one of the best metrics for HLV data.",
    "published": "2025-02-03T23:49:20Z",
    "updated": "2025-10-13T05:38:52Z",
    "link": "http://arxiv.org/pdf/2502.01891v5.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Kemal Kurniawan",
      "Meladel Mistica",
      "Timothy Baldwin",
      "Jey Han Lau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18748v2",
    "title": "Chronological Passage Assembling in RAG framework for Temporal Question\n  Answering",
    "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual f low in a limited context window.\nRetrievalaugmented generation (RAG) methods aim to address this challenge by\nselectively retrieving only necessary document segments. However, narrative\ntexts possess unique characteristics that limit the effectiveness of these\nexisting approaches. Specifically, understanding narrative texts requires more\nthan isolated segments, as the broader context and sequential relationships\nbetween segments are crucial for comprehension. To address these limitations,\nwe propose ChronoRAG, a novel RAG framework specialized for narrative texts.\nThis approach focuses on two essential aspects: refining dispersed document\ninformation into coherent and structured passages and preserving narrative flow\nby explicitly capturing and maintaining the temporal order among retrieved\npassages. We empirically demonstrate the effectiveness of ChronoRAG through\nexperiments on the NarrativeQA and GutenQAdataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.",
    "published": "2025-08-26T07:23:23Z",
    "updated": "2025-10-13T05:11:35Z",
    "link": "http://arxiv.org/pdf/2508.18748v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Byeongjeong Kim",
      "Jeonghyun Park",
      "Joonho Yang",
      "Hwanhee Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05431v2",
    "title": "Self-Filtered Distillation with LLMs-generated Trust Indicators for\n  Reliable Patent Classification",
    "summary": "Large language models (LLMs) increasingly generate natural language\nrationales to enhance interpretability, but these often contain logical errors,\nlabel mismatches, and domain-specific misalignments. Directly using such\nrationales as supervision risks propagating noise and undermining training\nstability. To address this challenge, we introduce Self-Filtered Distillation,\na framework specifically tailored for patent classification, which treats\nLLM-generated rationales as trust signals rather than ground-truth supervision.\nThe framework employs selective distillation guided by three unsupervised trust\nmetrics: (1) Self-Consistency, which measures the stability of LLM-generated\nrationales across multiple generations; (2) Class Entailment Alignment, which\nassesses semantic coherence with patent-specific class definitions; and (3) LLM\nAgreement Scoring, which validates rationale-label plausibility. These metrics\nare integrated into a unified trust score that primarily weights training\nsamples while optionally filtering out extremely low-trust cases, enabling\nreasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used\nbenchmark for patent classification, show that our method outperforms\nlabel-based learning and conventional distillation in accuracy, stability, and\ninterpretability, establishing a reliable paradigm for leveraging\nreasoning-aware trust indicators in patent analytics.",
    "published": "2025-10-06T22:50:01Z",
    "updated": "2025-10-13T05:04:30Z",
    "link": "http://arxiv.org/pdf/2510.05431v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yongmin Yoo",
      "Xu Zhang",
      "Longbing Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09895v3",
    "title": "Learning from Reference Answers: Versatile Language Model Alignment\n  without Binary Human Preference Data",
    "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In different alignment scenarios, such as safety, confidence, and\ngeneral preference alignment, binary preference data collection and reward\nmodeling are resource-intensive but play a central role in transferring human\npreferences. In this work, we explore using the similarity between sampled\ngenerations and reference answers as a supplementary reward function for\nalignment. When unary reference answers are available, such similarity-based\nrewards can circumvent the need for binary preference data and explicit reward\nmodeling. We introduce \\textit{RefAlign}, a versatile REINFORCE-style alignment\nalgorithm that does not rely on reward or reference models. RefAlign utilizes\nlanguage generation evaluation metrics, such as BERTScore, between sampled\ngenerations and reference answers as surrogate rewards. Beyond general\npreference optimization, RefAlign can be naturally extended to diverse\nscenarios, including safety and confidence alignment, by combining\nsimilarity-based rewards with task-specific objectives. Across multiple\nscenarios, RefAlign achieves performance comparable to prior alignment methods\nwhile operating without binary preference data or reward models. The code is\navailable at https://github.com/mzhaoshuai/RefAlign.",
    "published": "2025-04-14T05:43:21Z",
    "updated": "2025-10-13T05:00:17Z",
    "link": "http://arxiv.org/pdf/2504.09895v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shuai Zhao",
      "Yunqiu Xu",
      "Linchao Zhu",
      "Yi Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16415v4",
    "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven\n  Mechanistic Study of Context Attribution in Retrieval-Augmented Generation",
    "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning, gradient-calculation or surrogate\nmodelling. Evaluations on a wide range of RAG benchmarks, such as TyDi QA,\nHotpot QA, and Musique, using instruction-tuned LLMs in different scales\ndemonstrate superior accuracy and significant computational efficiency\nimprovements compared to the previous surrogate-based method. Furthermore, our\nmechanistic analysis reveals specific attention heads and multilayer perceptron\n(MLP) layers responsible for context attribution, providing valuable insights\ninto the internal workings of RAG models and how they affect RAG behaviours.\nOur code is available at https://github.com/ruizheliUOA/ARC_JSD.",
    "published": "2025-05-22T09:04:03Z",
    "updated": "2025-10-13T04:40:21Z",
    "link": "http://arxiv.org/pdf/2505.16415v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Ruizhe Li",
      "Chen Chen",
      "Yuchen Hu",
      "Yanjun Gao",
      "Xi Wang",
      "Emine Yilmaz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11004v1",
    "title": "Automating Structural Engineering Workflows with Large Language Model\n  Agents",
    "summary": "We introduce $\\textbf{MASSE}$, the first Multi-Agent System for Structural\nEngineering, effectively integrating large language model (LLM)-based agents\nwith real-world engineering workflows. Structural engineering is a fundamental\nyet traditionally stagnant domain, with core workflows remaining largely\nunchanged for decades despite its substantial economic impact and global market\nsize. Recent advancements in LLMs have significantly enhanced their ability to\nperform complex reasoning, long-horizon planning, and precise tool utilization\n-- capabilities well aligned with structural engineering tasks such as\ninterpreting design codes, executing load calculations, and verifying\nstructural capacities. We present a proof-of-concept showing that most\nreal-world structural engineering workflows can be fully automated through a\ntraining-free LLM-based multi-agent system. MASSE enables immediate deployment\nin professional environments, and our comprehensive validation on real-world\ncase studies demonstrates that it can reduce expert workload from approximately\ntwo hours to mere minutes, while enhancing both reliability and accuracy in\npractical engineering scenarios.",
    "published": "2025-10-13T04:38:46Z",
    "updated": "2025-10-13T04:38:46Z",
    "link": "http://arxiv.org/pdf/2510.11004v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "authors": [
      "Haoran Liang",
      "Yufa Zhou",
      "Mohammad Talebi Kalaleh",
      "Qipei Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11001v1",
    "title": "DND: Boosting Large Language Models with Dynamic Nested Depth",
    "summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves\nperformance for off-the-shelf LLMs by selecting critical tokens to reprocess in\na nested depth manner. Specifically, at the end of the given transformer layer,\nDND identifies more critical tokens with a router and feeds them back for an\nextra round of processing, effectively ``reviewing\" difficult tokens while\navoiding redundant computation for easier ones. The dynamic selection mechanism\nis tailored for precise control via two novel strategies: a router controlling\nloss to enhance token selection distinguishability, and a threshold control\nscheme to ensure selection stability. We demonstrate the effectiveness of DND\nby directly integrating it into pre-trained dense and MoE models during a\npost-training phase. On diverse benchmarks, this approach boosts the\nperformances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by\n0.87%, all with a minimal parameter and computing increase.",
    "published": "2025-10-13T04:22:57Z",
    "updated": "2025-10-13T04:22:57Z",
    "link": "http://arxiv.org/pdf/2510.11001v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tieyuan Chen",
      "Xiaodong Chen",
      "Haoxing Chen",
      "Zhenzhong Lan",
      "Weiyao Lin",
      "Jianguo Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10998v1",
    "title": "ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring\n  Scenarios",
    "summary": "Large language models (LLMs) are increasingly under scrutiny for perpetuating\nidentity-based discrimination in high-stakes domains such as hiring,\nparticularly against people with disabilities (PwD). However, existing research\nremains largely Western-centric, overlooking how intersecting forms of\nmarginalization--such as gender and caste--shape experiences of PwD in the\nGlobal South. We conduct a comprehensive audit of six LLMs across 2,820 hiring\nscenarios spanning diverse disability, gender, nationality, and caste profiles.\nTo capture subtle intersectional harms and biases, we introduce ABLEIST\n(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five\nableism-specific and three intersectional harm metrics grounded in disability\nstudies literature. Our results reveal significant increases in ABLEIST harms\ntowards disabled candidates--harms that many state-of-the-art models failed to\ndetect. These harms were further amplified by sharp increases in intersectional\nharms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,\nhighlighting critical blind spots in current safety tools and the need for\nintersectional safety evaluations of frontier models in high-stakes domains\nlike hiring.",
    "published": "2025-10-13T04:18:23Z",
    "updated": "2025-10-13T04:18:23Z",
    "link": "http://arxiv.org/pdf/2510.10998v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Mahika Phutane",
      "Hayoung Jung",
      "Matthew Kim",
      "Tanushree Mitra",
      "Aditya Vashistha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10994v1",
    "title": "DeepResearchGuard: Deep Research with Open-Domain Evaluation and\n  Multi-Stage Guardrails for Safety",
    "summary": "Deep research frameworks have shown promising capabilities in synthesizing\ncomprehensive reports from web sources. While deep research possesses\nsignificant potential to address complex issues through planning and research\ncycles, existing frameworks are deficient in sufficient evaluation procedures\nand stage-specific protections. They typically treat evaluation as exact match\naccuracy of question-answering, but overlook crucial aspects of report quality\nsuch as credibility, coherence, breadth, depth, and safety. This oversight may\nresult in hazardous or malicious sources being integrated into the final\nreport. To address these issues, we introduce DEEPRESEARCHGUARD, a\ncomprehensive framework featuring four-stage safeguards with open-domain\nevaluation of references and reports. We assess performance across multiple\nmetrics, e.g., defense success rate and over-refusal rate, and five key report\ndimensions. In the absence of a suitable safety benchmark, we introduce\nDRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation\nspans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,\nDeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success\nrate improvement of 18.16% while reducing over-refusal rate by 6%. The input\nguard provides the most substantial early-stage protection by filtering out\nobvious risks, while the plan and research guards enhance citation discipline\nand source credibility. Through extensive experiments, we show that\nDEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware\ndefenses that effectively block harmful content propagation, while\nsystematically improving report quality without excessive over-refusal rates.\nThe code can be found via https://github.com/Jasonya/DeepResearchGuard.",
    "published": "2025-10-13T04:11:21Z",
    "updated": "2025-10-13T04:11:21Z",
    "link": "http://arxiv.org/pdf/2510.10994v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Wei-Chieh Huang",
      "Henry Peng Zou",
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Weizhi Zhang",
      "Yangning Li",
      "Angelo Zangari",
      "Jizhou Guo",
      "Chunyu Miao",
      "Liancheng Fang",
      "Langzhou He",
      "Renhe Jiang",
      "Philip S. Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10991v1",
    "title": "A Survey on Agentic Multimodal Large Language Models",
    "summary": "With the recent emergence of revolutionary autonomous agentic systems,\nresearch community is witnessing a significant shift from traditional static,\npassive, and domain-specific AI agents toward more dynamic, proactive, and\ngeneralizable agentic AI. Motivated by the growing interest in agentic AI and\nits potential trajectory toward AGI, we present a comprehensive survey on\nAgentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we\nexplore the emerging paradigm of agentic MLLMs, delineating their conceptual\nfoundations and distinguishing characteristics from conventional MLLM-based\nagents. We establish a conceptual framework that organizes agentic MLLMs along\nthree fundamental dimensions: (i) Agentic internal intelligence functions as\nthe system's commander, enabling accurate long-horizon planning through\nreasoning, reflection, and memory; (ii) Agentic external tool invocation,\nwhereby models proactively use various external tools to extend their\nproblem-solving capabilities beyond their intrinsic knowledge; and (iii)\nAgentic environment interaction further situates models within virtual or\nphysical environments, allowing them to take actions, adapt strategies, and\nsustain goal-directed behavior in dynamic real-world scenarios. To further\naccelerate research in this area for the community, we compile open-source\ntraining frameworks, training and evaluation datasets for developing agentic\nMLLMs. Finally, we review the downstream applications of agentic MLLMs and\noutline future research directions for this rapidly evolving field. To\ncontinuously track developments in this rapidly evolving field, we will also\nactively update a public repository at\nhttps://github.com/HJYao00/Awesome-Agentic-MLLMs.",
    "published": "2025-10-13T04:07:01Z",
    "updated": "2025-10-13T04:07:01Z",
    "link": "http://arxiv.org/pdf/2510.10991v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Huanjin Yao",
      "Ruifei Zhang",
      "Jiaxing Huang",
      "Jingyi Zhang",
      "Yibo Wang",
      "Bo Fang",
      "Ruolin Zhu",
      "Yongcheng Jing",
      "Shunyu Liu",
      "Guanbin Li",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10990v1",
    "title": "Secret-Protected Evolution for Differentially Private Synthetic Text\n  Generation",
    "summary": "Text data has become extremely valuable on large language models (LLMs) and\neven lead to general artificial intelligence (AGI). A lot of high-quality text\nin the real world is private and cannot be freely used due to privacy concerns.\nTherefore, differentially private (DP) synthetic text generation has been\nproposed, aiming to produce high-utility synthetic data while protecting\nsensitive information. However, existing DP synthetic text generation imposes\nuniform guarantees that often overprotect non-sensitive content, resulting in\nsubstantial utility loss and computational overhead. Therefore, we propose\nSecret-Protected Evolution (SecPE), a novel framework that extends private\nevolution with secret-aware protection. Theoretically, we show that SecPE\nsatisfies $(\\mathrm{p}, \\mathrm{r})$-secret protection, constituting a\nrelaxation of Gaussian DP that enables tighter utility-privacy trade-offs,\nwhile also substantially reducing computational complexity relative to baseline\nmethods. Empirically, across the OpenReview, PubMed, and Yelp benchmarks, SecPE\nconsistently achieves lower Fr\\'echet Inception Distance (FID) and higher\ndownstream task accuracy than GDP-based Aug-PE baselines, while requiring less\nnoise to attain the same level of protection. Our results highlight that\nsecret-aware guarantees can unlock more practical and effective\nprivacy-preserving synthetic text generation.",
    "published": "2025-10-13T04:05:42Z",
    "updated": "2025-10-13T04:05:42Z",
    "link": "http://arxiv.org/pdf/2510.10990v1.pdf",
    "category": [
      "cs.CR",
      "cs.CL",
      "cs.NE"
    ],
    "authors": [
      "Tianze Wang",
      "Zhaoyu Chen",
      "Jian Du",
      "Yingtai Xiao",
      "Linjun Zhang",
      "Qiang Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.02359v2",
    "title": "Add-One-In: Incremental Sample Selection for Large Language Models via a\n  Choice-Based Greedy Paradigm",
    "summary": "Selecting high-quality and diverse training samples from extensive datasets\nplays a crucial role in reducing training overhead and enhancing the\nperformance of Large Language Models (LLMs). However, existing studies fall\nshort in assessing the overall value of selected data, focusing primarily on\nindividual quality, and struggle to strike an effective balance between\nensuring diversity and minimizing data point traversals. Therefore, this paper\nintroduces a novel choice-based sample selection framework that shifts the\nfocus from evaluating individual sample quality to comparing the contribution\nvalue of different samples when incorporated into the subset. Thanks to the\nadvanced language understanding capabilities of LLMs, we utilize LLMs to\nevaluate the value of each option during the selection process. Furthermore, we\ndesign a greedy sampling process where samples are incrementally added to the\nsubset, thereby improving efficiency by eliminating the need for exhaustive\ntraversal of the entire dataset with the limited budget. Extensive experiments\ndemonstrate that selected data from our method not only surpasses the\nperformance of the full dataset but also achieves competitive results with\nrecent powerful studies, while requiring fewer selections. Moreover, we\nvalidate our approach on a larger medical dataset, highlighting its practical\napplicability in real-world applications. Our code and data are available at\nhttps://github.com/BIRlz/comperative_sample_selection.",
    "published": "2025-03-04T07:32:41Z",
    "updated": "2025-10-13T03:37:25Z",
    "link": "http://arxiv.org/pdf/2503.02359v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhuo Li",
      "Yuhao Du",
      "Xiaoqi Jiao",
      "Yiwen Guo",
      "Yuege Feng",
      "Xiang Wan",
      "Anningzhe Gao",
      "Jinpeng Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10977v1",
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "summary": "Model merging, typically on Instruct and Thinking models, has shown\nremarkable performance for efficient reasoning. In this paper, we\nsystematically revisit the simplest merging method that interpolates two\nweights directly. Particularly, we observe that model interpolation follows a\nthree-stage evolutionary paradigm with distinct behaviors on the reasoning\ntrajectory. These dynamics provide a principled guide for navigating the\nperformance-cost trade-off. Empirical results demonstrate that a strategically\ninterpolated model surprisingly surpasses sophisticated model merging baselines\non both efficiency and effectiveness. We further validate our findings with\nextensive ablation studies on model layers, modules, and decoding strategies.\nUltimately, this work demystifies model interpolation and offers a practical\nframework for crafting models with precisely targeted reasoning capabilities.\nCode is available at \\href{https://github.com/wutaiqiang/MI}{Github}.",
    "published": "2025-10-13T03:30:01Z",
    "updated": "2025-10-13T03:30:01Z",
    "link": "http://arxiv.org/pdf/2510.10977v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Taiqiang Wu",
      "Runming Yang",
      "Tao Liu",
      "Jiahao Wang",
      "Ngai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10974v1",
    "title": "Enhancing Large Language Model Reasoning via Selective Critical Token\n  Fine-Tuning",
    "summary": "Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)\nas a key method to adapt pre-trained models to domain-specific tasks such as\nmathematical reasoning. However, standard SFT uniformly penalizes all tokens,\nneglecting that only a small subset of critical tokens determines reasoning\ncorrectness. This uniform supervision often causes reduced output diversity and\nlimited generalization. We propose Critical Token Fine-tuning (CFT), a simple\nyet effective approach that updates only tokens identified as functionally\nindispensable via counterfactual perturbations. By focusing gradient signals on\nthese decisive reasoning steps while preserving the diversity of non-critical\ntokens, CFT can enhance both generation and diversity. Extensive experiments on\nfive models across three families (Qwen, OLMo, LLaMA) and eleven mathematical\nreasoning benchmarks show that CFT, despite fine-tuning on less than 12% of\ntokens, consistently outperforms standard SFT. Moreover, CFT enables test-time\nscaling through improved sampling diversity and provides a stronger\ninitialization for reinforcement learning, sustaining performance gains in\nlater training stages while maintaining higher entropy for better exploration.\nThese results highlight CFT as a practical and general framework for efficient\nand robust LLM fine-tuning.",
    "published": "2025-10-13T03:25:36Z",
    "updated": "2025-10-13T03:25:36Z",
    "link": "http://arxiv.org/pdf/2510.10974v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhiwen Ruan",
      "Yixia Li",
      "He Zhu",
      "Yun Chen",
      "Peng Li",
      "Yang Liu",
      "Guanhua Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10971v1",
    "title": "RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech\n  Detection",
    "summary": "Hate speech remains prevalent in human society and continues to evolve in its\nforms and expressions. Modern advancements in internet and online anonymity\naccelerate its rapid spread and complicate its detection. However, hate speech\ndatasets exhibit diverse characteristics primarily because they are constructed\nfrom different sources and platforms, each reflecting different linguistic\nstyles and social contexts. Despite this diversity, prior studies on hate\nspeech detection often rely on fixed methodologies without adapting to\ndata-specific features. We introduce RV-HATE, a detection framework designed to\naccount for the dataset-specific characteristics of each hate speech dataset.\nRV-HATE consists of multiple specialized modules, where each module focuses on\ndistinct linguistic or contextual features of hate speech. The framework\nemploys reinforcement learning to optimize weights that determine the\ncontribution of each module for a given dataset. A voting mechanism then\naggregates the module outputs to produce the final decision. RV-HATE offers two\nprimary advantages: (1)~it improves detection accuracy by tailoring the\ndetection process to dataset-specific attributes, and (2)~it also provides\ninterpretable insights into the distinctive features of each dataset.\nConsequently, our approach effectively addresses implicit hate speech and\nachieves superior performance compared to conventional static methods. Our code\nis available at https://github.com/leeyejin1231/RV-HATE.",
    "published": "2025-10-13T03:21:51Z",
    "updated": "2025-10-13T03:21:51Z",
    "link": "http://arxiv.org/pdf/2510.10971v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Yejin Lee",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09255v2",
    "title": "DSPO: Stable and Efficient Policy Optimization for Agentic Search and\n  Reasoning",
    "summary": "Enhancing LLMs with the ability to actively search external knowledge is\ncrucial for complex and real-world tasks. Current approaches either rely on\nprompting to elicit the model's innate agent capabilities, or suffer from\nperformance ceilings and collapse when applying RL to complex interactive\ntasks, leaving their true agentic potential untapped. To address this, we\nintroduce \\textbf{D}ynamic-filter \\textbf{S}equence-level \\textbf{P}olicy\n\\textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust\nagent training through sequence-level optimization and dynamic sample\nfiltering. We train our model purely through RL to interleave multi-turn search\nand reasoning, obviating the need for supervised demonstration data. Across\nmultiple QA benchmarks, our DSPO-trained 7B model improves over a comparable\nprevious work by \\textbf{34.1\\%}, and even outperforms the 14B model from\nprevious work in complex multihop QA such as HotpotQA by nearly \\textbf{9\\%\nrelative}, maintaining exceptional training stability.",
    "published": "2025-10-10T10:53:25Z",
    "updated": "2025-10-13T03:20:43Z",
    "link": "http://arxiv.org/pdf/2510.09255v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Chenyang Gu",
      "Yewen Pu",
      "Bruce Yang",
      "Xiaofan Li",
      "Huan Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.04492v2",
    "title": "Dynamic Optimizations of LLM Ensembles with Two-Stage Reinforcement\n  Learning Agents",
    "summary": "The advancement of LLMs and their accessibility have triggered renewed\ninterest in multi-agent reinforcement learning as robust and adaptive\nframeworks for dynamically changing environments. This paper introduces\nRL-Focal, a two-stage RL agent framework that routes and ensembles LLMs. First,\nwe develop the Decider RL-agent, which learns to dynamically select an ensemble\nof small size ($m_i$) among $N$ LLMs ($m_i \\ll N$) for incoming queries from a\nuser-defined downstream task $i$, by maximizing both error-diversity and\nreasoning-performance of the selected ensemble through iterative updates of\ntask-adaptive rewards and policy. Second, to enable effective fusion of\ndynamically selected LLMs, we develop the stage-2 Fusion RL-agent, which learns\nto resolve reasoning conflicts from different LLMs and dynamically adapts to\ndifferent ensemble teams composed by the Decider Agent for different downstream\ntasks. Third, we introduce the focal diversity metric to better model the error\ncorrelations among multiple LLMs, further improving the generalization\nperformance of the Decider Agent, which actively prunes the ensemble\ncombinations. By focal diversity, we enhance performance across tasks by\neffectively promoting reward-aware and policy-adaptive ensemble selection and\ninference fusion. Extensive evaluations on five benchmarks show that RL-Focal\nachieves the performance improvement of 8.48\\% with an ensemble of small size\ncompared to the best individual LLM in a pool and offers stronger robustness.\nCode is available at https://github.com/sftekin/rl-focal",
    "published": "2025-02-06T20:44:26Z",
    "updated": "2025-10-13T03:17:23Z",
    "link": "http://arxiv.org/pdf/2502.04492v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Selim Furkan Tekin",
      "Fatih Ilhan",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Ling Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10965v1",
    "title": "Judge Before Answer: Can MLLM Discern the False Premise in Question?",
    "summary": "Multimodal large language models (MLLMs) have witnessed astonishing\nadvancements in recent years. Despite these successes, MLLMs remain vulnerable\nto flase premise problems. However, existing benchmarks targeting this issue\nare limited in scope: they often lack fine-grained categorization, exhibit\ninsufficient coverage, and thus fail to provide a rigorous evaluation of the\nability of models to recognize false premises. To bridge this gap, we introduce\na fully automated pipeline for constructing a comprehensive benchmark of false\npremise questions. Our method systematically categorizes the premises into\nthree main types and thirteen subtypes according to the abilities required to\nidentify the premises, resulting in the JBA dataset.Results show current MLLMs\nstill struggle with false premise recognition. Building upon this benchmark, we\nfurther propose a recognition enhancement framework tailored to strengthen the\nrobustness of MLLMs to detect false premises. Extensive experiments demonstrate\nthat models trained with our framework achieve significant improvements in\nfalse premise recognition.",
    "published": "2025-10-13T03:17:00Z",
    "updated": "2025-10-13T03:17:00Z",
    "link": "http://arxiv.org/pdf/2510.10965v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jidong Li",
      "Lingyong Fang",
      "Haodong Zhao",
      "Sufeng Duan",
      "Gongshen Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.20258v2",
    "title": "ARM: Adaptive Reasoning Model",
    "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage.",
    "published": "2025-05-26T17:38:50Z",
    "updated": "2025-10-13T03:13:52Z",
    "link": "http://arxiv.org/pdf/2505.20258v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Siye Wu",
      "Jian Xie",
      "Yikai Zhang",
      "Aili Chen",
      "Kai Zhang",
      "Yu Su",
      "Yanghua Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10961v1",
    "title": "KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification",
    "summary": "Toxic content has become an increasingly critical social issue with the rapid\nexpansion of online communication. While numerous studies explored methods for\ndetecting and detoxifying such content, most have focused primarily on English,\nleaving low-resource language underrepresented. Consequently, Large Language\nModels~(LLMs) often struggle to identify and neutralize toxic expressions in\nthese languages. This challenge becomes even more pronounced when user employ\nobfuscation techniques to evade detection systems. Therefore, we propose a\n\\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to\naddress this issue. We categorize various obfuscation approaches based on\nlinguistic characteristics of Korean and define a set of transformation rules\ngrounded in real-word examples. Using these rules, we construct three dataset\nversions (easy, normal, and hard) representing different levels of obfuscation\ndifficulty. This is the first dataset that simultaneously supports\ndeobfuscation and detoxification for the Korean language. We expect it to\nfacilitate better understanding and mitigating of obfuscated toxic content in\nLLM for low-resource languages. Our code and data are available at\nhttps://github.com/leeyejin1231/KOTOX.",
    "published": "2025-10-13T03:12:37Z",
    "updated": "2025-10-13T03:12:37Z",
    "link": "http://arxiv.org/pdf/2510.10961v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "authors": [
      "Yejin Lee",
      "Su-Hyeon Kim",
      "Hyundong Jin",
      "Dayoung Kim",
      "Yeonsoo Kim",
      "Yo-Sub Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10959v1",
    "title": "Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its\n  Potential for LLM Reinforcement Learning",
    "summary": "Reasoning ability has become a defining capability of Large Language Models\n(LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as\na key paradigm to enhance it. However, RLVR training often suffers from policy\nentropy collapse, where the policy becomes overly deterministic, hindering\nexploration and limiting reasoning performance. While entropy regularization is\na common remedy, its effectiveness is highly sensitive to the fixed\ncoefficient, making it unstable across tasks and models. In this work, we\nrevisit entropy regularization in RLVR and argue that its potential has been\nlargely underestimated. Our analysis shows that (i) tasks of varying difficulty\ndemand distinct exploration intensities, and (ii) balanced exploration may\nrequire the policy entropy to be maintained within a moderate range below its\ninitial level. Therefore, we propose Adaptive Entropy Regularization (AER)--a\nframework that dynamically balances exploration and exploitation via three\ncomponents: difficulty-aware coefficient allocation, initial-anchored target\nentropy, and dynamic global coefficient adjustment. Experiments on multiple\nmathematical reasoning benchmarks show that AER consistently outperforms\nbaselines, improving both reasoning accuracy and exploration capability.",
    "published": "2025-10-13T03:10:26Z",
    "updated": "2025-10-13T03:10:26Z",
    "link": "http://arxiv.org/pdf/2510.10959v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "authors": [
      "Xiaoyun Zhang",
      "Xiaojian Yuan",
      "Di Huang",
      "Wang You",
      "Chen Hu",
      "Jingqing Ruan",
      "Kejiang Chen",
      "Xing Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10951v1",
    "title": "Punctuation-aware treebank tree binarization",
    "summary": "This article presents a curated resource and evaluation suite for\npunctuation-aware treebank binarization. Standard binarization pipelines drop\npunctuation before head selection, which alters constituent shape and harms\nhead-child identification. We release (1) a reproducible pipeline that\npreserves punctuation as sibling nodes prior to binarization, (2) derived\nartifacts and metadata (intermediate @X markers, reversibility signatures,\nalignment indices), and (3) an accompanying evaluation suite covering\nhead-child prediction, round-trip reversibility, and structural compatibility\nwith derivational resources (CCGbank). On the Penn Treebank, punctuation-aware\npreprocessing improves head prediction accuracy from 73.66\\% (Collins rules)\nand 86.66\\% (MLP) to 91.85\\% with the same classifier, and achieves competitive\nalignment against CCGbank derivations. All code, configuration files, and\ndocumentation are released to enable replication and extension to other\ncorpora.",
    "published": "2025-10-13T03:02:38Z",
    "updated": "2025-10-13T03:02:38Z",
    "link": "http://arxiv.org/pdf/2510.10951v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Eitan Klinger",
      "Vivaan Wadhwa",
      "Jungyeul Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10943v1",
    "title": "The Social Cost of Intelligence: Emergence, Propagation, and\n  Amplification of Stereotypical Bias in Multi-Agent Systems",
    "summary": "Bias in large language models (LLMs) remains a persistent challenge,\nmanifesting in stereotyping and unfair treatment across social groups. While\nprior research has primarily focused on individual models, the rise of\nmulti-agent systems (MAS), where multiple LLMs collaborate and communicate,\nintroduces new and largely unexplored dynamics in bias emergence and\npropagation. In this work, we present a comprehensive study of stereotypical\nbias in MAS, examining how internal specialization, underlying LLMs and\ninter-agent communication protocols influence bias robustness, propagation, and\namplification. We simulate social contexts where agents represent different\nsocial groups and evaluate system behavior under various interaction and\nadversarial scenarios. Experiments on three bias benchmarks reveal that MAS are\ngenerally less robust than single-agent systems, with bias often emerging early\nthrough in-group favoritism. However, cooperative and debate-based\ncommunication can mitigate bias amplification, while more robust underlying\nLLMs improve overall system stability. Our findings highlight critical factors\nshaping fairness and resilience in multi-agent LLM systems.",
    "published": "2025-10-13T02:56:42Z",
    "updated": "2025-10-13T02:56:42Z",
    "link": "http://arxiv.org/pdf/2510.10943v1.pdf",
    "category": [
      "cs.MA",
      "cs.CL"
    ],
    "authors": [
      "Thi-Nhung Nguyen",
      "Linhao Luo",
      "Thuy-Trang Vu",
      "Dinh Phung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10936v1",
    "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A\n  Reproducibility Study",
    "summary": "We present a reproducibility study of the state-of-the-art neural\narchitecture for sequence labeling proposed by Ma and Hovy\n(2016)\\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines\ncharacter-level representations via Convolutional Neural Networks (CNNs),\nword-level context modeling through Bi-directional Long Short-Term Memory\nnetworks (BiLSTMs), and structured prediction using Conditional Random Fields\n(CRFs). This end-to-end approach eliminates the need for hand-crafted features\nwhile achieving excellent performance on named entity recognition (NER) and\npart-of-speech (POS) tagging tasks. Our implementation successfully reproduces\nthe key results, achieving 91.18\\% F1-score on CoNLL-2003 NER and demonstrating\nthe model's effectiveness across sequence labeling tasks. We provide a detailed\nanalysis of the architecture components and release an open-source PyTorch\nimplementation to facilitate further research.",
    "published": "2025-10-13T02:49:21Z",
    "updated": "2025-10-13T02:49:21Z",
    "link": "http://arxiv.org/pdf/2510.10936v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Anirudh Ganesh",
      "Jayavardhan Reddy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10930v1",
    "title": "Evaluating Language Models' Evaluations of Games",
    "summary": "Reasoning is not just about solving problems -- it is also about evaluating\nwhich problems are worth solving at all. Evaluations of artificial intelligence\n(AI) systems primarily focused on problem solving, historically by studying how\nmodels play games such as chess and Go. In this paper, we advocate for a new\nparadigm that assesses AI systems' evaluation of games. First, we introduce a\nformalism for evaluating such evaluations. We then leverage a large-scale\ndataset of over $100$ novel board games and over 450 human judgments to compare\nevaluations produced by modern language and reasoning models against those of\npeople and symbolic computational agents. We consider two kinds of evaluative\nqueries: assessing the payoff (or fairness) and the funness of games. These\nqueries span two dimensions relevant to the design of evaluations of AI\nevaluations: how complex a query is to compute and how difficult a query is to\nquantify. Our results show that reasoning models are generally more aligned to\npeople in their evaluations of games than non-reasoning language models.\nHowever, we observe a non-monotonic relationship: as models get closer to\ngame-theoretic optimal, their fit to human data weakens. We also observe more\n\"jaggedness\" across models for assessing funness, in line with the greater\ndifficulty of quantifying this query. Across queries and games, reasoning\nmodels show highly variable and unpredictable resource usage when assessing\nqueries, pointing to the importance of imbuing more resource-rational\nmeta-reasoning in language and reasoning models.",
    "published": "2025-10-13T02:45:37Z",
    "updated": "2025-10-13T02:45:37Z",
    "link": "http://arxiv.org/pdf/2510.10930v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Katherine M. Collins",
      "Cedegao E. Zhang",
      "Graham Todd",
      "Lance Ying",
      "Mauricio Barba da Costa",
      "Ryan Liu",
      "Prafull Sharma",
      "Adrian Weller",
      "Ionatan Kuperwajs",
      "Lionel Wong",
      "Joshua B. Tenenbaum",
      "Thomas L. Griffiths"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10927v1",
    "title": "GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity\n  Recognition",
    "summary": "In biomedical fields, one named entity may consist of a series of\nnon-adjacent tokens and overlap with other entities. Previous methods recognize\ndiscontinuous entities by connecting entity fragments or internal tokens, which\nface challenges of error propagation and decoding ambiguity due to the wide\nvariety of span or word combinations. To address these issues, we deeply\nexplore discontinuous entity structures and propose an effective Gap-aware grid\ntagging model for Discontinuous Named Entity Recognition, named GapDNER. Our\nGapDNER innovatively applies representation learning on the context gaps\nbetween entity fragments to resolve decoding ambiguity and enhance\ndiscontinuous NER performance. Specifically, we treat the context gap as an\nadditional type of span and convert span classification into a token-pair grid\ntagging task. Subsequently, we design two interactive components to\ncomprehensively model token-pair grid features from both intra- and inter-span\nperspectives. The intra-span regularity extraction module employs the biaffine\nmechanism along with linear attention to capture the internal regularity of\neach span, while the inter-span relation enhancement module utilizes\ncriss-cross attention to obtain semantic relations among different spans. At\nthe inference stage of entity decoding, we assign a directed edge to each\nentity fragment and context gap, then use the BFS algorithm to search for all\nvalid paths from the head to tail of grids with entity tags. Experimental\nresults on three datasets demonstrate that our GapDNER achieves new\nstate-of-the-art performance on discontinuous NER and exhibits remarkable\nadvantages in recognizing complex entity structures.",
    "published": "2025-10-13T02:40:14Z",
    "updated": "2025-10-13T02:40:14Z",
    "link": "http://arxiv.org/pdf/2510.10927v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yawen Yang",
      "Fukun Ma",
      "Shiao Meng",
      "Aiwei Liu",
      "Lijie Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10925v1",
    "title": "Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided\n  Multi-Teacher Distillation",
    "summary": "Training student models on synthetic data generated by strong teacher models\nis a promising way to distilling the capabilities of teachers. However, recent\nstudies show that stronger models are not always optimal teachers, revealing a\nmismatch between teacher outputs and student learnability. To address this\nissue, we propose PerSyn (Personalized data Synthesis), a novel synthesis\nstrategy that operates under a new ``Route then Generate'' paradigm to create\ndata tailored to each student model, enabling it to learn more effectively.\nSpecifically, PerSyn first assigns each prompt to its optimal teacher via a\nquery-level router that jointly considers student learnability and teacher\nresponse quality. Each teacher then synthesizes data only for its assigned\nprompts, making the process more efficient than the conventional ``Generate\nthen Select'' paradigm, where all teachers must generate parallel responses for\nthe entire prompt set before constructing the final dataset. Extensive\nexperiments across different model families and scales demonstrate that PerSyn\nconsistently achieves superior or comparable performance to all baselines in\ninstruct tuning and math reasoning settings. Further analysis verifies the\neffectiveness of PerSyn and offers extra insights to propel future research.",
    "published": "2025-10-13T02:36:36Z",
    "updated": "2025-10-13T02:36:36Z",
    "link": "http://arxiv.org/pdf/2510.10925v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Hengyuan Zhang",
      "Shiping Yang",
      "Xiao Liang",
      "Chenming Shang",
      "Yuxuan Jiang",
      "Chaofan Tao",
      "Jing Xiong",
      "Hayden Kwok-Hay So",
      "Ruobing Xie",
      "Angel X. Chang",
      "Ngai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10913v1",
    "title": "ADVICE: Answer-Dependent Verbalized Confidence Estimation",
    "summary": "Recent progress in large language models (LLMs) has enabled them to express\ntheir confidence in natural language, enhancing transparency and reliability.\nHowever, their confidence often exhibits overconfidence, the cause of which\nremains poorly understood. In this work, we conduct a detailed analysis of the\ndynamics underlying verbalized confidence and identify answer-independence as a\nkey factor, defined as the model's failure to condition confidence on its own\nanswer. To address this, we propose ADVICE (Answer-Dependent Verbalized\nConfidence Estimation), a fine-tuning framework that facilitates\nanswer-grounded confidence estimation. Extensive experiments show that ADVICE\nsubstantially improves confidence calibration while preserving task\nperformance. Further analyses confirm that ADVICE strengthens\nanswer-groundedness, leading to more balanced and well-calibrated confidence\ndistributions. Our findings shed light on the origin of overconfidence and\nestablish a framework for more trustworthy confidence verbalization.",
    "published": "2025-10-13T02:18:33Z",
    "updated": "2025-10-13T02:18:33Z",
    "link": "http://arxiv.org/pdf/2510.10913v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Ki Jung Seo",
      "Sehun Lim",
      "Taeuk Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00975v2",
    "title": "Self-Exploring Language Models for Explainable Link Forecasting on\n  Temporal Graphs via Reinforcement Learning",
    "summary": "Forecasting future links is a central task in temporal graph (TG) reasoning,\nrequiring models to leverage historical interactions to predict upcoming ones.\nTraditional neural approaches, such as temporal graph neural networks, achieve\nstrong performance but lack explainability and cannot be applied to unseen\ngraphs without retraining. Recent studies have begun to explore using large\nlanguage models (LLMs) for graph reasoning, but most of them are constrained to\nstatic graphs or small synthetic TGs and lack the evaluation of the quality of\nreasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced\nLearning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that\nfine-tunes LLMs to perform explainable link forecasting on real-world TGs.\nReaL-TG uses outcome-based reward to encourage models to self-explore reasoning\nstrategies from graph structure and to produce explanations that directly\njustify their predictions. To enable evaluation on LLM-generated reasoning\ntraces, we propose a new evaluation protocol combining ranking metrics with an\nLLM-as-a-Judge system that assesses both the quality of reasoning and the\nimpact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning\nQwen3-4B under our framework, show that it outperforms much larger frontier\nLLMs, including GPT-5 mini, on ranking metrics, while producing high-quality\nexplanations confirmed by both the LLM judge and human evaluation.",
    "published": "2025-08-31T19:47:01Z",
    "updated": "2025-10-13T02:09:16Z",
    "link": "http://arxiv.org/pdf/2509.00975v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Zifeng Ding",
      "Shenyang Huang",
      "Zeyu Cao",
      "Emma Kondrup",
      "Zachary Yang",
      "Xingyue Huang",
      "Yuan Sui",
      "Zhangdie Yuan",
      "Yuqicheng Zhu",
      "Xianglong Hu",
      "Yuan He",
      "Farimah Poursafaei",
      "Michael Bronstein",
      "Andreas Vlachos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09279v4",
    "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for\n  Clinically-Aligned Confidence Calibration in Multimodal Large Language Models",
    "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/prompt4trust.",
    "published": "2025-07-12T13:21:10Z",
    "updated": "2025-10-13T02:06:53Z",
    "link": "http://arxiv.org/pdf/2507.09279v4.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Anita Kriz",
      "Elizabeth Laura Janes",
      "Xing Shen",
      "Tal Arbel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09541v2",
    "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
    "summary": "Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.",
    "published": "2025-10-10T16:52:25Z",
    "updated": "2025-10-13T01:58:46Z",
    "link": "http://arxiv.org/pdf/2510.09541v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Chenyu Wang",
      "Paria Rashidinejad",
      "DiJia Su",
      "Song Jiang",
      "Sid Wang",
      "Siyan Zhao",
      "Cai Zhou",
      "Shannon Zejiang Shen",
      "Feiyu Chen",
      "Tommi Jaakkola",
      "Yuandong Tian",
      "Bo Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.15507v5",
    "title": "Steering LLMs for Formal Theorem Proving",
    "summary": "Recent advances in automated theorem proving use Large Language Models (LLMs)\nto translate informal mathematical statements into formal proofs. However,\ninformal cues are often ambiguous or lack strict logical structure, making it\nhard for models to interpret them precisely. While existing methods achieve\nstrong performance, little is known about how LLMs internally represent\ninformal cues, or how these influence proof generation. To address this, we\nexplore \\textit{activation steering}, an inference-time intervention that\nidentifies linear directions in residual activations associated with informal\nreasoning traces and adjusts them to improve proof construction without\nfine-tuning. This mechanism also yields interpretable information about how\nreasoning is internally encoded in the activation space of LLMs. We test our\nmethod for generating formal proofs from already-formalized theorems. Our\ncontributions are twofold: (1) a novel activation-based intervention for\nguiding proof synthesis in LLMs; and (2) demonstration that this intervention\nimproves performance under two decoding strategies (sampling and best-first\nsearch) without any further training.",
    "published": "2025-02-21T15:04:48Z",
    "updated": "2025-10-13T01:51:18Z",
    "link": "http://arxiv.org/pdf/2502.15507v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Shashank Kirtania",
      "Arun Iyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.03748v2",
    "title": "Rethinking the Residual Distribution of Locate-then-Editing Methods in\n  Model Editing",
    "summary": "Model editing enables targeted updates to the knowledge of large language\nmodels (LLMs) with minimal retraining. Among existing approaches,\nlocate-then-edit methods constitute a prominent paradigm: they first identify\ncritical layers, then compute residuals at the final critical layer based on\nthe target edit, and finally apply least-squares-based multi-layer updates via\n$\\textbf{residual distribution}$. While empirically effective, we identify a\ncounterintuitive failure mode: residual distribution, a core mechanism in these\nmethods, introduces weight shift errors that undermine editing precision.\nThrough theoretical and empirical analysis, we show that such errors increase\nwith the distribution distance, batch size, and edit sequence length,\nultimately leading to inaccurate or suboptimal edits. To address this, we\npropose the $\\textbf{B}$oundary $\\textbf{L}$ayer $\\textbf{U}$pdat$\\textbf{E\n(BLUE)}$ strategy to enhance locate-then-edit methods. Sequential batch editing\nexperiments on three LLMs and two datasets demonstrate that BLUE not only\ndelivers an average performance improvement of 35.59\\%, significantly advancing\nthe state of the art in model editing, but also enhances the preservation of\nLLMs' general capabilities. Our code is available at\nhttps://github.com/xpq-tech/BLUE.",
    "published": "2025-02-06T03:20:17Z",
    "updated": "2025-10-13T01:45:48Z",
    "link": "http://arxiv.org/pdf/2502.03748v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xiaopeng Li",
      "Shanwen Wang",
      "Shasha Li",
      "Shezheng Song",
      "Bin Ji",
      "Jun Ma",
      "Jie Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10890v1",
    "title": "LLM$\\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation\n  through a MCP-Driven Hierarchically Modular Agent System",
    "summary": "We introduce LLM x MapReduce-V3, a hierarchically modular agent system\ndesigned for long-form survey generation. Building on the prior work, LLM x\nMapReduce-V2, this version incorporates a multi-agent architecture where\nindividual functional components, such as skeleton initialization, digest\nconstruction, and skeleton refinement, are implemented as independent\nmodel-context-protocol (MCP) servers. These atomic servers can be aggregated\ninto higher-level servers, creating a hierarchically structured system. A\nhigh-level planner agent dynamically orchestrates the workflow by selecting\nappropriate modules based on their MCP tool descriptions and the execution\nhistory. This modular decomposition facilitates human-in-the-loop intervention,\naffording users greater control and customization over the research process.\nThrough a multi-turn interaction, the system precisely captures the intended\nresearch perspectives to generate a comprehensive skeleton, which is then\ndeveloped into an in-depth survey. Human evaluations demonstrate that our\nsystem surpasses representative baselines in both content depth and length,\nhighlighting the strength of MCP-based modular planning.",
    "published": "2025-10-13T01:38:37Z",
    "updated": "2025-10-13T01:38:37Z",
    "link": "http://arxiv.org/pdf/2510.10890v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yu Chao",
      "Siyu Lin",
      "xiaorong wang",
      "Zhu Zhang",
      "Zihan Zhou",
      "Haoyu Wang",
      "Shuo Wang",
      "Jie Zhou",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10885v1",
    "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time\n  Scaling Strategies in Text2SQL Tasks",
    "summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)\nsystems, enabling non-expert users to query industrial databases using natural\nlanguage. While test-time scaling strategies have shown promise in LLM-based\nsolutions, their effectiveness in real-world applications, especially with the\nlatest reasoning models, remains uncertain. In this work, we benchmark six\nlightweight, industry-oriented test-time scaling strategies and four LLMs,\nincluding two reasoning models, evaluating their performance on the BIRD\nMini-Dev benchmark. Beyond standard accuracy metrics, we also report inference\nlatency and token consumption, providing insights relevant for practical system\ndeployment. Our findings reveal that Divide-and-Conquer prompting and few-shot\ndemonstrations consistently enhance performance for both general-purpose and\nreasoning-focused LLMs. However, introducing additional workflow steps yields\nmixed results, and base model selection plays a critical role. This work sheds\nlight on the practical trade-offs between accuracy, efficiency, and complexity\nwhen deploying Text2SQL systems.",
    "published": "2025-10-13T01:29:54Z",
    "updated": "2025-10-13T01:29:54Z",
    "link": "http://arxiv.org/pdf/2510.10885v1.pdf",
    "category": [
      "cs.CL",
      "cs.DB"
    ],
    "authors": [
      "Jiajing Guo",
      "Kenil Patel",
      "Jorge Piazentin Ono",
      "Wenbin He",
      "Liu Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.15267v2",
    "title": "When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep\n  Secret or Forget Knowledge?",
    "summary": "The deployment of large language models (LLMs) like ChatGPT and Gemini has\nshown their powerful natural language generation capabilities. However, these\nmodels can inadvertently learn and retain sensitive information and harmful\ncontent during training, raising significant ethical and legal concerns. To\naddress these issues, machine unlearning has been introduced as a potential\nsolution. While existing unlearning methods take into account the specific\ncharacteristics of LLMs, they often suffer from high computational demands,\nlimited applicability, or the risk of catastrophic forgetting. To address these\nlimitations, we propose a lightweight behavioral unlearning framework based on\nRetrieval-Augmented Generation (RAG) technology. By modifying the external\nknowledge base of RAG, we simulate the effects of forgetting without directly\ninteracting with the unlearned LLM. We approach the construction of unlearned\nknowledge as a constrained optimization problem, deriving two key components\nthat underpin the effectiveness of RAG-based unlearning. This RAG-based\napproach is particularly effective for closed-source LLMs, where existing\nunlearning methods often fail. We evaluate our framework through extensive\nexperiments on both open-source and closed-source models, including ChatGPT,\nGemini, Llama-2-7b-chat, and PaLM 2. The results demonstrate that our approach\nmeets five key unlearning criteria: effectiveness, universality, harmlessness,\nsimplicity, and robustness. Meanwhile, this approach can extend to multimodal\nlarge language models and LLM-based agents.",
    "published": "2024-10-20T03:51:01Z",
    "updated": "2025-10-13T01:20:51Z",
    "link": "http://arxiv.org/pdf/2410.15267v2.pdf",
    "category": [
      "cs.CR",
      "cs.CL"
    ],
    "authors": [
      "Shang Wang",
      "Tianqing Zhu",
      "Dayong Ye",
      "Wanlei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01951v2",
    "title": "Self-ensemble: Mitigating Confidence Mis-calibration for Large Language\n  Models",
    "summary": "Although Large Language Models (LLMs) perform well in general fields, they\nexhibit a confidence distortion problem on multi-choice question-answering\n(MCQA), particularly as the number of answer choices increases. Specifically,\non MCQA with many choices, LLMs suffer from under-confidence in correct\npredictions and over-confidence in incorrect ones, leading to a substantially\ndegraded performance. To solve this problem, we propose Self-ensemble in this\nwork. Our method splits the choices into several groups and ensembles LLM\npredictions across these groups to reach a final decision. The advantage of\nSelf-ensemble is its plug-and-play nature, where it can be integrated into\nexisting LLM architecture based on a designed attention mask and positional\nencoding, without requiring labeled datasets for parameter tuning. Experimental\nresults on three LLMs and datasets demonstrate that Self-ensemble\ncomprehensively addresses the confidence distortion problem of LLMs,\noutperforming standard inference as well as baseline methods.",
    "published": "2025-06-02T17:59:29Z",
    "updated": "2025-10-13T01:06:12Z",
    "link": "http://arxiv.org/pdf/2506.01951v2.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Zicheng Xu",
      "Guanchu Wang",
      "Guangyao Zheng",
      "Yu-Neng Chuang",
      "Alexander Szalay",
      "Xia Hu",
      "Vladimir Braverman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.16783v3",
    "title": "SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven\n  Evaluation of Political and Demographic Perspectives in LLMs",
    "summary": "As increasingly capable large language models (LLMs) emerge, researchers have\nbegun exploring their potential for subjective tasks. While recent work\ndemonstrates that LLMs can be aligned with diverse human perspectives,\nevaluating this alignment on downstream tasks (e.g., hate speech detection)\nremains challenging due to the use of inconsistent datasets across studies. To\naddress this issue, in this resource paper we propose a two-step framework: we\n(1) introduce SubData, an open-source Python library designed for standardizing\nheterogeneous datasets to evaluate LLMs perspective alignment; and (2) present\na theory-driven approach leveraging this library to test how\ndifferently-aligned LLMs (e.g., aligned with different political viewpoints)\nclassify content targeting specific demographics. SubData's flexible mapping\nand taxonomy enable customization for diverse research needs, distinguishing it\nfrom existing resources. We illustrate its usage with an example application\nand invite contributions to extend our initial release into a multi-construct\nbenchmark suite for evaluating LLMs perspective alignment on natural language\nprocessing tasks.",
    "published": "2024-12-21T21:40:31Z",
    "updated": "2025-10-12T23:38:59Z",
    "link": "http://arxiv.org/pdf/2412.16783v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Pietro Bernardelle",
      "Leon Fröhling",
      "Stefano Civelli",
      "Gianluca Demartini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10846v1",
    "title": "DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language\n  Models",
    "summary": "As vision-language models become increasingly capable, maintaining a balance\nbetween safety and usefulness remains a central challenge. Safety mechanisms,\nwhile essential, can backfire, causing over-refusal, where models decline\nbenign requests out of excessive caution. Yet, no existing benchmark has\nsystematically addressed over-refusal in the visual modality. This setting\nintroduces unique challenges, such as dual-use cases where an instruction is\nharmless, but the accompanying image contains harmful content. Models\nfrequently fail in such scenarios, either refusing too conservatively or\ncompleting tasks unsafely, which highlights the need for more fine-grained\nalignment. The ideal behavior is safe completion, i.e., fulfilling the benign\nparts of a request while explicitly warning about any potentially harmful\nelements. To address this, we present DUAL-Bench, the first multimodal\nbenchmark focused on over-refusal and safe completion in VLMs. We evaluated 18\nVLMs across 12 hazard categories, with focus on their robustness under\nsemantics-preserving visual perturbations. The results reveal substantial room\nfor improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models\naverage 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster\nthe development of more nuanced alignment strategies that ensure models remain\nboth safe and useful in complex multimodal settings.",
    "published": "2025-10-12T23:21:34Z",
    "updated": "2025-10-12T23:21:34Z",
    "link": "http://arxiv.org/pdf/2510.10846v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kaixuan Ren",
      "Preslav Nakov",
      "Usman Naseem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.19073v4",
    "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral\n  Reasoning of LLMs through Multi-hop Hate Speech Explanation",
    "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via multi-hop hate speech explanation\nusing the Moral Foundations Theory. MFTCXplain comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Our results show a\nmisalignment between LLM outputs and human annotations in moral reasoning\ntasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their\nability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore,\nrationale alignment remains limited mainly in underrepresented languages. Our\nfindings show the limited capacity of current LLMs to internalize and reflect\nhuman moral reasoning",
    "published": "2025-06-23T19:44:21Z",
    "updated": "2025-10-12T23:04:15Z",
    "link": "http://arxiv.org/pdf/2506.19073v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jackson Trager",
      "Francielle Vargas",
      "Diego Alves",
      "Matteo Guida",
      "Mikel K. Ngueajio",
      "Ameeta Agrawal",
      "Yalda Daryani",
      "Farzan Karimi-Malekabadi",
      "Flor Miriam Plaza-del-Arco"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10827v1",
    "title": "Happiness is Sharing a Vocabulary: A Study of Transliteration Methods",
    "summary": "Transliteration has emerged as a promising means to bridge the gap between\nvarious languages in multilingual NLP, showing promising results especially for\nlanguages using non-Latin scripts. We investigate the degree to which shared\nscript, overlapping token vocabularies, and shared phonology contribute to\nperformance of multilingual models. To this end, we conduct controlled\nexperiments using three kinds of transliteration (romanization, phonemic\ntranscription, and substitution ciphers) as well as orthography. We evaluate\neach model on two downstream tasks -- named entity recognition (NER) and\nnatural language inference (NLI) -- and find that romanization significantly\noutperforms other input types in 7 out of 8 evaluation settings, largely\nconsistent with our hypothesis that it is the most effective approach. We\nfurther analyze how each factor contributed to the success, and suggest that\nhaving longer (subword) tokens shared with pre-trained languages leads to\nbetter utilization of the model.",
    "published": "2025-10-12T22:34:40Z",
    "updated": "2025-10-12T22:34:40Z",
    "link": "http://arxiv.org/pdf/2510.10827v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Haeji Jung",
      "Jinju Kim",
      "Kyungjin Kim",
      "Youjeong Roh",
      "David R. Mortensen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10815v1",
    "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems",
    "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.",
    "published": "2025-10-12T21:42:04Z",
    "updated": "2025-10-12T21:42:04Z",
    "link": "http://arxiv.org/pdf/2510.10815v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.SC"
    ],
    "authors": [
      "Meiru Zhang",
      "Philipp Borchert",
      "Milan Gritta",
      "Gerasimos Lampouras"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10806v1",
    "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based\n  Structures",
    "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.",
    "published": "2025-10-12T20:52:43Z",
    "updated": "2025-10-12T20:52:43Z",
    "link": "http://arxiv.org/pdf/2510.10806v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Mihir Gupte",
      "Paolo Giusto",
      "Ramesh S"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10801v1",
    "title": "Toward Human-Centered Readability Evaluation",
    "summary": "Text simplification is essential for making public health information\naccessible to diverse populations, including those with limited health\nliteracy. However, commonly used evaluation metrics in Natural Language\nProcessing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level\nfeatures and fail to account for human-centered qualities like clarity,\ntrustworthiness, tone, cultural relevance, and actionability. This limitation\nis particularly critical in high-stakes health contexts, where communication\nmust be not only simple but also usable, respectful, and trustworthy. To\naddress this gap, we propose the Human-Centered Readability Score (HCRS), a\nfive-dimensional evaluation framework grounded in Human-Computer Interaction\n(HCI) and health communication research. HCRS integrates automatic measures\nwith structured human feedback to capture the relational and contextual aspects\nof readability. We outline the framework, discuss its integration into\nparticipatory evaluation workflows, and present a protocol for empirical\nvalidation. This work aims to advance the evaluation of health text\nsimplification beyond surface metrics, enabling NLP systems that align more\nclosely with diverse users' needs, expectations, and lived experiences.",
    "published": "2025-10-12T20:38:32Z",
    "updated": "2025-10-12T20:38:32Z",
    "link": "http://arxiv.org/pdf/2510.10801v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Bahar İlgen",
      "Georges Hattab"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.08093v2",
    "title": "Culturally transmitted color categories in LLMs reflect a learning bias\n  toward efficient compression",
    "summary": "Converging evidence suggests that systems of semantic categories across human\nlanguages achieve near-optimal compression via the Information Bottleneck (IB)\ncomplexity-accuracy principle. Large language models (LLMs) are not trained for\nthis objective, which raises the question: are LLMs capable of evolving\nefficient human-like semantic systems? To address this question, we focus on\nthe domain of color as a key testbed of cognitive theories of categorization\nand replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two\ninfluential human behavioral studies. First, we conduct an English color-naming\nstudy, showing that Gemini aligns well with the naming patterns of native\nEnglish speakers and achieves a significantly high IB-efficiency score, while\nLlama exhibits an efficient but lower complexity system compared to English.\nSecond, to test whether LLMs simply mimic patterns in their training data or\nactually exhibit a human-like inductive bias toward IB-efficiency, we simulate\ncultural evolution of pseudo color-naming systems in LLMs via iterated\nin-context language learning. We find that akin to humans, LLMs iteratively\nrestructure initially random systems towards greater IB-efficiency and\nincreased alignment with patterns observed across the world's languages. These\nfindings demonstrate that LLMs are capable of evolving perceptually grounded,\nhuman-like semantic systems, driven by the same fundamental principle that\ngoverns semantic efficiency across human languages.",
    "published": "2025-09-09T19:00:10Z",
    "updated": "2025-10-12T20:33:56Z",
    "link": "http://arxiv.org/pdf/2509.08093v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nathaniel Imel",
      "Noga Zaslavsky"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13500v2",
    "title": "Noise Injection Systemically Degrades Large Language Model Safety\n  Guardrails",
    "summary": "Safety guardrails in large language models (LLMs) are a critical component in\npreventing harmful outputs. Yet, their resilience under perturbation remains\npoorly understood. In this paper, we investigate the robustness of safety\nfine-tuning in LLMs by systematically injecting Gaussian noise into model\nactivations. We show across multiple open-weight models that (1) Gaussian noise\nraises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety\nfine-tuning affords no extra protection, and (3) that chain-of-thought\nreasoning remains largely intact. The findings reveal critical vulnerabilities\nin current safety alignment techniques and highlight the potential of\nreasoning-based and reinforcement learning approaches as promising direction\nfor developing more robust AI safety systems. These results have important\nimplications for real-world deployment of LLMs in safety-critical applications\nas these results imply that widely-deployed safety tuning methods can fail even\nwithout adversarial prompts.",
    "published": "2025-05-16T01:33:25Z",
    "updated": "2025-10-12T20:22:06Z",
    "link": "http://arxiv.org/pdf/2505.13500v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Prithviraj Singh Shahani",
      "Kaveh Eskandari Miandoab",
      "Matthias Scheutz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.02785v2",
    "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for\n  Breaking the Efficiency-Quality Trade-off",
    "summary": "This paper introduces DrDiff, a novel framework for long-text generation that\novercomes the efficiency-quality trade-off through three core technologies.\nFirst, we design a dynamic expert scheduling mechanism that intelligently\nallocates computational resources during the diffusion process based on text\ncomplexity, enabling more efficient handling of text generation tasks of\nvarying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)\nmechanism that adaptively adjusts attention patterns according to a variety of\ninput lengths, reducing computational complexity from O($n^2$) to O($n$) while\nmaintaining model performance. Finally, we propose a soft absorption guidance\noptimization strategy that combines with DPM-solver++ to reduce diffusion\nsteps, significantly improving generation speed. Comprehensive experiments on\nvarious long-text generation benchmarks demonstrate the superiority of our\nDrDiff over the existing SOTA methods.",
    "published": "2025-09-02T19:38:49Z",
    "updated": "2025-10-12T20:19:21Z",
    "link": "http://arxiv.org/pdf/2509.02785v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jusheng Zhang",
      "Yijia Fan",
      "Kaitong Cai",
      "Zimeng Huang",
      "Xiaofei Sun",
      "Jian Wang",
      "Chengpei Tang",
      "Keze Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10787v1",
    "title": "Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG",
    "summary": "The performance gains of LLMs have historically been driven by scaling up\nmodel size and training data. However, the rapidly diminishing availability of\nhigh-quality training data is introducing a fundamental bottleneck, shifting\nthe focus of research toward inference-time scaling. This paradigm uses\nadditional computation at the time of deployment to substantially improve LLM\nperformance on downstream tasks without costly model re-training. This review\nsystematically surveys the diverse techniques contributing to this new era of\ninference-time scaling, organizing the rapidly evolving field into two\ncomprehensive perspectives: Output-focused and Input-focused methods.\nOutput-focused techniques encompass complex, multi-step generation strategies,\nincluding reasoning (e.g., CoT, ToT, ReAct), various search and decoding\nmethods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),\nand model ensemble methods. Input-focused techniques are primarily categorized\nby few-shot and RAG, with RAG as the central focus. The RAG section is further\ndetailed through a structured examination of query expansion, data, retrieval\nand reranker, LLM generation methods, and multi-modal RAG.",
    "published": "2025-10-12T20:09:07Z",
    "updated": "2025-10-12T20:09:07Z",
    "link": "http://arxiv.org/pdf/2510.10787v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Zhichao Wang",
      "Cheng Wan",
      "Dong Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10776v1",
    "title": "HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon",
    "summary": "The language of Hiligaynon, spoken predominantly by the people of Panay\nIsland, Negros Occidental, and Soccsksargen in the Philippines, remains\nunderrepresented in language processing research due to the absence of\nannotated corpora and baseline models. This study introduces HiligayNER, the\nfirst publicly available baseline model for the task of Named Entity\nRecognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains\nover 8,000 annotated sentences collected from publicly available news articles,\nsocial media posts, and literary texts. Two Transformer-based models, mBERT and\nXLM-RoBERTa, were fine-tuned on this collected corpus to build versions of\nHiligayNER. Evaluation results show strong performance, with both models\nachieving over 80% in precision, recall, and F1-score across entity types.\nFurthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates\npromising transferability, suggesting the broader applicability of HiligayNER\nfor multilingual NLP in low-resource settings. This work aims to contribute to\nlanguage technology development for underrepresented Philippine languages,\nspecifically for Hiligaynon, and support future research in regional language\nprocessing.",
    "published": "2025-10-12T19:34:22Z",
    "updated": "2025-10-12T19:34:22Z",
    "link": "http://arxiv.org/pdf/2510.10776v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "James Ald Teves",
      "Ray Daniel Cal",
      "Josh Magdiel Villaluz",
      "Jean Malolos",
      "Mico Magtira",
      "Ramon Rodriguez",
      "Mideth Abisado",
      "Joseph Marvin Imperial"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10762v1",
    "title": "Large Language Models for Full-Text Methods Assessment: A Case Study on\n  Mediation Analysis",
    "summary": "Systematic reviews are crucial for synthesizing scientific evidence but\nremain labor-intensive, especially when extracting detailed methodological\ninformation. Large language models (LLMs) offer potential for automating\nmethodological assessments, promising to transform evidence synthesis. Here,\nusing causal mediation analysis as a representative methodological domain, we\nbenchmarked state-of-the-art LLMs against expert human reviewers across 180\nfull-text scientific articles. Model performance closely correlated with human\njudgments (accuracy correlation 0.71; F1 correlation 0.97), achieving\nnear-human accuracy on straightforward, explicitly stated methodological\ncriteria. However, accuracy sharply declined on complex, inference-intensive\nassessments, lagging expert reviewers by up to 15%. Errors commonly resulted\nfrom superficial linguistic cues -- for instance, models frequently\nmisinterpreted keywords like \"longitudinal\" or \"sensitivity\" as automatic\nevidence of rigorous methodological approache, leading to systematic\nmisclassifications. Longer documents yielded lower model accuracy, whereas\npublication year showed no significant effect. Our findings highlight an\nimportant pattern for practitioners using LLMs for methods review and synthesis\nfrom full texts: current LLMs excel at identifying explicit methodological\nfeatures but require human oversight for nuanced interpretations. Integrating\nautomated information extraction with targeted expert review thus provides a\npromising approach to enhance efficiency and methodological rigor in evidence\nsynthesis across diverse scientific fields.",
    "published": "2025-10-12T19:04:22Z",
    "updated": "2025-10-12T19:04:22Z",
    "link": "http://arxiv.org/pdf/2510.10762v1.pdf",
    "category": [
      "cs.CL",
      "stat.AP"
    ],
    "authors": [
      "Wenqing Zhang",
      "Trang Nguyen",
      "Elizabeth A. Stuart",
      "Yiqun T. Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21750v3",
    "title": "Adversarial Defence without Adversarial Defence: Enhancing Language\n  Model Robustness via Instance-level Principal Component Removal",
    "summary": "Pre-trained language models (PLMs) have driven substantial progress in\nnatural language processing but remain vulnerable to adversarial attacks,\nraising concerns about their robustness in real-world applications. Previous\nstudies have sought to mitigate the impact of adversarial attacks by\nintroducing adversarial perturbations into the training process, either\nimplicitly or explicitly. While both strategies enhance robustness, they often\nincur high computational costs. In this work, we propose a simple yet effective\nadd-on module that enhances the adversarial robustness of PLMs by removing\ninstance-level principal components, without relying on conventional\nadversarial defences or perturbing the original training data. Our approach\ntransforms the embedding space to approximate Gaussian properties, thereby\nreducing its susceptibility to adversarial perturbations while preserving\nsemantic relationships. This transformation aligns embedding distributions in a\nway that minimises the impact of adversarial noise on decision boundaries,\nenhancing robustness without requiring adversarial examples or costly\ntraining-time augmentation. Evaluations on eight benchmark datasets show that\nour approach improves adversarial robustness while maintaining comparable\nbefore-attack accuracy to baselines, achieving a balanced trade-off between\nrobustness and generalisation.",
    "published": "2025-07-29T12:31:26Z",
    "updated": "2025-10-12T19:01:47Z",
    "link": "http://arxiv.org/pdf/2507.21750v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yang Wang",
      "Chenghao Xiao",
      "Yizhi Li",
      "Stuart E. Middleton",
      "Noura Al Moubayed",
      "Chenghua Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10729v1",
    "title": "Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular\n  Deep Learning Framework",
    "summary": "Sarcasm is a nuanced and often misinterpreted form of communication,\nespecially in text, where tone and body language are absent. This paper\nproposes a modular deep learning framework for sarcasm detection, leveraging\nDeep Convolutional Neural Networks (DCNNs) and contextual models such as BERT\nto analyze linguistic, emotional, and contextual cues. The system integrates\nsentiment analysis, contextual embeddings, linguistic feature extraction, and\nemotion detection through a multi-layer architecture. While the model is in the\nconceptual stage, it demonstrates feasibility for real-world applications such\nas chatbots and social media analysis.",
    "published": "2025-10-12T18:02:50Z",
    "updated": "2025-10-12T18:02:50Z",
    "link": "http://arxiv.org/pdf/2510.10729v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Manas Zambre",
      "Sarika Bobade"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11717v1",
    "title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event\n  Streams",
    "summary": "Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.",
    "published": "2025-10-13T17:59:55Z",
    "updated": "2025-10-13T17:59:55Z",
    "link": "http://arxiv.org/pdf/2510.11717v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Takuya Nakabayashi",
      "Navami Kairanda",
      "Hideo Saito",
      "Vladislav Golyanik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11715v1",
    "title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models",
    "summary": "Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.",
    "published": "2025-10-13T17:59:46Z",
    "updated": "2025-10-13T17:59:46Z",
    "link": "http://arxiv.org/pdf/2510.11715v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ayush Shrivastava",
      "Sanyam Mehta",
      "Daniel Geng",
      "Andrew Owens"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11712v1",
    "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
    "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
    "published": "2025-10-13T17:59:15Z",
    "updated": "2025-10-13T17:59:15Z",
    "link": "http://arxiv.org/pdf/2510.11712v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haoran Feng",
      "Dizhe Zhang",
      "Xiangtai Li",
      "Bo Du",
      "Lu Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11704v1",
    "title": "Bayesian Topological Convolutional Neural Nets",
    "summary": "Convolutional neural networks (CNNs) have been established as the main\nworkhorse in image data processing; nonetheless, they require large amounts of\ndata to train, often produce overconfident predictions, and frequently lack the\nability to quantify the uncertainty of their predictions. To address these\nconcerns, we propose a new Bayesian topological CNN that promotes a novel\ninterplay between topology-aware learning and Bayesian sampling. Specifically,\nit utilizes information from important manifolds to accelerate training while\nreducing calibration error by placing prior distributions on network parameters\nand properly learning appropriate posteriors. One important contribution of our\nwork is the inclusion of a consistency condition in the learning cost, which\ncan effectively modify the prior distributions to improve the performance of\nour novel network architecture. We evaluate the model on benchmark image\nclassification datasets and demonstrate its superiority over conventional CNNs,\nBayesian neural networks (BNNs), and topological CNNs. In particular, we supply\nevidence that our method provides an advantage in situations where training\ndata is limited or corrupted. Furthermore, we show that the new model allows\nfor better uncertainty quantification than standard BNNs since it can more\nreadily identify examples of out-of-distribution data on which it has not been\ntrained. Our results highlight the potential of our novel hybrid approach for\nmore efficient and robust image classification.",
    "published": "2025-10-13T17:57:43Z",
    "updated": "2025-10-13T17:57:43Z",
    "link": "http://arxiv.org/pdf/2510.11704v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sarah Harkins Dayton",
      "Hayden Everett",
      "Ioannis Schizas",
      "David L. Boothe Jr.",
      "Vasileios Maroulas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11690v1",
    "title": "Diffusion Transformers with Representation Autoencoders",
    "summary": "Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.",
    "published": "2025-10-13T17:51:39Z",
    "updated": "2025-10-13T17:51:39Z",
    "link": "http://arxiv.org/pdf/2510.11690v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Boyang Zheng",
      "Nanye Ma",
      "Shengbang Tong",
      "Saining Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11687v1",
    "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape\n  Estimation from a Single View",
    "summary": "Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI.",
    "published": "2025-10-13T17:49:15Z",
    "updated": "2025-10-13T17:49:15Z",
    "link": "http://arxiv.org/pdf/2510.11687v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jinyu Zhang",
      "Haitao Lin",
      "Jiashu Hou",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11650v1",
    "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
    "summary": "Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
    "published": "2025-10-13T17:29:55Z",
    "updated": "2025-10-13T17:29:55Z",
    "link": "http://arxiv.org/pdf/2510.11650v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuxuan Xue",
      "Xianghui Xie",
      "Margaret Kostyrko",
      "Gerard Pons-Moll"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11649v1",
    "title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from\n  a Single Image",
    "summary": "Reconstructing metrically accurate humans and their surrounding scenes from a\nsingle image is crucial for virtual reality, robotics, and comprehensive 3D\nscene understanding. However, existing methods struggle with depth ambiguity,\nocclusions, and physically inconsistent contacts. To address these challenges,\nwe introduce PhySIC, a framework for physically plausible Human-Scene\nInteraction and Contact reconstruction. PhySIC recovers metrically consistent\nSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within\na shared coordinate frame from a single RGB image. Starting from coarse\nmonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,\nfuses visible depth with unscaled geometry for a robust metric scaffold, and\nsynthesizes missing support surfaces like floors. A confidence-weighted\noptimization refines body pose, camera parameters, and global scale by jointly\nenforcing depth alignment, contact priors, interpenetration avoidance, and 2D\nreprojection consistency. Explicit occlusion masking safeguards invisible\nregions against implausible configurations. PhySIC is efficient, requiring only\n9 seconds for joint human-scene optimization and under 27 seconds end-to-end.\nIt naturally handles multiple humans, enabling reconstruction of diverse\ninteractions. Empirically, PhySIC outperforms single-image baselines, reducing\nmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,\nand improving contact F1 from 0.09 to 0.51. Qualitative results show realistic\nfoot-floor interactions, natural seating, and plausible reconstructions of\nheavily occluded furniture. By converting a single image into a physically\nplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.\nOur implementation is publicly available at https://yuxuan-xue.com/physic.",
    "published": "2025-10-13T17:29:51Z",
    "updated": "2025-10-13T17:29:51Z",
    "link": "http://arxiv.org/pdf/2510.11649v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pradyumna Yalandur Muralidhar",
      "Yuxuan Xue",
      "Xianghui Xie",
      "Margaret Kostyrko",
      "Gerard Pons-Moll"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11647v1",
    "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
    "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
    "published": "2025-10-13T17:27:08Z",
    "updated": "2025-10-13T17:27:08Z",
    "link": "http://arxiv.org/pdf/2510.11647v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yinan Chen",
      "Jiangning Zhang",
      "Teng Hu",
      "Yuxiang Zeng",
      "Zhucun Xue",
      "Qingdong He",
      "Chengjie Wang",
      "Yong Liu",
      "Xiaobin Hu",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01875v3",
    "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
    "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
    "published": "2025-08-03T18:15:42Z",
    "updated": "2025-10-13T17:15:14Z",
    "link": "http://arxiv.org/pdf/2508.01875v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haolin Yang",
      "Feilong Tang",
      "Lingxiao Zhao",
      "Xiang An",
      "Ming Hu",
      "Huifa Li",
      "Xinlin Zhuang",
      "Yifan Lu",
      "Xiaofeng Zhang",
      "Abdalla Swikir",
      "Junjun He",
      "Zongyuan Ge",
      "Imran Razzak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11613v1",
    "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid\n  Network",
    "summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.",
    "published": "2025-10-13T16:52:32Z",
    "updated": "2025-10-13T16:52:32Z",
    "link": "http://arxiv.org/pdf/2510.11613v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Feng Zhang",
      "Haoyou Deng",
      "Zhiqiang Li",
      "Lida Li",
      "Bin Xu",
      "Qingbo Lu",
      "Zisheng Cao",
      "Minchen Wei",
      "Changxin Gao",
      "Nong Sang",
      "Xiang Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11606v1",
    "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
    "published": "2025-10-13T16:45:28Z",
    "updated": "2025-10-13T16:45:28Z",
    "link": "http://arxiv.org/pdf/2510.11606v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yicheng Xu",
      "Yue Wu",
      "Jiashuo Yu",
      "Ziang Yan",
      "Tianxiang Jiang",
      "Yinan He",
      "Qingsong Zhao",
      "Kai Chen",
      "Yu Qiao",
      "Limin Wang",
      "Manabu Okumura",
      "Yi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11605v1",
    "title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through\n  Query Pre-Training",
    "summary": "Scene coordinate regression (SCR) has established itself as a promising\nlearning-based approach to visual relocalization. After mere minutes of\nscene-specific training, SCR models estimate camera poses of query images with\nhigh accuracy. Still, SCR methods fall short of the generalization capabilities\nof more classical feature-matching approaches. When imaging conditions of query\nimages, such as lighting or viewpoint, are too different from the training\nviews, SCR models fail. Failing to generalize is an inherent limitation of\nprevious SCR frameworks, since their training objective is to encode the\ntraining views in the weights of the coordinate regressor itself. The regressor\nessentially overfits to the training views, by design. We propose to separate\nthe coordinate regressor and the map representation into a generic transformer\nand a scene-specific map code. This separation allows us to pre-train the\ntransformer on tens of thousands of scenes. More importantly, it allows us to\ntrain the transformer to generalize from mapping images to unseen query images\nduring pre-training. We demonstrate on multiple challenging relocalization\ndatasets that our method, ACE-G, leads to significantly increased robustness\nwhile keeping the computational footprint attractive.",
    "published": "2025-10-13T16:45:17Z",
    "updated": "2025-10-13T16:45:17Z",
    "link": "http://arxiv.org/pdf/2510.11605v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Leonard Bruns",
      "Axel Barroso-Laguna",
      "Tommaso Cavallari",
      "Áron Monszpart",
      "Sowmya Munukutla",
      "Victor Adrian Prisacariu",
      "Eric Brachmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15818v2",
    "title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition",
    "summary": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.",
    "published": "2025-05-21T17:59:56Z",
    "updated": "2025-10-13T16:36:15Z",
    "link": "http://arxiv.org/pdf/2505.15818v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yijie Zheng",
      "Weijie Wu",
      "Qingyun Li",
      "Xuehui Wang",
      "Xu Zhou",
      "Aiai Ren",
      "Jun Shen",
      "Long Zhao",
      "Guoqing Li",
      "Xue Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11579v1",
    "title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis",
    "summary": "Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.",
    "published": "2025-10-13T16:23:32Z",
    "updated": "2025-10-13T16:23:32Z",
    "link": "http://arxiv.org/pdf/2510.11579v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Hongyu Zhu",
      "Lin Chen",
      "Mounim A. El-Yacoubi",
      "Mingsheng Shang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11576v1",
    "title": "Benchmarking foundation models for hyperspectral image classification:\n  Application to cereal crop type mapping",
    "summary": "Foundation models are transforming Earth observation, but their potential for\nhyperspectral crop mapping remains underexplored. This study benchmarks three\nfoundation models for cereal crop mapping using hyperspectral imagery:\nHyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth\ndataset (a large multitemporal hyperspectral archive). Models were fine-tuned\non manually labeled data from a training region and evaluated on an independent\ntest region. Performance was measured with overall accuracy (OA), average\naccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),\nDOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of\n93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved\n91%, highlighting the importance of model architecture for strong\ngeneralization across geographic regions and sensor platforms. These results\nprovide a systematic evaluation of foundation models for operational\nhyperspectral crop mapping and outline directions for future model development.",
    "published": "2025-10-13T16:21:59Z",
    "updated": "2025-10-13T16:21:59Z",
    "link": "http://arxiv.org/pdf/2510.11576v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Walid Elbarz",
      "Mohamed Bourriz",
      "Hicham Hajji",
      "Hamd Ait Abdelali",
      "François Bourzeix"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07723v2",
    "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view\n  Human Reconstruction",
    "summary": "Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.",
    "published": "2025-10-09T03:01:10Z",
    "updated": "2025-10-13T16:20:43Z",
    "link": "http://arxiv.org/pdf/2510.07723v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenyue Chen",
      "Peng Li",
      "Wangguandong Zheng",
      "Chengfeng Zhao",
      "Mengfei Li",
      "Yaolong Zhu",
      "Zhiyang Dou",
      "Ronggang Wang",
      "Yuan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08789v2",
    "title": "Q-Router: Agentic Video Quality Assessment with Expert Model Routing and\n  Artifact Localization",
    "summary": "Video quality assessment (VQA) is a fundamental computer vision task that\naims to predict the perceptual quality of a given video in alignment with human\njudgments. Existing performant VQA models trained with direct score supervision\nsuffer from (1) poor generalization across diverse content and tasks, ranging\nfrom user-generated content (UGC), short-form videos, to AI-generated content\n(AIGC), (2) limited interpretability, and (3) lack of extensibility to novel\nuse cases or content types. We propose Q-Router, an agentic framework for\nuniversal VQA with a multi-tier model routing system. Q-Router integrates a\ndiverse set of expert models and employs vision--language models (VLMs) as\nreal-time routers that dynamically reason and then ensemble the most\nappropriate experts conditioned on the input video semantics. We build a\nmulti-tiered routing system based on the computing budget, with the heaviest\ntier involving a specific spatiotemporal artifacts localization for\ninterpretability. This agentic design enables Q-Router to combine the\ncomplementary strengths of specialized experts, achieving both flexibility and\nrobustness in delivering consistent performance across heterogeneous video\nsources and tasks. Extensive experiments demonstrate that Q-Router matches or\nsurpasses state-of-the-art VQA models on a variety of benchmarks, while\nsubstantially improving generalization and interpretability. Moreover, Q-Router\nexcels on the quality-based question answering benchmark, Q-Bench-Video,\nhighlighting its promise as a foundation for next-generation VQA systems.\nFinally, we show that Q-Router capably localizes spatiotemporal artifacts,\nshowing potential as a reward function for post-training video generation\nmodels.",
    "published": "2025-10-09T20:11:38Z",
    "updated": "2025-10-13T16:16:11Z",
    "link": "http://arxiv.org/pdf/2510.08789v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuo Xing",
      "Soumik Dey",
      "Mingyang Wu",
      "Ashirbad Mishra",
      "Naveen Ravipati",
      "Binbin Li",
      "Hansi Wu",
      "Zhengzhong Tu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11567v1",
    "title": "A Framework for Low-Effort Training Data Generation for Urban Semantic\n  Segmentation",
    "summary": "Synthetic datasets are widely used for training urban scene recognition\nmodels, but even highly realistic renderings show a noticeable gap to real\nimagery. This gap is particularly pronounced when adapting to a specific target\ndomain, such as Cityscapes, where differences in architecture, vegetation,\nobject appearance, and camera characteristics limit downstream performance.\nClosing this gap with more detailed 3D modelling would require expensive asset\nand scene design, defeating the purpose of low-cost labelled data. To address\nthis, we present a new framework that adapts an off-the-shelf diffusion model\nto a target domain using only imperfect pseudo-labels. Once trained, it\ngenerates high-fidelity, target-aligned images from semantic maps of any\nsynthetic dataset, including low-effort sources created in hours rather than\nmonths. The method filters suboptimal generations, rectifies image-label\nmisalignments, and standardises semantics across datasets, transforming weak\nsynthetic data into competitive real-domain training sets. Experiments on five\nsynthetic datasets and two real target datasets show segmentation gains of up\nto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly\nconstructed synthetic datasets as effective as high-effort, time-intensive\nsynthetic datasets requiring extensive manual design. This work highlights a\nvaluable collaborative paradigm where fast semantic prototyping, combined with\ngenerative models, enables scalable, high-quality training data creation for\nurban scene understanding.",
    "published": "2025-10-13T16:12:29Z",
    "updated": "2025-10-13T16:12:29Z",
    "link": "http://arxiv.org/pdf/2510.11567v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "authors": [
      "Denis Zavadski",
      "Damjan Kalšan",
      "Tim Küchler",
      "Haebom Lee",
      "Stefan Roth",
      "Carsten Rother"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11566v1",
    "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative\n  Policy",
    "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.",
    "published": "2025-10-13T16:11:34Z",
    "updated": "2025-10-13T16:11:34Z",
    "link": "http://arxiv.org/pdf/2510.11566v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Kuanning Wang",
      "Yongchong Gu",
      "Yuqian Fu",
      "Zeyu Shangguan",
      "Sicheng He",
      "Xiangyang Xue",
      "Yanwei Fu",
      "Daniel Seita"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05034v4",
    "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
    "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
    "published": "2025-10-06T17:10:44Z",
    "updated": "2025-10-13T16:09:06Z",
    "link": "http://arxiv.org/pdf/2510.05034v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yolo Yunlong Tang",
      "Jing Bi",
      "Pinxin Liu",
      "Zhenyu Pan",
      "Zhangyun Tan",
      "Qianxiang Shen",
      "Jiani Liu",
      "Hang Hua",
      "Junjia Guo",
      "Yunzhong Xiao",
      "Chao Huang",
      "Zhiyuan Wang",
      "Susan Liang",
      "Xinyi Liu",
      "Yizhi Song",
      "Yuhe Nie",
      "Jia-Xing Zhong",
      "Bozheng Li",
      "Daiqing Qi",
      "Ziyun Zeng",
      "Ali Vosoughi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Daiki Shimada",
      "Han Liu",
      "Jiebo Luo",
      "Chenliang Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11565v1",
    "title": "SNAP: Towards Segmenting Anything in Any Point Cloud",
    "summary": "Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/",
    "published": "2025-10-13T16:07:00Z",
    "updated": "2025-10-13T16:07:00Z",
    "link": "http://arxiv.org/pdf/2510.11565v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Aniket Gupta",
      "Hanhui Wang",
      "Charles Saunders",
      "Aruni RoyChowdhury",
      "Hanumant Singh",
      "Huaizu Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11553v1",
    "title": "How many samples to label for an application given a foundation model?\n  Chest X-ray classification study",
    "summary": "Chest X-ray classification is vital yet resource-intensive, typically\ndemanding extensive annotated data for accurate diagnosis. Foundation models\nmitigate this reliance, but how many labeled samples are required remains\nunclear. We systematically evaluate the use of power-law fits to predict the\ntraining size necessary for specific ROC-AUC thresholds. Testing multiple\npathologies and foundation models, we find XrayCLIP and XraySigLIP achieve\nstrong performance with significantly fewer labeled examples than a ResNet-50\nbaseline. Importantly, learning curve slopes from just 50 labeled cases\naccurately forecast final performance plateaus. Our results enable\npractitioners to minimize annotation costs by labeling only the essential\nsamples for targeted performance.",
    "published": "2025-10-13T15:53:55Z",
    "updated": "2025-10-13T15:53:55Z",
    "link": "http://arxiv.org/pdf/2510.11553v1.pdf",
    "category": [
      "cs.CV",
      "68T07 (Primary) 68T45, 62H30, 62P10 (Secondary)"
    ],
    "authors": [
      "Nikolay Nechaev",
      "Evgenia Przhezdzetskaya",
      "Viktor Gombolevskiy",
      "Dmitry Umerenkov",
      "Dmitry Dylov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11549v1",
    "title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?",
    "summary": "Omnidirectional images (ODIs) provide full 360x180 view which are widely\nadopted in VR, AR and embodied intelligence applications. While multi-modal\nlarge language models (MLLMs) have demonstrated remarkable performance on\nconventional 2D image and video understanding benchmarks, their ability to\ncomprehend the immersive environments captured by ODIs remains largely\nunexplored. To address this gap, we first present ODI-Bench, a novel\ncomprehensive benchmark specifically designed for omnidirectional image\nunderstanding. ODI-Bench contains 2,000 high-quality omnidirectional images and\nover 4,000 manually annotated question-answering (QA) pairs across 10\nfine-grained tasks, covering both general-level and spatial-level ODI\nunderstanding. Extensive experiments are conducted to benchmark 20\nrepresentative MLLMs, including proprietary and open-source models, under both\nclose-ended and open-ended settings. Experimental results reveal that current\nMLLMs still struggle to capture the immersive context provided by ODIs. To this\nend, we further introduce Omni-CoT, a training-free method which significantly\nenhances MLLMs' comprehension ability in the omnidirectional environment\nthrough chain-of-thought reasoning across both textual information and visual\ncues. Both the benchmark and the code will be released upon the publication.",
    "published": "2025-10-13T15:51:47Z",
    "updated": "2025-10-13T15:51:47Z",
    "link": "http://arxiv.org/pdf/2510.11549v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Liu Yang",
      "Huiyu Duan",
      "Ran Tao",
      "Juntao Cheng",
      "Sijing Wu",
      "Yunhao Li",
      "Jing Liu",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11538v1",
    "title": "Massive Activations are the Key to Local Detail Synthesis in Diffusion\n  Transformers",
    "summary": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor visual generation. Recent observations reveal \\emph{Massive Activations}\n(MAs) in their internal feature maps, yet their function remains poorly\nunderstood. In this work, we systematically investigate these activations to\nelucidate their role in visual generation. We found that these massive\nactivations occur across all spatial tokens, and their distribution is\nmodulated by the input timestep embeddings. Importantly, our investigations\nfurther demonstrate that these massive activations play a key role in local\ndetail synthesis, while having minimal impact on the overall semantic content\nof output. Building on these insights, we propose \\textbf{D}etail\n\\textbf{G}uidance (\\textbf{DG}), a MAs-driven, training-free self-guidance\nstrategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG\nconstructs a degraded ``detail-deficient'' model by disrupting MAs and\nleverages it to guide the original network toward higher-quality detail\nsynthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),\nenabling further refinements of fine-grained details. Extensive experiments\ndemonstrate that our DG consistently improves fine-grained detail quality\nacross various pre-trained DiTs (\\eg, SD3, SD3.5, and Flux).",
    "published": "2025-10-13T15:39:13Z",
    "updated": "2025-10-13T15:39:13Z",
    "link": "http://arxiv.org/pdf/2510.11538v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chaofan Gan",
      "Zicheng Zhao",
      "Yuanpeng Tu",
      "Xi Chen",
      "Ziran Qin",
      "Tieyuan Chen",
      "Mehrtash Harandi",
      "Weiyao Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11520v1",
    "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
    "summary": "Walking assistance in extreme or complex environments remains a significant\nchallenge for people with blindness or low vision (BLV), largely due to the\nlack of a holistic scene understanding. Motivated by the real-world needs of\nthe BLV community, we build mmWalk, a simulated multi-modal dataset that\nintegrates multi-view sensor and accessibility-oriented features for outdoor\nsafe navigation. Our dataset comprises 120 manually controlled,\nscenario-categorized walking trajectories with 62k synchronized frames. It\ncontains over 559k panoramic images across RGB, depth, and semantic modalities.\nFurthermore, to emphasize real-world relevance, each trajectory involves\noutdoor corner cases and accessibility-specific landmarks for BLV users.\nAdditionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual\nquestion-answer triplets across 9 categories tailored for safe and informed\nwalking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)\nusing zero- and few-shot settings and found they struggle with our risk\nassessment and navigational tasks. We validate our mmWalk-finetuned model on\nreal-world datasets and show the effectiveness of our dataset for advancing\nmulti-modal walking assistance.",
    "published": "2025-10-13T15:25:52Z",
    "updated": "2025-10-13T15:25:52Z",
    "link": "http://arxiv.org/pdf/2510.11520v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kedi Ying",
      "Ruiping Liu",
      "Chongyan Chen",
      "Mingzhe Tao",
      "Hao Shi",
      "Kailun Yang",
      "Jiaming Zhang",
      "Rainer Stiefelhagen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11509v1",
    "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal\n  Large Language Model",
    "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.",
    "published": "2025-10-13T15:17:18Z",
    "updated": "2025-10-13T15:17:18Z",
    "link": "http://arxiv.org/pdf/2510.11509v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ruiping Liu",
      "Junwei Zheng",
      "Yufan Chen",
      "Zirui Wang",
      "Kunyu Peng",
      "Kailun Yang",
      "Jiaming Zhang",
      "Marc Pollefeys",
      "Rainer Stiefelhagen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11508v1",
    "title": "Towards Fast and Scalable Normal Integration using Continuous Components",
    "summary": "Surface normal integration is a fundamental problem in computer vision,\ndealing with the objective of reconstructing a surface from its corresponding\nnormal map. Existing approaches require an iterative global optimization to\njointly estimate the depth of each pixel, which scales poorly to larger normal\nmaps. In this paper, we address this problem by recasting normal integration as\nthe estimation of relative scales of continuous components. By constraining\npixels belonging to the same component to jointly vary their scale, we\ndrastically reduce the number of optimization variables. Our framework includes\na heuristic to accurately estimate continuous components from the start, a\nstrategy to rebalance optimization terms, and a technique to iteratively merge\ncomponents to further reduce the size of the problem. Our method achieves\nstate-of-the-art results on the standard normal integration benchmark in as\nlittle as a few seconds and achieves one-order-of-magnitude speedup over\npixel-level approaches on large-resolution normal maps.",
    "published": "2025-10-13T15:17:16Z",
    "updated": "2025-10-13T15:17:16Z",
    "link": "http://arxiv.org/pdf/2510.11508v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Francesco Milano",
      "Jen Jen Chung",
      "Lionel Ott",
      "Roland Siegwart"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.05970v3",
    "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval",
    "summary": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon.",
    "published": "2025-07-08T13:24:05Z",
    "updated": "2025-10-13T15:01:23Z",
    "link": "http://arxiv.org/pdf/2507.05970v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haiwen Li",
      "Delong Liu",
      "Zhaohui Hou",
      "Zhicheng Zhao",
      "Fei Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12094v2",
    "title": "Error Propagation Mechanisms and Compensation Strategies for Quantized\n  Diffusion",
    "summary": "Diffusion models have transformed image synthesis by establishing\nunprecedented quality and creativity benchmarks. Nevertheless, their\nlarge-scale deployment faces challenges due to computationally intensive\niterative denoising processes. Although post-training quantization (PTQ)\nprovides an effective pathway for accelerating sampling, the iterative nature\nof diffusion models causes stepwise quantization errors to accumulate\nprogressively during generation, inevitably compromising output fidelity. To\naddress this challenge, we develop a theoretical framework that mathematically\nformulates error propagation in Diffusion Models (DMs), deriving per-step\nquantization error propagation equations and establishing the first closed-form\nsolution for cumulative error. Building on this theoretical foundation, we\npropose a timestep-aware cumulative error compensation scheme. Extensive\nexperiments on multiple image datasets demonstrate that our compensation\nstrategy effectively mitigates error propagation, significantly enhancing\nexisting PTQ methods. Specifically, it achieves a 1.2 PSNR improvement over\nSVDQuant on SDXL W4A4, while incurring only an additional $<$ 0.5\\% time\noverhead.",
    "published": "2025-08-16T16:31:00Z",
    "updated": "2025-10-13T14:53:33Z",
    "link": "http://arxiv.org/pdf/2508.12094v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Songwei Liu",
      "Chao Zeng",
      "Chenqian Yan",
      "Xurui Peng",
      "Xing Wang",
      "Fangmin Chen",
      "Xing Mei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03215v3",
    "title": "Beyond [cls]: Exploring the true potential of Masked Image Modeling\n  representations",
    "summary": "Masked Image Modeling (MIM) has emerged as a promising approach for\nSelf-Supervised Learning (SSL) of visual representations. However, the\nout-of-the-box performance of MIMs is typically inferior to competing\napproaches. Most users cannot afford fine-tuning due to the need for large\namounts of data, high GPU consumption, and specialized user knowledge.\nTherefore, the practical use of MIM representations is limited. In this paper\nwe ask what is the reason for the poor out-of-the-box performance of MIMs. Is\nit due to weaker features produced by MIM models, or is it due to suboptimal\nusage? Through detailed analysis, we show that attention in MIMs is spread\nalmost uniformly over many patches, leading to ineffective aggregation by the\n[cls] token. Based on this insight, we propose Selective Aggregation to better\ncapture the rich semantic information retained in patch tokens, which\nsignificantly improves the out-of-the-box performance of MIM.",
    "published": "2024-12-04T11:08:32Z",
    "updated": "2025-10-13T14:50:31Z",
    "link": "http://arxiv.org/pdf/2412.03215v3.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Marcin Przewięźlikowski",
      "Randall Balestriero",
      "Wojciech Jasiński",
      "Marek Śmieja",
      "Bartosz Zieliński"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.10871v2",
    "title": "DDFusion:Degradation-Decoupled Fusion Framework for Robust Infrared and\n  Visible Images Fusion",
    "summary": "Conventional infrared and visible image fusion(IVIF) methods often assume\nhigh-quality inputs, neglecting real-world degradations such as low-light and\nnoise, which limits their practical applicability. To address this, we propose\na Degradation-Decoupled Fusion(DDFusion) framework, which achieves degradation\ndecoupling and jointly models degradation suppression and image fusion in a\nunified manner. Specifically, the Degradation-Decoupled Optimization\nNetwork(DDON) performs degradation-specific decomposition to decouple\ninter-degradation and degradation-information components, followed by\ncomponent-specific extraction paths for effective suppression of degradation\nand enhancement of informative features. The Interactive Local-Global Fusion\nNetwork (ILGFN) aggregates complementary features across multi-scale pathways\nand alleviates performance degradation caused by the decoupling between\ndegradation optimization and image fusion. Extensive experiments demonstrate\nthat DDFusion achieves superior fusion performance under both clean and\ndegraded conditions. Our code is available at\nhttps://github.com/Lmmh058/DDFusion.",
    "published": "2025-04-15T05:02:49Z",
    "updated": "2025-10-13T14:48:24Z",
    "link": "http://arxiv.org/pdf/2504.10871v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianpei Zhang",
      "Jufeng Zhao",
      "Yiming Zhu",
      "Guangmang Cui",
      "Yuxin Jing"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11473v1",
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via\n  View Alignment",
    "summary": "3D Gaussian Splatting has recently emerged as an efficient solution for\nhigh-quality and real-time novel view synthesis. However, its capability for\naccurate surface reconstruction remains underexplored. Due to the discrete and\nunstructured nature of Gaussians, supervision based solely on image rendering\nloss often leads to inaccurate geometry and inconsistent multi-view alignment.\nIn this work, we propose a novel method that enhances the geometric\nrepresentation of 3D Gaussians through view alignment (VA). Specifically, we\nincorporate edge-aware image cues into the rendering loss to improve surface\nboundary delineation. To enforce geometric consistency across views, we\nintroduce a visibility-aware photometric alignment loss that models occlusions\nand encourages accurate spatial relationships among Gaussians. To further\nmitigate ambiguities caused by lighting variations, we incorporate normal-based\nconstraints to refine the spatial orientation of Gaussians and improve local\nsurface estimation. Additionally, we leverage deep image feature embeddings to\nenforce cross-view consistency, enhancing the robustness of the learned\ngeometry under varying viewpoints and illumination. Extensive experiments on\nstandard benchmarks demonstrate that our method achieves state-of-the-art\nperformance in both surface reconstruction and novel view synthesis. The source\ncode is available at https://github.com/LeoQLi/VA-GS.",
    "published": "2025-10-13T14:44:50Z",
    "updated": "2025-10-13T14:44:50Z",
    "link": "http://arxiv.org/pdf/2510.11473v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Xun Gong",
      "Yu-Shen Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11456v1",
    "title": "Coupled Degradation Modeling and Fusion: A VLM-Guided\n  Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image\n  Fusion",
    "summary": "Existing Infrared and Visible Image Fusion (IVIF) methods typically assume\nhigh-quality inputs. However, when handing degraded images, these methods\nheavily rely on manually switching between different pre-processing techniques.\nThis decoupling of degradation handling and image fusion leads to significant\nperformance degradation. In this paper, we propose a novel VLM-Guided\nDegradation-Coupled Fusion network (VGDCFusion), which tightly couples\ndegradation modeling with the fusion process and leverages vision-language\nmodels (VLMs) for degradation-aware perception and guided suppression.\nSpecifically, the proposed Specific-Prompt Degradation-Coupled Extractor\n(SPDCE) enables modality-specific degradation awareness and establishes a joint\nmodeling of degradation suppression and intra-modal feature extraction. In\nparallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates\ncross-modal degradation perception and couples residual degradation filtering\nwith complementary cross-modal feature fusion. Extensive experiments\ndemonstrate that our VGDCFusion significantly outperforms existing\nstate-of-the-art fusion approaches under various degraded image scenarios. Our\ncode is available at https://github.com/Lmmh058/VGDCFusion.",
    "published": "2025-10-13T14:26:33Z",
    "updated": "2025-10-13T14:26:33Z",
    "link": "http://arxiv.org/pdf/2510.11456v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianpei Zhang",
      "Jufeng Zhao",
      "Yiming Zhu",
      "Guangmang Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11449v1",
    "title": "Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based\n  Fusion of Satellite and AIS for Vessel Characterization",
    "summary": "Maritime Domain Awareness (MDA) for inland waterways remains challenged by\ncooperative system vulnerabilities. This paper presents a novel framework that\nfuses high-resolution satellite imagery with vessel trajectory data from the\nAutomatic Identification System (AIS). This work addresses the limitations of\nAIS-based monitoring by leveraging non-cooperative satellite imagery and\nimplementing a fusion approach that links visual detections with AIS data to\nidentify dark vessels, validate cooperative traffic, and support advanced MDA.\nThe You Only Look Once (YOLO) v11 object detection model is used to detect and\ncharacterize vessels and barges by vessel type, barge cover, operational\nstatus, barge count, and direction of travel. An annotated data set of 4,550\ninstances was developed from $5{,}973~\\mathrm{mi}^2$ of Lower Mississippi River\nimagery. Evaluation on a held-out test set demonstrated vessel classification\n(tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1\nscore of 95.8\\%; barge cover (covered or uncovered) detection yielded an F1\nscore of 91.6\\%; operational status (staged or in motion) classification\nreached an F1 score of 99.4\\%. Directionality (upstream, downstream) yielded\n93.8\\% accuracy. The barge count estimation resulted in a mean absolute error\n(MAE) of 2.4 barges. Spatial transferability analysis across geographically\ndisjoint river segments showed accuracy was maintained as high as 98\\%. These\nresults underscore the viability of integrating non-cooperative satellite\nsensing with AIS fusion. This approach enables near-real-time fleet\ninventories, supports anomaly detection, and generates high-quality data for\ninland waterway surveillance. Future work will expand annotated datasets,\nincorporate temporal tracking, and explore multi-modal deep learning to further\nenhance operational scalability.",
    "published": "2025-10-13T14:19:58Z",
    "updated": "2025-10-13T14:19:58Z",
    "link": "http://arxiv.org/pdf/2510.11449v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Geoffery Agorku",
      "Sarah Hernandez",
      "Hayley Hames",
      "Cade Wagner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.09826v2",
    "title": "Isolated Channel Vision Transformers: From Single-Channel Pretraining to\n  Multi-Channel Finetuning",
    "summary": "Vision Transformers (ViTs) have achieved remarkable success in standard RGB\nimage processing tasks. However, applying ViTs to multi-channel imaging (MCI)\ndata, e.g., for medical and remote sensing applications, remains a challenge.\nIn particular, MCI data often consist of layers acquired from different\nmodalities. Directly training ViTs on such data can obscure complementary\ninformation and impair the performance. In this paper, we introduce a simple\nyet effective pretraining framework for large-scale MCI datasets. Our method,\nnamed Isolated Channel ViT (IC-ViT), patchifies image channels individually and\nthereby enables pretraining for multimodal multi-channel tasks. We show that\nthis channel-wise patchifying is a key technique for MCI processing. More\nimportantly, one can pretrain the IC-ViT on single channels and finetune it on\ndownstream multi-channel datasets. This pretraining framework captures\ndependencies between patches as well as channels and produces robust feature\nrepresentation. Experiments on various tasks and benchmarks, including JUMP-CP\nand CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging,\nshow that the proposed IC-ViT delivers 4-14 percentage points of performance\nimprovement over existing channel-adaptive approaches. Further, its efficient\ntraining makes it a suitable candidate for large-scale pretraining of\nfoundation models on heterogeneous data. Our code is available at\nhttps://github.com/shermanlian/IC-ViT.",
    "published": "2025-03-12T20:45:02Z",
    "updated": "2025-10-13T14:18:43Z",
    "link": "http://arxiv.org/pdf/2503.09826v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenyi Lian",
      "Patrick Micke",
      "Joakim Lindblad",
      "Nataša Sladoje"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11417v1",
    "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
    "summary": "Establishing object-level correspondence between egocentric and exocentric\nviews is essential for intelligent assistants to deliver precise and intuitive\nvisual guidance. However, this task faces numerous challenges, including\nextreme viewpoint variations, occlusions, and the presence of small objects.\nExisting approaches usually borrow solutions from video object segmentation\nmodels, but still suffer from the aforementioned challenges. Recently, the\nSegment Anything Model 2 (SAM 2) has shown strong generalization capabilities\nand excellent performance in video object segmentation. Yet, when simply\napplied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe\ndifficulties due to ineffective ego-exo feature fusion and limited long-term\nmemory capacity, especially for long videos. Addressing these problems, we\npropose a novel EEC framework based on SAM 2 with long-term memories by\npresenting a dual-memory architecture and an adaptive feature routing module\ninspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features\n(i) a Memory-View MoE module which consists of a dual-branch routing mechanism\nto adaptively assign contribution weights to each expert feature along both\nchannel and spatial dimensions, and (ii) a dual-memory bank system with a\nsimple yet effective compression strategy to retain critical long-term\ninformation while eliminating redundancy. In the extensive experiments on the\nchallenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new\nstate-of-the-art results and significantly outperforms existing methods and the\nSAM 2 baseline, showcasing its strong generalization across diverse scenarios.\nOur code and model are available at https://github.com/juneyeeHu/LM-EEC.",
    "published": "2025-10-13T13:54:12Z",
    "updated": "2025-10-13T13:54:12Z",
    "link": "http://arxiv.org/pdf/2510.11417v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yijun Hu",
      "Bing Fan",
      "Xin Gu",
      "Haiqing Ren",
      "Dongfang Liu",
      "Heng Fan",
      "Libo Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.07392v6",
    "title": "ID-Booth: Identity-consistent Face Generation with Diffusion Models",
    "summary": "Recent advances in generative modeling have enabled the generation of\nhigh-quality synthetic data that is applicable in a variety of domains,\nincluding face recognition. Here, state-of-the-art generative models typically\nrely on conditioning and fine-tuning of powerful pretrained diffusion models to\nfacilitate the synthesis of realistic images of a desired identity. Yet, these\nmodels often do not consider the identity of subjects during training, leading\nto poor consistency between generated and intended identities. In contrast,\nmethods that employ identity-based training objectives tend to overfit on\nvarious aspects of the identity, and in turn, lower the diversity of images\nthat can be generated. To address these issues, we present in this paper a\nnovel generative diffusion-based framework, called ID-Booth. ID-Booth consists\nof a denoising network responsible for data generation, a variational\nauto-encoder for mapping images to and from a lower-dimensional latent space\nand a text encoder that allows for prompt-based control over the generation\nprocedure. The framework utilizes a novel triplet identity training objective\nand enables identity-consistent image generation while retaining the synthesis\ncapabilities of pretrained diffusion models. Experiments with a\nstate-of-the-art latent diffusion model and diverse prompts reveal that our\nmethod facilitates better intra-identity consistency and inter-identity\nseparability than competing methods, while achieving higher image diversity. In\nturn, the produced data allows for effective augmentation of small-scale\ndatasets and training of better-performing recognition models in a\nprivacy-preserving manner. The source code for the ID-Booth framework is\npublicly available at https://github.com/dariant/ID-Booth.",
    "published": "2025-04-10T02:20:18Z",
    "updated": "2025-10-13T13:34:37Z",
    "link": "http://arxiv.org/pdf/2504.07392v6.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Darian Tomašević",
      "Fadi Boutros",
      "Chenhao Lin",
      "Naser Damer",
      "Vitomir Štruc",
      "Peter Peer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11387v1",
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent\n  Material Inference",
    "summary": "Modeling reflections from 2D images is essential for photorealistic rendering\nand novel view synthesis. Recent approaches enhance Gaussian primitives with\nreflection-related material attributes to enable physically based rendering\n(PBR) with Gaussian Splatting. However, the material inference often lacks\nsufficient constraints, especially under limited environment modeling,\nresulting in illumination aliasing and reduced generalization. In this work, we\nrevisit the problem from a multi-view perspective and show that multi-view\nconsistent material inference with more physically-based environment modeling\nis key to learning accurate reflections with Gaussian Splatting. To this end,\nwe enforce 2D Gaussians to produce multi-view consistent material maps during\ndeferred shading. We also track photometric variations across views to identify\nhighly reflective regions, which serve as strong priors for reflection strength\nterms. To handle indirect illumination caused by inter-object occlusions, we\nfurther introduce an environment modeling strategy through ray tracing with\n2DGS, enabling photorealistic rendering of indirect radiance. Experiments on\nwidely used benchmarks show that our method faithfully recovers both\nillumination and geometry, achieving state-of-the-art rendering quality in\nnovel views synthesis.",
    "published": "2025-10-13T13:29:20Z",
    "updated": "2025-10-13T13:29:20Z",
    "link": "http://arxiv.org/pdf/2510.11387v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenyuan Zhang",
      "Jimin Tang",
      "Weiqi Zhang",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.15702v3",
    "title": "Editable-DeepSC: Reliable Cross-Modal Semantic Communications for Facial\n  Editing",
    "summary": "Real-time computer vision (CV) plays a crucial role in various real-world\napplications, whose performance is highly dependent on communication networks.\nNonetheless, the data-oriented characteristics of conventional communications\noften do not align with the special needs of real-time CV tasks. To alleviate\nthis issue, the recently emerged semantic communications only transmit\ntask-related semantic information and exhibit a promising landscape to address\nthis problem. However, the communication challenges associated with Semantic\nFacial Editing, one of the most important real-time CV applications on social\nmedia, still remain largely unexplored. In this paper, we fill this gap by\nproposing Editable-DeepSC, a novel cross-modal semantic communication approach\nfor facial editing. Firstly, we theoretically discuss different transmission\nschemes that separately handle communications and editings, and emphasize the\nnecessity of Joint Editing-Channel Coding (JECC) via iterative attributes\nmatching, which integrates editings into the communication chain to preserve\nmore semantic mutual information. To compactly represent the high-dimensional\ndata, we leverage inversion methods via pre-trained StyleGAN priors for\nsemantic coding. To tackle the dynamic channel noise conditions, we propose\nSNR-aware channel coding via model fine-tuning. Extensive experiments indicate\nthat Editable-DeepSC can achieve superior editings while significantly saving\nthe transmission bandwidth, even under high-resolution and out-of-distribution\n(OOD) settings.",
    "published": "2024-11-24T04:07:33Z",
    "updated": "2025-10-13T13:17:15Z",
    "link": "http://arxiv.org/pdf/2411.15702v3.pdf",
    "category": [
      "cs.IT",
      "cs.CV",
      "cs.NI",
      "math.IT"
    ],
    "authors": [
      "Bin Chen",
      "Wenbo Yu",
      "Qinshan Zhang",
      "Tianqu Zhuang",
      "Yong Jiang",
      "Shu-Tao Xia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11369v1",
    "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in\n  Image Quality Assessment",
    "summary": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time.",
    "published": "2025-10-13T13:11:08Z",
    "updated": "2025-10-13T13:11:08Z",
    "link": "http://arxiv.org/pdf/2510.11369v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shijie Zhao",
      "Xuanyu Zhang",
      "Weiqi Li",
      "Junlin Li",
      "Li Zhang",
      "Tianfan Xue",
      "Jian Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.24000v2",
    "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for\n  Vision-Language Models",
    "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and make it difficult to assess their practical strengths\nand weaknesses. To address these challenges, we introduce TTA-VLM, a\ncomprehensive benchmark for evaluating TTA methods on VLMs. Our benchmark\nimplements 8 episodic TTA and 7 online TTA methods within a unified and\nreproducible framework, and evaluates them across 15 widely used datasets.\nUnlike prior studies focused solely on CLIP, we extend the evaluation to\nSigLIP--a model trained with a Sigmoid loss--and include training-time tuning\nmethods such as CoOp, MaPLe, and TeCoA to assess generality. Beyond\nclassification accuracy, TTA-VLM incorporates various evaluation metrics,\nincluding robustness, calibration, out-of-distribution detection, and\nstability, enabling a more holistic assessment of TTA methods. Through\nextensive experiments, we find that 1) existing TTA methods produce limited\ngains compared to the previous pioneering work; 2) current TTA methods exhibit\npoor collaboration with training-time fine-tuning methods; 3) accuracy gains\nfrequently come at the cost of reduced model trustworthiness. We release\nTTA-VLM to provide fair comparison and comprehensive evaluation of TTA methods\nfor VLMs, and we hope it encourages the community to develop more reliable and\ngeneralizable TTA strategies.",
    "published": "2025-06-30T16:05:55Z",
    "updated": "2025-10-13T13:09:11Z",
    "link": "http://arxiv.org/pdf/2506.24000v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Lijun Sheng",
      "Jian Liang",
      "Ran He",
      "Zilei Wang",
      "Tieniu Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.18254v2",
    "title": "Surface-Aware Distilled 3D Semantic Features",
    "summary": "Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by pairwise matching of semantic\nfeatures from pre-trained vision models. However, despite their power, these\nfeatures struggle to differentiate instances of the same semantic class such as\n``left hand'' versus ``right hand'' which leads to substantial mapping errors.\nTo solve this, we learn a surface-aware embedding space that is robust to these\nambiguities while facilitating shared mapping for an entire family of 3D\nshapes. Importantly, our approach is self-supervised and requires only a small\nnumber of unpaired training meshes to infer features for new possibly imperfect\n3D shapes at test time. We achieve this by introducing a contrastive loss that\npreserves the semantic content of the features distilled from foundational\nmodels while disambiguating features located far apart on the shape's surface.\nWe observe superior performance in correspondence matching benchmarks and\nenable downstream applications including 2D-to-3D and 3D-to-3D texture\ntransfer, in-part segmentation, pose alignment, and motion transfer in low-data\nregimes. Unlike previous pairwise approaches, our solution constructs a joint\nembedding space, where both seen and unseen 3D shapes are implicitly aligned\nwithout further optimization. The code is available at\nhttps://graphics.tudelft.nl/SurfaceAware3DFeatures.",
    "published": "2025-03-24T00:36:16Z",
    "updated": "2025-10-13T12:58:01Z",
    "link": "http://arxiv.org/pdf/2503.18254v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Lukas Uzolas",
      "Elmar Eisemann",
      "Petr Kellnhofer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.05839v2",
    "title": "Towards Robust and Realible Multimodal Fake News Detection with\n  Incomplete Modality",
    "summary": "Multimodal fake news detection (MFND) has become an urgent task with the\nemergence of huge multimodal fake content on social media platforms. Previous\nstudies mainly focus on complex feature extraction and fusion to learn\ndiscriminative information from multimodal content. However, in real-world\napplications, multimedia news may naturally lose some information during\ndissemination, resulting in modality incompleteness, which is detrimental to\nthe generalization and robustness of existing models. To this end, we propose a\nnovel generic and robust multimodal fusion strategy, termed Multi-expert\nModality-incomplete Learning Network (MMLNet), which is simple yet effective.\nIt consists of three key steps: (1) Multi-Expert Collaborative Reasoning to\ncompensate for missing modalities by dynamically leveraging complementary\ninformation through multiple experts. (2) Incomplete Modality Adapters\ncompensates for the missing information by leveraging the new feature\ndistribution. (3) Modality Missing Learning leveraging an label-aware adaptive\nweighting strategy to learn a robust representation with contrastive learning.\nWe evaluate MMLNet on three real-world benchmarks across two languages,\ndemonstrating superior performance compared to state-of-the-art methods while\nmaintaining relative simplicity. By ensuring the accuracy of fake news\ndetection in incomplete modality scenarios caused by information propagation,\nMMLNet effectively curbs the spread of malicious misinformation. Code is\npublicly available at https://github.com/zhyhome/MMLNet.",
    "published": "2025-10-07T12:03:17Z",
    "updated": "2025-10-13T12:48:53Z",
    "link": "http://arxiv.org/pdf/2510.05839v2.pdf",
    "category": [
      "cs.MM",
      "cs.CV"
    ],
    "authors": [
      "Hengyang Zhou",
      "Yiwei Wei",
      "Jian Yang",
      "Zhenyu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11344v1",
    "title": "MMAP: A Multi-Magnification and Prototype-Aware Architecture for\n  Predicting Spatial Gene Expression",
    "summary": "Spatial Transcriptomics (ST) enables the measurement of gene expression while\npreserving spatial information, offering critical insights into tissue\narchitecture and disease pathology. Recent developments have explored the use\nof hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict\ntranscriptome-wide gene expression profiles through deep neural networks. This\ntask is commonly framed as a regression problem, where each input corresponds\nto a localized image patch extracted from the WSI. However, predicting spatial\ngene expression from histological images remains a challenging problem due to\nthe significant modality gap between visual features and molecular signals.\nRecent studies have attempted to incorporate both local and global information\ninto predictive models. Nevertheless, existing methods still suffer from two\nkey limitations: (1) insufficient granularity in local feature extraction, and\n(2) inadequate coverage of global spatial context. In this work, we propose a\nnovel framework, MMAP (Multi-MAgnification and Prototype-enhanced\narchitecture), that addresses both challenges simultaneously. To enhance local\nfeature granularity, MMAP leverages multi-magnification patch representations\nthat capture fine-grained histological details. To improve global contextual\nunderstanding, it learns a set of latent prototype embeddings that serve as\ncompact representations of slide-level information. Extensive experimental\nresults demonstrate that MMAP consistently outperforms all existing\nstate-of-the-art methods across multiple evaluation metrics, including Mean\nAbsolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation\nCoefficient (PCC).",
    "published": "2025-10-13T12:41:09Z",
    "updated": "2025-10-13T12:41:09Z",
    "link": "http://arxiv.org/pdf/2510.11344v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hai Dang Nguyen",
      "Nguyen Dang Huy Pham",
      "The Minh Duc Nguyen",
      "Dac Thai Nguyen",
      "Hang Thi Nguyen",
      "Duong M. Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11341v1",
    "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language\n  Models",
    "summary": "General SVG modeling remains challenging due to fragmented datasets, limited\ntransferability of methods across tasks, and the difficulty of handling\nstructural complexity. In response, we leverage the strong transfer and\ngeneralization capabilities of multimodal large language models (MLLMs) to\nachieve unified modeling for SVG understanding, editing, and generation. We\npresent the InternSVG family, an integrated data-benchmark-model suite. At its\ncore is SAgoge, the largest and most comprehensive multimodal dataset for SVG\ntasks, encompassing both static graphics and dynamic animations. It covers\nicons, long-sequence illustrations, scientific diagrams, and dynamic\nanimations, supporting tasks of varied difficulty levels and providing deeper\nhierarchies with richer attributes compared to previous datasets. Based on this\nresource, we introduce SArena, a companion benchmark with comprehensive task\ndefinitions and standardized evaluation that aligns with the domains and\ndifficulty spectrum covered by SAgoge. Building on these foundations, we\npropose InternSVG, a unified MLLM for SVG understanding, editing, and\ngeneration with SVG-specific special tokens, subword-based embedding\ninitialization, and a two-stage training strategy that progresses from short\nstatic SVGs to long-sequence illustrations and complex animations. This unified\nformulation induces positive transfer and improves overall performance.\nExperiments on SArena and prior benchmark confirm that InternSVG achieves\nsubstantial gains and consistently outperforms leading open and proprietary\ncounterparts.",
    "published": "2025-10-13T12:38:04Z",
    "updated": "2025-10-13T12:38:04Z",
    "link": "http://arxiv.org/pdf/2510.11341v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haomin Wang",
      "Jinhui Yin",
      "Qi Wei",
      "Wenguang Zeng",
      "Lixin Gu",
      "Shenglong Ye",
      "Zhangwei Gao",
      "Yaohui Wang",
      "Yanting Zhang",
      "Yuanqi Li",
      "Yanwen Guo",
      "Wenhai Wang",
      "Kai Chen",
      "Yu Qiao",
      "Hongjie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11340v1",
    "title": "REACT3D: Recovering Articulations for Interactive Physical 3D Scenes",
    "summary": "Interactive 3D scenes are increasingly vital for embodied intelligence, yet\nexisting datasets remain limited due to the labor-intensive process of\nannotating part segmentation, kinematic types, and motion trajectories. We\npresent REACT3D, a scalable zero-shot framework that converts static 3D scenes\ninto simulation-ready interactive replicas with consistent geometry, enabling\ndirect use in diverse downstream tasks. Our contributions include: (i)\nopenable-object detection and segmentation to extract candidate movable parts\nfrom static scenes, (ii) articulation estimation that infers joint types and\nmotion parameters, (iii) hidden-geometry completion followed by interactive\nobject assembly, and (iv) interactive scene integration in widely supported\nformats to ensure compatibility with standard simulation platforms. We achieve\nstate-of-the-art performance on detection/segmentation and articulation metrics\nacross diverse indoor scenes, demonstrating the effectiveness of our framework\nand providing a practical foundation for scalable interactive scene generation,\nthereby lowering the barrier to large-scale research on articulated scene\nunderstanding. Our project page is\n\\textit{\\hypersetup{urlcolor=black}\\href{https://react3d.github.io/}{react3d.github.io}}.",
    "published": "2025-10-13T12:37:59Z",
    "updated": "2025-10-13T12:37:59Z",
    "link": "http://arxiv.org/pdf/2510.11340v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Zhao Huang",
      "Boyang Sun",
      "Alexandros Delitzas",
      "Jiaqi Chen",
      "Marc Pollefeys"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.14005v2",
    "title": "Real-Time Position-Aware View Synthesis from Single-View Input",
    "summary": "Recent advancements in view synthesis have significantly enhanced immersive\nexperiences across various computer graphics and multimedia applications,\nincluding telepresence and entertainment. By enabling the generation of new\nperspectives from a single input view, view synthesis allows users to better\nperceive and interact with their environment. However, many state-of-the-art\nmethods, while achieving high visual quality, face limitations in real-time\nperformance, which makes them less suitable for live applications where low\nlatency is critical. In this paper, we present a lightweight, position-aware\nnetwork designed for real-time view synthesis from a single input image and a\ntarget camera pose. The proposed framework consists of a Position Aware\nEmbedding, which efficiently maps positional information from the target pose\nto generate high dimensional feature maps. These feature maps, along with the\ninput image, are fed into a Rendering Network that merges features from dual\nencoder branches to resolve both high and low level details, producing a\nrealistic new view of the scene. Experimental results demonstrate that our\nmethod achieves superior efficiency and visual quality compared to existing\napproaches, particularly in handling complex translational movements without\nexplicit geometric operations like warping. This work marks a step toward\nenabling real-time live and interactive telepresence applications.",
    "published": "2024-12-18T16:20:21Z",
    "updated": "2025-10-13T12:18:29Z",
    "link": "http://arxiv.org/pdf/2412.14005v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "authors": [
      "Manu Gond",
      "Emin Zerman",
      "Sebastian Knorr",
      "Mårten Sjöström"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.11536v2",
    "title": "OVS Meets Continual Learning: Towards Sustainable Open-Vocabulary\n  Segmentation",
    "summary": "Open-Vocabulary Segmentation (OVS) aims to segment classes that are not\npresent in the training dataset. However, most existing studies assume that the\ntraining data is fixed in advance, overlooking more practical scenarios where\nnew datasets are continuously collected over time. To address this, we first\nanalyze how existing OVS models perform under such conditions. In this context,\nwe explore several approaches such as retraining, fine-tuning, and continual\nlearning but find that each of them has clear limitations. To address these\nissues, we propose ConOVS, a novel continual learning method based on a\nMixture-of-Experts framework. ConOVS dynamically combines expert decoders based\non the probability that an input sample belongs to the distribution of each\nincremental dataset. Through extensive experiments, we show that ConOVS\nconsistently outperforms existing methods across pre-training, incremental, and\nzero-shot test datasets, effectively expanding the recognition capabilities of\nOVS models when data is collected sequentially.",
    "published": "2024-10-15T12:11:41Z",
    "updated": "2025-10-13T11:59:16Z",
    "link": "http://arxiv.org/pdf/2410.11536v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dongjun Hwang",
      "Yejin Kim",
      "Minyoung Lee",
      "Seong Joon Oh",
      "Junsuk Choe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11305v1",
    "title": "Evaluating the effects of preprocessing, method selection, and\n  hyperparameter tuning on SAR-based flood mapping and water depth estimation",
    "summary": "Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)\nimagery are crucial for calibrating and validating hydraulic models. This study\nuses SAR imagery to evaluate various preprocessing (especially speckle noise\nreduction), flood mapping, and water depth estimation methods. The impact of\nthe choice of method at different steps and its hyperparameters is studied by\nconsidering an ensemble of preprocessed images, flood maps, and water depth\nfields. The evaluation is conducted for two flood events on the Garonne River\n(France) in 2019 and 2021, using hydrodynamic simulations and in-situ\nobservations as reference data. Results show that the choice of speckle filter\nalters flood extent estimations with variations of several square kilometers.\nFurthermore, the selection and tuning of flood mapping methods also affect\nperformance. While supervised methods outperformed unsupervised ones, tuned\nunsupervised approaches (such as local thresholding or change detection) can\nachieve comparable results. The compounded uncertainty from preprocessing and\nflood mapping steps also introduces high variability in the water depth field\nestimates. This study highlights the importance of considering the entire\nprocessing pipeline, encompassing preprocessing, flood mapping, and water depth\nestimation methods and their associated hyperparameters. Rather than relying on\na single configuration, adopting an ensemble approach and accounting for\nmethodological uncertainty should be privileged. For flood mapping, the method\nchoice has the most influence. For water depth estimation, the most influential\nprocessing step was the flood map input resulting from the flood mapping step\nand the hyperparameters of the methods.",
    "published": "2025-10-13T11:54:42Z",
    "updated": "2025-10-13T11:54:42Z",
    "link": "http://arxiv.org/pdf/2510.11305v1.pdf",
    "category": [
      "cs.CV",
      "physics.geo-ph"
    ],
    "authors": [
      "Jean-Paul Travert",
      "Cédric Goeury",
      "Sébastien Boyaval",
      "Vito Bacchi",
      "Fabrice Zaoui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11303v1",
    "title": "sketch2symm: Symmetry-aware sketch-to-shape generation via semantic\n  bridging",
    "summary": "Sketch-based 3D reconstruction remains a challenging task due to the abstract\nand sparse nature of sketch inputs, which often lack sufficient semantic and\ngeometric information. To address this, we propose Sketch2Symm, a two-stage\ngeneration method that produces geometrically consistent 3D shapes from\nsketches. Our approach introduces semantic bridging via sketch-to-image\ntranslation to enrich sparse sketch representations, and incorporates symmetry\nconstraints as geometric priors to leverage the structural regularity commonly\nfound in everyday objects. Experiments on mainstream sketch datasets\ndemonstrate that our method achieves superior performance compared to existing\nsketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's\nDistance, and F-Score, verifying the effectiveness of the proposed semantic\nbridging and symmetry-aware design.",
    "published": "2025-10-13T11:49:45Z",
    "updated": "2025-10-13T11:49:45Z",
    "link": "http://arxiv.org/pdf/2510.11303v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yan Zhou",
      "Mingji Li",
      "Xiantao Zeng",
      "Jie Lin",
      "Yuexia Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11296v1",
    "title": "$Δ\\mathrm{Energy}$: Optimizing Energy Change During\n  Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
    "summary": "Recent approaches for vision-language models (VLMs) have shown remarkable\nsuccess in achieving fast downstream adaptation. When applied to real-world\ndownstream tasks, VLMs inevitably encounter both the in-distribution (ID) data\nand out-of-distribution (OOD) data. The OOD datasets often include both\ncovariate shifts (e.g., known classes with changes in image styles) and\nsemantic shifts (e.g., test-time unseen classes). This highlights the\nimportance of improving VLMs' generalization ability to covariate-shifted OOD\ndata, while effectively detecting open-set semantic-shifted OOD classes. In\nthis paper, inspired by the substantial energy change observed in closed-set\ndata when re-aligning vision-language modalities (specifically by directly\nreducing the maximum cosine similarity to a low value), we introduce a novel\nOOD score, named {\\Delta}Energy. {\\Delta}Energy significantly outperforms the\nvanilla energy-based OOD score and provides a more reliable approach for OOD\ndetection. Furthermore, {\\Delta}Energy can simultaneously improve OOD\ngeneralization under covariate shifts, which is achieved by lower-bound\nmaximization for {\\Delta}Energy (termed EBM). EBM is theoretically proven to\nnot only enhance OOD detection but also yields a domain-consistent Hessian,\nwhich serves as a strong indicator for OOD generalization. Based on this\nfinding, we developed a unified fine-tuning framework that allows for improving\nVLMs' robustness in both OOD generalization and OOD detection. Extensive\nexperiments on challenging OOD detection and generalization benchmarks\ndemonstrate the superiority of our method, outperforming recent approaches by\n10% to 25% in AUROC.",
    "published": "2025-10-13T11:36:58Z",
    "updated": "2025-10-13T11:36:58Z",
    "link": "http://arxiv.org/pdf/2510.11296v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Lin Zhu",
      "Yifeng Yang",
      "Xinbing Wang",
      "Qinying Gu",
      "Nanyang Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11295v1",
    "title": "Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual\n  Question Answering",
    "summary": "Large vision-language models (VLMs) achieve strong performance in Visual\nQuestion Answering but still rely heavily on supervised fine-tuning (SFT) with\nmassive labeled datasets, which is costly due to human annotations. Crucially,\nreal-world datasets often exhibit human uncertainty (HU) -- variation in human\nconfidence across annotations -- but standard SFT simply optimizes toward the\nmost frequent label, disregarding HU distributions. This leaves two open\nquestions: How does HU affect SFT, and how can HU be effectively leveraged in\ntraining? In this work, we first conduct a systematic evaluation of VLMs across\nvarying HU levels. We have two key findings: (i) surprisingly, high-HU samples\ncontribute little or even degrade model performance, and (ii) naively training\non the full dataset yields under-calibrated models that fail to capture HU\ndistributions. Motivated by these findings, we introduce HaDola, a human\nuncertainty-aware data selection and automatic labeling framework. HaDola\noperates in four stages -- discriminate, self-annotate, error trigger, and\ntraining -- to iteratively identify harmful samples, prioritize informative\nones, and bootstrap from a small seed set (5\\% of data). Our approach\nsubstantially reduces reliance on costly HU annotations and makes VLMs more\naccurate and better calibrated. Extensive experiments on VQAv2 and VizWiz\ndatasets demonstrate that HaDola consistently matches or outperforms\nstate-of-the-art baselines with less training data. Our work highlights the\nimportance of explicitly modeling HU in SFT, suggesting that better utilization\nof HU is more effective than merely scaling up dataset size.",
    "published": "2025-10-13T11:35:30Z",
    "updated": "2025-10-13T11:35:30Z",
    "link": "http://arxiv.org/pdf/2510.11295v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jian Lan",
      "Zhicheng Liu",
      "Udo Schlegel",
      "Raoyuan Zhao",
      "Yihong Liu",
      "Hinrich Schütze",
      "Michael A. Hedderich",
      "Thomas Seidl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11287v1",
    "title": "EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable\n  Gating Mechanism",
    "summary": "Medical image segmentation is vital for diagnosis, treatment planning, and\ndisease monitoring but is challenged by complex factors like ambiguous edges\nand background noise. We introduce EEMS, a new model for segmentation,\ncombining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt\nGeneration Unit (MSPGU). EAEU enhances edge perception via multi-frequency\nfeature extraction, accurately defining boundaries. MSPGU integrates high-level\nsemantic and low-level spatial features using a prompt-guided approach,\nensuring precise target localization. The Dual-Source Adaptive Gated Fusion\nUnit (DAGFU) merges edge features from EAEU with semantic features from MSPGU,\nenhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018\nconfirm EEMS's superior performance and reliability as a clinical tool.",
    "published": "2025-10-13T11:21:57Z",
    "updated": "2025-10-13T11:21:57Z",
    "link": "http://arxiv.org/pdf/2510.11287v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Han Xia",
      "Quanjun Li",
      "Qian Li",
      "Zimeng Li",
      "Hongbin Ye",
      "Yupeng Liu",
      "Haolun Li",
      "Xuhang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11268v1",
    "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
    "summary": "Image classifiers play a critical role in detecting diseases in medical\nimaging and identifying anomalies in manufacturing processes. However, their\npredefined behaviors after extensive training make post hoc model editing\ndifficult, especially when it comes to forgetting specific classes or adapting\nto distribution shifts. Existing classifier editing methods either focus\nnarrowly on correcting errors or incur extensive retraining costs, creating a\nbottleneck for flexible editing. Moreover, such editing has seen limited\ninvestigation in image classification. To overcome these challenges, we\nintroduce Class Vectors, which capture class-specific representation\nadjustments during fine-tuning. Whereas task vectors encode task-level changes\nin weight space, Class Vectors disentangle each class's adaptation in the\nlatent space. We show that Class Vectors capture each class's semantic shift\nand that classifier editing can be achieved either by steering latent features\nalong these vectors or by mapping them into weight space to update the decision\nboundaries. We also demonstrate that the inherent linearity and orthogonality\nof Class Vectors support efficient, flexible, and high-level concept editing\nvia simple class arithmetic. Finally, we validate their utility in applications\nsuch as unlearning, environmental adaptation, adversarial defense, and\nadversarial trigger optimization.",
    "published": "2025-10-13T10:57:51Z",
    "updated": "2025-10-13T10:57:51Z",
    "link": "http://arxiv.org/pdf/2510.11268v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jaeik Kim",
      "Jaeyoung Do"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11260v1",
    "title": "A Large-Language-Model Assisted Automated Scale Bar Detection and\n  Extraction Framework for Scanning Electron Microscopic Images",
    "summary": "Microscopic characterizations, such as Scanning Electron Microscopy (SEM),\nare widely used in scientific research for visualizing and analyzing\nmicrostructures. Determining the scale bars is an important first step of\naccurate SEM analysis; however, currently, it mainly relies on manual\noperations, which is both time-consuming and prone to errors. To address this\nissue, we propose a multi-modal and automated scale bar detection and\nextraction framework that provides concurrent object detection, text detection\nand text recognition with a Large Language Model (LLM) agent. The proposed\nframework operates in four phases; i) Automatic Dataset Generation (Auto-DG)\nmodel to synthesize a diverse dataset of SEM images ensuring robust training\nand high generalizability of the model, ii) scale bar object detection, iii)\ninformation extraction using a hybrid Optical Character Recognition (OCR)\nsystem with DenseNet and Convolutional Recurrent Neural Network (CRNN) based\nalgorithms, iv) an LLM agent to analyze and verify accuracy of the results. The\nproposed model demonstrates a strong performance in object detection and\naccurate localization with a precision of 100%, recall of 95.8%, and a mean\nAverage Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The\nhybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the\nAuto-DG dataset, significantly outperforming several mainstream standalone\nengines, highlighting its reliability for scientific image analysis. The LLM is\nintroduced as a reasoning engine as well as an intelligent assistant that\nsuggests follow-up steps and verifies the results. This automated method\npowered by an LLM agent significantly enhances the efficiency and accuracy of\nscale bar detection and extraction in SEM images, providing a valuable tool for\nmicroscopic analysis and advancing the field of scientific imaging.",
    "published": "2025-10-13T10:50:54Z",
    "updated": "2025-10-13T10:50:54Z",
    "link": "http://arxiv.org/pdf/2510.11260v1.pdf",
    "category": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.data-an"
    ],
    "authors": [
      "Yuxuan Chen",
      "Ruotong Yang",
      "Zhengyang Zhang",
      "Mehreen Ahmed",
      "Yanming Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11259v1",
    "title": "DTEA: Dynamic Topology Weaving and Instability-Driven Entropic\n  Attenuation for Medical Image Segmentation",
    "summary": "In medical image segmentation, skip connections are used to merge global\ncontext and reduce the semantic gap between encoder and decoder. Current\nmethods often struggle with limited structural representation and insufficient\ncontextual modeling, affecting generalization in complex clinical scenarios. We\npropose the DTEA model, featuring a new skip connection framework with the\nSemantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG)\nmodules. STR reorganizes multi-scale semantic features into a dynamic\nhypergraph to better model cross-resolution anatomical dependencies, enhancing\nstructural and semantic representation. EPG assesses channel stability after\nperturbation and filters high-entropy channels to emphasize clinically\nimportant regions and improve spatial attention. Extensive experiments on three\nbenchmark datasets show our framework achieves superior segmentation accuracy\nand better generalization across various clinical settings. The code is\navailable at\n\\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.",
    "published": "2025-10-13T10:50:41Z",
    "updated": "2025-10-13T10:50:41Z",
    "link": "http://arxiv.org/pdf/2510.11259v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Weixuan Li",
      "Quanjun Li",
      "Guang Yu",
      "Song Yang",
      "Zimeng Li",
      "Chi-Man Pun",
      "Yupeng Liu",
      "Xuhang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11243v1",
    "title": "Nepali Sign Language Characters Recognition: Dataset Development and\n  Deep Learning Approaches",
    "summary": "Sign languages serve as essential communication systems for individuals with\nhearing and speech impairments. However, digital linguistic dataset resources\nfor underrepresented sign languages, such as Nepali Sign Language (NSL), remain\nscarce. This study introduces the first benchmark dataset for NSL, consisting\nof 36 gesture classes with 1,500 samples per class, designed to capture the\nstructural and visual features of the language. To evaluate recognition\nperformance, we fine-tuned MobileNetV2 and ResNet50 architectures on the\ndataset, achieving classification accuracies of 90.45% and 88.78%,\nrespectively. These findings demonstrate the effectiveness of convolutional\nneural networks in sign recognition tasks, particularly within low-resource\nsettings. To the best of our knowledge, this work represents the first\nsystematic effort to construct a benchmark dataset and assess deep learning\napproaches for NSL recognition, highlighting the potential of transfer learning\nand fine-tuning for advancing research in underexplored sign languages.",
    "published": "2025-10-13T10:29:08Z",
    "updated": "2025-10-13T10:29:08Z",
    "link": "http://arxiv.org/pdf/2510.11243v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Birat Poudel",
      "Satyam Ghimire",
      "Sijan Bhattarai",
      "Saurav Bhandari",
      "Suramya Sharma Dahal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19257v2",
    "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
    "published": "2025-08-15T12:03:34Z",
    "updated": "2025-10-13T10:18:34Z",
    "link": "http://arxiv.org/pdf/2508.19257v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Chenghao Liu",
      "Jiachen Zhang",
      "Chengxuan Li",
      "Zhimu Zhou",
      "Shixin Wu",
      "Songfang Huang",
      "Huiling Duan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11232v1",
    "title": "LightPneumoNet: Lightweight Pneumonia Classifier",
    "summary": "Effective pneumonia diagnosis is often challenged by the difficulty of\ndeploying large, computationally expensive deep learning models in\nresource-limited settings. This study introduces LightPneumoNet, an efficient,\nlightweight convolutional neural network (CNN) built from scratch to provide an\naccessible and accurate diagnostic solution for pneumonia detection from chest\nX-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.\nPreprocessing included image resizing to 224x224, grayscale conversion, and\npixel normalization, with data augmentation (rotation, zoom, shear) to prevent\noverfitting. The custom architecture features four blocks of stacked\nconvolutional layers and contains only 388,082 trainable parameters, resulting\nin a minimal 1.48 MB memory footprint. On the independent test set, our model\ndelivered exceptional performance, achieving an overall accuracy of 0.942,\nprecision of 0.92, and an F1-Score of 0.96. Critically, it obtained a\nsensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify\ntrue pneumonia cases and minimize clinically significant false negatives.\nNotably, LightPneumoNet achieves this high recall on the same dataset where\nexisting approaches typically require significantly heavier architectures or\nfail to reach comparable sensitivity levels. The model's efficiency enables\ndeployment on low-cost hardware, making advanced computer-aided diagnosis\naccessible in underserved clinics and serving as a reliable second-opinion tool\nto improve patient outcomes.",
    "published": "2025-10-13T10:14:17Z",
    "updated": "2025-10-13T10:14:17Z",
    "link": "http://arxiv.org/pdf/2510.11232v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Neilansh Chauhan",
      "Piyush Kumar Gupta",
      "Faraz Doja"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11223v1",
    "title": "Investigating Identity Signals in Conversational Facial Dynamics via\n  Disentangled Expression Features",
    "summary": "This work investigates whether individuals can be identified solely through\nthe pure dynamical components of their facial expressions, independent of\nstatic facial appearance. We leverage the FLAME 3D morphable model to achieve\nexplicit disentanglement between facial shape and expression dynamics,\nextracting frame-by-frame parameters from conversational videos while retaining\nonly expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers\nin naturalistic conversations, our Conformer model with supervised contrastive\nlearning achieves 61.14\\%accuracy on 1,429-way classification -- 458 times\nabove chance -- demonstrating that facial dynamics carry strong identity\nsignatures. We introduce a drift-to-noise ratio (DNR) that quantifies the\nreliability of shape expression separation by measuring across-session shape\nchanges relative to within-session variability. DNR strongly negatively\ncorrelates with recognition performance, confirming that unstable shape\nestimation compromises dynamic identification. Our findings reveal\nperson-specific signatures in conversational facial dynamics, with implications\nfor social perception and clinical assessment.",
    "published": "2025-10-13T10:06:25Z",
    "updated": "2025-10-13T10:06:25Z",
    "link": "http://arxiv.org/pdf/2510.11223v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Masoumeh Chapariniya",
      "Pierre Vuillecard",
      "Jean-Marc Odobez",
      "Volker Dellwo",
      "Teodora Vukovic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10264v2",
    "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating\n  Hallucinations in LVLMs",
    "summary": "Large Vision-Language Models (LVLMs) have shown strong performance across\nmultimodal tasks. However, they often produce hallucinations -- text that is\ninconsistent with visual input, due to the limited ability to verify\ninformation in different regions of the image. To address this, we propose\nMulti-Region Fusion Decoding (MRFD), a training-free decoding method that\nimproves factual grounding by modeling inter-region consistency. MRFD\nidentifies salient regions using cross-attention, generates initial responses\nfor each, and computes reliability weights based on Jensen-Shannon Divergence\n(JSD) among the responses. These weights guide a consistency-aware fusion of\nper-region predictions, using region-aware prompts inspired by Chain-of-Thought\nreasoning. Experiments across multiple LVLMs and benchmarks show that MRFD\nsignificantly reduces hallucinations and improves response factuality without\nrequiring model updates.",
    "published": "2025-08-14T01:17:39Z",
    "updated": "2025-10-13T09:52:34Z",
    "link": "http://arxiv.org/pdf/2508.10264v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haonan Ge",
      "Yiwei Wang",
      "Ming-Hsuan Yang",
      "Yujun Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08273v3",
    "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion\n  Models for Text-Guided Image Inpainting",
    "summary": "Text-guided image inpainting aims at reconstructing the masked regions as per\ntext prompts, where the longstanding challenges lie in the preservation for\nunmasked regions, while achieving the semantics consistency between unmasked\nand inpainted masked regions. Previous arts failed to address both of them,\nalways with either of them to be remedied. Such facts, as we observed, stem\nfrom the entanglement of the hybrid (e.g., mid-and-low) frequency bands that\nencode varied image properties, which exhibit different robustness to text\nprompts during the denoising process. In this paper, we propose a\nnull-text-null frequency-aware diffusion models, dubbed \\textbf{NTN-Diff}, for\ntext-guided image inpainting, by decomposing the semantics consistency across\nmasked and unmasked regions into the consistencies as per each frequency band,\nwhile preserving the unmasked regions, to circumvent two challenges in a row.\nBased on the diffusion process, we further divide the denoising process into\nearly (high-level noise) and late (low-level noise) stages, where the\nmid-and-low frequency bands are disentangled during the denoising process. As\nobserved, the stable mid-frequency band is progressively denoised to be\nsemantically aligned during text-guided denoising process, which, meanwhile,\nserves as the guidance to the null-text denoising process to denoise\nlow-frequency band for the masked regions, followed by a subsequent text-guided\ndenoising process at late stage, to achieve the semantics consistency for\nmid-and-low frequency bands across masked and unmasked regions, while preserve\nthe unmasked regions. Extensive experiments validate the superiority of\nNTN-Diff over the state-of-the-art diffusion models to text-guided diffusion\nmodels. Our code can be accessed from https://github.com/htyjers/NTN-Diff.",
    "published": "2025-10-09T14:30:34Z",
    "updated": "2025-10-13T09:49:07Z",
    "link": "http://arxiv.org/pdf/2510.08273v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haipeng Liu",
      "Yang Wang",
      "Meng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11204v1",
    "title": "Class Prototypes based Contrastive Learning for Classifying Multi-Label\n  and Fine-Grained Educational Videos",
    "summary": "The recent growth in the consumption of online media by children during early\nchildhood necessitates data-driven tools enabling educators to filter out\nappropriate educational content for young learners. This paper presents an\napproach for detecting educational content in online videos. We focus on two\nwidely used educational content classes: literacy and math. For each class, we\nchoose prominent codes (sub-classes) based on the Common Core Standards. For\nexample, literacy codes include `letter names', `letter sounds', and math codes\ninclude `counting', `sorting'. We pose this as a fine-grained multilabel\nclassification problem as videos can contain multiple types of educational\ncontent and the content classes can get visually similar (e.g., `letter names'\nvs `letter sounds'). We propose a novel class prototypes based supervised\ncontrastive learning approach that can handle fine-grained samples associated\nwith multiple labels. We learn a class prototype for each class and a loss\nfunction is employed to minimize the distances between a class prototype and\nthe samples from the class. Similarly, distances between a class prototype and\nthe samples from other classes are maximized. As the alignment between visual\nand audio cues are crucial for effective comprehension, we consider a\nmultimodal transformer network to capture the interaction between visual and\naudio cues in videos while learning the embedding for videos. For evaluation,\nwe present a dataset, APPROVE, employing educational videos from YouTube\nlabeled with fine-grained education classes by education researchers. APPROVE\nconsists of 193 hours of expert-annotated videos with 19 classes. The proposed\napproach outperforms strong baselines on APPROVE and other benchmarks such as\nYoutube-8M, and COIN. The dataset is available at\nhttps://github.com/rohit-gupta/MMContrast/tree/main/APPROVE",
    "published": "2025-10-13T09:36:26Z",
    "updated": "2025-10-13T09:36:26Z",
    "link": "http://arxiv.org/pdf/2510.11204v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rohit Gupta",
      "Anirban Roy",
      "Claire Christensen",
      "Sujeong Kim",
      "Sarah Gerard",
      "Madeline Cincebeaux",
      "Ajay Divakaran",
      "Todd Grindal",
      "Mubarak Shah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21524v2",
    "title": "Learning Shared Representations from Unpaired Data",
    "summary": "Learning shared representations is a primary area of multimodal\nrepresentation learning. The current approaches to achieve a shared embedding\nspace rely heavily on paired samples from each modality, which are\nsignificantly harder to obtain than unpaired ones. In this work, we demonstrate\nthat shared representations can be learned almost exclusively from unpaired\ndata. Our arguments are grounded in the spectral embeddings of the random walk\nmatrices constructed independently from each unimodal representation. Empirical\nresults in computer vision and natural language processing domains support its\npotential, revealing the effectiveness of unpaired data in capturing meaningful\ncross-modal relations, demonstrating high capabilities in retrieval tasks,\ngeneration, arithmetics, zero-shot, and cross-domain classification. This work,\nto the best of our knowledge, is the first to demonstrate these capabilities\nalmost exclusively from unpaired samples, giving rise to a cross-modal\nembedding that could be viewed as universal, i.e., independent of the specific\nmodalities of the data. Our project page:\nhttps://shaham-lab.github.io/SUE_page.",
    "published": "2025-05-23T11:13:04Z",
    "updated": "2025-10-13T09:35:27Z",
    "link": "http://arxiv.org/pdf/2505.21524v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Amitai Yacobi",
      "Nir Ben-Ari",
      "Ronen Talmon",
      "Uri Shaham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06427v2",
    "title": "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle\n  Detection",
    "summary": "Muzzle patterns are among the most effective biometric traits for cattle\nidentification. Fast and accurate detection of the muzzle region as the region\nof interest is critical to automatic visual cattle identification.. Earlier\napproaches relied on manual detection, which is labor-intensive and\ninconsistent. Recently, automated methods using supervised models like YOLO\nhave become popular for muzzle detection. Although effective, these methods\nrequire extensive annotated datasets and tend to be trained data-dependent,\nlimiting their performance on new or unseen cattle. To address these\nlimitations, this study proposes a zero-shot muzzle detection framework based\non Grounding DINO, a vision-language model capable of detecting muzzles without\nany task-specific training or annotated data. This approach leverages natural\nlanguage prompts to guide detection, enabling scalable and flexible muzzle\nlocalization across diverse breeds and environments. Our model achieves a mean\nAverage Precision (mAP)@0.5 of 76.8\\%, demonstrating promising performance\nwithout requiring annotated data. To our knowledge, this is the first research\nto provide a real-world, industry-oriented, and annotation-free solution for\ncattle muzzle detection. The framework offers a practical alternative to\nsupervised methods, promising improved adaptability and ease of deployment in\nlivestock monitoring applications.",
    "published": "2025-09-08T08:21:34Z",
    "updated": "2025-10-13T09:28:02Z",
    "link": "http://arxiv.org/pdf/2509.06427v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rabin Dulal",
      "Lihong Zheng",
      "Muhammad Ashad Kabir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11190v1",
    "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal\n  Large Language Models",
    "summary": "Multimodal large language models (MLLMs) face an inherent trade-off between\nfaithfulness and creativity, as different tasks require varying degrees of\nassociative reasoning. However, existing methods lack the flexibility to\nmodulate this reasoning strength, limiting MLLMs' adaptability across factual\nand creative scenarios. To bridge this gap, we propose equipping MLLMs with\nmechanisms that enable flexible control over associative reasoning. We begin by\ninvestigating the internal mechanisms underlying associative behavior in MLLMs\nand find that: (1) middle layers play a pivotal role in shaping model's\nassociative tendencies, (2) modifying representations in these layers\neffectively regulates associative reasoning strength, and (3) hallucinations\ncan be exploited to derive steering vectors that guide this modulation.\nBuilding on these findings, we introduce Flexible Association Control (FlexAC),\na lightweight and training-free framework for modulating associative behavior\nin MLLMs. FlexAC first induces hallucination-guided intermediate\nrepresentations to encode associative directions. Then, it selects\nhigh-association instances to construct effective associative steering vectors,\nwhose strengths are adaptively calibrated to balance creative guidance with\noutput stability. Finally, recognizing the multi-dimensional nature of\nassociative reasoning, FlexAC incorporates task-specific associative vectors\nderived from a forward pass on a few target-domain samples, enabling models to\nfollow diverse associative directions and better adapt to creative tasks.\nNotably, our method achieves up to a 5.8x improvement in creativity on\nCreation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing\nexisting baselines and demonstrating its effectiveness in enabling flexible\ncontrol over associative reasoning in MLLMs. Our code is available at\nhttps://github.com/ylhz/FlexAC.",
    "published": "2025-10-13T09:22:12Z",
    "updated": "2025-10-13T09:22:12Z",
    "link": "http://arxiv.org/pdf/2510.11190v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shengming Yuan",
      "Xinyu Lyu",
      "Shuailong Wang",
      "Beitao Chen",
      "Jingkuan Song",
      "Lianli Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11183v1",
    "title": "Saudi Sign Language Translation Using T5",
    "summary": "This paper explores the application of T5 models for Saudi Sign Language\n(SSL) translation using a novel dataset. The SSL dataset includes three\nchallenging testing protocols, enabling comprehensive evaluation across\ndifferent scenarios. Additionally, it captures unique SSL characteristics, such\nas face coverings, which pose challenges for sign recognition and translation.\nIn our experiments, we investigate the impact of pre-training on American Sign\nLanguage (ASL) data by comparing T5 models pre-trained on the YouTubeASL\ndataset with models trained directly on the SSL dataset. Experimental results\ndemonstrate that pre-training on YouTubeASL significantly improves models'\nperformance (roughly $3\\times$ in BLEU-4), indicating cross-linguistic\ntransferability in sign language models. Our findings highlight the benefits of\nleveraging large-scale ASL data to improve SSL translation and provide insights\ninto the development of more effective sign language translation systems. Our\ncode is publicly available at our GitHub repository.",
    "published": "2025-10-13T09:18:34Z",
    "updated": "2025-10-13T09:18:34Z",
    "link": "http://arxiv.org/pdf/2510.11183v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ali Alhejab",
      "Tomas Zelezny",
      "Lamya Alkanhal",
      "Ivan Gruber",
      "Yazeed Alharbi",
      "Jakub Straka",
      "Vaclav Javorek",
      "Marek Hruz",
      "Badriah Alkalifah",
      "Ahmed Ali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03356v2",
    "title": "Learned Display Radiance Fields with Lensless Cameras",
    "summary": "Calibrating displays is a basic and regular task that content creators must\nperform to maintain optimal visual experience, yet it remains a troublesome\nissue. Measuring display characteristics from different viewpoints often\nrequires specialized equipment and a dark room, making it inaccessible to most\nusers. To avoid specialized hardware requirements in display calibrations, our\nwork co-designs a lensless camera and an Implicit Neural Representation based\nalgorithm for capturing display characteristics from various viewpoints. More\nspecifically, our pipeline enables efficient reconstruction of light fields\nemitted from a display from a viewing cone of 46.6{\\deg} X 37.6{\\deg}. Our\nemerging pipeline paves the initial steps towards effortless display\ncalibration and characterization.",
    "published": "2025-10-02T23:11:04Z",
    "updated": "2025-10-13T09:18:28Z",
    "link": "http://arxiv.org/pdf/2510.03356v2.pdf",
    "category": [
      "cs.CV",
      "cs.ET"
    ],
    "authors": [
      "Ziyang Chen",
      "Yuta Itoh",
      "Kaan Akşit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11182v1",
    "title": "Generalisation of automatic tumour segmentation in histopathological\n  whole-slide images across multiple cancer types",
    "summary": "Deep learning is expected to aid pathologists by automating tasks such as\ntumour segmentation. We aimed to develop one universal tumour segmentation\nmodel for histopathological images and examine its performance in different\ncancer types. The model was developed using over 20 000 whole-slide images from\nover 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma.\nPerformance was validated in pre-planned analyses on external cohorts with over\n3 000 patients across six cancer types. Exploratory analyses included over 1\n500 additional patients from The Cancer Genome Atlas. Average Dice coefficient\nwas over 80% in all validation cohorts with en bloc resection specimens and in\nThe Cancer Genome Atlas cohorts. No loss of performance was observed when\ncomparing the universal model with models specialised on single cancer types.\nIn conclusion, extensive and rigorous evaluations demonstrate that generic\ntumour segmentation by a single model is possible across cancer types, patient\npopulations, sample preparations, and slide scanners.",
    "published": "2025-10-13T09:18:15Z",
    "updated": "2025-10-13T09:18:15Z",
    "link": "http://arxiv.org/pdf/2510.11182v1.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Ole-Johan Skrede",
      "Manohar Pradhan",
      "Maria Xepapadakis Isaksen",
      "Tarjei Sveinsgjerd Hveem",
      "Ljiljana Vlatkovic",
      "Arild Nesbakken",
      "Kristina Lindemann",
      "Gunnar B Kristensen",
      "Jenneke Kasius",
      "Alain G Zeimet",
      "Odd Terje Brustugun",
      "Lill-Tove Rasmussen Busund",
      "Elin H Richardsen",
      "Erik Skaaheim Haug",
      "Bjørn Brennhovd",
      "Emma Rewcastle",
      "Melinda Lillesand",
      "Vebjørn Kvikstad",
      "Emiel Janssen",
      "David J Kerr",
      "Knut Liestøl",
      "Fritz Albregtsen",
      "Andreas Kleppe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23606v2",
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
    "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
    "published": "2025-05-29T16:15:48Z",
    "updated": "2025-10-13T09:17:52Z",
    "link": "http://arxiv.org/pdf/2505.23606v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Qingyu Shi",
      "Jinbin Bai",
      "Zhuoran Zhao",
      "Wenhao Chai",
      "Kaidong Yu",
      "Jianzong Wu",
      "Shuangyong Song",
      "Yunhai Tong",
      "Xiangtai Li",
      "Xuelong Li",
      "Shuicheng Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24695v2",
    "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
    "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
    "published": "2025-09-29T12:28:09Z",
    "updated": "2025-10-13T09:12:27Z",
    "link": "http://arxiv.org/pdf/2509.24695v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Junsong Chen",
      "Yuyang Zhao",
      "Jincheng Yu",
      "Ruihang Chu",
      "Junyu Chen",
      "Shuai Yang",
      "Xianbang Wang",
      "Yicheng Pan",
      "Daquan Zhou",
      "Huan Ling",
      "Haozhe Liu",
      "Hongwei Yi",
      "Hao Zhang",
      "Muyang Li",
      "Yukang Chen",
      "Han Cai",
      "Sanja Fidler",
      "Ping Luo",
      "Song Han",
      "Enze Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11178v1",
    "title": "BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision\n  Language Models",
    "summary": "As vision-language models (VLMs) are deployed globally, their ability to\nunderstand culturally situated knowledge becomes essential. Yet, existing\nevaluations largely assess static recall or isolated visual grounding, leaving\nunanswered whether VLMs possess robust and transferable cultural understanding.\nWe introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to\nevaluate the robustness of everyday cultural knowledge in VLMs across\nlinguistic rephrasings and visual modalities. Building on the BLEnD dataset,\nBLEnD-Vis constructs 313 culturally grounded question templates spanning 16\nregions and generates three aligned multiple-choice formats: (i) a text-only\nbaseline querying from Region $\\to$ Entity, (ii) an inverted text-only variant\n(Entity $\\to$ Region), and (iii) a VQA-style version of (ii) with generated\nimages. The resulting benchmark comprises 4,916 images and over 21,000\nmultiple-choice question (MCQ) instances, validated through human annotation.\nBLEnD-Vis reveals significant fragility in current VLM cultural knowledge;\nmodels exhibit performance drops under linguistic rephrasing and, whilst visual\ncues often aid performance, low cross-modal consistency highlights challenges\nin robustly integrating textual and visual understanding, particularly for\nlower-resource regions. BLEnD-Vis thus provides a crucial testbed for\nsystematically analysing cultural robustness and multimodal grounding, exposing\nlimitations and guiding the development of more culturally competent VLMs.",
    "published": "2025-10-13T09:10:05Z",
    "updated": "2025-10-13T09:10:05Z",
    "link": "http://arxiv.org/pdf/2510.11178v1.pdf",
    "category": [
      "cs.CV",
      "cs.CY"
    ],
    "authors": [
      "Bryan Chen Zhengyu Tan",
      "Zheng Weihua",
      "Zhengyuan Liu",
      "Nancy F. Chen",
      "Hwaran Lee",
      "Kenny Tsu Wei Choo",
      "Roy Ka-Wei Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11176v1",
    "title": "G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation\n  Models via Knowledge Distillation",
    "summary": "Recent studies in pathology foundation models have shown that scaling\ntraining data, diversifying cancer types, and increasing model size\nconsistently improve their performance. However, giga-scale foundation models,\nwhich are trained on hundreds of thousands of slides covering tens of cancer\ntypes and contain billions of parameters, pose significant challenges for\npractical use due to their tremendous computational costs in both development\nand deployment. In this work, we present a novel strategy, named the G2L\nframework, to increase the performance of large-scale foundation models, which\nconsist of only $15\\%$ of the parameters of giga-scale models, to a comparable\nperformance level of giga-scale models in cancer-specific tasks. Our approach\napplies knowledge distillation, transferring the capabilities of a giga-scale\nmodel to a large-scale model, using just 1K pathology slides of a target cancer\n(e.g., breast, prostate, etc.). The resulting distilled model not only\noutperformed state-of-the-art models of the same size (i.e., large-scale)\nacross several benchmarks but also, interestingly, surpassed the giga-scale\nteacher and huge-scale models in some benchmarks. In addition, the distilled\nmodel exhibited a higher robustness index, indicating improved resilience to\nimage variations originating from multiple institutions. These findings suggest\nthat the proposed distillation approach for a large-scale model is a data- and\nparameter-efficient way to achieve giga-scale-level performance for\ncancer-specific applications without prohibitive computational burden.",
    "published": "2025-10-13T09:08:59Z",
    "updated": "2025-10-13T09:08:59Z",
    "link": "http://arxiv.org/pdf/2510.11176v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yesung Cho",
      "Sungmin Lee",
      "Geongyu Lee",
      "Minkyung Lee",
      "Jongbae Park",
      "Dongmyung Shin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11175v1",
    "title": "Reliable Cross-modal Alignment via Prototype Iterative Construction",
    "summary": "Cross-modal alignment is an important multi-modal task, aiming to bridge the\nsemantic gap between different modalities. The most reliable fundamention for\nachieving this objective lies in the semantic consistency between matched\npairs. Conventional methods implicitly assume embeddings contain solely\nsemantic information, ignoring the impact of non-semantic information during\nalignment, which inevitably leads to information bias or even loss. These\nnon-semantic information primarily manifest as stylistic variations in the\ndata, which we formally define as style information. An intuitive approach is\nto separate style from semantics, aligning only the semantic information.\nHowever, most existing methods distinguish them based on feature columns, which\ncannot represent the complex coupling relationship between semantic and style\ninformation. In this paper, we propose PICO, a novel framework for suppressing\nstyle interference during embedding interaction. Specifically, we quantify the\nprobability of each feature column representing semantic information, and\nregard it as the weight during the embedding interaction. To ensure the\nreliability of the semantic probability, we propose a prototype iterative\nconstruction method. The key operation of this method is a performance\nfeedback-based weighting function, and we have theoretically proven that the\nfunction can assign higher weight to prototypes that bring higher performance\nimprovements. Extensive experiments on various benchmarks and model backbones\ndemonstrate the superiority of PICO, outperforming state-of-the-art methods by\n5.2\\%-14.1\\%.",
    "published": "2025-10-13T09:08:27Z",
    "updated": "2025-10-13T09:08:27Z",
    "link": "http://arxiv.org/pdf/2510.11175v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiang Ma",
      "Litian Xu",
      "Lexin Fang",
      "Caiming Zhang",
      "Lizhen Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11173v1",
    "title": "CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning\n  Segmentation",
    "summary": "Existing works on reasoning segmentation either connect hidden features from\na language model directly to a mask decoder or represent positions in text,\nwhich limits interpretability and semantic detail. To solve this, we present\nCoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model\nthat bridges language reasoning to segmentation through a differentiable and\ninterpretable positional prior instantiated as a heatmap. By making the\nreasoning process clear via MCoT and expressing it as a dense, differentiable\nheatmap, this interface enhances interpretability and diagnostic analysis and\nyields more concentrated evidence on the target. A learnable concentration\ntoken aggregates features of the image and reasoning text to generate this\npositional prior, which is decoded to precise masks through a lightweight\ndecoder, providing a direct connection between reasoning and segmentation.\nAcross the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best\nreported metrics on each standard split under comparable protocols, with\nperformance at or above prior state of the art across both validation and test\npartitions. Extensive experiments reveal that the quality of the heatmap\nstrongly influences the resulting mask quality, supporting a consistent\nassociation between the reasoning output and downstream mask generation.\nCollectively, these findings support the utility of this paradigm in bridging\nreasoning and segmentation and show advantages in concentration driven by\nreasoning and predicting masks more precisely. Code, checkpoints and logs are\nreleased at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.",
    "published": "2025-10-13T09:07:54Z",
    "updated": "2025-10-13T09:07:54Z",
    "link": "http://arxiv.org/pdf/2510.11173v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Zhenyu Lu",
      "Liupeng Li",
      "Jinpeng Wang",
      "Yan Feng",
      "Bin Chen",
      "Ke Chen",
      "Yaowei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11171v1",
    "title": "Multiview Manifold Evidential Fusion for PolSAR Image Classification",
    "summary": "Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their\nextracted multi-features - such as scattering angle, entropy, texture, and\nboundary descriptors - provide complementary and physically interpretable\ninformation for image classification. Traditional fusion strategies typically\nconcatenate these features or employ deep learning networks to combine them.\nHowever, the covariance matrices and multi-features, as two complementary\nviews, lie on different manifolds with distinct geometric structures. Existing\nfusion methods also overlook the varying importance of different views and\nignore uncertainty, often leading to unreliable predictions. To address these\nissues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to\neffectively fuse these two views. It gives a new framework to integrate PolSAR\nmanifold learning and evidence fusion into a unified architecture.\nSpecifically, covariance matrices are represented on the Hermitian Positive\nDefinite (HPD) manifold, while multi-features are modeled on the Grassmann\nmanifold. Two different kernel metric learning networks are constructed to\nlearn their manifold representations. Subsequently, a trusted multiview\nevidence fusion, replacing the conventional softmax classifier, estimates\nbelief mass and quantifies the uncertainty of each view from the learned deep\nfeatures. Finally, a Dempster-Shafer theory-based fusion strategy combines\nevidence, enabling a more reliable and interpretable classification. Extensive\nexperiments on three real-world PolSAR datasets demonstrate that the proposed\nmethod consistently outperforms existing approaches in accuracy, robustness,\nand interpretability.",
    "published": "2025-10-13T09:05:51Z",
    "updated": "2025-10-13T09:05:51Z",
    "link": "http://arxiv.org/pdf/2510.11171v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junfei Shi",
      "Haojia Zhang",
      "Haiyan Jin",
      "Junhuai Li",
      "Xiaogang Song",
      "Yuanfan Guo",
      "Haonan Su",
      "Weisi Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.09073v2",
    "title": "Open Vocabulary Multi-Label Video Classification",
    "summary": "Pre-trained vision-language models (VLMs) have enabled significant progress\nin open vocabulary computer vision tasks such as image classification, object\ndetection and image segmentation. Some recent works have focused on extending\nVLMs to open vocabulary single label action classification in videos. However,\nprevious methods fall short in holistic video understanding which requires the\nability to simultaneously recognize multiple actions and entities e.g., objects\nin the video in an open vocabulary setting. We formulate this problem as open\nvocabulary multilabel video classification and propose a method to adapt a\npre-trained VLM such as CLIP to solve this task. We leverage large language\nmodels (LLMs) to provide semantic guidance to the VLM about class labels to\nimprove its open vocabulary performance with two key contributions. First, we\npropose an end-to-end trainable architecture that learns to prompt an LLM to\ngenerate soft attributes for the CLIP text-encoder to enable it to recognize\nnovel classes. Second, we integrate a temporal modeling module into CLIP's\nvision encoder to effectively model the spatio-temporal dynamics of video\nconcepts as well as propose a novel regularized finetuning technique to ensure\nstrong open vocabulary classification performance in the video domain. Our\nextensive experimentation showcases the efficacy of our approach on multiple\nbenchmark datasets.",
    "published": "2024-07-12T07:53:54Z",
    "updated": "2025-10-13T09:00:16Z",
    "link": "http://arxiv.org/pdf/2407.09073v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rohit Gupta",
      "Mamshad Nayeem Rizve",
      "Jayakrishnan Unnikrishnan",
      "Ashish Tawari",
      "Son Tran",
      "Mubarak Shah",
      "Benjamin Yao",
      "Trishul Chilimbi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11142v1",
    "title": "Validation of an Artificial Intelligence Tool for the Detection of Sperm\n  DNA Fragmentation Using the TUNEL In Situ Hybridization Assay",
    "summary": "Sperm DNA fragmentation (SDF) is a critical parameter in male fertility\nassessment that conventional semen analysis fails to evaluate. This study\npresents the validation of a novel artificial intelligence (AI) tool designed\nto detect SDF through digital analysis of phase contrast microscopy images,\nusing the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL)\nassay as the gold standard reference. Utilising the established link between\nsperm morphology and DNA integrity, the present work proposes a morphology\nassisted ensemble AI model that combines image processing techniques with\nstate-of-the-art transformer based machine learning models (GC-ViT) for the\nprediction of DNA fragmentation in sperm from phase contrast images. The\nensemble model is benchmarked against a pure transformer `vision' model as well\nas a `morphology-only` model. Promising results show the proposed framework is\nable to achieve sensitivity of 60\\% and specificity of 75\\%. This\nnon-destructive methodology represents a significant advancement in\nreproductive medicine by enabling real-time sperm selection based on DNA\nintegrity for clinical diagnostic and therapeutic applications.",
    "published": "2025-10-13T08:32:11Z",
    "updated": "2025-10-13T08:32:11Z",
    "link": "http://arxiv.org/pdf/2510.11142v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Byron Alexander Jacobs",
      "Aqeel Morris",
      "Ifthakaar Shaik",
      "Frando Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.05111v3",
    "title": "LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting",
    "summary": "We present LiDAR-GS, a Gaussian Splatting (GS) method for real-time,\nhigh-fidelity re-simulation of LiDAR scans in public urban road scenes. Recent\nGS methods proposed for cameras have achieved significant advancements in\nreal-time rendering beyond Neural Radiance Fields (NeRF). However, applying GS\nrepresentation to LiDAR, an active 3D sensor type, poses several challenges\nthat must be addressed to preserve high accuracy and unique characteristics.\nSpecifically, LiDAR-GS designs a differentiable laser beam splatting, using\nrange-view representation for precise surface splatting by projecting lasers\nonto micro cross-sections, effectively eliminating artifacts associated with\nlocal affine approximations. Furthermore, LiDAR-GS leverages Neural Gaussian\nRepresentation, which further integrate view-dependent clues, to represent key\nLiDAR properties that are influenced by the incident direction and external\nfactors. Combining these practices with some essential adaptations, e.g.,\ndynamic instances decomposition, LiDAR-GS succeeds in simultaneously\nre-simulating depth, intensity, and ray-drop channels, achieving\nstate-of-the-art results in both rendering frame rate and quality on publically\navailable large scene datasets when compared with the methods using explicit\nmesh or implicit NeRF. Our source code is publicly available at\nhttps://www.github.com/cqf7419/LiDAR-GS.",
    "published": "2024-10-07T15:07:56Z",
    "updated": "2025-10-13T08:28:29Z",
    "link": "http://arxiv.org/pdf/2410.05111v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qifeng Chen",
      "Sheng Yang",
      "Sicong Du",
      "Tao Tang",
      "Rengan Xie",
      "Peng Chen",
      "Yuchi Huo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02845v3",
    "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity\n  Environments",
    "summary": "Despite substantial progress in video understanding, most existing datasets\nare limited to Earth's gravitational conditions. However, microgravity alters\nhuman motion, interactions, and visual semantics, revealing a critical gap for\nreal-world vision systems. This presents a challenge for domain-robust video\nunderstanding in safety-critical space applications. To address this, we\nintroduce MicroG-4M, the first benchmark for spatio-temporal and semantic\nunderstanding of human activities in microgravity. Constructed from real-world\nspace missions and cinematic simulations, the dataset includes 4,759 clips\ncovering 50 actions, 1,238 context-rich captions, and over 7,000\nquestion-answer pairs on astronaut activities and scene understanding.\nMicroG-4M supports three core tasks: fine-grained multi-label action\nrecognition, temporal video captioning, and visual question answering, enabling\na comprehensive evaluation of both spatial localization and semantic reasoning\nin microgravity contexts. We establish baselines using state-of-the-art models.\nAll data, annotations, and code are available at\nhttps://github.com/LEI-QI-233/HAR-in-Space.",
    "published": "2025-06-03T13:15:19Z",
    "updated": "2025-10-13T08:22:34Z",
    "link": "http://arxiv.org/pdf/2506.02845v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Di Wen",
      "Lei Qi",
      "Kunyu Peng",
      "Kailun Yang",
      "Fei Teng",
      "Ao Luo",
      "Jia Fu",
      "Yufan Chen",
      "Ruiping Liu",
      "Yitian Shi",
      "M. Saquib Sarfraz",
      "Rainer Stiefelhagen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11129v1",
    "title": "video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via\n  Memory",
    "summary": "Continuous, high-frame-rate, high-resolution processing of long video streams\nis critical for future AI agents, yet current video-understanding LLMs struggle\nto scale. Offline, fixed-frame-number methods require the stream length to\nadapt frame rates; streaming methods constrain memory by merging or discarding\ntokens, losing information. We propose video-SALMONN S, a streaming\naudio-visual LLM that, to our knowledge, is the first to process 3-hour videos\nat 1 FPS and 360p resolution under a fixed memory budget. Our model introduces\n(i) a test-time-training (TTT) memory module that continually updates token\nrepresentations to capture long-range dependencies by replacing token merging,\nand (ii) a prompt-dependent memory reader that selectively retrieves\ncontext-relevant content from fixed-size memory. The TTT module is optimised\nwith a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient\nadaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),\nvideo-SALMONN S sustains high-quality understanding on multi-hour videos with\n10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and\n67.8% on the Video-MME long split, outperforming both offline and streaming\nbaselines.",
    "published": "2025-10-13T08:20:15Z",
    "updated": "2025-10-13T08:20:15Z",
    "link": "http://arxiv.org/pdf/2510.11129v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Guangzhi Sun",
      "Yixuan Li",
      "Xiaodong Wu",
      "Yudong Yang",
      "Wei Li",
      "Zejun Ma",
      "Chao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11128v1",
    "title": "Lightweight Facial Landmark Detection in Thermal Images via Multi-Level\n  Cross-Modal Knowledge Transfer",
    "summary": "Facial Landmark Detection (FLD) in thermal imagery is critical for\napplications in challenging lighting conditions, but it is hampered by the lack\nof rich visual cues. Conventional cross-modal solutions, like feature fusion or\nimage translation from RGB data, are often computationally expensive or\nintroduce structural artifacts, limiting their practical deployment. To address\nthis, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a\nnovel framework that decouples high-fidelity RGB-to-thermal knowledge transfer\nfrom model compression to create both accurate and efficient thermal FLD\nmodels. A central challenge during knowledge transfer is the profound modality\ngap between RGB and thermal data, where traditional unidirectional distillation\nfails to enforce semantic consistency across disparate feature spaces. To\novercome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a\nbidirectional mechanism designed specifically for this task. DIKD establishes a\nconnection between modalities: it not only guides the thermal student with rich\nRGB features but also validates the student's learned representations by\nfeeding them back into the frozen teacher's prediction head. This closed-loop\nsupervision forces the student to learn modality-invariant features that are\nsemantically aligned with the teacher, ensuring a robust and profound knowledge\ntransfer. Experiments show that our approach sets a new state-of-the-art on\npublic thermal FLD benchmarks, notably outperforming previous methods while\ndrastically reducing computational overhead.",
    "published": "2025-10-13T08:19:56Z",
    "updated": "2025-10-13T08:19:56Z",
    "link": "http://arxiv.org/pdf/2510.11128v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Qiyi Tong",
      "Olivia Nocentini",
      "Marta Lagomarsino",
      "Kuanqi Cai",
      "Marta Lorenzini",
      "Arash Ajoudani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11117v1",
    "title": "Demystifying Numerosity in Diffusion Models -- Limitations and Remedies",
    "summary": "Numerosity remains a challenge for state-of-the-art text-to-image generation\nmodels like FLUX and GPT-4o, which often fail to accurately follow counting\ninstructions in text prompts. In this paper, we aim to study a fundamental yet\noften overlooked question: Can diffusion models inherently generate the correct\nnumber of objects specified by a textual prompt simply by scaling up the\ndataset and model size? To enable rigorous and reproducible evaluation, we\nconstruct a clean synthetic numerosity benchmark comprising two complementary\ndatasets: GrayCount250 for controlled scaling studies, and NaturalCount6\nfeaturing complex naturalistic scenes. Second, we empirically show that the\nscaling hypothesis does not hold: larger models and datasets alone fail to\nimprove counting accuracy on our benchmark. Our analysis identifies a key\nreason: diffusion models tend to rely heavily on the noise initialization\nrather than the explicit numerosity specified in the prompt. We observe that\nnoise priors exhibit biases toward specific object counts. In addition, we\npropose an effective strategy for controlling numerosity by injecting\ncount-aware layout information into the noise prior. Our method achieves\nsignificant gains, improving accuracy on GrayCount250 from 20.0\\% to 85.3\\% and\non NaturalCount6 from 74.8\\% to 86.3\\%, demonstrating effective generalization\nacross settings.",
    "published": "2025-10-13T08:07:24Z",
    "updated": "2025-10-13T08:07:24Z",
    "link": "http://arxiv.org/pdf/2510.11117v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yaqi Zhao",
      "Xiaochen Wang",
      "Li Dong",
      "Wentao Zhang",
      "Yuhui Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11115v1",
    "title": "Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal\n  Models for Few-Shot Learning",
    "summary": "Few-shot learning (FSL) addresses the challenge of classifying novel classes\nwith limited training samples. While some methods leverage semantic knowledge\nfrom smaller-scale models to mitigate data scarcity, these approaches often\nintroduce noise and bias due to the data's inherent simplicity. In this paper,\nwe propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which\neffectively transfers diverse and complementary knowledge from large multimodal\nmodels to empower the off-the-shelf few-shot learner. Specifically, SynTrans\nemploys CLIP as a robust teacher and uses a few-shot vision encoder as a weak\nstudent, distilling semantic-aligned visual knowledge via an unsupervised proxy\ntask. Subsequently, a training-free synergistic knowledge mining module\nfacilitates collaboration among large multimodal models to extract high-quality\nsemantic knowledge. Building upon this, a visual-semantic bridging module\nenables bi-directional knowledge transfer between visual and semantic spaces,\ntransforming explicit visual and implicit semantic knowledge into\ncategory-specific classifier weights. Finally, SynTrans introduces a visual\nweight generator and a semantic weight reconstructor to adaptively construct\noptimal multimodal FSL classifiers. Experimental results on four FSL datasets\ndemonstrate that SynTrans, even when paired with a simple few-shot vision\nencoder, significantly outperforms current state-of-the-art methods.",
    "published": "2025-10-13T08:06:23Z",
    "updated": "2025-10-13T08:06:23Z",
    "link": "http://arxiv.org/pdf/2510.11115v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Hao Tang",
      "Shengfeng He",
      "Jing Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11112v1",
    "title": "Multimodal Disease Progression Modeling via Spatiotemporal\n  Disentanglement and Multiscale Alignment",
    "summary": "Longitudinal multimodal data, including electronic health records (EHR) and\nsequential chest X-rays (CXRs), is critical for modeling disease progression,\nyet remains underutilized due to two key challenges: (1) redundancy in\nconsecutive CXR sequences, where static anatomical regions dominate over\nclinically-meaningful dynamics, and (2) temporal misalignment between sparse,\nirregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a\nnovel framework that addresses these challenges through region-aware\ndisentanglement and multi-timescale alignment. First, we disentangle static\n(anatomy) and dynamic (pathology progression) features in sequential CXRs,\nprioritizing disease-relevant changes. Second, we hierarchically align these\nstatic and dynamic CXR features with asynchronous EHR data via local (pairwise\ninterval-level) and global (full-sequence) synchronization to model coherent\nprogression pathways. Extensive experiments on the MIMIC dataset demonstrate\nthat $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and\nachieve state-of-the-art performance on both disease progression identification\nand general ICU prediction tasks.",
    "published": "2025-10-13T08:02:36Z",
    "updated": "2025-10-13T08:02:36Z",
    "link": "http://arxiv.org/pdf/2510.11112v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chen Liu",
      "Wenfang Yao",
      "Kejing Yin",
      "William K. Cheung",
      "Jing Qin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.23719v2",
    "title": "PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary\n  Diagnosis of Parkinson's Disease",
    "summary": "Parkinson's disease (PD) is a common neurodegenerative disorder that severely\ndiminishes patients' quality of life. Its global prevalence has increased\nmarkedly in recent decades. Current diagnostic workflows are complex and\nheavily reliant on neurologists' expertise, often resulting in delays in early\ndetection and missed opportunities for timely intervention. To address these\nissues, we propose an end-to-end automated diagnostic method for PD, termed\nPD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly\nfrom raw MRI scans. This framework first introduces an MRI Pre-processing\nModule (MRI-Processor) to mitigate inter-subject and inter-scanner variability\nby flexibly integrating established medical imaging preprocessing tools. It\nthen incorporates two forms of clinical prior knowledge: (1)\nBrain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions\nstrongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior),\nwhich reflects the accelerated aging typically observed in PD-associated\nregions. Building on these priors, we design two dedicated modules: the\nRelevance-Prior Guided Feature Aggregation Module (Aggregator), which guides\nthe model to focus on PD-associated regions at the inter-subject level, and the\nAge-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps\nas auxiliary constraints at the intra-subject level to enhance diagnostic\naccuracy and clinical interpretability. Furthermore, we collected external test\ndata from our collaborating hospital. Experimental results show that\nPD-Diag-Net achieves 86\\% accuracy on external tests and over 96% accuracy in\nearly-stage diagnosis, outperforming existing advanced methods by more than\n20%.",
    "published": "2025-09-28T08:00:03Z",
    "updated": "2025-10-13T08:00:09Z",
    "link": "http://arxiv.org/pdf/2509.23719v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shuai Shao",
      "Shu Jiang",
      "Shiyuan Zhao",
      "Di Yang",
      "Yan Wang",
      "Yutong Bai",
      "Jianguo Zhang",
      "Jiangtao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11107v1",
    "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps",
    "summary": "This paper addresses the challenge of learning semantically and functionally\nmeaningful 3D motion priors from real-world videos, in order to enable\nprediction of future 3D scene motion from a single input image. We propose a\nnovel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,\nwhich can be generated from existing generative image models to facilitate\nefficient and effective motion prediction. To learn meaningful distributions\nover motion, we create a large-scale database of MoMaps from over 50,000 real\nvideos and train a diffusion model on these representations. Our motion\ngeneration not only synthesizes trajectories in 3D but also suggests a new\npipeline for 2D video synthesis: first generate a MoMap, then warp an image\naccordingly and complete the warped point-based renderings. Experimental\nresults demonstrate that our approach generates plausible and semantically\nconsistent 3D scene motion.",
    "published": "2025-10-13T07:56:19Z",
    "updated": "2025-10-13T07:56:19Z",
    "link": "http://arxiv.org/pdf/2510.11107v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiahui Lei",
      "Kyle Genova",
      "George Kopanas",
      "Noah Snavely",
      "Leonidas Guibas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11106v1",
    "title": "Compositional Zero-Shot Learning: A Survey",
    "summary": "Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision\nthat enables models to recognize unseen combinations of known attributes and\nobjects during inference, addressing the combinatorial challenge of requiring\ntraining data for every possible composition. This is particularly challenging\nbecause the visual appearance of primitives is highly contextual; for example,\n``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars\ndiffer significantly from ``wet'' cats. Effectively modeling this contextuality\nand the inherent compositionality is crucial for robust compositional zero-shot\nrecognition. This paper presents, to our knowledge, the first comprehensive\nsurvey specifically focused on Compositional Zero-Shot Learning. We\nsystematically review the state-of-the-art CZSL methods, introducing a taxonomy\ngrounded in disentanglement, with four families of approaches: no explicit\ndisentanglement, textual disentanglement, visual disentanglement, and\ncross-modal disentanglement. We provide a detailed comparative analysis of\nthese methods, highlighting their core advantages and limitations in different\nproblem settings, such as closed-world and open-world CZSL. Finally, we\nidentify the most significant open challenges and outline promising future\nresearch directions. This survey aims to serve as a foundational resource to\nguide and inspire further advancements in this fascinating and important field.\nPapers studied in this survey with their official code are available on our\ngithub: https://github.com/ans92/Compositional-Zero-Shot-Learning",
    "published": "2025-10-13T07:54:47Z",
    "updated": "2025-10-13T07:54:47Z",
    "link": "http://arxiv.org/pdf/2510.11106v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ans Munir",
      "Faisal Z. Qureshi",
      "Mohsen Ali",
      "Muhammad Haris Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11096v1",
    "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification\n  and Prompt Optimization",
    "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications.",
    "published": "2025-10-13T07:44:54Z",
    "updated": "2025-10-13T07:44:54Z",
    "link": "http://arxiv.org/pdf/2510.11096v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fengling Zhu",
      "Boshi Liu",
      "Jingyu Hua",
      "Sheng Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11092v1",
    "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory\n  Planning and Scene Evolution",
    "summary": "End-to-end autonomous driving methods aim to directly map raw sensor inputs\nto future driving actions such as planned trajectories, bypassing traditional\nmodular pipelines. While these approaches have shown promise, they often\noperate under a one-shot paradigm that relies heavily on the current scene\ncontext, potentially underestimating the importance of scene dynamics and their\ntemporal evolution. This limitation restricts the model's ability to make\ninformed and adaptive decisions in complex driving scenarios. We propose a new\nperspective: the future trajectory of an autonomous vehicle is closely\nintertwined with the evolving dynamics of its environment, and conversely, the\nvehicle's own future states can influence how the surrounding scene unfolds.\nMotivated by this bidirectional relationship, we introduce SeerDrive, a novel\nend-to-end framework that jointly models future scene evolution and trajectory\nplanning in a closed-loop manner. Our method first predicts future bird's-eye\nview (BEV) representations to anticipate the dynamics of the surrounding scene,\nthen leverages this foresight to generate future-context-aware trajectories.\nTwo key components enable this: (1) future-aware planning, which injects\npredicted BEV features into the trajectory planner, and (2) iterative scene\nmodeling and vehicle planning, which refines both future scene prediction and\ntrajectory generation through collaborative optimization. Extensive experiments\non the NAVSIM and nuScenes benchmarks show that SeerDrive significantly\noutperforms existing state-of-the-art methods.",
    "published": "2025-10-13T07:41:47Z",
    "updated": "2025-10-13T07:41:47Z",
    "link": "http://arxiv.org/pdf/2510.11092v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bozhou Zhang",
      "Nan Song",
      "Jingyu Li",
      "Xiatian Zhu",
      "Jiankang Deng",
      "Li Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11091v1",
    "title": "Text-Enhanced Panoptic Symbol Spotting in CAD Drawings",
    "summary": "With the widespread adoption of Computer-Aided Design(CAD) drawings in\nengineering, architecture, and industrial design, the ability to accurately\ninterpret and analyze these drawings has become increasingly critical. Among\nvarious subtasks, panoptic symbol spotting plays a vital role in enabling\ndownstream applications such as CAD automation and design retrieval. Existing\nmethods primarily focus on geometric primitives within the CAD drawings to\naddress this task, but they face following major problems: they usually\noverlook the rich textual annotations present in CAD drawings and they lack\nexplicit modeling of relationships among primitives, resulting in\nincomprehensive understanding of the holistic drawings. To fill this gap, we\npropose a panoptic symbol spotting framework that incorporates textual\nannotations. The framework constructs unified representations by jointly\nmodeling geometric and textual primitives. Then, using visual features extract\nby pretrained CNN as the initial representations, a Transformer-based backbone\nis employed, enhanced with a type-aware attention mechanism to explicitly model\nthe different types of spatial dependencies between various primitives.\nExtensive experiments on the real-world dataset demonstrate that the proposed\nmethod outperforms existing approaches on symbol spotting tasks involving\ntextual annotations, and exhibits superior robustness when applied to complex\nCAD drawings.",
    "published": "2025-10-13T07:41:15Z",
    "updated": "2025-10-13T07:41:15Z",
    "link": "http://arxiv.org/pdf/2510.11091v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xianlin Liu",
      "Yan Gong",
      "Bohao Li",
      "Jiajing Huang",
      "Bowen Du",
      "Junchen Ye",
      "Liyan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11090v1",
    "title": "Source-Free Object Detection with Detection Transformer",
    "summary": "Source-Free Object Detection (SFOD) enables knowledge transfer from a source\ndomain to an unsupervised target domain for object detection without access to\nsource data. Most existing SFOD approaches are either confined to conventional\nobject detection (OD) models like Faster R-CNN or designed as general solutions\nwithout tailored adaptations for novel OD architectures, especially Detection\nTransformer (DETR). In this paper, we introduce Feature Reweighting ANd\nContrastive Learning NetworK (FRANCK), a novel SFOD framework specifically\ndesigned to perform query-centric feature enhancement for DETRs. FRANCK\ncomprises four key components: (1) an Objectness Score-based Sample Reweighting\n(OSSR) module that computes attention-based objectness scores on multi-scale\nencoder feature maps, reweighting the detection loss to emphasize\nless-recognized regions; (2) a Contrastive Learning with Matching-based Memory\nBank (CMMB) module that integrates multi-level features into memory banks,\nenhancing class-wise contrastive learning; (3) an Uncertainty-weighted\nQuery-fused Feature Distillation (UQFD) module that improves feature\ndistillation through prediction quality reweighting and query feature fusion;\nand (4) an improved self-training pipeline with a Dynamic Teacher Updating\nInterval (DTUI) that optimizes pseudo-label quality. By leveraging these\ncomponents, FRANCK effectively adapts a source-pre-trained DETR model to a\ntarget domain with enhanced robustness and generalization. Extensive\nexperiments on several widely used benchmarks demonstrate that our method\nachieves state-of-the-art performance, highlighting its effectiveness and\ncompatibility with DETR-based SFOD models.",
    "published": "2025-10-13T07:35:04Z",
    "updated": "2025-10-13T07:35:04Z",
    "link": "http://arxiv.org/pdf/2510.11090v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Huizai Yao",
      "Sicheng Zhao",
      "Shuo Lu",
      "Hui Chen",
      "Yangyang Li",
      "Guoping Liu",
      "Tengfei Xing",
      "Chenggang Yan",
      "Jianhua Tao",
      "Guiguang Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09111v3",
    "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection",
    "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate predictions. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusions, and noise. Our benchmark, RoHOI,\nincludes 20 corruption types based on the HICO-DET and V-COCO datasets and a\nnew robustness-focused metric. We systematically analyze existing models in the\nHOI field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, thus dynamically adjusting the model's optimization to\nenhance robust feature learning. Extensive experiments show that our approach\noutperforms state-of-the-art methods, setting a new standard for robust HOI\ndetection. Benchmarks, datasets, and code are available at\nhttps://github.com/KratosWen/RoHOI.",
    "published": "2025-07-12T01:58:04Z",
    "updated": "2025-10-13T07:17:45Z",
    "link": "http://arxiv.org/pdf/2507.09111v3.pdf",
    "category": [
      "cs.CV",
      "cs.HC",
      "cs.RO",
      "eess.IV"
    ],
    "authors": [
      "Di Wen",
      "Kunyu Peng",
      "Kailun Yang",
      "Yufan Chen",
      "Ruiping Liu",
      "Junwei Zheng",
      "Alina Roitberg",
      "Danda Pani Paudel",
      "Luc Van Gool",
      "Rainer Stiefelhagen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.17490v3",
    "title": "Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval",
    "summary": "Structuring latent representations in a hierarchical manner enables models to\nlearn patterns at multiple levels of abstraction. However, most prevalent image\nunderstanding models focus on visual similarity, and learning visual\nhierarchies is relatively unexplored. In this work, for the first time, we\nintroduce a learning paradigm that can encode user-defined multi-level complex\nvisual hierarchies in hyperbolic space without requiring explicit hierarchical\nlabels. As a concrete example, first, we define a part-based image hierarchy\nusing object-level annotations within and across images. Then, we introduce an\napproach to enforce the hierarchy using contrastive loss with pairwise\nentailment metrics. Finally, we discuss new evaluation metrics to effectively\nmeasure hierarchical image retrieval. Encoding these complex relationships\nensures that the learned representations capture semantic and structural\ninformation that transcends mere visual similarity. Experiments in part-based\nimage retrieval show significant improvements in hierarchical retrieval tasks,\ndemonstrating the capability of our model in capturing visual hierarchies.",
    "published": "2024-11-26T14:58:06Z",
    "updated": "2025-10-13T07:13:01Z",
    "link": "http://arxiv.org/pdf/2411.17490v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziwei Wang",
      "Sameera Ramasinghe",
      "Chenchen Xu",
      "Julien Monteil",
      "Loris Bazzani",
      "Thalaiyasingam Ajanthan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11073v1",
    "title": "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible\n  Patient Face Anonymizer",
    "summary": "Patient face images provide a convenient mean for evaluating eye diseases,\nwhile also raising privacy concerns. Here, we introduce ROFI, a deep\nlearning-based privacy protection framework for ophthalmology. Using weakly\nsupervised learning and neural identity translation, ROFI anonymizes facial\nfeatures while retaining disease features (over 98\\% accuracy, $\\kappa >\n0.90$). It achieves 100\\% diagnostic sensitivity and high agreement ($\\kappa >\n0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\\% of\nimages. ROFI works with AI systems, maintaining original diagnoses ($\\kappa >\n0.80$), and supports secure image reversal (over 98\\% similarity), enabling\naudits and long-term care. These results show ROFI's effectiveness of\nprotecting patient privacy in the digital medicine era.",
    "published": "2025-10-13T07:12:23Z",
    "updated": "2025-10-13T07:12:23Z",
    "link": "http://arxiv.org/pdf/2510.11073v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuan Tian",
      "Min Zhou",
      "Yitong Chen",
      "Fang Li",
      "Lingzi Qi",
      "Shuo Wang",
      "Xieyang Xu",
      "Yu Yu",
      "Shiqiong Xu",
      "Chaoyu Lei",
      "Yankai Jiang",
      "Rongzhao Zhang",
      "Jia Tan",
      "Li Wu",
      "Hong Chen",
      "Xiaowei Liu",
      "Wei Lu",
      "Lin Li",
      "Huifang Zhou",
      "Xuefei Song",
      "Guangtao Zhai",
      "Xianqun Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11063v1",
    "title": "LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object\n  Segmentation",
    "summary": "This report presents an overview of the 7th Large-scale Video Object\nSegmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the\ntwo traditional tracks of LSVOS that jointly target robustness in realistic\nvideo scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition\nfeatures a newly introduced track, Complex VOS (MOSEv2). Building upon prior\ninsights, MOSEv2 substantially increases difficulty, introducing more\nchallenging but realistic scenarios including denser small objects, frequent\ndisappear/reappear events, severe occlusions, adverse weather and lighting,\netc., pushing long-term consistency and generalization beyond curated\nbenchmarks. The challenge retains standard ${J}$, $F$, and ${J\\&F}$ metrics for\nVOS and RVOS, while MOSEv2 adopts ${J\\&\\dot{F}}$ as the primary ranking metric\nto better evaluate objects across scales and disappearance cases. We summarize\ndatasets and protocols, highlight top-performing solutions, and distill\nemerging trends, such as the growing role of LLM/MLLM components and\nmemory-aware propagation, aiming to chart future directions for resilient,\nlanguage-aware video segmentation in the wild.",
    "published": "2025-10-13T07:02:09Z",
    "updated": "2025-10-13T07:02:09Z",
    "link": "http://arxiv.org/pdf/2510.11063v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chang Liu",
      "Henghui Ding",
      "Kaining Ying",
      "Lingyi Hong",
      "Ning Xu",
      "Linjie Yang",
      "Yuchen Fan",
      "Mingqi Gao",
      "Jingkun Chen",
      "Yunqi Miao",
      "Gengshen Wu",
      "Zhijin Qin",
      "Jungong Han",
      "Zhixiong Zhang",
      "Shuangrui Ding",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Jiaqi Wang",
      "Chang Soo Lim",
      "Joonyoung Moon",
      "Donghyeon Cho",
      "Tingmin Li",
      "Yixuan Li",
      "Yang Yang",
      "An Yan",
      "Leilei Cao",
      "Feng Lu",
      "Ran Hong",
      "Youhai Jiang",
      "Fengjie Zhu",
      "Yujie Xie",
      "Hongyang Zhang",
      "Zhihui Liu",
      "Shihai Ruan",
      "Quanzhu Niu",
      "Dengxian Gong",
      "Shihao Chen",
      "Tao Zhang",
      "Yikang Zhou",
      "Haobo Yuan",
      "Lu Qi",
      "Xiangtai Li",
      "Shunping Ji",
      "Ran Hong",
      "Feng Lu",
      "Leilei Cao",
      "An Yan",
      "Alexey Nekrasov",
      "Ali Athar",
      "Daan de Geus",
      "Alexander Hermans",
      "Bastian Leibe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01852v2",
    "title": "Context Guided Transformer Entropy Modeling for Video Compression",
    "summary": "Conditional entropy models effectively leverage spatio-temporal contexts to\nreduce video redundancy. However, incorporating temporal context often\nintroduces additional model complexity and increases computational cost. In\nparallel, many existing spatial context models lack explicit modeling the\nordering of spatial dependencies, which may limit the availability of relevant\ncontext during decoding. To address these issues, we propose the Context Guided\nTransformer (CGT) entropy model, which estimates probability mass functions of\nthe current frame conditioned on resampled temporal context and\ndependency-weighted spatial context. A temporal context resampler learns\npredefined latent queries to extract critical temporal information using\ntransformer encoders, reducing downstream computational overhead. Meanwhile, a\nteacher-student network is designed as dependency-weighted spatial context\nassigner to explicitly model the dependency of spatial context order. The\nteacher generates an attention map to represent token importance and an entropy\nmap to reflect prediction certainty from randomly masked inputs, guiding the\nstudent to select the weighted top-k tokens with the highest spatial\ndependency. During inference, only the student is used to predict undecoded\ntokens based on high-dependency context. Experimental results demonstrate that\nour CGT model reduces entropy modeling time by approximately 65% and achieves\nan 11% BD-Rate reduction compared to the previous state-of-the-art conditional\nentropy model.",
    "published": "2025-08-03T17:07:49Z",
    "updated": "2025-10-13T06:50:44Z",
    "link": "http://arxiv.org/pdf/2508.01852v2.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Junlong Tong",
      "Wei Zhang",
      "Yaohui Jin",
      "Xiaoyu Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11050v1",
    "title": "Zero-shot Face Editing via ID-Attribute Decoupled Inversion",
    "summary": "Recent advancements in text-guided diffusion models have shown promise for\ngeneral image editing via inversion techniques, but often struggle to maintain\nID and structural consistency in real face editing tasks. To address this\nlimitation, we propose a zero-shot face editing method based on ID-Attribute\nDecoupled Inversion. Specifically, we decompose the face representation into ID\nand attribute features, using them as joint conditions to guide both the\ninversion and the reverse diffusion processes. This allows independent control\nover ID and attributes, ensuring strong ID preservation and structural\nconsistency while enabling precise facial attribute manipulation. Our method\nsupports a wide range of complex multi-attribute face editing tasks using only\ntext prompts, without requiring region-specific input, and operates at a speed\ncomparable to DDIM inversion. Comprehensive experiments demonstrate its\npracticality and effectiveness.",
    "published": "2025-10-13T06:34:40Z",
    "updated": "2025-10-13T06:34:40Z",
    "link": "http://arxiv.org/pdf/2510.11050v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yang Hou",
      "Minggu Wang",
      "Jianjun Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11047v1",
    "title": "Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the\n  LaryngealCT Dataset",
    "summary": "Laryngeal cancer imaging research lacks standardised datasets to enable\nreproducible deep learning (DL) model development. We present LaryngealCT, a\ncurated benchmark of 1,029 computed tomography (CT) scans aggregated from six\ncollections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic\nvolumes of interest encompassing the larynx were extracted using a weakly\nsupervised parameter search framework validated by clinical experts. 3D DL\narchitectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i)\nearly (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification\ntasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892,\nF1-macro-0.646) respectively outperformed the other models in the two tasks.\nModel explainability assessed using 3D GradCAMs with thyroid cartilage overlays\nrevealed greater peri-cartilage attention in non-T4 cases and focal activations\nin T4 predictions. Through open-source data, pretrained models, and integrated\nexplainability tools, LaryngealCT offers a reproducible foundation for\nAI-driven research to support clinical decisions in laryngeal oncology.",
    "published": "2025-10-13T06:25:19Z",
    "updated": "2025-10-13T06:25:19Z",
    "link": "http://arxiv.org/pdf/2510.11047v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nivea Roy",
      "Son Tran",
      "Atul Sajjanhar",
      "K. Devaraja",
      "Prakashini Koteshwara",
      "Yong Xiang",
      "Divya Rao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.19159v3",
    "title": "GDO:Gradual Domain Osmosis",
    "summary": "In this paper, we propose a new method called Gradual Domain Osmosis, which\naims to solve the problem of smooth knowledge migration from source domain to\ntarget domain in Gradual Domain Adaptation (GDA). Traditional Gradual Domain\nAdaptation methods mitigate domain bias by introducing intermediate domains and\nself-training strategies, but often face the challenges of inefficient\nknowledge migration or missing data in intermediate domains. In this paper, we\ndesign an optimisation framework based on the hyperparameter $\\lambda$ by\ndynamically balancing the loss weights of the source and target domains, which\nenables the model to progressively adjust the strength of knowledge migration\n($\\lambda$ incrementing from 0 to 1) during the training process, thus\nachieving cross-domain generalisation more efficiently. Specifically, the\nmethod incorporates self-training to generate pseudo-labels and iteratively\nupdates the model by minimising a weighted loss function to ensure stability\nand robustness during progressive adaptation in the intermediate domain. The\nexperimental part validates the effectiveness of the method on rotated MNIST,\ncolour-shifted MNIST, portrait dataset and forest cover type dataset, and the\nresults show that it outperforms existing baseline methods. The paper further\nanalyses the impact of the dynamic tuning strategy of the hyperparameter\n$\\lambda$ on the performance through ablation experiments, confirming the\nadvantages of progressive domain penetration in mitigating the domain bias and\nenhancing the model generalisation capability. The study provides a theoretical\nsupport and practical framework for asymptotic domain adaptation and expands\nits application potential in dynamic environments.",
    "published": "2025-01-31T14:25:45Z",
    "updated": "2025-10-13T06:04:15Z",
    "link": "http://arxiv.org/pdf/2501.19159v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zixi Wang",
      "Yubo Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11028v1",
    "title": "Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with\n  Cascaded Prompts",
    "summary": "Recently, the powerful generalization ability exhibited by foundation models\nhas brought forth new solutions for zero-shot anomaly segmentation tasks.\nHowever, guiding these foundation models correctly to address downstream tasks\nremains a challenge. This paper proposes a novel two-stage framework, for\nzero-shot anomaly segmentation tasks in industrial anomaly detection. This\nframework excellently leverages the powerful anomaly localization capability of\nCLIP and the boundary perception ability of SAM.(1) To mitigate SAM's\ninclination towards object segmentation, we propose the Co-Feature Point Prompt\nGeneration (PPG) module. This module collaboratively utilizes CLIP and SAM to\ngenerate positive and negative point prompts, guiding SAM to focus on\nsegmenting anomalous regions rather than the entire object. (2) To further\noptimize SAM's segmentation results and mitigate rough boundaries and isolated\nnoise, we introduce the Cascaded Prompts for SAM (CPS) module. This module\nemploys hybrid prompts cascaded with a lightweight decoder of SAM, achieving\nprecise segmentation of anomalous regions. Across multiple datasets, consistent\nexperimental validation demonstrates that our approach achieves\nstate-of-the-art zero-shot anomaly segmentation results. Particularly\nnoteworthy is our performance on the Visa dataset, where we outperform the\nstate-of-the-art methods by 10.3\\% and 7.7\\% in terms of {$F_1$-max} and AP\nmetrics, respectively.",
    "published": "2025-10-13T05:53:49Z",
    "updated": "2025-10-13T05:53:49Z",
    "link": "http://arxiv.org/pdf/2510.11028v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yanning Hou",
      "Ke Xu",
      "Junfa Li",
      "Yanran Ruan",
      "Jianfeng Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11027v1",
    "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
    "summary": "While significant research has focused on developing embodied reasoning\ncapabilities using Vision-Language Models (VLMs) or integrating advanced VLMs\ninto Vision-Language-Action (VLA) models for end-to-end robot control, few\nstudies directly address the critical gap between upstream VLM-based reasoning\nand downstream VLA policy learning. In this work, we take an initial step\ntoward bridging embodied reasoning with VLA policy learning by introducing\nVlaser - a Vision-Language-Action Model with synergistic embodied reasoning\ncapability, which is a foundational vision-language model designed to integrate\nhigh-level reasoning with low-level control for embodied agents. Built upon the\nhigh-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance\nacross a range of embodied reasoning benchmarks - including spatial reasoning,\nembodied grounding, embodied QA, and task planning. Furthermore, we\nsystematically examine how different VLM initializations affect supervised VLA\nfine-tuning, offering novel insights into mitigating the domain shift between\ninternet-scale pre-training data and embodied-specific policy learning data.\nBased on these insights, our approach achieves state-of-the-art results on the\nWidowX benchmark and competitive performance on the Google Robot benchmark.",
    "published": "2025-10-13T05:51:22Z",
    "updated": "2025-10-13T05:51:22Z",
    "link": "http://arxiv.org/pdf/2510.11027v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ganlin Yang",
      "Tianyi Zhang",
      "Haoran Hao",
      "Weiyun Wang",
      "Yibin Liu",
      "Dehui Wang",
      "Guanzhou Chen",
      "Zijian Cai",
      "Junting Chen",
      "Weijie Su",
      "Wengang Zhou",
      "Yu Qiao",
      "Jifeng Dai",
      "Jiangmiao Pang",
      "Gen Luo",
      "Wenhai Wang",
      "Yao Mu",
      "Zhi Hou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11026v1",
    "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
    "summary": "Unified multimodal models integrate the reasoning capacity of large language\nmodels with both image understanding and generation, showing great promise for\nadvanced multimodal intelligence. However, the community still lacks a rigorous\nreasoning-centric benchmark to systematically evaluate the alignment between\nunderstanding and generation, and their generalization potential in complex\nvisual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive\nbenchmark that evaluates unified models across three complementary\nperspectives. Firstly, we investigate understanding-generation consistency\n(GIR-Bench-UGC), asking whether models can consistently leverage the same\nknowledge in both understanding and generation tasks. Secondly, we investigate\nwhether models can perform reasoning-centric text-to-image generation that\nrequires applying logical constraints and implicit knowledge to generate\nfaithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models\ncan handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,\nwe carefully design different task-specific evaluation pipelines tailored for\neach task. This enables fine-grained and interpretable evaluation while\nmitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive\nablations over various unified models and generation-only systems have shown\nthat: Although unified models are more capable of reasoning-driven visual\ntasks, they still exhibit a persistent gap between understanding and\ngeneration. The data and code for GIR-Bench are available at\n\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
    "published": "2025-10-13T05:50:44Z",
    "updated": "2025-10-13T05:50:44Z",
    "link": "http://arxiv.org/pdf/2510.11026v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hongxiang Li",
      "Yaowei Li",
      "Bin Lin",
      "Yuwei Niu",
      "Yuhang Yang",
      "Xiaoshuang Huang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Long Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16385v2",
    "title": "STAR: A Benchmark for Astronomical Star Fields Super-Resolution",
    "summary": "Super-resolution (SR) advances astronomical imaging by enabling\ncost-effective high-resolution capture, crucial for detecting faraway celestial\nobjects and precise structural analysis. However, existing datasets for\nastronomical SR (ASR) exhibit three critical limitations: flux inconsistency,\nobject-crop setting, and insufficient data diversity, significantly impeding\nASR development. We propose STAR, a large-scale astronomical SR dataset\ncontaining 54,738 flux-consistent star field image pairs covering wide\ncelestial regions. These pairs combine Hubble Space Telescope high-resolution\nobservations with physically faithful low-resolution counterparts generated\nthrough a flux-preserving data generation pipeline, enabling systematic\ndevelopment of field-level ASR models. To further empower the ASR community,\nSTAR provides a novel Flux Error (FE) to evaluate SR models in physical view.\nLeveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR)\nmodel that could accurately infer the flux-consistent high-resolution images\nfrom input photometry, suppressing several SR state-of-the-art methods by\n24.84% on a novel designed flux consistency metric, showing the priority of our\nmethod for astrophysics. Extensive experiments demonstrate the effectiveness of\nour proposed method and the value of our dataset. Code and models are available\nat https://github.com/GuoCheng12/STAR.",
    "published": "2025-07-22T09:28:28Z",
    "updated": "2025-10-13T05:45:48Z",
    "link": "http://arxiv.org/pdf/2507.16385v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kuo-Cheng Wu",
      "Guohang Zhuang",
      "Jinyang Huang",
      "Xiang Zhang",
      "Wanli Ouyang",
      "Yan Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11020v1",
    "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via\n  Cross-Modal Reward for Auxiliary Line Creation",
    "summary": "Auxiliary lines are essential for solving complex geometric problems but\nremain challenging for large vision-language models (LVLMs). Rather than\nediting diagrams to draw auxiliary lines, which current image editing models\nstruggle to render with geometric precision, we generate textual descriptions\nof auxiliary-line constructions to better align with the representational\nstrengths of LVLMs. To bridge the gap between textual descriptions and spatial\nstructure, we propose a reinforcement learning framework that enhances\ndiagram-text alignment. At the core of our approach is a cross-modal reward\nthat evaluates how well the generated auxiliary-line description for an\noriginal diagram matches a ground-truth auxiliary-line diagram. Built on this\nreward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line\nreasoning in solid geometry. This fine-grained signal drives a GRPO-based RL\nstage, yielding precise diagram-text alignment. To support training, we develop\na scalable data creation pipeline and construct AuxSolidMath, a dataset of\n3,018 real-exam geometry problems with paired diagrams and aligned textual\nfields. At the 3B and 7B scales, GeoVLMath achieves competitive and often\nsuperior performance compared with strong open-source and proprietary LVLMs on\nauxiliary-line reasoning benchmarks.",
    "published": "2025-10-13T05:33:51Z",
    "updated": "2025-10-13T05:33:51Z",
    "link": "http://arxiv.org/pdf/2510.11020v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shasha Guo",
      "Liang Pang",
      "Xi Wang",
      "Yanling Wang",
      "Huawei Shen",
      "Jing Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11018v1",
    "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
    "summary": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods.",
    "published": "2025-10-13T05:28:16Z",
    "updated": "2025-10-13T05:28:16Z",
    "link": "http://arxiv.org/pdf/2510.11018v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Pranav Ramesh",
      "Arjun Roy",
      "Deepak Ravikumar",
      "Kaushik Roy",
      "Gopalakrishnan Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11017v1",
    "title": "High-Resolution Spatiotemporal Modeling with Global-Local State Space\n  Models for Video-Based Human Pose Estimation",
    "summary": "Modeling high-resolution spatiotemporal representations, including both\nglobal dynamic contexts (e.g., holistic human motion tendencies) and local\nmotion details (e.g., high-frequency changes of keypoints), is essential for\nvideo-based human pose estimation (VHPE). Current state-of-the-art methods\ntypically unify spatiotemporal learning within a single type of modeling\nstructure (convolution or attention-based blocks), which inherently have\ndifficulties in balancing global and local dynamic modeling and may bias the\nnetwork to one of them, leading to suboptimal performance. Moreover, existing\nVHPE models suffer from quadratic complexity when capturing global\ndependencies, limiting their applicability especially for high-resolution\nsequences. Recently, the state space models (known as Mamba) have demonstrated\nsignificant potential in modeling long-range contexts with linear complexity;\nhowever, they are restricted to 1D sequential data. In this paper, we present a\nnovel framework that extends Mamba from two aspects to separately learn global\nand local high-resolution spatiotemporal representations for VHPE.\nSpecifically, we first propose a Global Spatiotemporal Mamba, which performs 6D\nselective space-time scan and spatial- and temporal-modulated scan merging to\nefficiently extract global representations from high-resolution sequences. We\nfurther introduce a windowed space-time scan-based Local Refinement Mamba to\nenhance the high-frequency details of localized keypoint motions. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed model\noutperforms state-of-the-art VHPE approaches while achieving better\ncomputational trade-offs.",
    "published": "2025-10-13T05:18:27Z",
    "updated": "2025-10-13T05:18:27Z",
    "link": "http://arxiv.org/pdf/2510.11017v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Runyang Feng",
      "Hyung Jin Chang",
      "Tze Ho Elden Tse",
      "Boeun Kim",
      "Yi Chang",
      "Yixing Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11014v1",
    "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of\n  Environment Uncertainty for Planning in Configuration Spaces",
    "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning.",
    "published": "2025-10-13T05:08:48Z",
    "updated": "2025-10-13T05:08:48Z",
    "link": "http://arxiv.org/pdf/2510.11014v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Subhransu S. Bhattacharjee",
      "Hao Lu",
      "Dylan Campbell",
      "Rahul Shome"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11012v1",
    "title": "COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced\n  Reasoning in Vision Language Models",
    "summary": "Compositional reasoning remains a persistent weakness of modern vision\nlanguage models (VLMs): they often falter when a task hinges on understanding\nhow multiple objects, attributes, and relations interact within an image.\nMultiple research works have attempted to improve compositionality performance\nby creative tricks such as improving prompt structure, chain of thought\nreasoning, etc. A more recent line of work attempts to impart additional\nreasoning in VLMs using well-trained Large Language Models (LLMs), which are\nfar superior in linguistic understanding than VLMs to compensate for the\nlimited linguistic prowess of VLMs. However, these approaches are either\nresource-intensive or do not provide an interpretable reasoning process. In\nthis paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs\nwith carefully designed neurosymbolic concept trees learned from LLMs to\nimprove VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning\nprocess boosts compositionality performance and provides a rationale behind VLM\npredictions. Empirical results on four compositionality benchmarks, Winoground,\nEqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with\nvarying sizes, demonstrate that COCO-Tree significantly improves compositional\ngeneralization by 5-10% over baselines.",
    "published": "2025-10-13T05:07:13Z",
    "updated": "2025-10-13T05:07:13Z",
    "link": "http://arxiv.org/pdf/2510.11012v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sanchit Sinha",
      "Guangzhi Xiong",
      "Aidong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23042v2",
    "title": "Goal-Based Vision-Language Driving",
    "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.",
    "published": "2025-07-30T19:12:42Z",
    "updated": "2025-10-13T04:53:24Z",
    "link": "http://arxiv.org/pdf/2507.23042v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO",
      "I.2.6; I.2.9; I.2.10; C.3.3"
    ],
    "authors": [
      "Santosh Patapati",
      "Trisanth Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11005v1",
    "title": "Frequency Domain Unlocks New Perspectives for Abdominal Medical Image\n  Segmentation",
    "summary": "Accurate segmentation of tumors and adjacent normal tissues in medical images\nis essential for surgical planning and tumor staging. Although foundation\nmodels generally perform well in segmentation tasks, they often struggle to\nfocus on foreground areas in complex, low-contrast backgrounds, where some\nmalignant tumors closely resemble normal organs, complicating contextual\ndifferentiation. To address these challenges, we propose the Foreground-Aware\nSpectrum Segmentation (FASS) framework. First, we introduce a foreground-aware\nmodule to amplify the distinction between background and the entire volume\nspace, allowing the model to concentrate more effectively on target areas.\nNext, a feature-level frequency enhancement module, based on wavelet transform,\nextracts discriminative high-frequency features to enhance boundary recognition\nand detail perception. Eventually, we introduce an edge constraint module to\npreserve geometric continuity in segmentation boundaries. Extensive experiments\non multiple medical datasets demonstrate superior performance across all\nmetrics, validating the effectiveness of our framework, particularly in\nrobustness under complex conditions and fine structure recognition. Our\nframework significantly enhances segmentation of low-contrast images, paving\nthe way for applications in more diverse and complex medical imaging scenarios.",
    "published": "2025-10-13T04:44:43Z",
    "updated": "2025-10-13T04:44:43Z",
    "link": "http://arxiv.org/pdf/2510.11005v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kai Han",
      "Siqi Ma",
      "Chengxuan Qian",
      "Jun Chen",
      "Chongwen Lyu",
      "Yuqing Song",
      "Zhe Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.08980v2",
    "title": "Learning Diffusion Models with Flexible Representation Guidance",
    "summary": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.",
    "published": "2025-07-11T19:29:02Z",
    "updated": "2025-10-13T04:40:50Z",
    "link": "http://arxiv.org/pdf/2507.08980v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Chenyu Wang",
      "Cai Zhou",
      "Sharut Gupta",
      "Zongyu Lin",
      "Stefanie Jegelka",
      "Stephen Bates",
      "Tommi Jaakkola"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01738v2",
    "title": "DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image\n  Segmentation through Loopback Synergy",
    "summary": "Referring Image Segmentation (RIS) is a challenging task that aims to segment\nobjects in an image based on natural language expressions. While prior studies\nhave predominantly concentrated on improving vision-language interactions and\nachieving fine-grained localization, a systematic analysis of the fundamental\nbottlenecks in existing RIS frameworks remains underexplored. To bridge this\ngap, we propose DeRIS, a novel framework that decomposes RIS into two key\ncomponents: perception and cognition. This modular decomposition facilitates a\nsystematic analysis of the primary bottlenecks impeding RIS performance. Our\nfindings reveal that the predominant limitation lies not in perceptual\ndeficiencies, but in the insufficient multi-modal cognitive capacity of current\nmodels. To mitigate this, we propose a Loopback Synergy mechanism, which\nenhances the synergy between the perception and cognition modules, thereby\nenabling precise segmentation while simultaneously improving robust image-text\ncomprehension. Additionally, we analyze and introduce a simple non-referent\nsample conversion data augmentation to address the long-tail distribution issue\nrelated to target existence judgement in general scenarios. Notably, DeRIS\ndemonstrates inherent adaptability to both non- and multi-referents scenarios\nwithout requiring specialized architectural modifications, enhancing its\ngeneral applicability. The codes and models are available at\nhttps://github.com/Dmmm1997/DeRIS.",
    "published": "2025-07-02T14:14:35Z",
    "updated": "2025-10-13T04:39:10Z",
    "link": "http://arxiv.org/pdf/2507.01738v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ming Dai",
      "Wenxuan Cheng",
      "Jiang-jiang Liu",
      "Sen Yang",
      "Wenxiao Cai",
      "Yanpeng Sun",
      "Wankou Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24469v2",
    "title": "LaMoGen: Laban Movement-Guided Diffusion for Text-to-Motion Generation",
    "summary": "Diverse human motion generation is an increasingly important task, having\nvarious applications in computer vision, human-computer interaction and\nanimation. While text-to-motion synthesis using diffusion models has shown\nsuccess in generating high-quality motions, achieving fine-grained expressive\nmotion control remains a significant challenge. This is due to the lack of\nmotion style diversity in datasets and the difficulty of expressing\nquantitative characteristics in natural language. Laban movement analysis has\nbeen widely used by dance experts to express the details of motion including\nmotion quality as consistent as possible. Inspired by that, this work aims for\ninterpretable and expressive control of human motion generation by seamlessly\nintegrating the quantification methods of Laban Effort and Shape components\ninto the text-guided motion generation models. Our proposed zero-shot,\ninference-time optimization method guides the motion generation model to have\ndesired Laban Effort and Shape components without any additional motion data by\nupdating the text embedding of pretrained diffusion models during the sampling\nstep. We demonstrate that our approach yields diverse expressive motion\nqualities while preserving motion identity by successfully manipulating motion\nattributes according to target Laban tags.",
    "published": "2025-09-29T08:48:49Z",
    "updated": "2025-10-13T04:36:32Z",
    "link": "http://arxiv.org/pdf/2509.24469v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Heechang Kim",
      "Gwanghyun Kim",
      "Se Young Chun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.09282v2",
    "title": "VideoAds for Fast-Paced Video Understanding",
    "summary": "Advertisement videos serve as a rich and valuable source of purpose-driven\ninformation, encompassing high-quality visual, textual, and contextual cues\ndesigned to engage viewers. They are often more complex than general videos of\nsimilar duration due to their structured narratives and rapid scene\ntransitions, posing significant challenges to multi-modal large language models\n(MLLMs). In this work, we introduce VideoAds, the first dataset tailored for\nbenchmarking the performance of MLLMs on advertisement videos. VideoAds\ncomprises well-curated advertisement videos with complex temporal structures,\naccompanied by \\textbf{manually} annotated diverse questions across three core\ntasks: visual finding, video summary, and visual reasoning. We propose a\nquantitative measure to compare VideoAds against existing benchmarks in terms\nof video complexity. Through extensive experiments, we find that\nQwen2.5-VL-72B, an opensource MLLM, achieves 73.35\\% accuracy on VideoAds,\noutperforming GPT-4o (66.82\\%) and Gemini-1.5 Pro (69.66\\%); the two\nproprietary models especially fall behind the opensource model in video\nsummarization and reasoning, but perform the best in visual finding. Notably,\nhuman experts easily achieve a remarkable accuracy of 94.27\\%. These results\nunderscore the necessity of advancing MLLMs' temporal modeling capabilities and\nhighlight VideoAds as a potentially pivotal benchmark for future research in\nunderstanding video that requires high FPS sampling. The dataset and evaluation\ncode will be publicly available at https://videoadsbenchmark.netlify.app.",
    "published": "2025-04-12T17:05:35Z",
    "updated": "2025-10-13T04:33:41Z",
    "link": "http://arxiv.org/pdf/2504.09282v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zheyuan Zhang",
      "Monica Dou",
      "Linkai Peng",
      "Hongyi Pan",
      "Ulas Bagci",
      "Boqing Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23064v3",
    "title": "Vision-Language Cross-Attention for Real-Time Autonomous Driving",
    "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model. On the MD-NEX Outdoor-Driving benchmark\nXYZ-Drive attains 95% success and 0.80 Success weighted by Path Length (SPL),\nsurpassing PhysNav-DG by 15%. and halving collisions, all while significantly\nimproving efficiency by using only a single branch. Sixteen ablations explain\nthe gains. Removing any modality (vision, waypoint, map) drops success by up to\n11%, confirming their complementary roles and rich connections. Replacing\ngoal-centered attention with simple concatenation cuts 3% in performance,\nshowing query-based fusion injects map knowledge more effectively. Keeping the\ntransformer frozen loses 5%, showing the importance of fine-tuning when\napplying VLMs for specific tasks such as autonomous driving. Coarsening map\nresolution from 10 cm to 40 cm blurs lane edges and raises crash rate. Overall,\nthese results demonstrate that early, token-level fusion of intent and map\nlayout enables accurate, transparent, real-time driving.",
    "published": "2025-07-30T19:51:23Z",
    "updated": "2025-10-13T04:30:09Z",
    "link": "http://arxiv.org/pdf/2507.23064v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"
    ],
    "authors": [
      "Santosh Patapati",
      "Trisanth Srinivasan",
      "Murari Ambati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11000v1",
    "title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent\n  Multi-Instance Generation",
    "summary": "Multi-instance image generation (MIG) remains a significant challenge for\nmodern diffusion models due to key limitations in achieving precise control\nover object layout and preserving the identity of multiple distinct subjects.\nTo address these limitations, we introduce ContextGen, a novel Diffusion\nTransformer framework for multi-instance generation that is guided by both\nlayout and reference images. Our approach integrates two key technical\ncontributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates\nthe composite layout image into the generation context to robustly anchor the\nobjects in their desired positions, and Identity Consistency Attention (ICA),\nan innovative attention mechanism that leverages contextual reference images to\nensure the identity consistency of multiple instances. Recognizing the lack of\nlarge-scale, hierarchically-structured datasets for this task, we introduce\nIMIG-100K, the first dataset with detailed layout and identity annotations.\nExtensive experiments demonstrate that ContextGen sets a new state-of-the-art,\noutperforming existing methods in control precision, identity fidelity, and\noverall visual quality.",
    "published": "2025-10-13T04:21:19Z",
    "updated": "2025-10-13T04:21:19Z",
    "link": "http://arxiv.org/pdf/2510.11000v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ruihang Xu",
      "Dewei Zhou",
      "Fan Ma",
      "Yi Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10993v1",
    "title": "Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency",
    "summary": "3D Gaussian inpainting, a critical technique for numerous applications in\nvirtual reality and multimedia, has made significant progress with pretrained\ndiffusion models. However, ensuring multi-view consistency, an essential\nrequirement for high-quality inpainting, remains a key challenge. In this work,\nwe present PAInpainter, a novel approach designed to advance 3D Gaussian\ninpainting by leveraging perspective-aware content propagation and consistency\nverification across multi-view inpainted images. Our method iteratively refines\ninpainting and optimizes the 3D Gaussian representation with multiple views\nadaptively sampled from a perspective graph. By propagating inpainted images as\nprior information and verifying consistency across neighboring views,\nPAInpainter substantially enhances global consistency and texture fidelity in\nrestored 3D scenes. Extensive experiments demonstrate the superiority of\nPAInpainter over existing methods. Our approach achieves superior 3D inpainting\nquality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and\nNeRFiller datasets, respectively, highlighting its effectiveness and\ngeneralization capability.",
    "published": "2025-10-13T04:10:39Z",
    "updated": "2025-10-13T04:10:39Z",
    "link": "http://arxiv.org/pdf/2510.10993v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuxin Cheng",
      "Binxiao Huang",
      "Taiqiang Wu",
      "Wenyong Zhou",
      "Chenchen Ding",
      "Zhengwu Liu",
      "Graziano Chesi",
      "Ngai Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14481v2",
    "title": "LSP-ST: Ladder Shape-Biased Side-Tuning for Robust Infrared Small Target\n  Detection",
    "summary": "Fine-tuning the Segment Anything Model (SAM) for infrared small target\ndetection poses significant challenges due to severe domain shifts. Existing\nadaptation methods often incorporate handcrafted priors to bridge this gap, yet\nsuch designs limit generalization and scalability. We identify a fundamental\ntexture bias in foundation models, which overly depend on local texture cues\nfor target localization. To address this, we propose Ladder Shape-Biased\nSide-Tuning (LSP-ST), a novel approach that introduces a shape-aware inductive\nbias to facilitate effective adaptation beyond texture cues. In contrast to\nprior work that injects explicit edge or contour features, LSP-ST models shape\nas a global structural prior, integrating both boundaries and internal layouts.\nWe design a Shape-Enhanced Large-Kernel Attention Module to hierarchically and\nimplicitly capture structural information in a fully differentiable manner,\nwithout task-specific handcrafted guidance. A theoretical analysis grounded in\nmatched filtering and backpropagation reveals the mechanism by which the\nproposed attention improves structure-aware learning. With only 4.72M learnable\nparameters, LSP-ST achieves state-of-the-art performance on multiple infrared\nsmall target detection benchmarks. Furthermore, its strong generalization is\nvalidated across tasks such as mirror detection, shadow detection, and\ncamouflaged object detection, while maintaining stable performance on\ntexture-driven tasks like salient object detection, demonstrating that the\nintroduced shape bias complements rather than competes with texture-based\nreasoning.",
    "published": "2025-04-20T04:12:38Z",
    "updated": "2025-10-13T04:09:07Z",
    "link": "http://arxiv.org/pdf/2504.14481v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Guoyi Zhang",
      "Siyang Chen",
      "Guangsheng Xu",
      "Han Wang",
      "Donghe Wang",
      "Xiaohu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10986v1",
    "title": "Mixup Helps Understanding Multimodal Video Better",
    "summary": "Multimodal video understanding plays a crucial role in tasks such as action\nrecognition and emotion classification by combining information from different\nmodalities. However, multimodal models are prone to overfitting strong\nmodalities, which can dominate learning and suppress the contributions of\nweaker ones. To address this challenge, we first propose Multimodal Mixup (MM),\nwhich applies the Mixup strategy at the aggregated multimodal feature level to\nmitigate overfitting by generating virtual feature-label pairs. While MM\neffectively improves generalization, it treats all modalities uniformly and\ndoes not account for modality imbalance during training. Building on MM, we\nfurther introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts\nthe mixing ratios for each modality based on their relative contributions to\nthe learning objective. Extensive experiments on several datasets demonstrate\nthe effectiveness of our methods in improving generalization and multimodal\nrobustness.",
    "published": "2025-10-13T03:53:25Z",
    "updated": "2025-10-13T03:53:25Z",
    "link": "http://arxiv.org/pdf/2510.10986v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoyu Ma",
      "Ding Ding",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10980v1",
    "title": "On the Optimal Representation Efficiency of Barlow Twins: An\n  Information-Geometric Interpretation",
    "summary": "Self-supervised learning (SSL) has achieved remarkable success by learning\nmeaningful representations without labeled data. However, a unified theoretical\nframework for understanding and comparing the efficiency of different SSL\nparadigms remains elusive. In this paper, we introduce a novel\ninformation-geometric framework to quantify representation efficiency. We\ndefine representation efficiency $\\eta$ as the ratio between the effective\nintrinsic dimension of the learned representation space and its ambient\ndimension, where the effective dimension is derived from the spectral\nproperties of the Fisher Information Matrix (FIM) on the statistical manifold\ninduced by the encoder. Within this framework, we present a theoretical\nanalysis of the Barlow Twins method. Under specific but natural assumptions, we\nprove that Barlow Twins achieves optimal representation efficiency ($\\eta = 1$)\nby driving the cross-correlation matrix of representations towards the identity\nmatrix, which in turn induces an isotropic FIM. This work provides a rigorous\ntheoretical foundation for understanding the effectiveness of Barlow Twins and\noffers a new geometric perspective for analyzing SSL algorithms.",
    "published": "2025-10-13T03:41:27Z",
    "updated": "2025-10-13T03:41:27Z",
    "link": "http://arxiv.org/pdf/2510.10980v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV",
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T07, 62B11, 94A17, 53B12",
      "I.2.6; I.5.1; G.3; H.1.1"
    ],
    "authors": [
      "Di Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07961v2",
    "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent\n  Space Regularization and Controllable Refinement",
    "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between\ncomputational efficiency and high-frequency detail retention. While Variational\nAutoencoders (VAEs) improve efficiency via latent-space processing, their\nGaussian constraint often discards degradation-specific high-frequency\ninformation, hurting reconstruction fidelity. To overcome this, we propose\nLatent Harmony, a two-stage framework that redefines VAEs for UHD restoration\nby jointly regularizing the latent space and enforcing high-frequency-aware\nreconstruction.In Stage One, we introduce LH-VAE, which enhances semantic\nrobustness through visual semantic constraints and progressive degradation\nperturbations, while latent equivariance strengthens high-frequency\nreconstruction.Stage Two jointly trains this refined VAE with a restoration\nmodel using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA\nguided by a fidelity-oriented high-frequency alignment loss to recover\nauthentic details, and a decoder LoRA driven by a perception-oriented loss to\nsynthesize realistic textures. Both LoRA modules are trained via alternating\noptimization with selective gradient propagation to preserve the pretrained\nlatent structure.At inference, a tunable parameter {\\alpha} enables flexible\nfidelity-perception trade-offs.Experiments show Latent Harmony achieves\nstate-of-the-art performance across UHD and standard-resolution tasks,\neffectively balancing efficiency, perceptual quality, and reconstruction\naccuracy.",
    "published": "2025-10-09T08:54:26Z",
    "updated": "2025-10-13T03:30:40Z",
    "link": "http://arxiv.org/pdf/2510.07961v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yidi Liu",
      "Xueyang Fu",
      "Jie Huang",
      "Jie Xiao",
      "Dong Li",
      "Wenlong Zhang",
      "Lei Bai",
      "Zheng-Jun Zha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10973v1",
    "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for\n  Explainable Chart Reasoning",
    "summary": "The capabilities of Large Vision-Language Models (LVLMs) have reached\nstate-of-the-art on many visual reasoning tasks, including chart reasoning, yet\nthey still falter on out-of-distribution (OOD) data, and degrade further when\nasked to produce their chain-of-thought (CoT) rationales, limiting\nexplainability. We present Chart-RVR, a general framework that fine-tunes LVLMs\nto be more robust and explainable for chart reasoning by coupling Group\nRelative Policy Optimization (GRPO) with automatically verifiable rewards. Our\nframework comprises of three rewards that maximize: (i) correct chart-type\nclassification, (ii) faithful chart table reconstruction, and (iii) process\nconformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently\noutperforms standard supervised fine-tuning (SFT) on both in-distribution and\nout-of-distribution datasets, closing the OOD performance gap while improving\nrationale fidelity. The resulting models, the Chart-RVR-3B series, achieve\nstate-of-the-art results on six chart-reasoning benchmarks spanning in-domain\nand OOD settings, surpassing all existing models of comparable size. Beyond\naccuracy, Chart-RVR yields more interpretable CoT rationales, strengthening\ntrust and reliability - showcasing the power of verifiable rewards with GRPO\nfor training reliable, interpretable chart-reasoning models.",
    "published": "2025-10-13T03:25:35Z",
    "updated": "2025-10-13T03:25:35Z",
    "link": "http://arxiv.org/pdf/2510.10973v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Sanchit Sinha",
      "Oana Frunza",
      "Kashif Rasul",
      "Yuriy Nevmyvaka",
      "Aidong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10969v1",
    "title": "IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation",
    "summary": "Existing vision language models (VLMs), including GPT-4 and DALL-E, often\nstruggle to preserve logic, object identity, and style in multimodal image-text\ngeneration. This limitation significantly hinders the generalization capability\nof VLMs in complex image-text input-output scenarios. To address this issue, we\npropose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which\nenhances existing interleaved VLMs through explicit structured reasoning,\nthereby mitigating context drift in logic, entity identity, and style. The\nproposed framework operates in two stages. (1) A dynamic IUT-Plug extraction\nmodule parses visual scenes into hierarchical symbolic structures. (2) A\ncoordinated narrative-flow and image synthesis mechanism ensures cross-modal\nconsistency. To evaluate our approach, we construct a novel benchmark based on\n3,000 real human-generated question-answer pairs over fine-tuned large models,\nintroducing a dynamic evaluation protocol for quantifying context drift in\ninterleaved VLMs. Experimental results demonstrate that IUT-Plug not only\nimproves accuracy on established benchmarks but also effectively alleviates the\nthree critical forms of context drift across diverse multimodal question\nanswering (QA) scenarios.",
    "published": "2025-10-13T03:19:45Z",
    "updated": "2025-10-13T03:19:45Z",
    "link": "http://arxiv.org/pdf/2510.10969v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zeteng Lin",
      "Xingxing Li",
      "Wen You",
      "Xiaoyang Li",
      "Zehan Lu",
      "Yujun Cai",
      "Jing Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.00161v2",
    "title": "SAVeD: Learning to Denoise Low-SNR Video for Improved Downstream\n  Performance",
    "summary": "Low signal-to-noise ratio videos -- such as those from underwater sonar,\nultrasound, and microscopy -- pose significant challenges for computer vision\nmodels, particularly when paired clean imagery is unavailable. We present\nSpatiotemporal Augmentations and denoising in Video for Downstream Tasks\n(SAVeD), a novel self-supervised method that denoises low-SNR sensor videos\nusing only raw noisy data. By leveraging distinctions between foreground and\nbackground motion and exaggerating objects with stronger motion signal, SAVeD\nenhances foreground object visibility and reduces background and camera noise\nwithout requiring clean video. SAVeD has a set of architectural optimizations\nthat lead to faster throughput, training, and inference than existing deep\nlearning methods. We also introduce a new denoising metric, FBD, which\nindicates foreground-background divergence for detection datasets without\nrequiring clean imagery. Our approach achieves state-of-the-art results for\nclassification, detection, tracking, and counting tasks, and it does so with\nfewer training resource requirements than existing deep-learning-based\ndenoising methods. Project page: https://suzanne-stathatos.github.io/SAVeD Code\npage: https://github.com/suzanne-stathatos/SAVeD",
    "published": "2025-03-31T19:14:48Z",
    "updated": "2025-10-13T03:16:27Z",
    "link": "http://arxiv.org/pdf/2504.00161v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Suzanne Stathatos",
      "Michael Hobley",
      "Pietro Perona",
      "Markus Marks"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10954v1",
    "title": "Comparative Evaluation of Neural Network Architectures for Generalizable\n  Human Spatial Preference Prediction in Unseen Built Environments",
    "summary": "The capacity to predict human spatial preferences within built environments\nis instrumental for developing Cyber-Physical-Social Infrastructure Systems\n(CPSIS). A significant challenge in this domain is the generalizability of\npreference models, particularly their efficacy in predicting preferences within\nenvironmental configurations not encountered during training. While deep\nlearning models have shown promise in learning complex spatial and contextual\ndependencies, it remains unclear which neural network architectures are most\neffective at generalizing to unseen layouts. To address this, we conduct a\ncomparative study of Graph Neural Networks, Convolutional Neural Networks, and\nstandard feedforward Neural Networks using synthetic data generated from a\nsimplified and synthetic pocket park environment. Beginning with this\nillustrative case study, allows for controlled analysis of each model's ability\nto transfer learned preference patterns to unseen spatial scenarios. The models\nare evaluated based on their capacity to predict preferences influenced by\nheterogeneous physical, environmental, and social features. Generalizability\nscore is calculated using the area under the precision-recall curve for the\nseen and unseen layouts. This generalizability score is appropriate for\nimbalanced data, providing insights into the suitability of each neural network\narchitecture for preference-aware human behavior modeling in unseen built\nenvironments.",
    "published": "2025-10-13T03:04:48Z",
    "updated": "2025-10-13T03:04:48Z",
    "link": "http://arxiv.org/pdf/2510.10954v1.pdf",
    "category": [
      "cs.CE",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Maral Doctorarastoo",
      "Katherine A. Flanigan",
      "Mario Bergés",
      "Christopher McComb"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23484v3",
    "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion\n  Inversion Sensitivity",
    "summary": "AI-generated content (AIGC) enables efficient visual creation but raises\ncopyright and authenticity risks. As a common technique for integrity\nverification and source tracing, digital image watermarking is regarded as a\npotential solution to above issues. However, the widespread adoption and\nadvancing capabilities of generative image editing tools have amplified\nmalicious tampering risks, while simultaneously posing new challenges to\npassive tampering detection and watermark robustness. To address these\nchallenges, this paper proposes a Tamper-Aware Generative image WaterMarking\nmethod named TAG-WM. The proposed method comprises four key modules: a\ndual-mark joint sampling (DMJS) algorithm for embedding copyright and\nlocalization watermarks into the latent space while preserving generative\nquality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a\ndense variation region detector (DVRD) leveraging diffusion inversion\nsensitivity to identify tampered areas via statistical deviation analysis, and\nthe tamper-aware decoding (TAD) guided by localization results. The\nexperimental results demonstrate that TAG-WM achieves state-of-the-art\nperformance in both tampering robustness and localization capability even under\ndistortion, while preserving lossless generation quality and maintaining a\nwatermark capacity of 256 bits. The code is available at:\nhttps://github.com/Suchenl/TAG-WM.",
    "published": "2025-06-30T03:14:07Z",
    "updated": "2025-10-13T02:59:11Z",
    "link": "http://arxiv.org/pdf/2506.23484v3.pdf",
    "category": [
      "cs.MM",
      "cs.CV",
      "eess.IV",
      "I.3.3; I.4.9"
    ],
    "authors": [
      "Yuzhuo Chen",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10947v1",
    "title": "Towards Distribution-Shift Uncertainty Estimation for Inverse Problems\n  with Generative Priors",
    "summary": "Generative models have shown strong potential as data-driven priors for\nsolving inverse problems such as reconstructing medical images from\nundersampled measurements. While these priors improve reconstruction quality\nwith fewer measurements, they risk hallucinating features when test images lie\noutside the training distribution. Existing uncertainty quantification methods\nin this setting (i) require an in-distribution calibration dataset, which may\nnot be available, (ii) provide heuristic rather than statistical estimates, or\n(iii) quantify uncertainty from model capacity or limited measurements rather\nthan distribution shift. We propose an instance-level, calibration-free\nuncertainty indicator that is sensitive to distribution shift, requires no\nknowledge of the training distribution, and incurs no retraining cost. Our key\nhypothesis is that reconstructions of in-distribution images remain stable\nunder random measurement variations, while reconstructions of\nout-of-distribution (OOD) images exhibit greater instability. We use this\nstability as a proxy for detecting distribution shift. Our proposed OOD\nindicator is efficiently computable for any computational imaging inverse\nproblem; we demonstrate it on tomographic reconstruction of MNIST digits, where\na learned proximal network trained only on digit \"0\" is evaluated on all ten\ndigits. Reconstructions of OOD digits show higher variability and\ncorrespondingly higher reconstruction error, validating this indicator. These\nresults suggest a deployment strategy that pairs generative priors with\nlightweight guardrails, enabling aggressive measurement reduction for\nin-distribution cases while automatically warning when priors are applied out\nof distribution.",
    "published": "2025-10-13T02:58:26Z",
    "updated": "2025-10-13T02:58:26Z",
    "link": "http://arxiv.org/pdf/2510.10947v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Namhoon Kim",
      "Sara Fridovich-Keil"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07249v2",
    "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video\n  Generation",
    "summary": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning.",
    "published": "2025-10-08T17:16:09Z",
    "updated": "2025-10-13T02:46:39Z",
    "link": "http://arxiv.org/pdf/2510.07249v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaben Chen",
      "Zixin Wang",
      "Ailing Zeng",
      "Yang Fu",
      "Xueyang Yu",
      "Siyuan Cen",
      "Julian Tanke",
      "Yihang Chen",
      "Koichi Saito",
      "Yuki Mitsufuji",
      "Chuang Gan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10933v1",
    "title": "DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose\n  Estimation of Textureless Objects",
    "summary": "6D pose estimation of textureless objects is valuable for industrial robotic\napplications, yet remains challenging due to the frequent loss of depth\ninformation. Current multi-view methods either rely on depth data or\ninsufficiently exploit multi-view geometric cues, limiting their performance.\nIn this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level\nfusion using only multi-view RGB images as input. We design a three-stage\nprogressive pose optimization strategy that leverages dense multi-view keypoint\ngeometry information. To enable effective dense keypoint fusion, we enhance the\nkeypoint network with attentional aggregation and symmetry-aware training,\nimproving prediction accuracy and resolving ambiguities on symmetric objects.\nExtensive experiments on the ROBI dataset demonstrate that DKPMV outperforms\nstate-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods\nin the majority of cases. The code will be available soon.",
    "published": "2025-10-13T02:45:55Z",
    "updated": "2025-10-13T02:45:55Z",
    "link": "http://arxiv.org/pdf/2510.10933v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Jiahong Chen",
      "Jinghao Wang",
      "Zi Wang",
      "Ziwen Wang",
      "Banglei Guan",
      "Qifeng Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10921v1",
    "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
    "summary": "Fine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models like CLIP\nperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduce FG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrich fine-grained supervision, including region-text matching and long-caption\nmodeling, alongside multiple discriminative objectives. We further introduce\nthe Textual Intra-modal Contrastive (TIC) loss to better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuring long-caption retrieval and bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthat FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.",
    "published": "2025-10-13T02:32:07Z",
    "updated": "2025-10-13T02:32:07Z",
    "link": "http://arxiv.org/pdf/2510.10921v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chunyu Xie",
      "Bin Wang",
      "Fanjing Kong",
      "Jincheng Li",
      "Dawei Liang",
      "Ji Ao",
      "Dawei Leng",
      "Yuhui Yin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10918v1",
    "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models",
    "summary": "The exponential growth of the global makeup market has paralleled\nadvancements in virtual makeup simulation technology. Despite the progress led\nby GANs, their application still encounters significant challenges, including\ntraining instability and limited customization capabilities. Addressing these\nchallenges, we introduce DreamMakup - a novel training-free Diffusion model\nbased Makeup Customization method, leveraging the inherent advantages of\ndiffusion models for superior controllability and precise real-image editing.\nDreamMakeup employs early-stopped DDIM inversion to preserve the facial\nstructure and identity while enabling extensive customization through various\nconditioning inputs such as reference images, specific RGB colors, and textual\ndescriptions. Our model demonstrates notable improvements over existing\nGAN-based and recent diffusion-based frameworks - improved customization,\ncolor-matching capabilities, identity preservation and compatibility with\ntextual descriptions or LLMs with affordable computational costs.",
    "published": "2025-10-13T02:29:23Z",
    "updated": "2025-10-13T02:29:23Z",
    "link": "http://arxiv.org/pdf/2510.10918v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Geon Yeong Park",
      "Inhwa Han",
      "Serin Yang",
      "Yeobin Hong",
      "Seongmin Jeong",
      "Heechan Jeon",
      "Myeongjin Goh",
      "Sung Won Yi",
      "Jin Nam",
      "Jong Chul Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.07501v3",
    "title": "FormCoach: Lift Smarter, Not Harder",
    "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI.",
    "published": "2025-08-10T22:33:43Z",
    "updated": "2025-10-13T02:14:50Z",
    "link": "http://arxiv.org/pdf/2508.07501v3.pdf",
    "category": [
      "cs.CV",
      "cs.HC"
    ],
    "authors": [
      "Xiaoye Zuo",
      "Nikos Athanasiou",
      "Ginger Delmas",
      "Yiming Huang",
      "Xingyu Fu",
      "Lingjie Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2406.04765v2",
    "title": "SMC++: Masked Learning of Unsupervised Video Semantic Compression",
    "summary": "Most video compression methods focus on human visual perception, neglecting\nsemantic preservation. This leads to severe semantic loss during the\ncompression, hampering downstream video analysis tasks. In this paper, we\npropose a Masked Video Modeling (MVM)-powered compression framework that\nparticularly preserves video semantics, by jointly mining and compressing the\nsemantics in a self-supervised manner. While MVM is proficient at learning\ngeneralizable semantics through the masked patch prediction task, it may also\nencode non-semantic information like trivial textural details, wasting bitcost\nand bringing semantic noises. To suppress this, we explicitly regularize the\nnon-semantic entropy of the compressed video in the MVM token space. The\nproposed framework is instantiated as a simple Semantic-Mining-then-Compression\n(SMC) model. Furthermore, we extend SMC as an advanced SMC++ model from several\naspects. First, we equip it with a masked motion prediction objective, leading\nto better temporal semantic learning ability. Second, we introduce a\nTransformer-based compression module, to improve the semantic compression\nefficacy. Considering that directly mining the complex redundancy among\nheterogeneous features in different coding stages is non-trivial, we introduce\na compact blueprint semantic representation to align these features into a\nsimilar form, fully unleashing the power of the Transformer-based compression\nmodule. Extensive results demonstrate the proposed SMC and SMC++ models show\nremarkable superiority over previous traditional, learnable, and perceptual\nquality-oriented video codecs, on three video analysis tasks and seven\ndatasets. \\textit{Codes and model are available at:\nhttps://github.com/tianyuan168326/VideoSemanticCompression-Pytorch.",
    "published": "2024-06-07T09:06:40Z",
    "updated": "2025-10-13T02:12:10Z",
    "link": "http://arxiv.org/pdf/2406.04765v2.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Yuan Tian",
      "Xiaoyue Ling",
      "Cong Geng",
      "Qiang Hu",
      "Guo Lu",
      "Guangtao Zhai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10910v1",
    "title": "SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework\n  with Diffusion Model",
    "summary": "With the rapid development of diffusion models, style transfer has made\nremarkable progress. However, flexible and localized style editing for scene\ntext remains an unsolved challenge. Although existing scene text editing\nmethods have achieved text region editing, they are typically limited to\ncontent replacement and simple styles, which lack the ability of free-style\ntransfer. In this paper, we introduce SceneTextStylizer, a novel training-free\ndiffusion-based framework for flexible and high-fidelity style transfer of text\nin scene images. Unlike prior approaches that either perform global style\ntransfer or focus solely on textual content modification, our method enables\nprompt-guided style transformation specifically for text regions, while\npreserving both text readability and stylistic consistency. To achieve this, we\ndesign a feature injection module that leverages diffusion model inversion and\nself-attention to transfer style features effectively. Additionally, a region\ncontrol mechanism is introduced by applying a distance-based changing mask at\neach denoising step, enabling precise spatial control. To further enhance\nvisual quality, we incorporate a style enhancement module based on the Fourier\ntransform to reinforce stylistic richness. Extensive experiments demonstrate\nthat our method achieves superior performance in scene text style\ntransformation, outperforming existing state-of-the-art methods in both visual\nfidelity and text preservation.",
    "published": "2025-10-13T02:11:57Z",
    "updated": "2025-10-13T02:11:57Z",
    "link": "http://arxiv.org/pdf/2510.10910v1.pdf",
    "category": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Honghui Yuan",
      "Keiji Yanai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.23555v3",
    "title": "LH2Face: Loss function for Hard High-quality Face",
    "summary": "In current practical face authentication systems, most face recognition (FR)\nalgorithms are based on cosine similarity with softmax classification. Despite\nits reliable classification performance, this method struggles with hard\nsamples. A popular strategy to improve FR performance is incorporating angular\nor cosine margins. However, it does not take face quality or recognition\nhardness into account, simply increasing the margin value and thus causing an\noverly uniform training strategy. To address this problem, a novel loss\nfunction is proposed, named Loss function for Hard High-quality Face (LH2Face).\nFirstly, a similarity measure based on the von Mises-Fisher (vMF) distribution\nis stated, specifically focusing on the logarithm of the Probability Density\nFunction (PDF), which represents the distance between a probability\ndistribution and a vector. Then, an adaptive margin-based multi-classification\nmethod using softmax, called the Uncertainty-Aware Margin Function, is\nimplemented in the article. Furthermore, proxy-based loss functions are used to\napply extra constraints between the proxy and sample to optimize their\nrepresentation space distribution. Finally, a renderer is constructed that\noptimizes FR through face reconstruction and vice versa. Our LH2Face is\nsuperior to similiar schemes on hard high-quality face datasets, achieving\n49.39% accuracy on the IJB-B dataset, which surpasses the second-place method\nby 2.37%.",
    "published": "2025-06-30T06:59:02Z",
    "updated": "2025-10-13T02:05:40Z",
    "link": "http://arxiv.org/pdf/2506.23555v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fan Xie",
      "Yang Wang",
      "Yikang Jiao",
      "Zhenyu Yuan",
      "Congxi Chen",
      "Chuanxin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10889v1",
    "title": "Topological Alignment of Shared Vision-Language Embedding Space",
    "summary": "Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot\ncapabilities. However, their cross-modal alignment remains biased toward\nEnglish due to limited multilingual multimodal data. Recent multilingual\nextensions have alleviated this gap but enforce instance-level alignment while\nneglecting the global geometry of the shared embedding space. We address this\nproblem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a\ntopology-aware framework aligning embedding spaces with topology-preserving\nconstraints. The proposed method applies persistent homology to define a\ntopological alignment loss and approximates persistence diagram with\ntheoretical error bounds using graph sparsification strategy. This work\nvalidates the proposed approach, showing enhanced structural coherence of\nmultilingual representations, higher zero-shot accuracy on the CIFAR-100, and\nstronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the\nproposed approach provides a general method for incorporating topological\nalignment into representation learning.",
    "published": "2025-10-13T01:36:38Z",
    "updated": "2025-10-13T01:36:38Z",
    "link": "http://arxiv.org/pdf/2510.10889v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Junwon You",
      "Dasol Kang",
      "Jae-Hun Jung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21451v2",
    "title": "One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a\n  Practical Visual Specialist",
    "summary": "Image captioning is fundamental for applications like video-grounded chatbot\nsystems and navigation robots, yet deploying such models on local devices is\nchallenging due to the high computational demands of multimodal LLMs (MLLMs).\nTo address this, we first build lightweight captioning models using a\n125M-parameter language model, 56 times smaller than LLaMA-7B, and evaluate\ntheir performance not only on single-sentence but on detailed captioning tasks.\nWe obtain surprising results showing that our model can achieve performance\ncomparable to MLLMs, suggesting its potential to serve as a strong captioning\nspecialist for on-device applications. While promising, our model also exhibits\na limitation: like other MLLMs, it suffers from occasional captioning errors.\nWe investigate the underlying causes and observe that the problems stem from\nineffective attention mechanisms and limited visual representations. To\nalleviate them, we develop a novel captioning framework, Sharp-Eyed Refinement,\nwhich enhances caption quality by refining coarse descriptions into more\nprecise captions. At its core, DeepLens improves visual grounding by\nre-examining the informative regions identified in the initial glance.\nExperimental results demonstrate the superiority of our model over both recent\nlightweight captioning methods and MLLMs in detailed captioning and even in\nlong-range video QA tasks.",
    "published": "2025-08-29T09:29:27Z",
    "updated": "2025-10-13T01:25:01Z",
    "link": "http://arxiv.org/pdf/2508.21451v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junha Song",
      "Yongsik Jo",
      "So Yeon Min",
      "Quanting Xie",
      "Taehwan Kim",
      "Yonatan Bisk",
      "Jaegul Choo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10880v1",
    "title": "Where on Earth? A Vision-Language Benchmark for Probing Model\n  Geolocation Skills Across Scales",
    "summary": "Vision-language models (VLMs) have advanced rapidly, yet their capacity for\nimage-grounded geolocation in open-world conditions, a task that is challenging\nand of demand in real life, has not been comprehensively evaluated. We present\nEarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates\nvisual recognition, step-by-step reasoning, and evidence use. EarthWhere\ncomprises 810 globally distributed images across two complementary geolocation\nscales: WhereCountry (i.e., 500 multiple-choice question-answering, with\ncountry-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained\nstreet-level identification tasks requiring multi-step reasoning with optional\nweb search). For evaluation, we adopt the final-prediction metrics: location\naccuracies within k km (Acc@k) for coordinates and hierarchical path scores for\ntextual localization. Beyond this, we propose to explicitly score intermediate\nreasoning chains using human-verified key visual clues and a Shapley-reweighted\nthinking score that attributes credit to each clue's marginal contribution. We\nbenchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere\nand report different types of final answer accuracies as well as the calibrated\nmodel thinking scores. Overall, Gemini-2.5-Pro achieves the best average\naccuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches\n34.71%. We reveal that web search and reasoning do not guarantee improved\nperformance when visual clues are limited, and models exhibit regional biases,\nachieving up to 42.7% higher scores in certain areas than others. These\nfindings highlight not only the promise but also the persistent challenges of\nmodels to mitigate bias and achieve robust, fine-grained localization. We\nopen-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.",
    "published": "2025-10-13T01:12:21Z",
    "updated": "2025-10-13T01:12:21Z",
    "link": "http://arxiv.org/pdf/2510.10880v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhaofang Qian",
      "Hardy Chen",
      "Zeyu Wang",
      "Li Zhang",
      "Zijun Wang",
      "Xiaoke Huang",
      "Hui Liu",
      "Xianfeng Tang",
      "Zeyu Zheng",
      "Haoqin Tu",
      "Cihang Xie",
      "Yuyin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10876v1",
    "title": "rareboost3d: a synthetic lidar dataset with enhanced rare classes",
    "summary": "Real-world point cloud datasets have made significant contributions to the\ndevelopment of LiDAR-based perception technologies, such as object segmentation\nfor autonomous driving. However, due to the limited number of instances in some\nrare classes, the long-tail problem remains a major challenge in existing\ndatasets. To address this issue, we introduce a novel, synthetic point cloud\ndataset named RareBoost3D, which complements existing real-world datasets by\nproviding significantly more instances for object classes that are rare in\nreal-world datasets. To effectively leverage both synthetic and real-world\ndata, we further propose a cross-domain semantic alignment method named CSC\nloss that aligns feature representations of the same class across different\ndomains. Experimental results demonstrate that this alignment significantly\nenhances the performance of LiDAR point cloud segmentation models over\nreal-world data.",
    "published": "2025-10-13T01:02:33Z",
    "updated": "2025-10-13T01:02:33Z",
    "link": "http://arxiv.org/pdf/2510.10876v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shutong Lin",
      "Zhengkang Xiang",
      "Jianzhong Qi",
      "Kourosh Khoshelham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.01720v2",
    "title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "summary": "Customization of text-to-image models enables users to insert new concepts or\nobjects and generate them in unseen settings. Existing methods either rely on\ncomparatively expensive test-time optimization or train encoders on\nsingle-image datasets without multi-image supervision, which can limit image\nquality. We propose a simple approach to address these challenges. We first\nleverage existing text-to-image models and 3D datasets to create a high-quality\nSynthetic Customization Dataset (SynCD) consisting of multiple images of the\nsame object in different lighting, backgrounds, and poses. Using this dataset,\nwe train an encoder-based model that incorporates fine-grained visual details\nfrom reference images via a shared attention mechanism. Finally, we propose an\ninference technique that normalizes text and image guidance vectors to mitigate\noverexposure issues in sampled images. Through extensive experiments, we show\nthat our encoder-based model, trained on SynCD, and with the proposed inference\nalgorithm, improves upon existing encoder-based methods on standard\ncustomization benchmarks.",
    "published": "2025-02-03T18:59:41Z",
    "updated": "2025-10-13T00:38:43Z",
    "link": "http://arxiv.org/pdf/2502.01720v2.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "authors": [
      "Nupur Kumari",
      "Xi Yin",
      "Jun-Yan Zhu",
      "Ishan Misra",
      "Samaneh Azadi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10868v1",
    "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding",
    "summary": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have\nachieved strong performance but often suffer from high computational cost and\ncomplexity due to deep transformer architectures and redundant tokens. In this\npaper, we introduce two HMR-specific merging strategies: Error-Constrained\nLayer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM\nselectively merges transformer layers that have minimal impact on the Mean Per\nJoint Position Error (MPJPE), while Mask-ToMe focuses on merging background\ntokens that contribute little to the final prediction. To further address the\npotential performance drop caused by merging, we propose a diffusion-based\ndecoder that incorporates temporal context and leverages pose priors learned\nfrom large-scale motion capture datasets. Experiments across multiple\nbenchmarks demonstrate that our method achieves up to 2.3x speed-up while\nslightly improving performance over the baseline.",
    "published": "2025-10-13T00:23:17Z",
    "updated": "2025-10-13T00:23:17Z",
    "link": "http://arxiv.org/pdf/2510.10868v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Soroush Mehraban",
      "Andrea Iaboni",
      "Babak Taati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.13964v2",
    "title": "Precise Mobile Manipulation of Small Everyday Objects",
    "summary": "Many everyday mobile manipulation tasks require precise interaction with\nsmall objects, such as grasping a knob to open a cabinet or pressing a light\nswitch. In this paper, we develop Servoing with Vision Models (SVM), a\nclosed-loop framework that enables a mobile manipulator to tackle such precise\ntasks involving the manipulation of small objects. SVM uses state-of-the-art\nvision foundation models to generate 3D targets for visual servoing to enable\ndiverse tasks in novel environments. Naively doing so fails because of\nocclusion by the end-effector. SVM mitigates this using vision models that\nout-paint the end-effector, thereby significantly enhancing target\nlocalization. We demonstrate that aided by out-painting methods,\nopen-vocabulary object detectors can serve as a drop-in module for SVM to seek\nsemantic targets (e.g. knobs) and point tracking methods can help SVM reliably\npursue interaction sites indicated by user clicks. We conduct a large-scale\nevaluation spanning experiments in 10 novel environments across 6 buildings\nincluding 72 different object instances. SVM obtains a 71% zero-shot success\nrate on manipulating unseen objects in novel environments in the real world,\noutperforming an open-loop control method by an absolute 42% and an imitation\nlearning baseline trained on 1000+ demonstrations also by an absolute success\nrate of 50%.",
    "published": "2025-02-19T18:59:17Z",
    "updated": "2025-10-13T00:08:12Z",
    "link": "http://arxiv.org/pdf/2502.13964v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Arjun Gupta",
      "Rishik Sathua",
      "Saurabh Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.07381v2",
    "title": "Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting\n  in Videos",
    "summary": "Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as the Gate\nShift Module (GSM) or the Gate Shift Fuse to enrich 2D CNN feature extractors\nwith temporal context. However, these modules are limited in both temporal\nreceptive field and spatial adaptability. We propose a Multi-Scale Attention\nGate Shift Module (MSAGSM) that enhances GSM with multi-scale temporal shifts\nand channel grouped spatial attention, enabling efficient modeling of both\nshort and long-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight, plug-and-play module that integrates seamlessly with diverse 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia dataset, the first PES benchmark for table tennis containing over\n4,800 precisely annotated events. Extensive experiments across four PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.",
    "published": "2025-07-10T02:30:07Z",
    "updated": "2025-10-12T23:21:46Z",
    "link": "http://arxiv.org/pdf/2507.07381v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hao Xu",
      "Sam Wells",
      "Mohamed Reda Bouadjenek",
      "Richard Dazeley",
      "Sunil Aryal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.00313v2",
    "title": "Streamlining Image Editing with Layered Diffusion Brushes",
    "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
    "published": "2024-05-01T04:30:03Z",
    "updated": "2025-10-12T23:17:39Z",
    "link": "http://arxiv.org/pdf/2405.00313v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peyman Gholami",
      "Robert Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26618v2",
    "title": "DA$^2$: Depth Anything in Any Direction",
    "summary": "Panorama has a full FoV (360$^\\circ\\times$180$^\\circ$), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\n$\\textbf{DA}$$^{\\textbf{2}}$: $\\textbf{D}$epth $\\textbf{A}$nything in\n$\\textbf{A}$ny $\\textbf{D}$irection, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create $\\sim$543K panoramic\nRGB-depth pairs, bringing the total to $\\sim$607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA$^{2}$'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA$^{2}$ even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA$^{2}$ exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data has be released.\nProject page: https://depth-any-in-any-dir.github.io/.",
    "published": "2025-09-30T17:55:37Z",
    "updated": "2025-10-12T22:41:42Z",
    "link": "http://arxiv.org/pdf/2509.26618v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haodong Li",
      "Wangguangdong Zheng",
      "Jing He",
      "Yuhao Liu",
      "Xin Lin",
      "Xin Yang",
      "Ying-Cong Chen",
      "Chunchao Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10822v1",
    "title": "From Detection to Mitigation: Addressing Bias in Deep Learning Models\n  for Chest X-Ray Diagnosis",
    "summary": "Deep learning models have shown promise in improving diagnostic accuracy from\nchest X-rays, but they also risk perpetuating healthcare disparities when\nperformance varies across demographic groups. In this work, we present a\ncomprehensive bias detection and mitigation framework targeting sex, age, and\nrace-based disparities when performing diagnostic tasks with chest X-rays. We\nextend a recent CNN-XGBoost pipeline to support multi-label classification and\nevaluate its performance across four medical conditions. We show that replacing\nthe final layer of CNN with an eXtreme Gradient Boosting classifier improves\nthe fairness of the subgroup while maintaining or improving the overall\npredictive performance. To validate its generalizability, we apply the method\nto different backbones, namely DenseNet-121 and ResNet-50, and achieve\nsimilarly strong performance and fairness outcomes, confirming its\nmodel-agnostic design. We further compare this lightweight adapter training\nmethod with traditional full-model training bias mitigation techniques,\nincluding adversarial training, reweighting, data augmentation, and active\nlearning, and find that our approach offers competitive or superior bias\nreduction at a fraction of the computational cost. Finally, we show that\ncombining eXtreme Gradient Boosting retraining with active learning yields the\nlargest reduction in bias across all demographic subgroups, both in and out of\ndistribution on the CheXpert and MIMIC datasets, establishing a practical and\neffective path toward equitable deep learning deployment in clinical radiology.",
    "published": "2025-10-12T22:20:08Z",
    "updated": "2025-10-12T22:20:08Z",
    "link": "http://arxiv.org/pdf/2510.10822v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Clemence Mottez",
      "Louisa Fay",
      "Maya Varma",
      "Sophie Ostmeier",
      "Curtis Langlotz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10802v1",
    "title": "MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral\n  Cloud Segmentation",
    "summary": "Clouds remain a critical challenge in optical satellite imagery, hindering\nreliable analysis for environmental monitoring, land cover mapping, and climate\nresearch. To overcome this, we propose MSCloudCAM, a Cross-Attention with\nMulti-Scale Context Network tailored for multispectral and multi-sensor cloud\nsegmentation. Our framework exploits the spectral richness of Sentinel-2\n(CloudSEN12) and Landsat-8 (L8Biome) data to classify four semantic categories:\nclear sky, thin cloud, thick cloud, and cloud shadow. MSCloudCAM combines a\nSwin Transformer backbone for hierarchical feature extraction with multi-scale\ncontext modules ASPP and PSP for enhanced scale-aware learning. A\nCross-Attention block enables effective multisensor and multispectral feature\nfusion, while the integration of an Efficient Channel Attention Block (ECAB)\nand a Spatial Attention Module adaptively refine feature representations.\nComprehensive experiments on CloudSEN12 and L8Biome demonstrate that MSCloudCAM\ndelivers state-of-the-art segmentation accuracy, surpassing leading baseline\narchitectures while maintaining competitive parameter efficiency and FLOPs.\nThese results underscore the model's effectiveness and practicality, making it\nwell-suited for large-scale Earth observation tasks and real-world\napplications.",
    "published": "2025-10-12T20:40:22Z",
    "updated": "2025-10-12T20:40:22Z",
    "link": "http://arxiv.org/pdf/2510.10802v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "F.2.2, I.2.7"
    ],
    "authors": [
      "Md Abdullah Al Mazid",
      "Liangdong Deng",
      "Naphtali Rishe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10797v1",
    "title": "Full segmentation annotations of 3D time-lapse microscopy images of\n  MDA231 cells",
    "summary": "High-quality, publicly available segmentation annotations of image and video\ndatasets are critical for advancing the field of image processing. In\nparticular, annotations of volumetric images of a large number of targets are\ntime-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we\npresented the first publicly available full 3D time-lapse segmentation\nannotations of migrating cells with complex dynamic shapes. Concretely, three\ndistinct humans annotated two sequences of MDA231 human breast carcinoma cells\n(Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).\n  This paper aims to provide a comprehensive description of the dataset and\naccompanying experiments that were not included in (Melnikova, A., & Matula,\nP., 2025) due to limitations in publication space. Namely, we show that the\ncreated annotations are consistent with the previously published tracking\nmarkers provided by the CTC organizers and the segmentation accuracy measured\nbased on the 2D gold truth of CTC is within the inter-annotator variability\nmargins. We compared the created 3D annotations with automatically created\nsilver truth provided by CTC. We have found the proposed annotations better\nrepresent the complexity of the input images. The presented annotations can be\nused for testing and training cell segmentation, or analyzing 3D shapes of\nhighly dynamic objects.",
    "published": "2025-10-12T20:31:40Z",
    "updated": "2025-10-12T20:31:40Z",
    "link": "http://arxiv.org/pdf/2510.10797v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Aleksandra Melnikova",
      "Petr Matula"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10793v1",
    "title": "ImHead: A Large-scale Implicit Morphable Model for Localized Head\n  Modeling",
    "summary": "Over the last years, 3D morphable models (3DMMs) have emerged as a\nstate-of-the-art methodology for modeling and generating expressive 3D avatars.\nHowever, given their reliance on a strict topology, along with their linear\nnature, they struggle to represent complex full-head shapes. Following the\nadvent of deep implicit functions, we propose imHead, a novel implicit 3DMM\nthat not only models expressive 3D head avatars but also facilitates localized\nediting of the facial features. Previous methods directly divided the latent\nspace into local components accompanied by an identity encoding to capture the\nglobal shape variations, leading to expensive latent sizes. In contrast, we\nretain a single compact identity space and introduce an intermediate\nregion-specific latent representation to enable local edits. To train imHead,\nwe curate a large-scale dataset of 4K distinct identities, making a\nstep-towards large scale 3D head modeling. Under a series of experiments we\ndemonstrate the expressive power of the proposed model to represent diverse\nidentities and expressions outperforming previous approaches. Additionally, the\nproposed approach provides an interpretable solution for 3D face manipulation,\nallowing the user to make localized edits.",
    "published": "2025-10-12T20:17:34Z",
    "updated": "2025-10-12T20:17:34Z",
    "link": "http://arxiv.org/pdf/2510.10793v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rolandos Alexandros Potamias",
      "Stathis Galanakis",
      "Jiankang Deng",
      "Athanasios Papaioannou",
      "Stefanos Zafeiriou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10782v1",
    "title": "DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic\n  Underwater Image Generation",
    "summary": "In this paper, we propose a novel framework, Disentangled Style-Content GAN\n(DISC-GAN), which integrates style-content disentanglement with a\ncluster-specific training strategy towards photorealistic underwater image\nsynthesis. The quality of synthetic underwater images is challenged by optical\ndue to phenomena such as color attenuation and turbidity. These phenomena are\nrepresented by distinct stylistic variations across different waterbodies, such\nas changes in tint and haze. While generative models are well-suited to capture\ncomplex patterns, they often lack the ability to model the non-uniform\nconditions of diverse underwater environments. To address these challenges, we\nemploy K-means clustering to partition a dataset into style-specific domains.\nWe use separate encoders to get latent spaces for style and content; we further\nintegrate these latent representations via Adaptive Instance Normalization\n(AdaIN) and decode the result to produce the final synthetic image. The model\nis trained independently on each style cluster to preserve domain-specific\ncharacteristics. Our framework demonstrates state-of-the-art performance,\nobtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak\nSignal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance\n(FID) of 13.3728.",
    "published": "2025-10-12T19:56:20Z",
    "updated": "2025-10-12T19:56:20Z",
    "link": "http://arxiv.org/pdf/2510.10782v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Sneha Varur",
      "Anirudh R Hanchinamani",
      "Tarun S Bagewadi",
      "Uma Mudenagudi",
      "Chaitra D Desai",
      "Sujata C",
      "Padmashree Desai",
      "Sumit Meharwade"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10779v1",
    "title": "Structured Spectral Graph Learning for Multi-label Abnormality\n  Classification in 3D Chest CT Scans",
    "summary": "With the growing volume of CT examinations, there is an increasing demand for\nautomated tools such as organ segmentation, abnormality detection, and report\ngeneration to support radiologists in managing their clinical workload.\nMulti-label classification of 3D Chest CT scans remains a critical yet\nchallenging problem due to the complex spatial relationships inherent in\nvolumetric data and the wide variability of abnormalities. Existing methods\nbased on 3D convolutional neural networks struggle to capture long-range\ndependencies, while Vision Transformers often require extensive pre-training on\nlarge-scale, domain-specific datasets to perform competitively. In this work,\nwe propose a 2.5D alternative by introducing a new graph-based framework that\nrepresents 3D CT volumes as structured graphs, where axial slice triplets serve\nas nodes processed through spectral graph convolution, enabling the model to\nreason over inter-slice dependencies while maintaining complexity compatible\nwith clinical deployment. Our method, trained and evaluated on 3 datasets from\nindependent institutions, achieves strong cross-dataset generalization, and\nshows competitive performance compared to state-of-the-art visual encoders. We\nfurther conduct comprehensive ablation studies to evaluate the impact of\nvarious aggregation strategies, edge-weighting schemes, and graph connectivity\npatterns. Additionally, we demonstrate the broader applicability of our\napproach through transfer experiments on automated radiology report generation\nand abdominal CT data.\\\\ This work extends our previous contribution presented\nat the MICCAI 2025 EMERGE Workshop.",
    "published": "2025-10-12T19:49:51Z",
    "updated": "2025-10-12T19:49:51Z",
    "link": "http://arxiv.org/pdf/2510.10779v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Theo Di Piazza",
      "Carole Lazarus",
      "Olivier Nempont",
      "Loic Boussel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10765v1",
    "title": "EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird\n  Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse\n  Condition",
    "summary": "Identifying drones and birds correctly is essential for keeping the skies\nsafe and improving security systems. Using the VIP CUP 2025 dataset, which\nprovides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a\nnew lightweight yet powerful model for object detection. The model improves how\nimage features are captured and understood, making detection more accurate and\nefficient. It uses smart design changes and attention layers to focus on\nimportant details while reducing the amount of computation needed. A special\ndetection head helps the model adapt to objects of different shapes and sizes.\nWe trained three versions: one using RGB images, one using IR images, and one\ncombining both. The combined model achieved the best accuracy and reliability\nwhile running fast enough for real-time use on common GPUs.",
    "published": "2025-10-12T19:05:16Z",
    "updated": "2025-10-12T19:05:16Z",
    "link": "http://arxiv.org/pdf/2510.10765v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sudipto Sarkar",
      "Mohammad Asif Hasan",
      "Khondokar Ashik Shahriar",
      "Fablia Labiba",
      "Nahian Tasnim",
      "Sheikh Anawarul Haq Fattah"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10764v1",
    "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior\n  Efficiency",
    "summary": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively.",
    "published": "2025-10-12T19:05:04Z",
    "updated": "2025-10-12T19:05:04Z",
    "link": "http://arxiv.org/pdf/2510.10764v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Shaharyar Ahmed Khan Tareen",
      "Filza Khan Tareen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10753v1",
    "title": "Restricted Receptive Fields for Face Verification",
    "summary": "Understanding how deep neural networks make decisions is crucial for\nanalyzing their behavior and diagnosing failure cases. In computer vision, a\ncommon approach to improve interpretability is to assign importance to\nindividual pixels using post-hoc methods. Although they are widely used to\nexplain black-box models, their fidelity to the model's actual reasoning is\nuncertain due to the lack of reliable evaluation metrics. This limitation\nmotivates an alternative approach, which is to design models whose decision\nprocesses are inherently interpretable. To this end, we propose a face\nsimilarity metric that breaks down global similarity into contributions from\nrestricted receptive fields. Our method defines the similarity between two face\nimages as the sum of patch-level similarity scores, providing a locally\nadditive explanation without relying on post-hoc analysis. We show that the\nproposed approach achieves competitive verification performance even with\npatches as small as 28x28 within 112x112 face images, and surpasses\nstate-of-the-art methods when using 56x56 patches.",
    "published": "2025-10-12T18:46:56Z",
    "updated": "2025-10-12T18:46:56Z",
    "link": "http://arxiv.org/pdf/2510.10753v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kagan Ozturk",
      "Aman Bhatta",
      "Haiyu Wu",
      "Patrick Flynn",
      "Kevin W. Bowyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10750v1",
    "title": "Uncovering Anomalous Events for Marine Environmental Monitoring via\n  Visual Anomaly Detection",
    "summary": "Underwater video monitoring is a promising strategy for assessing marine\nbiodiversity, but the vast volume of uneventful footage makes manual inspection\nhighly impractical. In this work, we explore the use of visual anomaly\ndetection (VAD) based on deep neural networks to automatically identify\ninteresting or anomalous events. We introduce AURA, the first multi-annotator\nbenchmark dataset for underwater VAD, and evaluate four VAD models across two\nmarine scenes. We demonstrate the importance of robust frame selection\nstrategies to extract meaningful video segments. Our comparison against\nmultiple annotators reveals that VAD performance of current models varies\ndramatically and is highly sensitive to both the amount of training data and\nthe variability in visual content that defines \"normal\" scenes. Our results\nhighlight the value of soft and consensus labels and offer a practical approach\nfor supporting scientific exploration and scalable biodiversity monitoring.",
    "published": "2025-10-12T18:39:34Z",
    "updated": "2025-10-12T18:39:34Z",
    "link": "http://arxiv.org/pdf/2510.10750v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Laura Weihl",
      "Nejc Novak",
      "Stefan H. Bengtson",
      "Malte Pedersen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10742v1",
    "title": "Seeing My Future: Predicting Situated Interaction Behavior in Virtual\n  Reality",
    "summary": "Virtual and augmented reality systems increasingly demand intelligent\nadaptation to user behaviors for enhanced interaction experiences. Achieving\nthis requires accurately understanding human intentions and predicting future\nsituated behaviors - such as gaze direction and object interactions - which is\nvital for creating responsive VR/AR environments and applications like\npersonalized assistants. However, accurate behavioral prediction demands\nmodeling the underlying cognitive processes that drive human-environment\ninteractions. In this work, we introduce a hierarchical, intention-aware\nframework that models human intentions and predicts detailed situated behaviors\nby leveraging cognitive mechanisms. Given historical human dynamics and the\nobservation of scene contexts, our framework first identifies potential\ninteraction targets and forecasts fine-grained future behaviors. We propose a\ndynamic Graph Convolutional Network (GCN) to effectively capture\nhuman-environment relationships. Extensive experiments on challenging\nreal-world benchmarks and live VR environment demonstrate the effectiveness of\nour approach, achieving superior performance across all metrics and enabling\npractical applications for proactive VR systems that anticipate user behaviors\nand adapt virtual environments accordingly.",
    "published": "2025-10-12T18:29:01Z",
    "updated": "2025-10-12T18:29:01Z",
    "link": "http://arxiv.org/pdf/2510.10742v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Yuan Xu",
      "Zimu Zhang",
      "Xiaoxuan Ma",
      "Wentao Zhu",
      "Yu Qiao",
      "Yizhou Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11711v1",
    "title": "Reinforced sequential Monte Carlo for amortised sampling",
    "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods.",
    "published": "2025-10-13T17:59:11Z",
    "updated": "2025-10-13T17:59:11Z",
    "link": "http://arxiv.org/pdf/2510.11711v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Sanghyeok Choi",
      "Sarthak Mittal",
      "Víctor Elvira",
      "Jinkyoo Park",
      "Nikolay Malkin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11691v1",
    "title": "Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player\n  Zero-Sum Games",
    "summary": "In two-player zero-sum games, the learning dynamic based on optimistic Hedge\nachieves one of the best-known regret upper bounds among strongly-uncoupled\nlearning dynamics. With an appropriately chosen learning rate, the social and\nindividual regrets can be bounded by $O(\\log(mn))$ in terms of the numbers of\nactions $m$ and $n$ of the two players. This study investigates the optimality\nof the dependence on $m$ and $n$ in the regret of optimistic Hedge. To this\nend, we begin by refining existing regret analysis and show that, in the\nstrongly-uncoupled setting where the opponent's number of actions is known,\nboth the social and individual regret bounds can be improved to $O(\\sqrt{\\log m\n\\log n})$. In this analysis, we express the regret upper bound as an\noptimization problem with respect to the learning rates and the coefficients of\ncertain negative terms, enabling refined analysis of the leading constants. We\nthen show that the existing social regret bound as well as these new social and\nindividual regret upper bounds cannot be further improved for optimistic Hedge\nby providing algorithm-dependent individual regret lower bounds. Importantly,\nthese social regret upper and lower bounds match exactly including the constant\nfactor in the leading term. Finally, building on these results, we improve the\nlast-iterate convergence rate and the dynamic regret of a learning dynamic\nbased on optimistic Hedge, and complement these bounds with algorithm-dependent\ndynamic regret lower bounds that match the improved bounds.",
    "published": "2025-10-13T17:52:01Z",
    "updated": "2025-10-13T17:52:01Z",
    "link": "http://arxiv.org/pdf/2510.11691v1.pdf",
    "category": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "authors": [
      "Taira Tsuchiya"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11677v1",
    "title": "Chronologically Consistent Generative AI",
    "summary": "We introduce a family of chronologically consistent, instruction-following\nlarge language models to eliminate lookahead bias. Each model is trained only\non data available before a clearly defined knowledge-cutoff date, ensuring\nstrict temporal separation from any post-cutoff data. The resulting framework\noffers (i) a simple, conversational chat interface, (ii) fully open, fixed\nmodel weights that guarantee replicability, and (iii) a conservative lower\nbound on forecast accuracy, isolating the share of predictability that survives\nonce training leakage is removed. Together, these features provide researchers\nwith an easy-to-use generative AI tool useful for a wide range of prediction\ntasks that is free of lookahead bias.",
    "published": "2025-10-13T17:45:24Z",
    "updated": "2025-10-13T17:45:24Z",
    "link": "http://arxiv.org/pdf/2510.11677v1.pdf",
    "category": [
      "cs.LG",
      "q-fin.GN"
    ],
    "authors": [
      "Songrun He",
      "Linying Lv",
      "Asaf Manela",
      "Jimmy Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23389v2",
    "title": "Causal Explanation of Concept Drift -- A Truly Actionable Approach",
    "summary": "In a world that constantly changes, it is crucial to understand how those\nchanges impact different systems, such as industrial manufacturing or critical\ninfrastructure. Explaining critical changes, referred to as concept drift in\nthe field of machine learning, is the first step towards enabling targeted\ninterventions to avoid or correct model failures, as well as malfunctions and\nerrors in the physical world. Therefore, in this work, we extend model-based\ndrift explanations towards causal explanations, which increases the\nactionability of the provided explanations. We evaluate our explanation\nstrategy on a number of use cases, demonstrating the practical usefulness of\nour framework, which isolates the causally relevant features impacted by\nconcept drift and, thus, allows for targeted intervention.",
    "published": "2025-07-31T10:02:28Z",
    "updated": "2025-10-13T17:36:55Z",
    "link": "http://arxiv.org/pdf/2507.23389v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "David Komnick",
      "Kathrin Lammers",
      "Barbara Hammer",
      "Valerie Vaquet",
      "Fabian Hinder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11657v1",
    "title": "An Eulerian Perspective on Straight-Line Sampling",
    "summary": "We study dynamic measure transport for generative modeling: specifically,\nflows induced by stochastic processes that bridge a specified source and target\ndistribution. The conditional expectation of the process' velocity defines an\nODE whose flow map achieves the desired transport. We ask \\emph{which processes\nproduce straight-line flows} -- i.e., flows whose pointwise acceleration\nvanishes and thus are exactly integrable with a first-order method? We provide\na concise PDE characterization of straightness as a balance between conditional\nacceleration and the divergence of a weighted covariance (Reynolds) tensor.\nUsing this lens, we fully characterize affine-in-time interpolants and show\nthat straightness occurs exactly under deterministic endpoint couplings. We\nalso derive necessary conditions that constrain flow geometry for general\nprocesses, offering broad guidance for designing transports that are easier to\nintegrate.",
    "published": "2025-10-13T17:33:58Z",
    "updated": "2025-10-13T17:33:58Z",
    "link": "http://arxiv.org/pdf/2510.11657v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Panos Tsimpos",
      "Youssef Marzouk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11640v1",
    "title": "Continual Release of Densest Subgraphs: Privacy Amplification &\n  Sublinear Space via Subsampling",
    "summary": "We study the sublinear space continual release model for edge-differentially\nprivate (DP) graph algorithms, with a focus on the densest subgraph problem\n(DSG) in the insertion-only setting. Our main result is the first continual\nrelease DSG algorithm that matches the additive error of the best static DP\nalgorithms and the space complexity of the best non-private streaming\nalgorithms, up to constants. The key idea is a refined use of subsampling that\nsimultaneously achieves privacy amplification and sparsification, a connection\nnot previously formalized in graph DP. Via a simple black-box reduction to the\nstatic setting, we obtain both pure and approximate-DP algorithms with $O(\\log\nn)$ additive error and $O(n\\log n)$ space, improving both accuracy and space\ncomplexity over the previous state of the art. Along the way, we introduce\ngraph densification in the graph DP setting, adding edges to trigger earlier\nsubsampling, which removes the extra logarithmic factors in error and space\nincurred by prior work [ELMZ25]. We believe this simple idea may be of\nindependent interest.",
    "published": "2025-10-13T17:20:13Z",
    "updated": "2025-10-13T17:20:13Z",
    "link": "http://arxiv.org/pdf/2510.11640v1.pdf",
    "category": [
      "cs.DS",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Felix Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.12453v3",
    "title": "Cost-aware Stopping for Bayesian Optimization",
    "summary": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nusually matches or outperforms other acquisition-function--stopping-rule pairs\nin terms of cost-adjusted simple regret, a metric capturing trade-offs between\nsolution quality and cumulative evaluation cost.",
    "published": "2025-07-16T17:54:14Z",
    "updated": "2025-10-13T17:20:11Z",
    "link": "http://arxiv.org/pdf/2507.12453v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Qian Xie",
      "Linda Cai",
      "Alexander Terenin",
      "Peter I. Frazier",
      "Ziv Scully"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11617v1",
    "title": "Lecture Notes on Verifying Graph Neural Networks",
    "summary": "In these lecture notes, we first recall the connection between graph neural\nnetworks, Weisfeiler-Lehman tests and logics such as first-order logic and\ngraded modal logic. We then present a modal logic in which counting modalities\nappear in linear inequalities in order to solve verification tasks on graph\nneural networks. We describe an algorithm for the satisfiability problem of\nthat logic. It is inspired from the tableau method of vanilla modal logic,\nextended with reasoning in quantifier-free fragment Boolean algebra with\nPresburger arithmetic.",
    "published": "2025-10-13T16:57:20Z",
    "updated": "2025-10-13T16:57:20Z",
    "link": "http://arxiv.org/pdf/2510.11617v1.pdf",
    "category": [
      "cs.LO",
      "cs.LG"
    ],
    "authors": [
      "François Schwarzentruber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20982v3",
    "title": "Provably faster randomized and quantum algorithms for $k$-means\n  clustering via uniform sampling",
    "summary": "The $k$-means algorithm (Lloyd's algorithm) is a widely used method for\nclustering unlabeled data. A key bottleneck of the $k$-means algorithm is that\neach iteration requires time linear in the number of data points, which can be\nexpensive in big data applications. This was improved in recent works proposing\nquantum and quantum-inspired classical algorithms to approximate the $k$-means\nalgorithm locally, in time depending only logarithmically on the number of data\npoints (along with data dependent parameters) [q-means: A quantum algorithm for\nunsupervised machine learning, Kerenidis, Landman, Luongo, and Prakash, NeurIPS\n2019; Do you know what $q$-means?, Cornelissen, Doriguello, Luongo, Tang, QTML\n2025]. In this work, we describe a simple randomized mini-batch $k$-means\nalgorithm and a quantum algorithm inspired by the classical algorithm. We\ndemonstrate that the worst case guarantees of these algorithms can\nsignificantly improve upon the bounds for algorithms in prior work. Our\nimprovements are due to a careful use of uniform sampling, which preserves\ncertain symmetries of the $k$-means problem that are not preserved in previous\nalgorithms that use data norm-based sampling.",
    "published": "2025-04-29T17:51:29Z",
    "updated": "2025-10-13T16:44:52Z",
    "link": "http://arxiv.org/pdf/2504.20982v3.pdf",
    "category": [
      "quant-ph",
      "cs.DS",
      "cs.LG"
    ],
    "authors": [
      "Tyler Chen",
      "Archan Ray",
      "Akshay Seshadri",
      "Dylan Herman",
      "Bao Bach",
      "Pranav Deshpande",
      "Abhishek Som",
      "Niraj Kumar",
      "Marco Pistoia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11590v1",
    "title": "Diffusion-DFL: Decision-focused Diffusion Models for Stochastic\n  Optimization",
    "summary": "Decision-focused learning (DFL) integrates predictive modeling and\noptimization by training predictors to optimize the downstream decision target\nrather than merely minimizing prediction error. To date, existing DFL methods\ntypically rely on deterministic point predictions, which are often insufficient\nto capture the intrinsic stochasticity of real-world environments. To address\nthis challenge, we propose the first diffusion-based DFL approach, which trains\na diffusion model to represent the distribution of uncertain parameters and\noptimizes the decision by solving a stochastic optimization with samples drawn\nfrom the diffusion model. Our contributions are twofold. First, we formulate\ndiffusion DFL using the reparameterization trick, enabling end-to-end training\nthrough diffusion. While effective, it is memory and compute-intensive due to\nthe need to differentiate through the diffusion sampling process. Second, we\npropose a lightweight score function estimator that uses only several forward\ndiffusion passes and avoids backpropagation through the sampling. This follows\nfrom our results that backpropagating through stochastic optimization can be\napproximated by a weighted score function formulation. We empirically show that\nour diffusion DFL approach consistently outperforms strong baselines in\ndecision quality. The source code for all experiments is available at the\nproject repository: https://github.com/GT-KOALA/Diffusion_DFL.",
    "published": "2025-10-13T16:31:17Z",
    "updated": "2025-10-13T16:31:17Z",
    "link": "http://arxiv.org/pdf/2510.11590v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Zihao Zhao",
      "Christopher Yeh",
      "Lingkai Kong",
      "Kai Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11561v1",
    "title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in\n  Python",
    "summary": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn.",
    "published": "2025-10-13T16:04:06Z",
    "updated": "2025-10-13T16:04:06Z",
    "link": "http://arxiv.org/pdf/2510.11561v1.pdf",
    "category": [
      "cs.LG",
      "cs.SC"
    ],
    "authors": [
      "Caglar Demir",
      "Alkid Baci",
      "N'Dah Jean Kouagou",
      "Leonie Nora Sieger",
      "Stefan Heindorf",
      "Simon Bin",
      "Lukas Blübaum",
      "Alexander Bigerl",
      "Axel-Cyrille Ngonga Ngomo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.23305v2",
    "title": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and\n  Source Extraction",
    "summary": "We present MGE-LDM, a unified latent diffusion framework for simultaneous\nmusic generation, source imputation, and query-driven source separation. Unlike\nprior approaches constrained to fixed instrument classes, MGE-LDM learns a\njoint distribution over full mixtures, submixtures, and individual stems within\na single compact latent diffusion model. At inference, MGE-LDM enables (1)\ncomplete mixture generation, (2) partial generation (i.e., source imputation),\nand (3) text-conditioned extraction of arbitrary sources. By formulating both\nseparation and imputation as conditional inpainting tasks in the latent space,\nour approach supports flexible, class-agnostic manipulation of arbitrary\ninstrument sources. Notably, MGE-LDM can be trained jointly across\nheterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without\nrelying on predefined instrument categories. Audio samples are available at our\nproject page: https://yoongi43.github.io/MGELDM_Samples/.",
    "published": "2025-05-29T10:04:24Z",
    "updated": "2025-10-13T15:49:43Z",
    "link": "http://arxiv.org/pdf/2505.23305v2.pdf",
    "category": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Yunkee Chae",
      "Kyogu Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11546v1",
    "title": "Efficient Group Lasso Regularized Rank Regression with Data-Driven\n  Parameter Determination",
    "summary": "High-dimensional regression often suffers from heavy-tailed noise and\noutliers, which can severely undermine the reliability of least-squares based\nmethods. To improve robustness, we adopt a non-smooth Wilcoxon score based rank\nobjective and incorporate structured group sparsity regularization, a natural\ngeneralization of the lasso, yielding a group lasso regularized rank regression\nmethod. By extending the tuning-free parameter selection scheme originally\ndeveloped for the lasso, we introduce a data-driven, simulation-based tuning\nrule and further establish a finite-sample error bound for the resulting\nestimator. On the computational side, we develop a proximal augmented\nLagrangian method for solving the associated optimization problem, which\neliminates the singularity issues encountered in existing methods, thereby\nenabling efficient semismooth Newton updates for the subproblems. Extensive\nnumerical experiments demonstrate the robustness and effectiveness of our\nproposed estimator against alternatives, and showcase the scalability of the\nalgorithm across both simulated and real-data settings.",
    "published": "2025-10-13T15:45:58Z",
    "updated": "2025-10-13T15:45:58Z",
    "link": "http://arxiv.org/pdf/2510.11546v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.OC",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Meixia Lin",
      "Meijiao Shi",
      "Yunhai Xiao",
      "Qian Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.16614v2",
    "title": "ORN-CBF: Learning Observation-conditioned Residual Neural Control\n  Barrier Functions via Hypernetworks",
    "summary": "Control barrier functions (CBFs) have been demonstrated as an effective\nmethod for safety-critical control of autonomous systems. Although CBFs are\nsimple to deploy, their design remains challenging, motivating the development\nof learning-based approaches. Yet, issues such as suboptimal safe sets,\napplicability in partially observable environments, and lack of rigorous safety\nguarantees persist. In this work, we propose observation-conditioned neural\nCBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately\nrecover the maximal safe sets. We exploit certain mathematical properties of\nthe HJ value function, ensuring that the predicted safe set never intersects\nwith the observed failure set. Moreover, we leverage a hypernetwork-based\narchitecture that is particularly suitable for the design of\nobservation-conditioned safety filters. The proposed method is examined both in\nsimulation and hardware experiments for a ground robot and a quadcopter. The\nresults show improved success rates and generalization to out-of-domain\nenvironments compared to the baselines.",
    "published": "2025-09-20T10:29:36Z",
    "updated": "2025-10-13T15:32:55Z",
    "link": "http://arxiv.org/pdf/2509.16614v2.pdf",
    "category": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Bojan Derajić",
      "Sebastian Bernhard",
      "Wolfgang Hönig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07569v2",
    "title": "Automated Machine Learning for Unsupervised Tabular Tasks",
    "summary": "In this work, we present LOTUS (Learning to Learn with Optimal Transport for\nUnsupervised Scenarios), a simple yet effective method to perform model\nselection for multiple unsupervised machine learning(ML) tasks such as outlier\ndetection and clustering. Our intuition behind this work is that a machine\nlearning pipeline will perform well in a new dataset if it previously worked\nwell on datasets with a similar underlying data distribution. We use Optimal\nTransport distances to find this similarity between unlabeled tabular datasets\nand recommend machine learning pipelines with one unified single method on two\ndownstream unsupervised tasks: outlier detection and clustering. We present the\neffectiveness of our approach with experiments against strong baselines and\nshow that LOTUS is a very promising first step toward model selection for\nmultiple unsupervised ML tasks.",
    "published": "2025-10-08T21:31:22Z",
    "updated": "2025-10-13T15:27:23Z",
    "link": "http://arxiv.org/pdf/2510.07569v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Prabhant Singh",
      "Pieter Gijsbers",
      "Elif Ceren Gok Yildirim",
      "Murat Onur Yildirim",
      "Joaquin Vanschoren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12989v3",
    "title": "Query Complexity of Classical and Quantum Channel Discrimination",
    "summary": "Quantum channel discrimination has been studied from an information-theoretic\nperspective, wherein one is interested in the optimal decay rate of error\nprobabilities as a function of the number of unknown channel accesses. In this\npaper, we study the query complexity of quantum channel discrimination, wherein\nthe goal is to determine the minimum number of channel uses needed to reach a\ndesired error probability. To this end, we show that the query complexity of\nbinary channel discrimination depends logarithmically on the inverse error\nprobability and inversely on the negative logarithm of the (geometric and\nHolevo) channel fidelity. As a special case of these findings, we precisely\ncharacterize the query complexity of discriminating two classical channels and\ntwo classical-quantum channels. Furthermore, by obtaining an optimal\ncharacterization of the sample complexity of quantum hypothesis testing,\nincluding prior probabilities, we provide a more precise characterization of\nquery complexity when the error probability does not exceed a fixed threshold.\nWe also provide lower and upper bounds on the query complexity of binary\nasymmetric channel discrimination and multiple quantum channel discrimination.\nFor the former, the query complexity depends on the geometric R\\'enyi and Petz\nR\\'enyi channel divergences, while for the latter, it depends on the negative\nlogarithm of the (geometric and Uhlmann) channel fidelity. For multiple channel\ndiscrimination, the upper bound scales as the logarithm of the number of\nchannels.",
    "published": "2025-04-17T14:54:00Z",
    "updated": "2025-10-13T15:15:40Z",
    "link": "http://arxiv.org/pdf/2504.12989v3.pdf",
    "category": [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Theshani Nuradha",
      "Mark M. Wilde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11505v1",
    "title": "Knowledge-Guided Machine Learning Models to Upscale Evapotranspiration\n  in the U.S. Midwest",
    "summary": "Evapotranspiration (ET) plays a critical role in the land-atmosphere\ninteractions, yet its accurate quantification across various spatiotemporal\nscales remains a challenge. In situ measurement approaches, like eddy\ncovariance (EC) or weather station-based ET estimation, allow for measuring ET\nat a single location. Agricultural uses of ET require estimates for each field\nover broad areas, making it infeasible to deploy sensing systems at each\nlocation. This study integrates tree-based and knowledge-guided machine\nlearning (ML) techniques with multispectral remote sensing data, griddled\nmeteorology and EC data to upscale ET across the Midwest United States. We\ncompare four tree-based models - Random Forest, CatBoost, XGBoost, LightGBM -\nand a simple feed-forward artificial neural network in combination with\nfeatures engineered using knowledge-guided ML principles. Models were trained\nand tested on EC towers located in the Midwest of the United States using\nk-fold cross validation with k=5 and site-year, biome stratified train-test\nsplit to avoid data leakage. Results show that LightGBM with knowledge-guided\nfeatures outperformed other methods with an R2=0.86, MSE=14.99 W m^-2 and MAE =\n8.82 W m^-2 according to grouped k-fold validation (k=5). Feature importance\nanalysis shows that knowledge-guided features were most important for\npredicting evapotranspiration. Using the best performing model, we provide a\ndata product at 500 m spatial and one-day temporal resolution for gridded ET\nfor the period of 2019-2024. Intercomparison between the new gridded product\nand state-level weather station-based ET estimates show best-in-class\ncorrespondence.",
    "published": "2025-10-13T15:15:40Z",
    "updated": "2025-10-13T15:15:40Z",
    "link": "http://arxiv.org/pdf/2510.11505v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Aleksei Rozanov",
      "Samikshya Subedi",
      "Vasudha Sharma",
      "Bryan C. Runck"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.07841v2",
    "title": "Learning Satellite Attitude Dynamics with Physics-Informed Normalising\n  Flow",
    "summary": "Attitude control is a fundamental aspect of spacecraft operations. Model\nPredictive Control (MPC) has emerged as a powerful strategy for these tasks,\nrelying on accurate models of the system dynamics to optimize control actions\nover a prediction horizon. In scenarios where physics models are incomplete,\ndifficult to derive, or computationally expensive, machine learning offers a\nflexible alternative by learning the system behavior directly from data.\nHowever, purely data-driven models often struggle with generalization and\nstability, especially when applied to inputs outside their training domain. To\naddress these limitations, we investigate the benefits of incorporating\nPhysics-Informed Neural Networks (PINNs) into the learning of spacecraft\nattitude dynamics, comparing their performance with that of purely data-driven\napproaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network\narchitecture with a self-attention mechanism, we trained several models on\nsimulated data generated with the Basilisk simulator. Two training strategies\nwere considered: a purely data-driven baseline and a physics-informed variant\nto improve robustness and stability. Our results demonstrate that the inclusion\nof physics-based information significantly enhances the performance in terms of\nthe mean relative error of the best architectures found by 27.08%. These\nadvantages are particularly evident when the learned models are integrated into\nan MPC framework, where PINN-based models consistently outperform their purely\ndata-driven counterparts in terms of control accuracy and robustness, yielding\nimprovements of up to 42.86% in performance stability error and increased\nrobustness-to-noise.",
    "published": "2025-08-11T10:50:49Z",
    "updated": "2025-10-13T15:14:16Z",
    "link": "http://arxiv.org/pdf/2508.07841v2.pdf",
    "category": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Carlo Cena",
      "Mauro Martini",
      "Marcello Chiaberge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11502v1",
    "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key\n  Errors",
    "summary": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests).",
    "published": "2025-10-13T15:10:38Z",
    "updated": "2025-10-13T15:10:38Z",
    "link": "http://arxiv.org/pdf/2510.11502v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Alexis Ross",
      "Jacob Andreas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11501v1",
    "title": "Context-Aware Model-Based Reinforcement Learning for Autonomous Racing",
    "summary": "Autonomous vehicles have shown promising potential to be a groundbreaking\ntechnology for improving the safety of road users. For these vehicles, as well\nas many other safety-critical robotic technologies, to be deployed in\nreal-world applications, we require algorithms that can generalize well to\nunseen scenarios and data. Model-based reinforcement learning algorithms (MBRL)\nhave demonstrated state-of-the-art performance and data efficiency across a\ndiverse set of domains. However, these algorithms have also shown\nsusceptibility to changes in the environment and its transition dynamics.\n  In this work, we explore the performance and generalization capabilities of\nMBRL algorithms for autonomous driving, specifically in the simulated\nautonomous racing environment, Roboracer (formerly F1Tenth). We frame the\nhead-to-head racing task as a learning problem using contextual Markov decision\nprocesses and parameterize the driving behavior of the adversaries using the\ncontext of the episode, thereby also parameterizing the transition and reward\ndynamics. We benchmark the behavior of MBRL algorithms in this environment and\npropose a novel context-aware extension of the existing literature, cMask. We\ndemonstrate that context-aware MBRL algorithms generalize better to\nout-of-distribution adversary behaviors relative to context-free approaches. We\nalso demonstrate that cMask displays strong generalization capabilities, as\nwell as further performance improvement relative to other context-aware MBRL\napproaches when racing against adversaries with in-distribution behaviors.",
    "published": "2025-10-13T15:09:09Z",
    "updated": "2025-10-13T15:09:09Z",
    "link": "http://arxiv.org/pdf/2510.11501v1.pdf",
    "category": [
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Emran Yasser Moustafa",
      "Ivana Dusparic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11495v1",
    "title": "How Reinforcement Learning After Next-Token Prediction Facilitates\n  Learning",
    "summary": "Recent advances in reasoning domains with neural networks have primarily been\nenabled by a training recipe that optimizes Large Language Models, previously\ntrained to predict the next-token in a sequence, with reinforcement learning\nalgorithms. We introduce a framework to study the success of this paradigm, and\nwe theoretically expose the optimization mechanisms by which reinforcement\nlearning improves over next-token prediction in this setting. We study learning\nfrom mixture distributions of short and long ``chain-of-thought'' sequences\nencoding a single task. In particular, when the task consists of predicting the\nparity of $d$ bits and long sequences are rare, we show how reinforcement\nlearning after next-token prediction enables autoregressive transformers to\ngeneralize, whereas mere next-token prediction requires extreme statistical or\ncomputational resources to do so. We further explain how reinforcement learning\nleverages increased test-time computation, manifested in longer responses, to\nfacilitate this learning process. In a simplified setting, we theoretically\nprove that autoregressive linear models following this training recipe can\nefficiently learn to predict the parity of $d$ bits as long as the proportion\nof long demonstrations in the data mix is not exponentially small in the input\ndimension $d$. Finally, we demonstrate these same phenomena in other settings,\nincluding the post-training of Llama-series models on mixture variations of\ncommon mathematical reasoning benchmarks.",
    "published": "2025-10-13T15:04:00Z",
    "updated": "2025-10-13T15:04:00Z",
    "link": "http://arxiv.org/pdf/2510.11495v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Nikolaos Tsilivis",
      "Eran Malach",
      "Karen Ullrich",
      "Julia Kempe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11491v1",
    "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling",
    "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods.",
    "published": "2025-10-13T14:59:28Z",
    "updated": "2025-10-13T14:59:28Z",
    "link": "http://arxiv.org/pdf/2510.11491v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Murad Dawood",
      "Usama Ahmed Siddiquie",
      "Shahram Khorshidi",
      "Maren Bennewitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11484v1",
    "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning\n  Models on Full-Integer Hardware",
    "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems.",
    "published": "2025-10-13T14:55:34Z",
    "updated": "2025-10-13T14:55:34Z",
    "link": "http://arxiv.org/pdf/2510.11484v1.pdf",
    "category": [
      "cs.LG",
      "cs.AR"
    ],
    "authors": [
      "Lion Mueller",
      "Alberto Garcia-Ortiz",
      "Ardalan Najafi",
      "Adam Fuks",
      "Lennart Bamberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14067v2",
    "title": "Online Selective Generation with Adversarial Bandit Feedback",
    "summary": "Large language generative models increasingly interact with humans, while\ntheir falsified responses raise concerns. To mitigate this hallucination\neffect, selectively abstaining from answering, called selective generation,\nprovides an effective way for generators to control the hallucination when\nuncertain about their answers. However, as selective generators interact under\nadversarial environments and receive partial feedback from users on selected\ngeneration (e.g., thumbs up or down on the selected answer), learning methods\nfor selective generation under such practical setups are crucial but currently\nmissing. To address this limitation, we propose an online learning algorithm\nfor selective generation with partial feedback under an adaptive adversary. In\nparticular, we re-purpose an adversarial bandit algorithm to design an online\nselective generation method with controllable false discovery rates (FDR),\nwhich measures the rate of hallucination. The key building blocks include a\nnovel conversion lemma from regret of any bandit algorithm to the FDR, and the\nexploitation of a unique structure of selective generation to reuse partial\nfeedback, which we call feedback unlocking. We empirically evaluate the\nefficacy of the proposed online selective generation algorithm with partial\nfeedback over diverse learning environments, demonstrating its ability to\ncontrol the FDR, while maintaining reasonable selection efficiency, i.e., the\nratio of non-abstaining answers, compared to baselines.",
    "published": "2025-06-16T23:51:30Z",
    "updated": "2025-10-13T14:48:23Z",
    "link": "http://arxiv.org/pdf/2506.14067v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Minjae Lee",
      "Yoonjae Jung",
      "Sangdon Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11472v1",
    "title": "Differentiable Fast Top-K Selection for Large-Scale Recommendation",
    "summary": "Cascade ranking is a widely adopted paradigm in large-scale information\nretrieval systems for Top-K item selection. However, the Top-K operator is\nnon-differentiable, hindering end-to-end training. Existing methods include\nLearning-to-Rank approaches (e.g., LambdaLoss), which optimize ranking metrics\nlike NDCG and suffer from objective misalignment, and differentiable\nsorting-based methods (e.g., ARF, LCRON), which relax permutation matrices for\ndirect Top-K optimization but introduce gradient conflicts through matrix\naggregation. A promising alternative is to directly construct a differentiable\napproximation of the Top-K selection operator, bypassing the use of soft\npermutation matrices. However, even state-of-the-art differentiable Top-K\noperator (e.g., LapSum) require $O(n \\log n)$ complexity due to their\ndependence on sorting for solving the threshold. Thus, we propose DFTopK, a\nnovel differentiable Top-K operator achieving optimal $O(n)$ time complexity.\nBy relaxing normalization constraints, DFTopK admits a closed-form solution and\navoids sorting. DFTopK also avoids the gradient conflicts inherent in\ndifferentiable sorting-based methods. We evaluate DFTopK on both the public\nbenchmark RecFLow and an industrial system. Experimental results show that\nDFTopK significantly improves training efficiency while achieving superior\nperformance, which enables us to scale up training samples more efficiently. In\nthe online A/B test, DFTopK yielded a +1.77\\% revenue lift with the same\ncomputational budget compared to the baseline. To the best of our knowledge,\nthis work is the first to introduce differentiable Top-K operators into\nrecommendation systems and the first to achieve theoretically optimal\nlinear-time complexity for Top-K selection. We have open-sourced our\nimplementation to facilitate future research in both academia and industry.",
    "published": "2025-10-13T14:40:49Z",
    "updated": "2025-10-13T14:40:49Z",
    "link": "http://arxiv.org/pdf/2510.11472v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yanjie Zhu",
      "Zhen Zhang",
      "Yunli Wang",
      "Zhiqiang Wang",
      "Yu Li",
      "Rufan Zhou",
      "Shiyang Wen",
      "Peng Jiang",
      "Chenhao Lin",
      "Jian Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14161v2",
    "title": "Personalized Bayesian Federated Learning with Wasserstein Barycenter\n  Aggregation",
    "summary": "Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client\ndata and quantifies uncertainty by combining personalization with Bayesian\ninference. However, existing PBFL methods face two limitations: restrictive\nparametric assumptions in client posterior inference and naive parameter\naveraging for server aggregation. To overcome these issues, we propose FedWBA,\na novel PBFL method that enhances both local inference and global aggregation.\nAt the client level, we use particle-based variational inference for\nnonparametric posterior representation. At the server level, we introduce\nparticle-based Wasserstein barycenter aggregation, offering a more\ngeometrically meaningful approach. Theoretically, we provide local and global\nconvergence guarantees for FedWBA. Locally, we prove a KL divergence decrease\nlower bound per iteration for variational inference convergence. Globally, we\nshow that the Wasserstein barycenter converges to the true parameter as the\nclient data size increases. Empirically, experiments show that FedWBA\noutperforms baselines in prediction accuracy, uncertainty calibration, and\nconvergence rate, with ablation studies confirming its robustness.",
    "published": "2025-05-20T10:14:32Z",
    "updated": "2025-10-13T14:09:17Z",
    "link": "http://arxiv.org/pdf/2505.14161v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ting Wei",
      "Biao Mei",
      "Junliang Lyu",
      "Renquan Zhang",
      "Feng Zhou",
      "Yifan Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18530v2",
    "title": "Re-uploading quantum data: A universal function approximator for quantum\n  inputs",
    "summary": "Quantum data re-uploading has proved powerful for classical inputs, where\nrepeatedly encoding features into a small circuit yields universal function\napproximation. Extending this idea to quantum inputs remains underexplored, as\nthe information contained in a quantum state is not directly accessible in\nclassical form. We propose and analyze a quantum data re-uploading architecture\nin which a qubit interacts sequentially with fresh copies of an arbitrary input\nstate. The circuit can approximate any bounded continuous function using only\none ancilla qubit and single-qubit measurements. By alternating entangling\nunitaries with mid-circuit resets of the input register, the architecture\nrealizes a discrete cascade of completely positive and trace-preserving maps,\nanalogous to collision models in open quantum system dynamics. Our framework\nprovides a qubit-efficient and expressive approach to designing quantum machine\nlearning models that operate directly on quantum data.",
    "published": "2025-09-23T01:50:37Z",
    "updated": "2025-10-13T14:00:58Z",
    "link": "http://arxiv.org/pdf/2509.18530v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Hyunho Cha",
      "Daniel K. Park",
      "Jungwoo Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11418v1",
    "title": "Forward-Forward Autoencoder Architectures for Energy-Efficient Wireless\n  Communications",
    "summary": "The application of deep learning to the area of communications systems has\nbeen a growing field of interest in recent years. Forward-forward (FF) learning\nis an efficient alternative to the backpropagation (BP) algorithm, which is the\ntypically used training procedure for neural networks. Among its several\nadvantages, FF learning does not require the communication channel to be\ndifferentiable and does not rely on the global availability of partial\nderivatives, allowing for an energy-efficient implementation. In this work, we\ndesign end-to-end learned autoencoders using the FF algorithm and numerically\nevaluate their performance for the additive white Gaussian noise and Rayleigh\nblock fading channels. We demonstrate their competitiveness with BP-trained\nsystems in the case of joint coding and modulation, and in a scenario where a\nfixed, non-differentiable modulation stage is applied. Moreover, we provide\nfurther insights into the design principles of the FF network, its training\nconvergence behavior, and significant memory and processing time savings\ncompared to BP-based approaches.",
    "published": "2025-10-13T13:54:50Z",
    "updated": "2025-10-13T13:54:50Z",
    "link": "http://arxiv.org/pdf/2510.11418v1.pdf",
    "category": [
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "authors": [
      "Daniel Seifert",
      "Onur Günlü",
      "Rafael F. Schaefer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03134v2",
    "title": "Enhancing XAI Narratives through Multi-Narrative Refinement and\n  Knowledge Distillation",
    "summary": "Explainable Artificial Intelligence has become a crucial area of research,\naiming to demystify the decision-making processes of deep learning models.\nAmong various explainability techniques, counterfactual explanations have been\nproven particularly promising, as they offer insights into model behavior by\nhighlighting minimal changes that would alter a prediction. Despite their\npotential, these explanations are often complex and technical, making them\ndifficult for non-experts to interpret. To address this challenge, we propose a\nnovel pipeline that leverages Language Models, large and small, to compose\nnarratives for counterfactual explanations. We employ knowledge distillation\ntechniques along with a refining mechanism to enable Small Language Models to\nperform comparably to their larger counterparts while maintaining robust\nreasoning abilities. In addition, we introduce a simple but effective\nevaluation method to assess natural language narratives, designed to verify\nwhether the models' responses are in line with the factual, counterfactual\nground truth. As a result, our proposed pipeline enhances both the reasoning\ncapabilities and practical performance of student models, making them more\nsuitable for real-world use cases.",
    "published": "2025-10-03T16:04:09Z",
    "updated": "2025-10-13T13:50:02Z",
    "link": "http://arxiv.org/pdf/2510.03134v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Flavio Giorgi",
      "Matteo Silvestri",
      "Cesare Campagnano",
      "Fabrizio Silvestri",
      "Gabriele Tolomei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11409v1",
    "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic\n  Literature Reviews",
    "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows.",
    "published": "2025-10-13T13:48:29Z",
    "updated": "2025-10-13T13:48:29Z",
    "link": "http://arxiv.org/pdf/2510.11409v1.pdf",
    "category": [
      "cs.LG",
      "cs.DL",
      "cs.HC"
    ],
    "authors": [
      "Lucas Joos",
      "Daniel A. Keim",
      "Maximilian T. Fischer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11400v1",
    "title": "FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid\n  Tensor Management",
    "summary": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, one fundamental and prevailing challenge that hinders the\ndeployment of FL on mobile devices is the memory limitation. This paper\nproposes \\textit{FedHybrid}, a novel framework that effectively reduces the\nmemory footprint during the training process while guaranteeing the model\naccuracy and the overall training progress. Specifically, \\textit{FedHybrid}\nfirst selects the participating devices for each training round by jointly\nevaluating their memory budget, computing capability, and data diversity. After\nthat, it judiciously analyzes the computational graph and generates an\nexecution plan for each selected client in order to meet the corresponding\nmemory budget while minimizing the training delay through employing a hybrid of\nrecomputation and compression techniques according to the characteristic of\neach tensor. During the local training process, \\textit{FedHybrid} carries out\nthe execution plan with a well-designed activation compression technique to\neffectively achieve memory reduction with minimum accuracy loss. We conduct\nextensive experiments to evaluate \\textit{FedHybrid} on both simulation and\noff-the-shelf mobile devices. The experiment results demonstrate that\n\\textit{FedHybrid} achieves up to a 39.1\\% increase in model accuracy and a\n15.5$\\times$ reduction in wall clock time under various memory budgets compared\nwith the baselines.",
    "published": "2025-10-13T13:43:55Z",
    "updated": "2025-10-13T13:43:55Z",
    "link": "http://arxiv.org/pdf/2510.11400v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kahou Tam",
      "Chunlin Tian",
      "Li Li",
      "Haikai Zhao",
      "ChengZhong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07624v3",
    "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum\n  Likelihood Estimation",
    "summary": "Generative models form the backbone of modern machine learning, underpinning\nstate-of-the-art systems in text, vision, and multimodal applications. While\nMaximum Likelihood Estimation has traditionally served as the dominant training\nparadigm, recent work have highlighted its limitations, particularly in\ngeneralization and susceptibility to catastrophic forgetting compared to\nReinforcement Learning techniques, such as Policy Gradient methods. However,\nthese approaches depend on explicit reward signals, which are often unavailable\nin practice, leaving open the fundamental problem of how to align generative\nmodels when only high-quality datasets are accessible. In this work, we address\nthis challenge via a Bilevel Optimization framework, where the reward function\nis treated as the optimization variable of an outer-level problem, while a\npolicy gradient objective defines the inner-level. We then conduct a\ntheoretical analysis of this optimization problem in a tractable setting and\nextract insights that, as we demonstrate, generalize to applications such as\ntabular classification and model-based reinforcement learning. We release the\ncode at https://github.com/abenechehab/nll_to_po .",
    "published": "2025-10-08T23:45:37Z",
    "updated": "2025-10-13T13:24:41Z",
    "link": "http://arxiv.org/pdf/2510.07624v3.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Abdelhakim Benechehab",
      "Gabriel Singer",
      "Corentin Léger",
      "Youssef Attia El Hili",
      "Giuseppe Paolo",
      "Albert Thomas",
      "Maurizio Filippone",
      "Balázs Kégl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00761v3",
    "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in\n  LLM Unlearning",
    "summary": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality.",
    "published": "2025-10-01T10:50:14Z",
    "updated": "2025-10-13T12:38:53Z",
    "link": "http://arxiv.org/pdf/2510.00761v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yicheng Lang",
      "Yihua Zhang",
      "Chongyu Fan",
      "Changsheng Wang",
      "Jinghan Jia",
      "Sijia Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11335v1",
    "title": "DiffStyleTS: Diffusion Model for Style Transfer in Time Series",
    "summary": "Style transfer combines the content of one signal with the style of another.\nIt supports applications such as data augmentation and scenario simulation,\nhelping machine learning models generalize in data-scarce domains. While well\ndeveloped in vision and language, style transfer methods for time series data\nremain limited. We introduce DiffTSST, a diffusion-based framework that\ndisentangles a time series into content and style representations via\nconvolutional encoders and recombines them through a self-supervised\nattention-based diffusion process. At inference, encoders extract content and\nstyle from two distinct series, enabling conditional generation of novel\nsamples to achieve style transfer. We demonstrate both qualitatively and\nquantitatively that DiffTSST achieves effective style transfer. We further\nvalidate its real-world utility by showing that data augmentation with DiffTSST\nimproves anomaly detection in data-scarce regimes.",
    "published": "2025-10-13T12:30:10Z",
    "updated": "2025-10-13T12:30:10Z",
    "link": "http://arxiv.org/pdf/2510.11335v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mayank Nagda",
      "Phil Ostheimer",
      "Justus Arweiler",
      "Indra Jungjohann",
      "Jennifer Werner",
      "Dennis Wagner",
      "Aparna Muraleedharan",
      "Pouya Jafari",
      "Jochen Schmid",
      "Fabian Jirasek",
      "Jakob Burger",
      "Michael Bortz",
      "Hans Hasse",
      "Stephan Mandt",
      "Marius Kloft",
      "Sophie Fellenz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08288v2",
    "title": "On Experiments",
    "summary": "The scientific process is a means to turn the results of experiments into\nknowledge about the world in which we live. Much research effort has been\ndirected toward automating this process. To do this, one needs to formulate the\nscientific process in a precise mathematical language. This paper outlines one\nsuch language. What is presented here is hardly new. The material is based on\ngreat thinkers from times past well as more modern contributions. The novel\ncontributions of this paper are: A new general data processing inequality, a\nbias variance decomposition for canonical losses, streamlined proofs of the\nBlackwell-Sherman-Stein and Randomization theorems. means of calculating\ndeficiency through linear programming.",
    "published": "2025-08-05T00:54:08Z",
    "updated": "2025-10-13T11:48:38Z",
    "link": "http://arxiv.org/pdf/2508.08288v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST",
      "stat.TH"
    ],
    "authors": [
      "Brendan van Rooyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09174v2",
    "title": "Robustness and Regularization in Hierarchical Re-Basin",
    "summary": "This paper takes a closer look at Git Re-Basin, an interesting new approach\nto merge trained models. We propose a hierarchical model merging scheme that\nsignificantly outperforms the standard MergeMany algorithm. With our new\nalgorithm, we find that Re-Basin induces adversarial and perturbation\nrobustness into the merged models, with the effect becoming stronger the more\nmodels participate in the hierarchical merging scheme. However, in our\nexperiments Re-Basin induces a much bigger performance drop than reported by\nthe original authors.",
    "published": "2025-10-10T09:17:10Z",
    "updated": "2025-10-13T11:42:48Z",
    "link": "http://arxiv.org/pdf/2510.09174v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Benedikt Franke",
      "Florian Heinrich",
      "Markus Lange",
      "Arne Raulf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11291v1",
    "title": "Network-Optimised Spiking Neural Network (NOS) Scheduling for 6G O-RAN:\n  Spectral Margin and Delay-Tail Control",
    "summary": "This work presents a Network-Optimised Spiking (NOS) delay-aware scheduler\nfor 6G radio access. The scheme couples a bounded two-state kernel to a\nclique-feasible proportional-fair (PF) grant head: the excitability state acts\nas a finite-buffer proxy, the recovery state suppresses repeated grants, and\nneighbour pressure is injected along the interference graph via delayed spikes.\nA small-signal analysis yields a delay-dependent threshold $k_\\star(\\Delta)$\nand a spectral margin $\\delta = k_\\star(\\Delta) - gH\\rho(W)$ that compress\ntopology, controller gain, and delay into a single design parameter. Under\nlight assumptions on arrivals, we prove geometric ergodicity for $\\delta>0$ and\nderive sub-Gaussian backlog and delay tail bounds with exponents proportional\nto $\\delta$. A numerical study, aligned with the analysis and a DU compute\nbudget, compares NOS with PF and delayed backpressure (BP) across interference\ntopologies over a $5$--$20$\\,ms delay sweep. With a single gain fixed at the\nworst spectral radius, NOS sustains higher utilisation and a smaller\n99.9th-percentile delay while remaining clique-feasible on integer PRBs.",
    "published": "2025-10-13T11:28:28Z",
    "updated": "2025-10-13T11:28:28Z",
    "link": "http://arxiv.org/pdf/2510.11291v1.pdf",
    "category": [
      "cs.NI",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "68M20, 60K25, 93C23, 93D05, 90B18, 68M10, 68T07",
      "C.2.1; C.2.3; C.4; I.2.6; I.6.5; G.3"
    ],
    "authors": [
      "Muhammad Bilal",
      "Xiaolong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.06390v2",
    "title": "Edge Delayed Deep Deterministic Policy Gradient: efficient continuous\n  control for edge scenarios",
    "summary": "Deep Reinforcement Learning is gaining increasing attention thanks to its\ncapability to learn complex policies in high-dimensional settings. Recent\nadvancements utilize a dual-network architecture to learn optimal policies\nthrough the Q-learning algorithm. However, this approach has notable drawbacks,\nsuch as an overestimation bias that can disrupt the learning process and\ndegrade the performance of the resulting policy. To address this, novel\nalgorithms have been developed that mitigate overestimation bias by employing\nmultiple Q-functions. Edge scenarios, which prioritize privacy, have recently\ngained prominence. In these settings, limited computational resources pose a\nsignificant challenge for complex Machine Learning approaches, making the\nefficiency of algorithms crucial for their performance. In this work, we\nintroduce a novel Reinforcement Learning algorithm tailored for edge scenarios,\ncalled Edge Delayed Deep Deterministic Policy Gradient (EdgeD3). EdgeD3\nenhances the Deep Deterministic Policy Gradient (DDPG) algorithm, achieving\nsignificantly improved performance with $25\\%$ less Graphics Process Unit (GPU)\ntime while maintaining the same memory usage. Additionally, EdgeD3 consistently\nmatches or surpasses the performance of state-of-the-art methods across various\nbenchmarks, all while using $30\\%$ fewer computational resources and requiring\n$30\\%$ less memory.",
    "published": "2024-12-09T11:17:04Z",
    "updated": "2025-10-13T11:26:46Z",
    "link": "http://arxiv.org/pdf/2412.06390v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Alberto Sinigaglia",
      "Niccolò Turcato",
      "Ruggero Carli",
      "Gian Antonio Susto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08078v2",
    "title": "Detecting and Mitigating Insertion Hallucination in Video-to-Audio\n  Generation",
    "summary": "Video-to-Audio generation has made remarkable strides in automatically\nsynthesizing sound for video. However, existing evaluation metrics, which focus\non semantic and temporal alignment, overlook a critical failure mode: models\noften generate acoustic events, particularly speech and music, that have no\ncorresponding visual source. We term this phenomenon Insertion Hallucination\nand identify it as a systemic risk driven by dataset biases, such as the\nprevalence of off-screen sounds, that remains completely undetected by current\nmetrics. To address this challenge, we first develop a systematic evaluation\nframework that employs a majority-voting ensemble of multiple audio event\ndetectors. We also introduce two novel metrics to quantify the prevalence and\nseverity of this issue: IH@vid (the fraction of videos with hallucinations) and\nIH@dur (the fraction of hallucinated duration). Building on this, we propose\nPosterior Feature Correction, a novel training-free inference-time method that\nmitigates IH. PFC operates in a two-pass process: it first generates an initial\naudio output to detect hallucinated segments, and then regenerates the audio\nafter masking the corresponding video features at those timestamps. Experiments\non several mainstream V2A benchmarks first reveal that state-of-the-art models\nsuffer from severe IH. In contrast, our PFC method reduces both the prevalence\nand duration of hallucinations by over 50\\% on average, without degrading, and\nin some cases even improving, conventional metrics for audio quality and\ntemporal synchronization. Our work is the first to formally define,\nsystematically measure, and effectively mitigate Insertion Hallucination,\npaving the way for more reliable and faithful V2A models.",
    "published": "2025-10-09T11:08:07Z",
    "updated": "2025-10-13T11:22:24Z",
    "link": "http://arxiv.org/pdf/2510.08078v2.pdf",
    "category": [
      "cs.SD",
      "cs.LG"
    ],
    "authors": [
      "Liyang Chen",
      "Hongkai Chen",
      "Yujun Cai",
      "Sifan Li",
      "Qingwen Ye",
      "Yiwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11283v1",
    "title": "Gym-TORAX: Open-source software for integrating RL with plasma control\n  simulators",
    "summary": "This paper presents Gym-TORAX, a Python package enabling the implementation\nof Reinforcement Learning (RL) environments for simulating plasma dynamics and\ncontrol in tokamaks. Users define succinctly a set of control actions and\nobservations, and a control objective from which Gym-TORAX creates a Gymnasium\nenvironment that wraps TORAX for simulating the plasma dynamics. The objective\nis formulated through rewards depending on the simulated state of the plasma\nand control action to optimize specific characteristics of the plasma, such as\nperformance and stability. The resulting environment instance is then\ncompatible with a wide range of RL algorithms and libraries and will facilitate\nRL research in plasma control. In its current version, one environment is\nreadily available, based on a ramp-up scenario of the International\nThermonuclear Experimental Reactor (ITER).",
    "published": "2025-10-13T11:16:25Z",
    "updated": "2025-10-13T11:16:25Z",
    "link": "http://arxiv.org/pdf/2510.11283v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Antoine Mouchamps",
      "Arthur Malherbe",
      "Adrien Bolland",
      "Damien Ernst"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11282v1",
    "title": "Vision-LLMs for Spatiotemporal Traffic Forecasting",
    "summary": "Accurate spatiotemporal traffic forecasting is a critical prerequisite for\nproactive resource management in dense urban mobile networks. While Large\nLanguage Models (LLMs) have shown promise in time series analysis, they\ninherently struggle to model the complex spatial dependencies of grid-based\ntraffic data. Effectively extending LLMs to this domain is challenging, as\nrepresenting the vast amount of information from dense geographical grids can\nbe inefficient and overwhelm the model's context. To address these challenges,\nwe propose ST-Vision-LLM, a novel framework that reframes spatiotemporal\nforecasting as a vision-language fusion problem. Our approach leverages a\nVision-LLM visual encoder to process historical global traffic matrices as\nimage sequences, providing the model with a comprehensive global view to inform\ncell-level predictions. To overcome the inefficiency of LLMs in handling\nnumerical data, we introduce an efficient encoding scheme that represents\nfloating-point values as single tokens via a specialized vocabulary, coupled\nwith a two-stage numerical alignment fine-tuning process. The model is first\ntrained with Supervised Fine-Tuning (SFT) and then further optimized for\npredictive accuracy using Group Relative Policy Optimization (GRPO), a\nmemory-efficient reinforcement learning method. Evaluations on real-world\nmobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing\nmethods by 15.6% in long-term prediction accuracy and exceeds the second-best\nbaseline by over 30.04% in cross-domain few-shot scenarios. Our extensive\nexperiments validate the model's strong generalization capabilities across\nvarious data-scarce environments.",
    "published": "2025-10-13T11:15:56Z",
    "updated": "2025-10-13T11:15:56Z",
    "link": "http://arxiv.org/pdf/2510.11282v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ning Yang",
      "Hengyu Zhong",
      "Haijun Zhang",
      "Randall Berry"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.11740v2",
    "title": "Simple and Effective Specialized Representations for Fair Classifiers",
    "summary": "Fair classification is a critical challenge that has gained increasing\nimportance due to international regulations and its growing use in high-stakes\ndecision-making settings. Existing methods often rely on adversarial learning\nor distribution matching across sensitive groups; however, adversarial learning\ncan be unstable, and distribution matching can be computationally intensive. To\naddress these limitations, we propose a novel approach based on the\ncharacteristic function distance. Our method ensures that the learned\nrepresentation contains minimal sensitive information while maintaining high\neffectiveness for downstream tasks. By utilizing characteristic functions, we\nachieve a more stable and efficient solution compared to traditional methods.\nAdditionally, we introduce a simple relaxation of the objective function that\nguarantees fairness in common classification models with no performance\ndegradation. Experimental results on benchmark datasets demonstrate that our\napproach consistently matches or achieves better fairness and predictive\naccuracy than existing methods. Moreover, our method maintains robustness and\ncomputational efficiency, making it a practical solution for real-world\napplications.",
    "published": "2025-05-16T22:59:46Z",
    "updated": "2025-10-13T11:14:09Z",
    "link": "http://arxiv.org/pdf/2505.11740v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Alberto Sinigaglia",
      "Davide Sartor",
      "Marina Ceccon",
      "Gian Antonio Susto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11275v1",
    "title": "SeFEF: A Seizure Forecasting Evaluation Framework",
    "summary": "The lack of standardization in seizure forecasting slows progress in the\nfield and limits the clinical translation of forecasting models. In this work,\nwe introduce a Python-based framework aimed at streamlining the development,\nassessment, and documentation of individualized seizure forecasting algorithms.\n  The framework automates data labeling, cross-validation splitting, forecast\npost-processing, performance evaluation, and reporting. It supports various\nforecasting horizons and includes a model card that documents implementation\ndetails, training and evaluation settings, and performance metrics. Three\ndifferent models were implemented as a proof-of-concept. The models leveraged\nfeatures extracted from time series data and seizure periodicity. Model\nperformance was assessed using time series cross-validation and key\ndeterministic and probabilistic metrics.\n  Implementation of the three models was successful, demonstrating the\nflexibility of the framework. The results also emphasize the importance of\ncareful model interpretation due to variations in probability scaling,\ncalibration, and subject-specific differences. Although formal usability\nmetrics were not recorded, empirical observations suggest reduced development\ntime and methodological consistency, minimizing unintentional variations that\ncould affect the comparability of different approaches.\n  As a proof-of-concept, this validation is inherently limited, relying on a\nsingle-user experiment without statistical analyses or replication across\nindependent datasets. At this stage, our objective is to make the framework\npublicly available to foster community engagement, facilitate experimentation,\nand gather feedback. In the long term, we aim to contribute to the\nestablishment of a consensus on a standardized methodology for the development\nand validation of seizure forecasting algorithms in people with epilepsy.",
    "published": "2025-10-13T11:10:27Z",
    "updated": "2025-10-13T11:10:27Z",
    "link": "http://arxiv.org/pdf/2510.11275v1.pdf",
    "category": [
      "q-bio.QM",
      "cs.LG"
    ],
    "authors": [
      "Ana Sofia Carmo",
      "Lourenço Abrunhosa Rodrigues",
      "Ana Rita Peralta",
      "Ana Fred",
      "Carla Bentes",
      "Hugo Plácido da Silva"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11274v1",
    "title": "FedLoRA-Optimizer: Federated LoRA Fine-Tuning with Global and Local\n  Optimization in Heterogeneous Data Scenarios",
    "summary": "Federated efficient fine-tuning has emerged as an approach that leverages\ndistributed data and computational resources across nodes to address the\nchallenges of large-scale fine-tuning and privacy preservation. The Low-Rank\nAdaptation (LoRA) enables efficient fine-tuning of large-scale pre-trained\nmodels by introducing trainable low-rank matrices into weight updates.However,\nin heterogeneous data scenarios, client drift weakens the generalization of the\nglobal model, and local models often fail to meet the personalized needs of\nindividual clients.Moreover, existing federated LoRA efficient fine-tuning\ntechniques overlook fine-grained analysis of the tuning matrices. To address\nthis, we conducted preliminary experiments and found that different LoRA\nmatrices exhibit different sensitivity to changes in the direction and\nmagnitude of their vectors.We thus propose a fine-grained federated LoRA tuning\nmethod. By fine-tuning the more sensitive directional vectors in the A matrix,\nwhich encode shared knowledge, our method learns shared features more\neffectively across clients and enhances global generalization. Simultaneously,\nby fine-tuning the more sensitive magnitude vectors in the B matrix, which\nencode personalized knowledge, our method better captures personalized\nknowledge, enabling detailed adaptation to local data. The method uses a\npipeline combining global and local optimizers. Global optimization further\nimproves local models, achieving collaborative optimization between global and\nlocal levels. This improves both the generalization ability of the global model\nand the personalized adaptation of local models under heterogeneous data\nscenarios. Experiments on Databricks-Dolly-15k and Natural Instructions with\nLLaMA2-7B and Deepseek-7B confirm that our method improves global performance\nby 0.39% and local performance by 0.59%.",
    "published": "2025-10-13T11:06:36Z",
    "updated": "2025-10-13T11:06:36Z",
    "link": "http://arxiv.org/pdf/2510.11274v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jianzhe Zhao",
      "Hailin Zhu",
      "Yu Zhang",
      "Ziqi Chen",
      "Guibing Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11258v1",
    "title": "DemoHLM: From One Demonstration to Generalizable Humanoid\n  Loco-Manipulation",
    "summary": "Loco-manipulation is a fundamental challenge for humanoid robots to achieve\nversatile interactions in human environments. Although recent studies have made\nsignificant progress in humanoid whole-body control, loco-manipulation remains\nunderexplored and often relies on hard-coded task definitions or costly\nreal-world data collection, which limits autonomy and generalization. We\npresent DemoHLM, a framework for humanoid loco-manipulation that enables\ngeneralizable loco-manipulation on a real humanoid robot from a single\ndemonstration in simulation. DemoHLM adopts a hierarchy that integrates a\nlow-level universal whole-body controller with high-level manipulation policies\nfor multiple tasks. The whole-body controller maps whole-body motion commands\nto joint torques and provides omnidirectional mobility for the humanoid robot.\nThe manipulation policies, learned in simulation via our data generation and\nimitation learning pipeline, command the whole-body controller with closed-loop\nvisual feedback to execute challenging loco-manipulation tasks. Experiments\nshow a positive correlation between the amount of synthetic data and policy\nperformance, underscoring the effectiveness of our data generation pipeline and\nthe data efficiency of our approach. Real-world experiments on a Unitree G1\nrobot equipped with an RGB-D camera validate the sim-to-real transferability of\nDemoHLM, demonstrating robust performance under spatial variations across ten\nloco-manipulation tasks.",
    "published": "2025-10-13T10:49:40Z",
    "updated": "2025-10-13T10:49:40Z",
    "link": "http://arxiv.org/pdf/2510.11258v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Yuhui Fu",
      "Feiyang Xie",
      "Chaoyi Xu",
      "Jing Xiong",
      "Haoqi Yuan",
      "Zongqing Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11257v1",
    "title": "MIEO: encoding clinical data to enhance cardiovascular event prediction",
    "summary": "As clinical data are becoming increasingly available, machine learning\nmethods have been employed to extract knowledge from them and predict clinical\nevents. While promising, approaches suffer from at least two main issues: low\navailability of labelled data and data heterogeneity leading to missing values.\nThis work proposes the use of self-supervised auto-encoders to efficiently\naddress these challenges. We apply our methodology to a clinical dataset from\npatients with ischaemic heart disease. Patient data is embedded in a latent\nspace, built using unlabelled data, which is then used to train a neural\nnetwork classifier to predict cardiovascular death. Results show improved\nbalanced accuracy compared to applying the classifier directly to the raw data,\ndemonstrating that this solution is promising, especially in conditions where\navailability of unlabelled data could increase.",
    "published": "2025-10-13T10:47:49Z",
    "updated": "2025-10-13T10:47:49Z",
    "link": "http://arxiv.org/pdf/2510.11257v1.pdf",
    "category": [
      "cs.LG",
      "q-bio.QM"
    ],
    "authors": [
      "Davide Borghini",
      "Davide Marchi",
      "Angelo Nardone",
      "Giordano Scerra",
      "Silvia Giulia Galfrè",
      "Alessandro Pingitore",
      "Giuseppe Prencipe",
      "Corrado Priami",
      "Alina Sîrbu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11251v1",
    "title": "Large Language Models Are Effective Code Watermarkers",
    "summary": "The widespread use of large language models (LLMs) and open-source code has\nraised ethical and security concerns regarding the distribution and attribution\nof source code, including unauthorized redistribution, license violations, and\nmisuse of code for malicious purposes. Watermarking has emerged as a promising\nsolution for source attribution, but existing techniques rely heavily on\nhand-crafted transformation rules, abstract syntax tree (AST) manipulation, or\ntask-specific training, limiting their scalability and generality across\nlanguages. Moreover, their robustness against attacks remains limited. To\naddress these limitations, we propose CodeMark-LLM, an LLM-driven watermarking\nframework that embeds watermark into source code without compromising its\nsemantics or readability. CodeMark-LLM consists of two core components: (i)\nSemantically Consistent Embedding module that applies functionality-preserving\ntransformations to encode watermark bits, and (ii) Differential Comparison\nExtraction module that identifies the applied transformations by comparing the\noriginal and watermarked code. Leveraging the cross-lingual generalization\nability of LLM, CodeMark-LLM avoids language-specific engineering and training\npipelines. Extensive experiments across diverse programming languages and\nattack scenarios demonstrate its robustness, effectiveness, and scalability.",
    "published": "2025-10-13T10:40:24Z",
    "updated": "2025-10-13T10:40:24Z",
    "link": "http://arxiv.org/pdf/2510.11251v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Rui Xu",
      "Jiawei Chen",
      "Zhaoxia Yin",
      "Cong Kong",
      "Xinpeng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11250v1",
    "title": "FUSE: Fast Semi-Supervised Node Embedding Learning via Structural and\n  Label-Aware Optimization",
    "summary": "Graph-based learning is a cornerstone for analyzing structured data, with\nnode classification as a central task. However, in many real-world graphs,\nnodes lack informative feature vectors, leaving only neighborhood connectivity\nand class labels as available signals. In such cases, effective classification\nhinges on learning node embeddings that capture structural roles and\ntopological context. We introduce a fast semi-supervised embedding framework\nthat jointly optimizes three complementary objectives: (i) unsupervised\nstructure preservation via scalable modularity approximation, (ii) supervised\nregularization to minimize intra-class variance among labeled nodes, and (iii)\nsemi-supervised propagation that refines unlabeled nodes through\nrandom-walk-based label spreading with attention-weighted similarity. These\ncomponents are unified into a single iterative optimization scheme, yielding\nhigh-quality node embeddings. On standard benchmarks, our method consistently\nachieves classification accuracy at par with or superior to state-of-the-art\napproaches, while requiring significantly less computational cost.",
    "published": "2025-10-13T10:39:58Z",
    "updated": "2025-10-13T10:39:58Z",
    "link": "http://arxiv.org/pdf/2510.11250v1.pdf",
    "category": [
      "cs.LG",
      "I.5.2"
    ],
    "authors": [
      "Sujan Chakraborty",
      "Rahul Bordoloi",
      "Anindya Sengupta",
      "Olaf Wolkenhauer",
      "Saptarshi Bej"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11245v1",
    "title": "Learning the Structure of Connection Graphs",
    "summary": "Connection graphs (CGs) extend traditional graph models by coupling network\ntopology with orthogonal transformations, enabling the representation of global\ngeometric consistency. They play a key role in applications such as\nsynchronization, Riemannian signal processing, and neural sheaf diffusion. In\nthis work, we address the inverse problem of learning CGs directly from\nobserved signals. We propose a principled framework based on maximum\npseudo-likelihood under a consistency assumption, which enforces spectral\nproperties linking the connection Laplacian to the underlying combinatorial\nLaplacian. Based on this formulation, we introduce the Structured Connection\nGraph Learning (SCGL) algorithm, a block-optimization procedure over Riemannian\nmanifolds that jointly infers network topology, edge weights, and geometric\nstructure. Our experiments show that SCGL consistently outperforms existing\nbaselines in both topological recovery and geometric fidelity, while remaining\ncomputationally efficient.",
    "published": "2025-10-13T10:33:31Z",
    "updated": "2025-10-13T10:33:31Z",
    "link": "http://arxiv.org/pdf/2510.11245v1.pdf",
    "category": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Leonardo Di Nino",
      "Gabriele D'Acunto",
      "Sergio Barbarossa",
      "Paolo Di Lorenzo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11242v1",
    "title": "Analyzing Data Quality and Decay in Mega-Constellations: A\n  Physics-Informed Machine Learning Approach",
    "summary": "In the era of mega-constellations, the need for accurate and publicly\navailable information has become fundamental for satellite operators to\nguarantee the safety of spacecrafts and the Low Earth Orbit (LEO) space\nenvironment. This study critically evaluates the accuracy and reliability of\npublicly available ephemeris data for a LEO mega-constellation - Starlink. The\ngoal of this work is twofold: (i) compare and analyze the quality of the data\nagainst high-precision numerical propagation. (ii) Leverage Physics-Informed\nMachine Learning to extract relevant satellite quantities, such as\nnon-conservative forces, during the decay process. By analyzing two months of\nreal orbital data for approximately 1500 Starlink satellites, we identify\ndiscrepancies between high precision numerical algorithms and the published\nephemerides, recognizing the use of simplified dynamics at fixed thresholds,\nplanned maneuvers, and limitations in uncertainty propagations. Furthermore, we\ncompare data obtained from multiple sources to track and analyze deorbiting\nsatellites over the same period. Empirically, we extract the acceleration\nprofile of satellites during deorbiting and provide insights relating to the\neffects of non-conservative forces during reentry. For non-deorbiting\nsatellites, the position Root Mean Square Error (RMSE) was approximately 300 m,\nwhile for deorbiting satellites it increased to about 600 m. Through this\nin-depth analysis, we highlight potential limitations in publicly available\ndata for accurate and robust Space Situational Awareness (SSA), and\nimportantly, we propose a data-driven model of satellite decay in\nmega-constellations.",
    "published": "2025-10-13T10:28:23Z",
    "updated": "2025-10-13T10:28:23Z",
    "link": "http://arxiv.org/pdf/2510.11242v1.pdf",
    "category": [
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.LG"
    ],
    "authors": [
      "Katarina Dyreby",
      "Francisco Caldas",
      "Cláudia Soares"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.12010v3",
    "title": "Incentivize Contribution and Learn Parameters Too: Federated Learning\n  with Strategic Data Owners",
    "summary": "Classical federated learning (FL) assumes that the clients have a limited\namount of noisy data with which they voluntarily participate and contribute\ntowards learning a global, more accurate model in a principled manner. The\nlearning happens in a distributed fashion without sharing the data with the\ncenter. However, these methods do not consider the incentive of an agent for\nparticipating and contributing to the process, given that data collection and\nrunning a distributed algorithm is costly for the clients. The question of\nrationality of contribution has been asked recently in the literature and some\nresults exist that consider this problem. This paper addresses the question of\nsimultaneous parameter learning and incentivizing contribution in a truthful\nmanner, which distinguishes it from the extant literature. Our first mechanism\nincentivizes each client to contribute to the FL process at a Nash equilibrium\nand simultaneously learn the model parameters. We also ensure that agents are\nincentivized to truthfully reveal information in the intermediate stages of the\nalgorithm. However, this equilibrium outcome can be away from the optimal,\nwhere clients contribute with their full data and the algorithm learns the\noptimal parameters. We propose a second mechanism that enables the full data\ncontribution along with optimal parameter learning. Large scale experiments\nwith real (federated) datasets (CIFAR-10, FEMNIST, and Twitter) show that these\nalgorithms converge quite fast in practice, yield good welfare guarantees and\nbetter model performance for all agents.",
    "published": "2025-05-17T14:04:20Z",
    "updated": "2025-10-13T10:24:17Z",
    "link": "http://arxiv.org/pdf/2505.12010v3.pdf",
    "category": [
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Drashthi Doshi",
      "Aditya Vema Reddy Kesari",
      "Avishek Ghosh",
      "Swaprava Nath",
      "Suhas S Kowshik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11234v1",
    "title": "Neural Weight Compression for Language Models",
    "summary": "The efficient storage and transmission of language model weights is becoming\nincreasingly important, as their scale and adoption continue to grow. However,\nas our understanding of this new data modality is limited, designing a good\ncompression algorithm for language model weights heavily relies on manual,\ntrial-and-error approaches. In this paper, we propose a learned compression\nframework that trains neural codecs directly from pretrained language model\nweights. Unlike conventional data (e.g., images), language model weights pose\nunique challenges: the sizes and shapes of weight tensors vary significantly,\nand the reconstruction quality must be judged by downstream model predictions\nrather than na\\\"ive MSE loss. To address this, we introduce Neural Weight\nCompression (NWC), a novel autoencoder-based neural codec tailored to model\nweight compression. The proposed method inherits the advantages of\nautoencoder-based codecs while incorporating three technical components: (1)\ncolumn-wise tensor chunking and normalization; (2) an importance-aware training\nloss; (3) an inference-time error compensation mechanism guided by model\noutputs. Experiments on open-weight language models show that NWC achieves\ncompetitive or state-of-the-art accuracy-compression tradeoffs, with\nparticularly strong results at 4-6 bit precisions where accuracy remains nearly\non par with FP16 models.",
    "published": "2025-10-13T10:16:20Z",
    "updated": "2025-10-13T10:16:20Z",
    "link": "http://arxiv.org/pdf/2510.11234v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jegwang Ryu",
      "Minkyu Kim",
      "Seungjun Shin",
      "Hee Min Choi",
      "Dokwan Oh",
      "Jaeho Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11227v1",
    "title": "Enforcing convex constraints in Graph Neural Networks",
    "summary": "Many machine learning applications require outputs that satisfy complex,\ndynamic constraints. This task is particularly challenging in Graph Neural\nNetwork models due to the variable output sizes of graph-structured data. In\nthis paper, we introduce ProjNet, a Graph Neural Network framework which\nsatisfies input-dependant constraints. ProjNet combines a sparse vector\nclipping method with the Component-Averaged Dykstra (CAD) algorithm, an\niterative scheme for solving the best-approximation problem. We establish a\nconvergence result for CAD and develop a GPU-accelerated implementation capable\nof handling large-scale inputs efficiently. To enable end-to-end training, we\nintroduce a surrogate gradient for CAD that is both computationally efficient\nand better suited for optimization than the exact gradient. We validate ProjNet\non four classes of constrained optimisation problems: linear programming, two\nclasses of non-convex quadratic programs, and radio transmit power\noptimization, demonstrating its effectiveness across diverse problem settings.",
    "published": "2025-10-13T10:10:46Z",
    "updated": "2025-10-13T10:10:46Z",
    "link": "http://arxiv.org/pdf/2510.11227v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ahmed Rashwan",
      "Keith Briggs",
      "Chris Budd",
      "Lisa Kreusser"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26003v2",
    "title": "Scaling Equilibrium Propagation to Deeper Neural Network Architectures",
    "summary": "Equilibrium propagation has been proposed as a biologically plausible\nalternative to the backpropagation algorithm. The local nature of gradient\ncomputations, combined with the use of convergent RNNs to reach equilibrium\nstates, make this approach well-suited for implementation on neuromorphic\nhardware. However, previous studies on equilibrium propagation have been\nrestricted to networks containing only dense layers or relatively small\narchitectures with a few convolutional layers followed by a final dense layer.\nThese networks have a significant gap in accuracy compared to similarly sized\nfeedforward networks trained with backpropagation. In this work, we introduce\nthe Hopfield-Resnet architecture, which incorporates residual (or skip)\nconnections in Hopfield networks with clipped $\\mathrm{ReLU}$ as the activation\nfunction. The proposed architectural enhancements enable the training of\nnetworks with nearly twice the number of layers reported in prior works. For\nexample, Hopfield-Resnet13 achieves 93.92\\% accuracy on CIFAR-10, which is\n$\\approx$3.5\\% higher than the previous best result and comparable to that\nprovided by Resnet13 trained using backpropagation.",
    "published": "2025-09-30T09:34:44Z",
    "updated": "2025-10-13T10:06:01Z",
    "link": "http://arxiv.org/pdf/2509.26003v2.pdf",
    "category": [
      "cs.NE",
      "cs.LG"
    ],
    "authors": [
      "Sankar Vinayak Elayedam",
      "Gopalakrishnan Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16690v2",
    "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator",
    "summary": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.",
    "published": "2025-05-22T13:55:39Z",
    "updated": "2025-10-13T09:59:08Z",
    "link": "http://arxiv.org/pdf/2505.16690v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Beier Luo",
      "Shuoyuan Wang",
      "Yixuan Li",
      "Hongxin Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.15567v4",
    "title": "Towards Unified and Lossless Latent Space for 3D Molecular Latent\n  Diffusion Modeling",
    "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose \\textbf{U}nified\nVariational \\textbf{A}uto-\\textbf{E}ncoder for \\textbf{3D} Molecular Latent\nDiffusion Modeling (\\textbf{UAE-3D}), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both \\textit{de novo} and conditional 3D molecule generation, achieving\nleading efficiency and quality. On GEOM-Drugs, it reduces FCD by 72.6\\% over\nthe previous best result, while achieving over 70\\% relative average\nimprovements in geometric fidelity. Our code is released at\nhttps://github.com/lyc0930/UAE-3D/.",
    "published": "2025-03-19T08:56:13Z",
    "updated": "2025-10-13T09:55:25Z",
    "link": "http://arxiv.org/pdf/2503.15567v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yanchen Luo",
      "Zhiyuan Liu",
      "Yi Zhao",
      "Sihang Li",
      "Hengxing Cai",
      "Kenji Kawaguchi",
      "Tat-Seng Chua",
      "Yang Zhang",
      "Xiang Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.18164v6",
    "title": "Faster Convergence of Riemannian Stochastic Gradient Descent with\n  Increasing Batch Size",
    "summary": "We theoretically analyzed the convergence behavior of Riemannian stochastic\ngradient descent (RSGD) and found that using an increasing batch size leads to\nfaster convergence than using a constant batch size, not only with a constant\nlearning rate but also with a decaying learning rate, such as cosine annealing\ndecay and polynomial decay. The convergence rate improves from $O(T^{-1}+C)$\nwith a constant batch size to $O(T^{-1})$ with an increasing batch size, where\n$T$ denotes the total number of iterations and $C$ is a constant. Using\nprincipal component analysis and low-rank matrix completion, we investigated,\nboth theoretically and numerically, how an increasing batch size affects\ncomputational time as quantified by stochastic first-order oracle (SFO)\ncomplexity. An increasing batch size was found to reduce the SFO complexity of\nRSGD. Furthermore, an increasing batch size was found to offer the advantages\nof both small and large constant batch sizes.",
    "published": "2025-01-30T06:23:28Z",
    "updated": "2025-10-13T09:54:32Z",
    "link": "http://arxiv.org/pdf/2501.18164v6.pdf",
    "category": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Kanata Oowada",
      "Hideaki Iiduka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02765v3",
    "title": "Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse\n  Plasticity",
    "summary": "Gradient-based algorithms are a cornerstone of artificial neural network\ntraining, yet it remains unclear whether biological neural networks use similar\ngradient-based strategies during learning. Experiments often discover a\ndiversity of synaptic plasticity rules, but whether these amount to an\napproximation to gradient descent is unclear. Here we investigate a previously\noverlooked possibility: that learning dynamics may include fundamentally\nnon-gradient \"curl\"-like components while still being able to effectively\noptimize a loss function. Curl terms naturally emerge in networks with\ninhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,\nresulting in learning dynamics that cannot be framed as gradient descent on any\nobjective. To investigate the impact of these curl terms, we analyze\nfeedforward networks within an analytically tractable student-teacher\nframework, systematically introducing non-gradient dynamics through neurons\nexhibiting rule-flipped plasticity. Small curl terms preserve the stability of\nthe original solution manifold, resulting in learning dynamics similar to\ngradient descent. Beyond a critical value, strong curl terms destabilize the\nsolution manifold. Depending on the network architecture, this loss of\nstability can lead to chaotic learning dynamics that destroy performance. In\nother cases, the curl terms can counterintuitively speed learning compared to\ngradient descent by allowing the weight dynamics to escape saddles by\ntemporarily ascending the loss. Our results identify specific architectures\ncapable of supporting robust learning via diverse learning rules, providing an\nimportant counterpoint to normative theories of gradient-based learning in\nneural networks.",
    "published": "2025-10-03T06:54:40Z",
    "updated": "2025-10-13T09:45:32Z",
    "link": "http://arxiv.org/pdf/2510.02765v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hugo Ninou",
      "Jonathan Kadmon",
      "N. Alex Cayco-Gajic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11209v1",
    "title": "Cross-Scale Reservoir Computing for large spatio-temporal forecasting\n  and modeling",
    "summary": "We propose a new reservoir computing method for forecasting high-resolution\nspatiotemporal datasets. By combining multi-resolution inputs from coarser to\nfiner layers, our architecture better captures both local and global dynamics.\nApplied to Sea Surface Temperature data, it outperforms standard parallel\nreservoir models in long-term forecasting, demonstrating the effectiveness of\ncross-layers coupling in improving predictive accuracy. Finally, we show that\nthe optimal network dynamics in each layer become increasingly linear,\nrevealing the slow modes propagated to subsequent layers.",
    "published": "2025-10-13T09:43:29Z",
    "updated": "2025-10-13T09:43:29Z",
    "link": "http://arxiv.org/pdf/2510.11209v1.pdf",
    "category": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Nicola Alboré",
      "Gabriele Di Antonio",
      "Fabrizio Coccetti",
      "Andrea Gabrielli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11202v1",
    "title": "Evaluating Line-level Localization Ability of Learning-based Code\n  Vulnerability Detection Models",
    "summary": "To address the extremely concerning problem of software vulnerability, system\nsecurity is often entrusted to Machine Learning (ML) algorithms. Despite their\nnow established detection capabilities, such models are limited by design to\nflagging the entire input source code function as vulnerable, rather than\nprecisely localizing the concerned code lines. However, the detection\ngranularity is crucial to support human operators during software development,\nensuring that such predictions reflect the true code semantics to help debug,\nevaluate, and fix the detected vulnerabilities. To address this issue, recent\nwork made progress toward improving the detector's localization ability, thus\nnarrowing down the vulnerability detection \"window\" and providing more\nfine-grained predictions. Such approaches, however, implicitly disregard the\npresence of spurious correlations and biases in the data, which often\npredominantly influence the performance of ML algorithms. In this work, we\ninvestigate how detectors comply with this requirement by proposing an\nexplainability-based evaluation procedure. Our approach, defined as Detection\nAlignment (DA), quantifies the agreement between the input source code lines\nthat most influence the prediction and the actual localization of the\nvulnerability as per the ground truth. Through DA, which is model-agnostic and\nadaptable to different detection tasks, not limited to our use case, we analyze\nmultiple learning-based vulnerability detectors and datasets. As a result, we\nshow how the predictions of such models are consistently biased by\nnon-vulnerable lines, ultimately highlighting the high impact of biases and\nspurious correlations. The code is available at\nhttps://github.com/pralab/vuln-localization-eval.",
    "published": "2025-10-13T09:34:40Z",
    "updated": "2025-10-13T09:34:40Z",
    "link": "http://arxiv.org/pdf/2510.11202v1.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Marco Pintore",
      "Giorgio Piras",
      "Angelo Sotgiu",
      "Maura Pintor",
      "Battista Biggio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11192v1",
    "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs",
    "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations.",
    "published": "2025-10-13T09:25:48Z",
    "updated": "2025-10-13T09:25:48Z",
    "link": "http://arxiv.org/pdf/2510.11192v1.pdf",
    "category": [
      "cs.AR",
      "cs.LG"
    ],
    "authors": [
      "João Paulo Cardoso de Lima",
      "Marc Dietrich",
      "Jeronimo Castrillon",
      "Asif Ali Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11188v1",
    "title": "Protein as a Second Language for LLMs",
    "summary": "Deciphering the function of unseen protein sequences is a fundamental\nchallenge with broad scientific impact, yet most existing methods depend on\ntask-specific adapters or large-scale supervised fine-tuning. We introduce the\n\"Protein-as-Second-Language\" framework, which reformulates amino-acid sequences\nas sentences in a novel symbolic language that large language models can\ninterpret through contextual exemplars. Our approach adaptively constructs\nsequence-question-answer triples that reveal functional cues in a zero-shot\nsetting, without any further training. To support this process, we curate a\nbilingual corpus of 79,926 protein-QA instances spanning attribute prediction,\ndescriptive understanding, and extended reasoning. Empirically, our method\ndelivers consistent gains across diverse open-source LLMs and GPT-4, achieving\nup to 17.2% ROUGE-L improvement (average +7%) and even surpassing fine-tuned\nprotein-specific language models. These results highlight that generic LLMs,\nwhen guided with protein-as-language cues, can outperform domain-specialized\nmodels, offering a scalable pathway for protein understanding in foundation\nmodels.",
    "published": "2025-10-13T09:21:45Z",
    "updated": "2025-10-13T09:21:45Z",
    "link": "http://arxiv.org/pdf/2510.11188v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "authors": [
      "Xinhui Chen",
      "Zuchao Li",
      "Mengqi Gao",
      "Yufeng Zhang",
      "Chak Tou Leong",
      "Haoyang Li",
      "Jiaqi Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08722v2",
    "title": "Enhancing Self-Supervised Learning with Semantic Pairs A New Dataset and\n  Empirical Study",
    "summary": "Instance discrimination is a self-supervised representation learning paradigm\nwherein individual instances within a dataset are treated as distinct classes.\nThis is typically achieved by generating two disparate views of each instance\nby applying stochastic transformations, encouraging the model to learn\nrepresentations invariant to the common underlying object across these views.\nWhile this approach facilitates the acquisition of invariant representations\nfor dataset instances under various handcrafted transformations (e.g., random\ncropping, colour jittering), an exclusive reliance on such data transformations\nfor achieving invariance may inherently limit the model's generalizability to\nunseen datasets and diverse downstream tasks. The inherent limitation stems\nfrom the fact that the finite set of transformations within the data processing\npipeline is unable to encompass the full spectrum of potential data variations.\nIn this study, we provide the technical foundation for leveraging semantic\npairs to enhance the generalizability of the model's representation and\nempirically demonstrate that incorporating semantic pairs mitigates the issue\nof limited transformation coverage. Specifically, we propose that by exposing\nthe model to semantic pairs (i.e., two instances belonging to the same semantic\ncategory), we introduce varied real-world scene contexts, thereby fostering the\ndevelopment of more generalizable object representations. To validate this\nhypothesis, we constructed and released a novel dataset comprising curated\nsemantic pairs and conducted extensive experimentation to empirically establish\nthat their inclusion enables the model to learn more general representations,\nultimately leading to improved performance across diverse downstream tasks.",
    "published": "2025-10-09T18:31:55Z",
    "updated": "2025-10-13T09:09:06Z",
    "link": "http://arxiv.org/pdf/2510.08722v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mohammad Alkhalefi",
      "Georgios Leontidis",
      "Mingjun Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11174v1",
    "title": "Machine Learning-Integrated Hybrid Fluid-Kinetic Framework for Quantum\n  Electrodynamic Laser Plasma Simulations",
    "summary": "High-intensity laser plasma interactions create complex computational\nproblems because they involve both fluid and kinetic regimes, which need models\nthat maintain physical precision while keeping computational speed. The\nresearch introduces a machine learning-based three-dimensional hybrid\nfluid-particle-in-cell (PIC) system, which links relativistic plasma behavior\nto automatic regime transitions. The technique employs fluid approximations for\nstable areas but activates the PIC solver when SwitchNet directs it to unstable\nsections through its training on physics-based synthetic data. The model uses a\nsmooth transition between Ammosov-Delone-Krainov (ADK) tunneling and\nmultiphoton ionization rates to simulate ionization, while Airy-function\napproximations simulate quantum electrodynamic (QED) effects for radiation\nreaction and pair production. The convolutional neural network applies energy\nconservation through physics-based loss functions, which operate on normalized\nfields per channel. Monte Carlo dropout provides uncertainty measurement. The\nhybrid model produces precise predictions with coefficient of determination\n(R^2) values above 0.95 and mean squared errors below 10^-4 for all field\ncomponents. This adaptive approach enhances the accuracy and scalability of\nlaser-plasma simulations, providing a unified predictive framework for\nhigh-energy-density and particle acceleration applications.",
    "published": "2025-10-13T09:07:59Z",
    "updated": "2025-10-13T09:07:59Z",
    "link": "http://arxiv.org/pdf/2510.11174v1.pdf",
    "category": [
      "physics.plasm-ph",
      "cs.LG"
    ],
    "authors": [
      "Sadra Saremi",
      "Amirhossein Ahmadkhan Kordbacheh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11169v1",
    "title": "PAC-Bayesian Bounds on Constrained f-Entropic Risk Measures",
    "summary": "PAC generalization bounds on the risk, when expressed in terms of the\nexpected loss, are often insufficient to capture imbalances between subgroups\nin the data. To overcome this limitation, we introduce a new family of risk\nmeasures, called constrained f-entropic risk measures, which enable finer\ncontrol over distributional shifts and subgroup imbalances via f-divergences,\nand include the Conditional Value at Risk (CVaR), a well-known risk measure. We\nderive both classical and disintegrated PAC-Bayesian generalization bounds for\nthis family of risks, providing the first disintegratedPAC-Bayesian guarantees\nbeyond standard risks. Building on this theory, we design a self-bounding\nalgorithm that minimizes our bounds directly, yielding models with guarantees\nat the subgroup level. Finally, we empirically demonstrate the usefulness of\nour approach.",
    "published": "2025-10-13T09:02:13Z",
    "updated": "2025-10-13T09:02:13Z",
    "link": "http://arxiv.org/pdf/2510.11169v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Hind Atbir",
      "Farah Cherfaoui",
      "Guillaume Metzler",
      "Emilie Morvant",
      "Paul Viallard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08808v2",
    "title": "TinyGraphEstimator: Adapting Lightweight Language Models for Graph\n  Structure Inference",
    "summary": "Graphs provide a universal framework for representing complex relational\nsystems, and inferring their structural properties is a core challenge in graph\nanalysis and reasoning. While large language models have recently demonstrated\nemerging abilities to perform symbolic and numerical reasoning, the potential\nof smaller, resource-efficient models in this context remains largely\nunexplored. This paper investigates whether compact transformer-based language\nmodels can infer graph-theoretic parameters directly from graph\nrepresentations. To enable systematic evaluation, we introduce the\nTinyGraphEstimator dataset - a balanced collection of connected graphs\ngenerated from multiple random graph models and annotated with detailed\nstructural metadata. We evaluate several small open models on their ability to\npredict key graph parameters such as density, clustering, and chromatic number.\nFurthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation\n(LoRA) technique, achieving consistent improvements across all evaluated\nmetrics. The results demonstrate that small language models possess non-trivial\nreasoning capacity over graph-structured data and can be effectively adapted\nfor structural inference tasks through efficient parameter tuning.",
    "published": "2025-10-09T20:47:07Z",
    "updated": "2025-10-13T09:00:15Z",
    "link": "http://arxiv.org/pdf/2510.08808v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Michal Podstawski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06743v3",
    "title": "Long-Range Graph Wavelet Networks",
    "summary": "Modeling long-range interactions, the propagation of information across\ndistant parts of a graph, is a central challenge in graph machine learning.\nGraph wavelets, inspired by multi-resolution signal processing, provide a\nprincipled way to capture both local and global structures. However, existing\nwavelet-based graph neural networks rely on finite-order polynomial\napproximations, which limit their receptive fields and hinder long-range\npropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which\ndecompose wavelet filters into complementary local and global components. Local\naggregation is handled with efficient low-order polynomials, while long-range\ninteractions are captured through a flexible spectral-domain parameterization.\nThis hybrid design unifies short- and long-distance information flow within a\nprincipled wavelet framework. Experiments show that LR-GWN achieves\nstate-of-the-art performance among wavelet-based methods on long-range\nbenchmarks, while remaining competitive on short-range datasets.",
    "published": "2025-09-08T14:35:30Z",
    "updated": "2025-10-13T08:56:32Z",
    "link": "http://arxiv.org/pdf/2509.06743v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Filippo Guerranti",
      "Fabrizio Forte",
      "Simon Geisler",
      "Stephan Günnemann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11164v1",
    "title": "Beyond single-model XAI: aggregating multi-model explanations for\n  enhanced trustworthiness",
    "summary": "The use of Artificial Intelligence (AI) models in real-world and high-risk\napplications has intensified the discussion about their trustworthiness and\nethical usage, from both a technical and a legislative perspective. The field\nof eXplainable Artificial Intelligence (XAI) addresses this challenge by\nproposing explanations that bring to light the decision-making processes of\ncomplex black-box models. Despite being an essential property, the robustness\nof explanations is often an overlooked aspect during development: only robust\nexplanation methods can increase the trust in the system as a whole. This paper\ninvestigates the role of robustness through the usage of a feature importance\naggregation derived from multiple models ($k$-nearest neighbours, random forest\nand neural networks). Preliminary results showcase the potential in increasing\nthe trustworthiness of the application, while leveraging multiple model's\npredictive power.",
    "published": "2025-10-13T08:55:45Z",
    "updated": "2025-10-13T08:55:45Z",
    "link": "http://arxiv.org/pdf/2510.11164v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ilaria Vascotto",
      "Alex Rodriguez",
      "Alessandro Bonaita",
      "Luca Bortolussi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11162v1",
    "title": "Emergence of hybrid computational dynamics through reinforcement\n  learning",
    "summary": "Understanding how learning algorithms shape the computational strategies that\nemerge in neural networks remains a fundamental challenge in machine\nintelligence. While network architectures receive extensive attention, the role\nof the learning paradigm itself in determining emergent dynamics remains\nlargely unexplored. Here we demonstrate that reinforcement learning (RL) and\nsupervised learning (SL) drive recurrent neural networks (RNNs) toward\nfundamentally different computational solutions when trained on identical\ndecision-making tasks. Through systematic dynamical systems analysis, we reveal\nthat RL spontaneously discovers hybrid attractor architectures, combining\nstable fixed-point attractors for decision maintenance with quasi-periodic\nattractors for flexible evidence integration. This contrasts sharply with SL,\nwhich converges almost exclusively to simpler fixed-point-only solutions. We\nfurther show that RL sculpts functionally balanced neural populations through a\npowerful form of implicit regularization -- a structural signature that\nenhances robustness and is conspicuously absent in the more heterogeneous\nsolutions found by SL-trained networks. The prevalence of these complex\ndynamics in RL is controllably modulated by weight initialization and\ncorrelates strongly with performance gains, particularly as task complexity\nincreases. Our results establish the learning algorithm as a primary\ndeterminant of emergent computation, revealing how reward-based optimization\nautonomously discovers sophisticated dynamical mechanisms that are less\naccessible to direct gradient-based optimization. These findings provide both\nmechanistic insights into neural computation and actionable principles for\ndesigning adaptive AI systems.",
    "published": "2025-10-13T08:53:59Z",
    "updated": "2025-10-13T08:53:59Z",
    "link": "http://arxiv.org/pdf/2510.11162v1.pdf",
    "category": [
      "cs.LG",
      "cs.NE",
      "nlin.AO",
      "q-bio.NC"
    ],
    "authors": [
      "Roman A. Kononov",
      "Nikita A. Pospelov",
      "Konstantin V. Anokhin",
      "Vladimir V. Nekorkin",
      "Oleg V. Maslennikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24138v2",
    "title": "AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in\n  AMS Circuits",
    "summary": "Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated\ncircuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit\ndesign has remained a longstanding challenge due to its difficulty and\ncomplexity. Although recent advances in Multi-modal Large Language Models\n(MLLMs) offer promising potential for supporting AMS circuit analysis and\ndesign, current research typically evaluates MLLMs on isolated tasks within the\ndomain, lacking a comprehensive benchmark that systematically assesses model\ncapabilities across diverse AMS-related challenges. To address this gap, we\nintroduce AMSbench, a benchmark suite designed to evaluate MLLM performance\nacross critical tasks including circuit schematic perception, circuit analysis,\nand circuit design. AMSbench comprises approximately 8000 test questions\nspanning multiple difficulty levels and assesses eight prominent models,\nencompassing both open-source and proprietary solutions such as Qwen 2.5-VL and\nGemini 2.5 Pro. Our evaluation highlights significant limitations in current\nMLLMs, particularly in complex multi-modal reasoning and sophisticated circuit\ndesign tasks. These results underscore the necessity of advancing MLLMs'\nunderstanding and effective application of circuit-specific knowledge, thereby\nnarrowing the existing performance gap relative to human expertise and moving\ntoward fully automated AMS circuit design workflows. Our data is released at\nthis URL.",
    "published": "2025-05-30T02:17:45Z",
    "updated": "2025-10-13T08:48:52Z",
    "link": "http://arxiv.org/pdf/2505.24138v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yichen Shi",
      "Ze Zhang",
      "Hongyang Wang",
      "Zhuofu Tao",
      "Zhongyi Li",
      "Bingyu Chen",
      "Yaxin Wang",
      "Zhen huang",
      "Xuhua Liu",
      "Quan Chen",
      "Zhiping Yu",
      "Ting-Jung Lin",
      "Lei He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.18224v2",
    "title": "FSA: An Alternative Efficient Implementation of Native Sparse Attention\n  Kernel",
    "summary": "Recent advance in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), one\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance boost while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA forces a loop order that is only efficient with a\nrelatively large number of query heads in each Grouped Query Attention (GQA)\ngroup, whereas existing LLMs widely adopt much smaller number of query heads in\neach GQA group -- such an inconsistency significantly limits the applicability\nof this sparse algorithmic advance. In this work, we propose Flash Sparse\nAttention (FSA), an alternative kernel implementation that enables efficient\nNSA computation across a wide range of popular LLMs with varied smaller number\nof heads in each GQA group on modern GPUs. Compared to vanilla NSA kernel\nimplementation, our empirical evaluation demonstrates that FSA achieves (i) up\nto 3.5x and on average 1.6x kernel-level latency reduction, (ii) up to 1.25x\nand 1.09x on average end-to-end training speedup on state-of-the-art LLMs, and\n(iii) up to 1.36x and 1.11x on average for prefill-phase speedup in LLM\ngenerative inference. Github Repo at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.",
    "published": "2025-08-25T17:22:15Z",
    "updated": "2025-10-13T08:46:58Z",
    "link": "http://arxiv.org/pdf/2508.18224v2.pdf",
    "category": [
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Ran Yan",
      "Youhe Jiang",
      "Zhuoming Chen",
      "Haohui Mai",
      "Beidi Chen",
      "Binhang Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09692v4",
    "title": "AB-UPT: Scaling Neural CFD Surrogates for High-Fidelity Automotive\n  Aerodynamics Simulations via Anchored-Branched Universal Physics Transformers",
    "summary": "Recent advances in neural surrogate modeling offer the potential for\ntransformative innovations in applications such as automotive aerodynamics.\nYet, industrial-scale problems often involve volumetric meshes with cell counts\nreaching 100 million, presenting major scalability challenges. Complex\ngeometries further complicate modeling through intricate surface-volume\ninteractions, while quantities such as vorticity are highly nonlinear and must\nsatisfy strict divergence-free constraints. To address these requirements, we\nintroduce AB-UPT as a novel modeling scheme for building neural surrogates for\nCFD simulations. AB-UPT is designed to: (i) decouple geometry encoding and\nprediction tasks via multi-branch operators; (ii) enable scalability to\nhigh-resolution outputs via neural simulation in a low-dimensional latent\nspace, coupled with anchored neural field decoders to predict high-fidelity\noutputs; (iii) enforce physics consistency by a divergence-free formulation. We\nshow that AB-UPT yields state-of-the-art predictive accuracy of surface and\nvolume fields on automotive CFD simulations ranging from 33 thousand up to 150\nmillion mesh cells. Furthermore, our anchored neural field architecture enables\nthe enforcement of hard physical constraints on the physics predictions without\ndegradation in performance, exemplified by modeling divergence-free vorticity\nfields. Notably, the proposed models can be trained on a single GPU in less\nthan a day and predict industry-standard surface and volume fields within\nseconds. Additionally, we show that the flexible design of our method enables\nneural simulation from a CAD geometry alone, thereby eliminating the need for\ncostly CFD meshing procedures for inference.",
    "published": "2025-02-13T17:58:07Z",
    "updated": "2025-10-13T08:45:57Z",
    "link": "http://arxiv.org/pdf/2502.09692v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Benedikt Alkin",
      "Maurits Bleeker",
      "Richard Kurle",
      "Tobias Kronlachner",
      "Reinhard Sonnleitner",
      "Matthias Dorfer",
      "Johannes Brandstetter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03959v3",
    "title": "Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage\n  Machine Learning Model",
    "summary": "Thunderstorm-driven outages are difficult to predict because most storms do\nnot cause damage, convective processes occur rapidly and chaotically, and the\navailable public data are both noisy and incomplete. We develop a 24-48 h\nearly-warning model for summer, thunderstorm-related outages in Michigan using\nonly open sources (EAGLE-I for ground truth; METAR for weather). We use the\npublicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge\nNational Laboratory for the U.S. Department of Energy. The pipeline preserves\nconvective micro-signals from a sparse station network via parameter-specific\nkriging with hourly variograms and targeted overdrafting to retain extremes,\nand builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW\nspatial aggregates) capturing precursors of severe convection (moisture\nadvection, wind shifts, and pressure drops). The two-stage model design,\ncombining a logistic gate and an LSTM regressor, limits routine periods and\nreduces noise exposure. The study uses event-centric metrics (cluster-based\nhits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour\nwindows around state-level peaks (>= 50,000), with uncertainty quantified by\nhourly moving-block bootstrap.\n  On the test sample, Two-Stage detects more reference peaks across all windows\n(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra\nfalse alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at\n+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48\nh (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP\nanalysis confirms moisture-advection and wind/gust precursors, underscoring the\nvalue of the feature engineering. Despite open-data noise, the feature-driven\npipeline yields actionable, event-focused early warnings for thunderstorm\noutages.",
    "published": "2025-10-04T21:59:45Z",
    "updated": "2025-10-13T08:41:56Z",
    "link": "http://arxiv.org/pdf/2510.03959v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Iryna Stanishevska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11148v1",
    "title": "Enhanced Sampling for Efficient Learning of Coarse-Grained Machine\n  Learning Potentials",
    "summary": "Coarse-graining (CG) enables molecular dynamics (MD) simulations of larger\nsystems and longer timescales that are otherwise infeasible with atomistic\nmodels. Machine learning potentials (MLPs), with their capacity to capture\nmany-body interactions, can provide accurate approximations of the potential of\nmean force (PMF) in CG models. Current CG MLPs are typically trained in a\nbottom-up manner via force matching, which in practice relies on configurations\nsampled from the unbiased equilibrium Boltzmann distribution to ensure\nthermodynamic consistency. This convention poses two key limitations: first,\nsufficiently long atomistic trajectories are needed to reach convergence; and\nsecond, even once equilibrated, transition regions remain poorly sampled. To\naddress these issues, we employ enhanced sampling to bias along CG degrees of\nfreedom for data generation, and then recompute the forces with respect to the\nunbiased potential. This strategy simultaneously shortens the simulation time\nrequired to produce equilibrated data and enriches sampling in transition\nregions, while preserving the correct PMF. We demonstrate its effectiveness on\nthe M\\\"uller-Brown potential and capped alanine, achieving notable\nimprovements. Our findings support the use of enhanced sampling for force\nmatching as a promising direction to improve the accuracy and reliability of CG\nMLPs.",
    "published": "2025-10-13T08:40:13Z",
    "updated": "2025-10-13T08:40:13Z",
    "link": "http://arxiv.org/pdf/2510.11148v1.pdf",
    "category": [
      "physics.chem-ph",
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Weilong Chen",
      "Franz Görlich",
      "Paul Fuchs",
      "Julija Zavadlav"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11147v1",
    "title": "torchsom: The Reference PyTorch Library for Self-Organizing Maps",
    "summary": "This paper introduces torchsom, an open-source Python library that provides a\nreference implementation of the Self-Organizing Map (SOM) in PyTorch. This\npackage offers three main features: (i) dimensionality reduction, (ii)\nclustering, and (iii) friendly data visualization. It relies on a PyTorch\nbackend, enabling (i) fast and efficient training of SOMs through GPU\nacceleration, and (ii) easy and scalable integrations with PyTorch ecosystem.\nMoreover, torchsom follows the scikit-learn API for ease of use and\nextensibility. The library is released under the Apache 2.0 license with 90%\ntest coverage, and its source code and documentation are available at\nhttps://github.com/michelin/TorchSOM.",
    "published": "2025-10-13T08:40:00Z",
    "updated": "2025-10-13T08:40:00Z",
    "link": "http://arxiv.org/pdf/2510.11147v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "68T07, 68T05, 68-04",
      "I.2.6; I.5.1; I.5.3"
    ],
    "authors": [
      "Louis Berthier",
      "Ahmed Shokry",
      "Maxime Moreaud",
      "Guillaume Ramelet",
      "Eric Moulines"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11141v1",
    "title": "A Comprehensive Forecasting-Based Framework for Time Series Anomaly\n  Detection: Benchmarking on the Numenta Anomaly Benchmark (NAB)",
    "summary": "Time series anomaly detection is critical for modern digital infrastructures,\nyet existing methods lack systematic cross-domain evaluation. We present a\ncomprehensive forecasting-based framework unifying classical methods\n(Holt-Winters, SARIMA) with deep learning architectures (LSTM, Informer) under\na common residual-based detection interface. Our modular pipeline integrates\npreprocessing (normalization, STL decomposition), four forecasting models, four\ndetection methods, and dual evaluation through forecasting metrics (MAE, RMSE,\nPCC) and detection metrics (Precision, Recall, F1, AUC). We conduct the first\ncomplete evaluation on the Numenta Anomaly Benchmark (58 datasets, 7\ncategories) with 232 model training runs and 464 detection evaluations\nachieving 100\\% success rate. LSTM achieves best performance (F1: 0.688,\nranking first or second on 81\\% of datasets) with exceptional correlation on\ncomplex patterns (PCC: 0.999). Informer provides competitive accuracy (F1:\n0.683) with 30\\% faster training. Classical methods achieve perfect predictions\non simple synthetic data with 60 lower cost but show 2-3 worse F1-scores on\nreal-world datasets. Forecasting quality dominates detection performance:\ndifferences between detection methods (F1: 0.621-0.688) are smaller than\nbetween forecasting models (F1: 0.344-0.688). Our findings provide\nevidence-based guidance: use LSTM for complex patterns, Informer for\nefficiency-critical deployments, and classical methods for simple periodic data\nwith resource constraints. The complete implementation and results establish\nbaselines for future forecasting-based anomaly detection research.",
    "published": "2025-10-13T08:31:42Z",
    "updated": "2025-10-13T08:31:42Z",
    "link": "http://arxiv.org/pdf/2510.11141v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mohammad Karami",
      "Mostafa Jalali",
      "Fatemeh Ghassemi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11140v1",
    "title": "DUAL: Learning Diverse Kernels for Aggregated Two-sample and\n  Independence Testing",
    "summary": "To adapt kernel two-sample and independence testing to complex structured\ndata, aggregation of multiple kernels is frequently employed to boost testing\npower compared to single-kernel tests. However, we observe a phenomenon that\ndirectly maximizing multiple kernel-based statistics may result in highly\nsimilar kernels that capture highly overlapping information, limiting the\neffectiveness of aggregation. To address this, we propose an aggregated\nstatistic that explicitly incorporates kernel diversity based on the covariance\nbetween different kernels. Moreover, we identify a fundamental challenge: a\ntrade-off between the diversity among kernels and the test power of individual\nkernels, i.e., the selected kernels should be both effective and diverse. This\nmotivates a testing framework with selection inference, which leverages\ninformation from the training phase to select kernels with strong individual\nperformance from the learned diverse kernel pool. We provide rigorous\ntheoretical statements and proofs to show the consistency on the test power and\ncontrol of Type-I error, along with asymptotic analysis of the proposed\nstatistics. Lastly, we conducted extensive empirical experiments demonstrating\nthe superior performance of our proposed approach across various benchmarks for\nboth two-sample and independence testing.",
    "published": "2025-10-13T08:30:42Z",
    "updated": "2025-10-13T08:30:42Z",
    "link": "http://arxiv.org/pdf/2510.11140v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhijian Zhou",
      "Xunye Tian",
      "Liuhua Peng",
      "Chao Lei",
      "Antonin Schrab",
      "Danica J. Sutherland",
      "Feng Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.24510v2",
    "title": "Specialization after Generalization: Towards Understanding Test-Time\n  Training in Foundation Models",
    "summary": "Recent empirical studies have explored the idea of continuing to train a\nmodel at test-time for a given task, known as test-time training (TTT), and\nhave found it to yield significant performance improvements. However, there is\nlimited understanding of why and when TTT is effective. Earlier explanations\nmostly focused on the observation that TTT may help when applied to\nout-of-distribution adaptation or used with privileged data. However, the\ngrowing scale of foundation models with most test data being in-distribution\nquestions these explanations. We instead posit that foundation models remain\nglobally underparameterized, with TTT providing a mechanism for specialization\nafter generalization, focusing capacity on concepts relevant to the test task.\nSpecifically, under the linear representation hypothesis, we propose a model in\nwhich TTT achieves a substantially smaller in-distribution test error than\nglobal training. We empirically validate our model's key assumptions by\ntraining a sparse autoencoder on ImageNet, showing that semantically related\ndata points are explained by only a few shared concepts. Finally, we perform\nscaling studies across image and language tasks that confirm the practical\nimplications of our model, identifying the regimes where specialization is most\neffective.",
    "published": "2025-09-29T09:24:52Z",
    "updated": "2025-10-13T08:29:09Z",
    "link": "http://arxiv.org/pdf/2509.24510v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jonas Hübotter",
      "Patrik Wolf",
      "Alexander Shevchenko",
      "Dennis Jüni",
      "Andreas Krause",
      "Gil Kur"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11133v1",
    "title": "Test-Time Adaptation by Causal Trimming",
    "summary": "Test-time adaptation aims to improve model robustness under distribution\nshifts by adapting models with access to unlabeled target samples. A primary\ncause of performance degradation under such shifts is the model's reliance on\nfeatures that lack a direct causal relationship with the prediction target. We\nintroduce Test-time Adaptation by Causal Trimming (TACT), a method that\nidentifies and removes non-causal components from representations for test\ndistributions. TACT applies data augmentations that preserve causal features\nwhile varying non-causal ones. By analyzing the changes in the representations\nusing Principal Component Analysis, TACT identifies the highest variance\ndirections associated with non-causal features. It trims the representations by\nremoving their projections on the identified directions, and uses the trimmed\nrepresentations for the predictions. During adaptation, TACT continuously\ntracks and refines these directions to get a better estimate of non-causal\nfeatures. We theoretically analyze the effectiveness of this approach and\nempirically validate TACT on real-world out-of-distribution benchmarks. TACT\nconsistently outperforms state-of-the-art methods by a significant margin.",
    "published": "2025-10-13T08:22:38Z",
    "updated": "2025-10-13T08:22:38Z",
    "link": "http://arxiv.org/pdf/2510.11133v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yingnan Liu",
      "Rui Qiao",
      "Mong Li Lee",
      "Wynne Hsu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11121v1",
    "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
    "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
    "published": "2025-10-13T08:08:58Z",
    "updated": "2025-10-13T08:08:58Z",
    "link": "http://arxiv.org/pdf/2510.11121v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Rongjie Zhu",
      "Cong Zhang",
      "Zhiguang Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11110v1",
    "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for\n  Physiological Signals with Missing Modalities",
    "summary": "Missing or corrupted modalities are common in physiological signal-based\nmedical applications owing to hardware constraints or motion artifacts.\nHowever, most existing methods assume the availability of all modalities,\nresulting in substantial performance degradation in the absence of any\nmodality. To overcome this limitation, this study proposes PhysioME, a robust\nframework designed to ensure reliable performance under missing modality\nconditions. PhysioME adopts: (1) a multimodal self-supervised learning approach\nthat combines contrastive learning with masked prediction; (2) a\nDual-PathNeuroNet backbone tailored to capture the temporal dynamics of each\nphysiological signal modality; and (3) a restoration decoder that reconstructs\nmissing modality tokens, enabling flexible processing of incomplete inputs. The\nexperimental results show that PhysioME achieves high consistency and\ngeneralization performance across various missing modality scenarios. These\nfindings highlight the potential of PhysioME as a reliable tool for supporting\nclinical decision-making in real-world settings with imperfect data\navailability.",
    "published": "2025-10-13T08:00:55Z",
    "updated": "2025-10-13T08:00:55Z",
    "link": "http://arxiv.org/pdf/2510.11110v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Cheol-Hui Lee",
      "Hwa-Yeon Lee",
      "Min-Kyung Jung",
      "Dong-Joo Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11109v1",
    "title": "Graph Neural Network-Based Multicast Routing for On-Demand Streaming\n  Services in 6G Networks",
    "summary": "The increase of bandwidth-intensive applications in sixth-generation (6G)\nwireless networks, such as real-time volumetric streaming and multi-sensory\nextended reality, demands intelligent multicast routing solutions capable of\ndelivering differentiated quality-of-service (QoS) at scale. Traditional\nshortest-path and multicast routing algorithms are either computationally\nprohibitive or structurally rigid, and they often fail to support heterogeneous\nuser demands, leading to suboptimal resource utilization. Neural network-based\napproaches, while offering improved inference speed, typically lack topological\ngeneralization and scalability. To address these limitations, this paper\npresents a graph neural network (GNN)-based multicast routing framework that\njointly minimizes total transmission cost and supports user-specific video\nquality requirements. The routing problem is formulated as a constrained\nminimum-flow optimization task, and a reinforcement learning algorithm is\ndeveloped to sequentially construct efficient multicast trees by reusing paths\nand adapting to network dynamics. A graph attention network (GAT) is employed\nas the encoder to extract context-aware node embeddings, while a long\nshort-term memory (LSTM) module models the sequential dependencies in routing\ndecisions. Extensive simulations demonstrate that the proposed method closely\napproximates optimal dynamic programming-based solutions while significantly\nreducing computational complexity. The results also confirm strong\ngeneralization to large-scale and dynamic network topologies, highlighting the\nmethod's potential for real-time deployment in 6G multimedia delivery\nscenarios. Code is available at https://github.com/UNIC-Lab/GNN-Routing.",
    "published": "2025-10-13T08:00:45Z",
    "updated": "2025-10-13T08:00:45Z",
    "link": "http://arxiv.org/pdf/2510.11109v1.pdf",
    "category": [
      "cs.NI",
      "cs.LG"
    ],
    "authors": [
      "Xiucheng Wang",
      "Zien Wang",
      "Nan Cheng",
      "Wenchao Xu",
      "Wei Quan",
      "Xuemin Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.10315v2",
    "title": "A Vision-Language Pre-training Model-Guided Approach for Mitigating\n  Backdoor Attacks in Federated Learning",
    "summary": "Defending backdoor attacks in Federated Learning (FL) under heterogeneous\nclient data distributions encounters limitations balancing effectiveness and\nprivacy-preserving, while most existing methods highly rely on the assumption\nof homogeneous client data distributions or the availability of a clean serve\ndataset. In this paper, we propose an FL backdoor defense framework, named\nCLIP-Fed, that utilizes the zero-shot learning capabilities of vision-language\npre-training models. Our scheme overcomes the limitations of Non-IID imposed on\ndefense effectiveness by integrating pre-aggregation and post-aggregation\ndefense strategies. CLIP-Fed aligns the knowledge of the global model and CLIP\non the augmented dataset using prototype contrastive loss and Kullback-Leibler\ndivergence, so that class prototype deviations caused by backdoor samples are\nensured and the correlation between trigger patterns and target labels is\neliminated. In order to balance privacy-preserving and coverage enhancement of\nthe dataset against diverse triggers, we further construct and augment the\nserver dataset via using the multimodal large language model and frequency\nanalysis without any client samples. Extensive experiments on representative\ndatasets evidence the effectiveness of CLIP-Fed. Comparing to other existing\nmethods, CLIP-Fed achieves an average reduction in Attack Success Rate, {\\em\ni.e.}, 2.03\\% on CIFAR-10 and 1.35\\% on CIFAR-10-LT, while improving average\nMain Task Accuracy by 7.92\\% and 0.48\\%, respectively. Our codes are available\nat https://anonymous.4open.science/r/CLIP-Fed.",
    "published": "2025-08-14T03:39:54Z",
    "updated": "2025-10-13T07:55:09Z",
    "link": "http://arxiv.org/pdf/2508.10315v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Keke Gai",
      "Dongjue Wang",
      "Jing Yu",
      "Liehuang Zhu",
      "Qi Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01711v2",
    "title": "Contrastive Representation Regularization for Vision-Language-Action\n  Models",
    "summary": "Vision-Language-Action (VLA) models have shown its capabilities in robot\nmanipulation by leveraging rich representations from pre-trained\nVision-Language Models (VLMs). However, their representations arguably remain\nsuboptimal, lacking sensitivity to robotic signals such as control actions and\nproprioceptive states. To address the issue, we introduce Robot State-aware\nContrastive Loss (RS-CL), a simple and effective representation regularization\nfor VLA models, designed to bridge the gap between VLM representations and\nrobotic signals. In particular, RS-CL aligns the representations more closely\nwith the robot's proprioceptive states, by using relative distances between the\nstates as soft supervision. Complementing the original action prediction\nobjective, RS-CL effectively enhances control-relevant representation learning,\nwhile being lightweight and fully compatible with standard VLA training\npipeline. Our empirical results demonstrate that RS-CL substantially improves\nthe manipulation performance of state-of-the-art VLA models; it pushes the\nprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,\nthrough more accurate positioning during grasping and placing, and boosts\nsuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.",
    "published": "2025-10-02T06:41:22Z",
    "updated": "2025-10-13T07:50:27Z",
    "link": "http://arxiv.org/pdf/2510.01711v2.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Taeyoung Kim",
      "Jimin Lee",
      "Myungkyu Koo",
      "Dongyoung Kim",
      "Kyungmin Lee",
      "Changyeon Kim",
      "Younggyo Seo",
      "Jinwoo Shin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.14167v4",
    "title": "Structured Generative Modeling with the Thermodynamic Kolmogorov-Arnold\n  Model",
    "summary": "Learning an energy-based model (EBM) in the latent space of a top-down\ngenerative model offers a versatile framework for generation across multiple\ndata modalities. However, it remains unclear how its interpretability can be\nused to guide model design, improve generative quality, and reduce training\ntime. Moreover, the reliance on Langevin Monte Carlo (LMC) sampling presents\nchallenges in efficiency and sampling multimodal latent distributions. In this\nwork, we propose a novel adaptation of the Kolmogorov-Arnold representation\ntheorem for generative modeling and introduce the Thermodynamic\nKolmogorov-Arnold Model (T-KAM) to take advantage of structural and inductive\nbiases. By constraining the prior to univariate relationships, T-KAM enables\nfast and exact inference via the inverse transform method. With the low\ndimensionality of the latent space and suitable inductive biases encoded, we\ndemonstrate that importance sampling (IS) becomes a viable, unbiased, and\nhighly efficient posterior sampler. For situations where IS fails, we\ninvestigate a novel strategy using population-based LMC, which decomposes\nposterior sampling into a sequence of annealed distributions to improve\nmultimodal sampling. T-KAM elegantly balances common trade-offs in generative\nmodeling, offering fast inference, interpretability, and stable training, while\nbeing naturally suited to upcoming Zettascale Computing Corp. hardware.",
    "published": "2025-06-17T04:07:32Z",
    "updated": "2025-10-13T07:37:27Z",
    "link": "http://arxiv.org/pdf/2506.14167v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Prithvi Raj"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11084v1",
    "title": "Causal Disentanglement Learning for Accurate Anomaly Detection in\n  Multivariate Time Series",
    "summary": "Disentangling complex causal relationships is important for accurate\ndetection of anomalies. In multivariate time series analysis, dynamic\ninteractions among data variables over time complicate the interpretation of\ncausal relationships. Traditional approaches assume statistical independence\nbetween variables in unsupervised settings, whereas recent methods capture\nfeature correlations through graph representation learning. However, their\nrepresentations fail to explicitly infer the causal relationships over\ndifferent time periods. To solve the problem, we propose Causally Disentangled\nRepresentation Learning for Anomaly Detection (CDRL4AD) to detect anomalies and\nidentify their causal relationships in multivariate time series. First, we\ndesign the causal process as model input, the temporal heterogeneous graph, and\ncausal relationships. Second, our representation identifies causal\nrelationships over different time periods and disentangles latent variables to\ninfer the corresponding causal factors. Third, our experiments on real-world\ndatasets demonstrate that CDRL4AD outperforms state-of-the-art methods in terms\nof accuracy and root cause analysis. Fourth, our model analysis validates\nhyperparameter sensitivity and the time complexity of CDRL4AD. Last, we conduct\na case study to show how our approach assists human experts in diagnosing the\nroot causes of anomalies.",
    "published": "2025-10-13T07:26:20Z",
    "updated": "2025-10-13T07:26:20Z",
    "link": "http://arxiv.org/pdf/2510.11084v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wonah Kim",
      "Jeonghyeon Park",
      "Dongsan Jun",
      "Jungkyu Han",
      "Sejin Chun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18470v2",
    "title": "Discrete-Time Diffusion-Like Models for Speech Synthesis",
    "summary": "Diffusion models have attracted a lot of attention in recent years. These\nmodels view speech generation as a continuous-time process. For efficient\ntraining, this process is typically restricted to additive Gaussian noising,\nwhich is limiting. For inference, the time is typically discretized, leading to\nthe mismatch between continuous training and discrete sampling conditions.\nRecently proposed discrete-time processes, on the other hand, usually do not\nhave these limitations, may require substantially fewer inference steps, and\nare fully consistent between training/inference conditions. This paper explores\nsome diffusion-like discrete-time processes and proposes some new variants.\nThese include processes applying additive Gaussian noise, multiplicative\nGaussian noise, blurring noise and a mixture of blurring and Gaussian noises.\nThe experimental results suggest that discrete-time processes offer comparable\nsubjective and objective speech quality to their widely popular continuous\ncounterpart, with more efficient and consistent training and inference schemas.",
    "published": "2025-09-22T23:19:24Z",
    "updated": "2025-10-13T07:19:10Z",
    "link": "http://arxiv.org/pdf/2509.18470v2.pdf",
    "category": [
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Xiaozhou Tan",
      "Minghui Zhao",
      "Anton Ragni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11072v1",
    "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene\n  Interaction System",
    "summary": "Deploying humanoid robots to interact with real-world environments--such as\ncarrying objects or sitting on chairs--requires generalizable, lifelike motions\nand robust scene perception. Although prior approaches have advanced each\ncapability individually, combining them in a unified system is still an ongoing\nchallenge. In this work, we present a physical-world humanoid-scene interaction\nsystem, PhysHSI, that enables humanoids to autonomously perform diverse\ninteraction tasks while maintaining natural and lifelike behaviors. PhysHSI\ncomprises a simulation training pipeline and a real-world deployment system. In\nsimulation, we adopt adversarial motion prior-based policy learning to imitate\nnatural humanoid-scene interaction data across diverse scenarios, achieving\nboth generalization and lifelike behaviors. For real-world deployment, we\nintroduce a coarse-to-fine object localization module that combines LiDAR and\ncamera inputs to provide continuous and robust scene perception. We validate\nPhysHSI on four representative interactive tasks--box carrying, sitting, lying,\nand standing up--in both simulation and real-world settings, demonstrating\nconsistently high success rates, strong generalization across diverse task\ngoals, and natural motion patterns.",
    "published": "2025-10-13T07:11:37Z",
    "updated": "2025-10-13T07:11:37Z",
    "link": "http://arxiv.org/pdf/2510.11072v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Huayi Wang",
      "Wentao Zhang",
      "Runyi Yu",
      "Tao Huang",
      "Junli Ren",
      "Feiyu Jia",
      "Zirui Wang",
      "Xiaojie Niu",
      "Xiao Chen",
      "Jiahe Chen",
      "Qifeng Chen",
      "Jingbo Wang",
      "Jiangmiao Pang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11068v1",
    "title": "Efficient Edge Test-Time Adaptation via Latent Feature Coordinate\n  Correction",
    "summary": "Edge devices face significant challenges due to limited computational\nresources and distribution shifts, making efficient and adaptable machine\nlearning essential. Existing test-time adaptation (TTA) methods often rely on\ngradient-based optimization or batch processing, which are inherently\nunsuitable for resource-constrained edge scenarios due to their reliance on\nbackpropagation and high computational demands. Gradient-free alternatives\naddress these issues but often suffer from limited learning capacity, lack\nflexibility, or impose architectural constraints. To overcome these\nlimitations, we propose a novel single-instance TTA method tailored for edge\ndevices (TED), which employs forward-only coordinate optimization in the\nprincipal subspace of latent using the covariance matrix adaptation evolution\nstrategy (CMA-ES). By updating a compact low-dimensional vector, TED not only\nenhances output confidence but also aligns the latent representation closer to\nthe source latent distribution within the latent principal subspace. This is\nachieved without backpropagation, keeping the model parameters frozen, and\nenabling efficient, forgetting-free adaptation with minimal memory and\ncomputational overhead. Experiments on image classification and keyword\nspotting tasks across the ImageNet and Google Speech Commands series datasets\ndemonstrate that TED achieves state-of-the-art performance while\n$\\textit{reducing computational complexity by up to 63 times}$, offering a\npractical and scalable solution for real-world edge applications. Furthermore,\nwe successfully $\\textit{deployed TED on the ZYNQ-7020 platform}$,\ndemonstrating its feasibility and effectiveness for resource-constrained edge\ndevices in real-world deployments.",
    "published": "2025-10-13T07:08:52Z",
    "updated": "2025-10-13T07:08:52Z",
    "link": "http://arxiv.org/pdf/2510.11068v1.pdf",
    "category": [
      "cs.LG",
      "eess.AS",
      "eess.IV"
    ],
    "authors": [
      "Xinyu Luo",
      "Jie Liu",
      "Kecheng Chen",
      "Junyi Yang",
      "Bo Ding",
      "Arindam Basu",
      "Haoliang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13881v6",
    "title": "TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias\n  Intrinsically from Regression Models in Recommender Systems",
    "summary": "Regression models are crucial in recommender systems. However,\nretransformation bias problem has been conspicuously neglected within the\ncommunity. While many works in other fields have devised effective bias\ncorrection methods, all of them are post-hoc cures externally to the model,\nfacing practical challenges when applied to real-world recommender systems.\nHence, we propose a preemptive paradigm to eradicate the bias intrinsically\nfrom the models via minor model refinement. Specifically, a novel TranSUN\nmethod is proposed with a joint bias learning manner to offer theoretically\nguaranteed unbiasedness under empirical superior convergence. It is further\ngeneralized into a novel generic regression model family, termed Generalized\nTranSUN (GTS), which not only offers more theoretical insights but also serves\nas a generic framework for flexibly developing various bias-free models.\nComprehensive experimental results demonstrate the superiority of our methods\nacross data from various domains, which have been successfully deployed in two\nreal-world industrial recommendation scenarios, i.e. product and short video\nrecommendation scenarios in Guess What You Like business domain in the homepage\nof Taobao App (a leading e-commerce platform with DAU > 300M), to serve the\nmajor online traffic.",
    "published": "2025-05-20T03:36:54Z",
    "updated": "2025-10-13T07:05:05Z",
    "link": "http://arxiv.org/pdf/2505.13881v6.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Jiahao Yu",
      "Haozhuang Liu",
      "Yeqiu Yang",
      "Lu Chen",
      "Jian Wu",
      "Yuning Jiang",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11062v1",
    "title": "Stronger Together: On-Policy Reinforcement Learning for Collaborative\n  LLMs",
    "summary": "Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to\nenhance the agentic capabilities of large language models (LLMs). MAS improves\ntask performance through role-based orchestration, while RL uses environmental\nrewards to learn stronger policies, such as GRPO-style optimization. However,\napplying on-policy RL to MAS remains underexplored and presents unique\nchallenges. Algorithmically, standard GRPO grouping assumptions break down\nbecause prompts vary by role and by turn. System-wise, the training stack must\nsupport MAS-workflow rollouts and on-policy updates for both single-policy and\nmulti-policy models.\n  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL\nalgorithm tailored to MAS and (ii) a training system that supports both single-\nand multi-policy regimes. Across game, planning, coding, and math tasks,\nAT-GRPO delivers substantial gains. On long-horizon planning, it increases\naccuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5\npercent. It also improves reasoning performance, with average gains of 3.87 to\n7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and\nenvironments are available at: https://github.com/pettingllms-ai/PettingLLMs.",
    "published": "2025-10-13T06:55:09Z",
    "updated": "2025-10-13T06:55:09Z",
    "link": "http://arxiv.org/pdf/2510.11062v1.pdf",
    "category": [
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Yujie Zhao",
      "Lanxiang Hu",
      "Yang Wang",
      "Minmin Hou",
      "Hao Zhang",
      "Ke Ding",
      "Jishen Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.09734v2",
    "title": "Modeling AdaGrad, RMSProp, and Adam with Integro-Differential Equations",
    "summary": "In this paper, we propose a continuous-time formulation for the AdaGrad,\nRMSProp, and Adam optimization algorithms by modeling them as first-order\nintegro-differential equations. We perform numerical simulations of these\nequations, along with stability and convergence analyses, to demonstrate their\nvalidity as accurate approximations of the original algorithms. Our results\nindicate a strong agreement between the behavior of the continuous-time models\nand the discrete implementations, thus providing a new perspective on the\ntheoretical understanding of adaptive optimization methods.",
    "published": "2024-11-14T19:00:01Z",
    "updated": "2025-10-13T06:50:24Z",
    "link": "http://arxiv.org/pdf/2411.09734v2.pdf",
    "category": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "math.OC"
    ],
    "authors": [
      "Carlos Heredia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11058v1",
    "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks",
    "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems.",
    "published": "2025-10-13T06:47:44Z",
    "updated": "2025-10-13T06:47:44Z",
    "link": "http://arxiv.org/pdf/2510.11058v1.pdf",
    "category": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "I Chiu",
      "Yu-Tung Liu",
      "Kuan-Chen Wang",
      "Hung-Yu Wei",
      "Yu Tsao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11057v1",
    "title": "Temporal Alignment Guidance: On-Manifold Sampling in Diffusion Models",
    "summary": "Diffusion models have achieved remarkable success as generative models.\nHowever, even a well-trained model can accumulate errors throughout the\ngeneration process. These errors become particularly problematic when arbitrary\nguidance is applied to steer samples toward desired properties, which often\nbreaks sample fidelity. In this paper, we propose a general solution to address\nthe off-manifold phenomenon observed in diffusion models. Our approach\nleverages a time predictor to estimate deviations from the desired data\nmanifold at each timestep, identifying that a larger time gap is associated\nwith reduced generation quality. We then design a novel guidance mechanism,\n`Temporal Alignment Guidance' (TAG), attracting the samples back to the desired\nmanifold at every timestep during generation. Through extensive experiments, we\ndemonstrate that TAG consistently produces samples closely aligned with the\ndesired manifold at each timestep, leading to significant improvements in\ngeneration quality across various downstream tasks.",
    "published": "2025-10-13T06:46:57Z",
    "updated": "2025-10-13T06:46:57Z",
    "link": "http://arxiv.org/pdf/2510.11057v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Youngrok Park",
      "Hojung Jung",
      "Sangmin Bae",
      "Se-Young Yun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07392v3",
    "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep\n  Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in\n  UAV Swarm Networks",
    "summary": "The proliferation of UAVs has enabled a wide range of mission-critical\napplications and is becoming a cornerstone of low-altitude networks, supporting\nsmart cities, emergency response, and more. However, the open wireless\nenvironment, dynamic topology, and resource constraints of UAVs expose\nlow-altitude networks to severe DoS threats. Traditional defense approaches,\nwhich rely on fixed configurations or centralized decision-making, cannot\neffectively respond to the rapidly changing conditions in UAV swarm\nenvironments. To address these challenges, we propose a novel federated\nmulti-agent deep reinforcement learning (FMADRL)-driven moving target defense\n(MTD) framework for proactive DoS mitigation in low-altitude networks.\nSpecifically, we design lightweight and coordinated MTD mechanisms, including\nleader switching, route mutation, and frequency hopping, to disrupt attacker\nefforts and enhance network resilience. The defense problem is formulated as a\nmulti-agent partially observable Markov decision process, capturing the\nuncertain nature of UAV swarms under attack. Each UAV is equipped with a policy\nagent that autonomously selects MTD actions based on partial observations and\nlocal experiences. By employing a policy gradient-based algorithm, UAVs\ncollaboratively optimize their policies via reward-weighted aggregation.\nExtensive simulations demonstrate that our approach significantly outperforms\nstate-of-the-art baselines, achieving up to a 34.6% improvement in attack\nmitigation rate, a reduction in average recovery time of up to 94.6%, and\ndecreases in energy consumption and defense cost by as much as 29.3% and 98.3%,\nrespectively, under various DoS attack strategies. These results highlight the\npotential of intelligent, distributed defense mechanisms to protect\nlow-altitude networks, paving the way for reliable and scalable low-altitude\neconomy.",
    "published": "2025-06-09T03:33:04Z",
    "updated": "2025-10-13T06:45:46Z",
    "link": "http://arxiv.org/pdf/2506.07392v3.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "68",
      "F.2.2"
    ],
    "authors": [
      "Yuyang Zhou",
      "Guang Cheng",
      "Kang Du",
      "Zihan Chen",
      "Tian Qin",
      "Yuyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18171v2",
    "title": "FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation\n  Method for Domain-Robust Federated Graph Learning on Node Classification",
    "summary": "Federated Graph Learning (FGL) under domain skew -- as observed on platforms\nsuch as \\emph{Twitch Gamers} and multilingual \\emph{Wikipedia} networks --\ndrives client models toward incompatible representations, rendering naive\naggregation both unstable and ineffective. We find that the culprit is not the\nweighting scheme but the \\emph{noisy gradient signal}: empirical analysis of\nbaseline methods suggests that a vast majority of gradient dimensions can be\ndominated by domain-specific variance. We therefore shift focus from\n\"aggregation-first\" to a \\emph{projection-first} strategy that denoises client\nupdates \\emph{before} they are combined. The proposed FedIA framework realises\nthis \\underline{I}mportance-\\underline{A}ware idea through a two-stage,\nplug-and-play pipeline: (i) a server-side top-$\\rho$ mask keeps only the most\ninformative about 5% of coordinates, and (ii) a lightweight\ninfluence-regularised momentum weight suppresses outlier clients. FedIA adds\n\\emph{no extra uplink traffic and only negligible server memory}, making it\nreadily deployable. On both homogeneous (Twitch Gamers) and heterogeneous\n(Wikipedia) graphs, it yields smoother, more stable convergence and higher\nfinal accuracy than nine strong baselines. A convergence sketch further shows\nthat dynamic projection maintains the optimal\n$\\mathcal{O}(\\sigma^{2}/\\sqrt{T})$ rate.",
    "published": "2025-09-17T13:04:11Z",
    "updated": "2025-10-13T06:41:25Z",
    "link": "http://arxiv.org/pdf/2509.18171v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhanting Zhou",
      "KaHou Tam",
      "Zeqin Wu",
      "Pengzhao Sun",
      "Jinbo Wang",
      "Fengli Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.03941v3",
    "title": "Discovering and Reasoning of Causality in the Hidden World with Large\n  Language Models",
    "summary": "Revealing hidden causal variables alongside the underlying causal mechanisms\nis essential to the development of science. Despite the progress in the past\ndecades, existing practice in causal discovery (CD) heavily relies on\nhigh-quality measured variables, which are usually given by human experts. In\nfact, the lack of well-defined high-level variables behind unstructured data\nhas been a longstanding roadblock to a broader real-world application of CD.\nThis procedure can naturally benefit from an automated process that can suggest\npotential hidden variables in the system. Interestingly, Large language models\n(LLMs) are trained on massive observations of the world and have demonstrated\ngreat capability in processing unstructured data. To leverage the power of\nLLMs, we develop a new framework termed Causal representatiOn AssistanT (COAT)\nthat incorporates the rich world knowledge of LLMs to propose useful measured\nvariables for CD with respect to high-value target variables on their paired\nunstructured data. Instead of directly inferring causality with LLMs, COAT\nconstructs feedback from intermediate CD results to LLMs to refine the proposed\nvariables. Given the target variable and the paired unstructured data, we first\ndevelop COAT-MB that leverages the predictivity of the proposed variables to\niteratively uncover the Markov Blanket of the target variable. Built upon\nCOAT-MB, COAT-PAG further extends to uncover a more complete causal graph,\ni.e., Partial Ancestral Graph, by iterating over the target variables and\nactively seeking new high-level variables. Moreover, the reliable CD\ncapabilities of COAT also extend the debiased causal inference to unstructured\ndata by discovering an adjustment set. We establish theoretical guarantees for\nthe CD results and verify their efficiency and reliability across realistic\nbenchmarks and real-world case studies.",
    "published": "2024-02-06T12:18:54Z",
    "updated": "2025-10-13T06:34:20Z",
    "link": "http://arxiv.org/pdf/2402.03941v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "authors": [
      "Chenxi Liu",
      "Yongqiang Chen",
      "Tongliang Liu",
      "Mingming Gong",
      "James Cheng",
      "Bo Han",
      "Kun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11049v1",
    "title": "Conformal Inference for Time Series over Graphs",
    "summary": "Trustworthy decision making in networked, dynamic environments calls for\ninnovative uncertainty quantification substrates in predictive models for graph\ntime series. Existing conformal prediction (CP) methods have been applied\nseparately to multivariate time series and static graphs, but they either\nignore the underlying graph topology or neglect temporal dynamics. To bridge\nthis gap, here we develop a CP-based sequential prediction region framework\ntailored for graph time series. A key technical innovation is to leverage the\ngraph structure and thus capture pairwise dependencies across nodes, while\nproviding user-specified coverage guarantees on the predictive outcomes. We\nformally establish that our scheme yields an exponential shrinkage in the\nvolume of the ellipsoidal prediction set relative to its graph-agnostic\ncounterpart. Using real-world datasets, we demonstrate that the novel\nuncertainty quantification framework maintains desired empirical coverage while\nachieving markedly smaller (up to 80% reduction) prediction regions than\nexisting approaches.",
    "published": "2025-10-13T06:32:09Z",
    "updated": "2025-10-13T06:32:09Z",
    "link": "http://arxiv.org/pdf/2510.11049v1.pdf",
    "category": [
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Sonakshi Dua",
      "Gonzalo Mateos",
      "Sundeep Prabhakar Chepuri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09294v2",
    "title": "Mitigating Model Drift in Developing Economies Using Synthetic Data and\n  Outliers",
    "summary": "Machine Learning models in finance are highly susceptible to model drift,\nwhere predictive performance declines as data distributions shift. This issue\nis especially acute in developing economies such as those in Central Asia and\nthe Caucasus - including Tajikistan, Uzbekistan, Kazakhstan, and Azerbaijan -\nwhere frequent and unpredictable macroeconomics shocks destabilize financial\ndata. To the best of our knowledge, this is among the first studies to examine\ndrift mitigation methods on financial datasets from these regions. We\ninvestigate the use of synthetic outliers, a largely unexplored approach, to\nimprove model stability against unforeseen shocks. To evaluate effectiveness,\nwe introduce a two-level framework that measures both the extent of performance\ndegradation and the severity of shocks. Our experiments on macroeconomic\ntabular datasets show that adding a small proportion of synthetic outliers\ngenerally improves stability compared to baseline models, though the optimal\namount varies by dataset and model",
    "published": "2025-10-10T11:36:49Z",
    "updated": "2025-10-13T06:30:24Z",
    "link": "http://arxiv.org/pdf/2510.09294v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ilyas Varshavskiy",
      "Bonu Boboeva",
      "Shuhrat Khalilbekov",
      "Azizjon Azimi",
      "Sergey Shulgin",
      "Akhlitdin Nizamitdinov",
      "Haitz Sáez de Ocáriz Borde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25263v2",
    "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A\n  Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data",
    "summary": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.",
    "published": "2025-09-28T03:21:24Z",
    "updated": "2025-10-13T06:17:07Z",
    "link": "http://arxiv.org/pdf/2509.25263v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph",
      "stat.ML"
    ],
    "authors": [
      "Yifang Zhang",
      "Pengfei Duan",
      "Henan Wang",
      "Wenjie Yin",
      "Chen Zhou",
      "Shengwu Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14741v2",
    "title": "Communication-Efficient Diffusion Denoising Parallelization via\n  Reuse-then-Predict Mechanism",
    "summary": "Diffusion models have emerged as a powerful class of generative models across\nvarious modalities, including image, video, and audio synthesis. However, their\ndeployment is often limited by significant inference latency, primarily due to\nthe inherently sequential nature of the denoising process. While existing\nparallelization strategies attempt to accelerate inference by distributing\ncomputation across multiple devices, they typically incur high communication\noverhead, hindering deployment on commercial hardware. To address this\nchallenge, we propose \\textbf{ParaStep}, a novel parallelization method based\non a reuse-then-predict mechanism that parallelizes diffusion inference by\nexploiting similarity between adjacent denoising steps. Unlike prior approaches\nthat rely on layer-wise or stage-wise communication, ParaStep employs\nlightweight, step-wise communication, substantially reducing overhead. ParaStep\nachieves end-to-end speedups of up to \\textbf{3.88}$\\times$ on SVD,\n\\textbf{2.43}$\\times$ on CogVideoX-2b, and \\textbf{6.56}$\\times$ on\nAudioLDM2-large, while maintaining generation quality. These results highlight\nParaStep as a scalable and communication-efficient solution for accelerating\ndiffusion inference, particularly in bandwidth-constrained environments.",
    "published": "2025-05-20T06:58:40Z",
    "updated": "2025-10-13T06:04:36Z",
    "link": "http://arxiv.org/pdf/2505.14741v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kunyun Wang",
      "Bohan Li",
      "Kai Yu",
      "Minyi Guo",
      "Jieru Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06940v2",
    "title": "Revisiting Node Affinity Prediction in Temporal Graphs",
    "summary": "Node affinity prediction is a common task that is widely used in temporal\ngraph learning with applications in social and financial networks, recommender\nsystems, and more. Recent works have addressed this task by adapting\nstate-of-the-art dynamic link property prediction models to node affinity\nprediction. However, simple heuristics, such as Persistent Forecast or Moving\nAverage, outperform these models. In this work, we analyze the challenges in\ntraining current Temporal Graph Neural Networks for node affinity prediction\nand suggest appropriate solutions. Combining the solutions, we develop NAViS -\nNode Affinity prediction model using Virtual State, by exploiting the\nequivalence between heuristics and state space models. While promising,\ntraining NAViS is non-trivial. Therefore, we further introduce a novel loss\nfunction for node affinity prediction. We evaluate NAViS on TGB and show that\nit outperforms the state-of-the-art, including heuristics. Our source code is\navailable at https://github.com/orfeld415/NAVIS",
    "published": "2025-10-08T12:21:52Z",
    "updated": "2025-10-13T05:39:12Z",
    "link": "http://arxiv.org/pdf/2510.06940v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Krishna Sri Ipsit Mantri",
      "Or Feldman",
      "Moshe Eliasof",
      "Chaim Baskin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11016v1",
    "title": "Instruction-aware User Embedding via Synergistic Language and\n  Representation Modeling",
    "summary": "User representation modeling has become increasingly crucial for personalized\napplications, yet existing approaches struggle with generalizability across\ndomains and sensitivity to noisy behavioral signals. We present InstructUE, an\ninstruction-aware user embedding foundation model that leverages large language\nmodels (LLMs) to generate general and instruction-aware user representations.\nInstructUE introduces a multi-encoder architecture with a lightweight adapter\nthat efficiently processes heterogeneous data from six different sources while\npreserving their structural characteristics. Additionally, it proposes a novel\ncontrastive-autoregressive training framework that bridges language and\nrepresentation spaces through a curated UserQA dataset. The\ncontrastive-autoregressive training framework simultaneously leverages\nautoregressive learning to capture domain knowledge in language space and\ncontrastive learning to align user-text embeddings in representation space,\nthereby enhancing the instruction-awareness and noise-robustness of user\nembeddings. Through extensive experiments on real-world applications, we\ndemonstrate that InstructUE significantly outperforms existing methods across\nmultiple domains including user prediction, marketing, and recommendation\nscenarios. Our results show that instruction-aware user modeling can\neffectively achieve instruction-guided denoising of user information in\nspecific scenarios, paving the way for more generalizable and robust user\nrepresentation learning.",
    "published": "2025-10-13T05:15:34Z",
    "updated": "2025-10-13T05:15:34Z",
    "link": "http://arxiv.org/pdf/2510.11016v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ziyi Gao",
      "Yike Xu",
      "Jiahao Yuan",
      "Baokun Wang",
      "Jinyong Wen",
      "Xiaotong Lin",
      "Yun Liu",
      "Xing Fu",
      "Yu Cheng",
      "Yongchao Liu",
      "Weiqiang Wang",
      "Zhongle Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11011v1",
    "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
    "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
    "published": "2025-10-13T05:03:23Z",
    "updated": "2025-10-13T05:03:23Z",
    "link": "http://arxiv.org/pdf/2510.11011v1.pdf",
    "category": [
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Farzaneh Zirak",
      "Farhana Choudhury",
      "Renata Borovica-Gajic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25279v2",
    "title": "RL in the Wild: Characterizing RLVR Training in LLM Deployment",
    "summary": "Large Language Models (LLMs) are now widely used across many domains. With\ntheir rapid development, Reinforcement Learning with Verifiable Rewards (RLVR)\nhas surged in recent months to enhance their reasoning and understanding\nabilities. However, its complex data flows and diverse tasks pose substantial\nchallenges to RL training systems, and there is limited understanding of RLVR\nfrom a system perspective. To thoroughly understand the system challenges\nintroduced by RLVR, we present a characterization study of RLVR tasks in our\nLLM deployment. Specifically, we investigate the distribution and variation\ntrends of workloads across different RL tasks across training steps. We\nidentify issues such as GPU idling caused by skewed sequence length\ndistribution, inefficient parallel strategies in dynamically varying workloads,\ninefficient data management mechanisms, and load imbalance. We describe our\nobservations and call for further investigation into the remaining open\nchallenges. Furthermore, we propose PolyTrace benchmark suite to conduct\nevaluation with realistic workloads, and a practical use case validates that\nPolyTrace benchmark suite exhibits 94.7% accuracy.",
    "published": "2025-09-29T03:09:27Z",
    "updated": "2025-10-13T05:01:17Z",
    "link": "http://arxiv.org/pdf/2509.25279v2.pdf",
    "category": [
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Jiecheng Zhou",
      "Qinghao Hu",
      "Yuyang Jin",
      "Zerui Wang",
      "Peng Sun",
      "Yuzhe Gu",
      "Wenwei Zhang",
      "Mingshu Zhai",
      "Xingcheng Zhang",
      "Weiming Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.23258v2",
    "title": "Joint Source-Environment Adaptation of Data-Driven Underwater Acoustic\n  Source Ranging Based on Model Uncertainty",
    "summary": "Adapting pre-trained deep learning models to new and unknown environments\nremains a major challenge in underwater acoustic localization. We show that\nalthough the performance of pre-trained models suffers from mismatch between\nthe training and test data, they generally exhibit a higher uncertainty in\nenvironments where there is more mismatch. Additionally, in the presence of\nenvironmental mismatch, spurious peaks can appear in the output of\nclassification-based localization approaches, which inspires us to define and\nuse a method to quantify the \"implied uncertainty\" based on the number of model\noutput peaks. Leveraging this notion of implied uncertainty, we partition the\ntest samples into sets with more certain and less certain samples, and\nimplement a method to adapt the model to new environments by using the certain\nsamples to improve the labeling for uncertain samples, which helps to adapt the\nmodel. Thus, using this efficient method for model uncertainty quantification,\nwe showcase an innovative approach to adapt a pre-trained model to unseen\nunderwater environments at test time. This eliminates the need for labeled data\nfrom the target environment or the original training data. This adaptation is\nenhanced by integrating an independent estimate based on the received signal\nenergy. We validate the approach extensively using real experimental data, as\nwell as synthetic data consisting of model-generated signals with real ocean\nnoise. The results demonstrate significant improvements in model prediction\naccuracy, underscoring the potential of the method to enhance underwater\nacoustic localization in diverse, noisy, and unknown environments.",
    "published": "2025-03-30T00:00:17Z",
    "updated": "2025-10-13T05:00:41Z",
    "link": "http://arxiv.org/pdf/2503.23258v2.pdf",
    "category": [
      "cs.SD",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "authors": [
      "Dariush Kari",
      "Hari Vishnu",
      "Andrew C. Singer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24183v4",
    "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while even exceeding the\nperformance of 671B DeepSeek-R1 on RTLLM. We have released our model, training\ncode, and dataset to facilitate research in EDA and LLM communities.",
    "published": "2025-05-30T03:51:06Z",
    "updated": "2025-10-13T04:22:09Z",
    "link": "http://arxiv.org/pdf/2505.24183v4.pdf",
    "category": [
      "cs.LG",
      "cs.AR",
      "cs.PL"
    ],
    "authors": [
      "Yaoyu Zhu",
      "Di Huang",
      "Hanqi Lyu",
      "Xiaoyun Zhang",
      "Chongxiao Li",
      "Wenxuan Shi",
      "Yutong Wu",
      "Jianan Mu",
      "Jinghua Wang",
      "Yang Zhao",
      "Pengwei Jin",
      "Shuyao Cheng",
      "Shengwen Liang",
      "Xishan Zhang",
      "Rui Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu",
      "Yunji Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21027v2",
    "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data",
    "summary": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks are well studied in unstructured domains such as\nimages, their behaviour on tabular data remains underexplored due to mixed\nfeature types and complex inter-feature dependencies. This study introduces a\ncomprehensive benchmark that evaluates adversarial attacks on tabular datasets\nwith respect to both effectiveness and imperceptibility. We assess five\nwhite-box attack algorithms (FGSM, BIM, PGD, DeepFool, and C\\&W) across four\nrepresentative models (LR, MLP, TabTransformer and FT-Transformer) using eleven\ndatasets spanning finance, energy, and healthcare domains. The benchmark\nemploys four quantitative imperceptibility metrics (proximity, sparsity,\ndeviation, and sensitivity) to characterise perturbation realism. The analysis\nquantifies the trade-off between these two aspects and reveals consistent\ndifferences between attack types, with $\\ell_\\infty$-based attacks achieving\nhigher success but lower subtlety, and $\\ell_2$-based attacks offering more\nrealistic perturbations. The benchmark findings offer actionable insights for\ndesigning more imperceptible adversarial attacks, advancing the understanding\nof adversarial vulnerability in tabular machine learning.",
    "published": "2025-05-27T11:01:32Z",
    "updated": "2025-10-13T03:56:19Z",
    "link": "http://arxiv.org/pdf/2505.21027v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhipeng He",
      "Chun Ouyang",
      "Lijie Wen",
      "Cong Liu",
      "Catarina Moreira"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10988v1",
    "title": "Adversarial Robustness in One-Stage Learning-to-Defer",
    "summary": "Learning-to-Defer (L2D) enables hybrid decision-making by routing inputs\neither to a predictor or to external experts. While promising, L2D is highly\nvulnerable to adversarial perturbations, which can not only flip predictions\nbut also manipulate deferral decisions. Prior robustness analyses focus solely\non two-stage settings, leaving open the end-to-end (one-stage) case where\npredictor and allocation are trained jointly. We introduce the first framework\nfor adversarial robustness in one-stage L2D, covering both classification and\nregression. Our approach formalizes attacks, proposes cost-sensitive\nadversarial surrogate losses, and establishes theoretical guarantees including\n$\\mathcal{H}$, $(\\mathcal{R }, \\mathcal{F})$, and Bayes consistency.\nExperiments on benchmark datasets confirm that our methods improve robustness\nagainst untargeted and targeted attacks while preserving clean performance.",
    "published": "2025-10-13T03:55:55Z",
    "updated": "2025-10-13T03:55:55Z",
    "link": "http://arxiv.org/pdf/2510.10988v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Yannis Montreuil",
      "Letian Yu",
      "Axel Carlier",
      "Lai Xing Ng",
      "Wei Tsang Ooi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10982v1",
    "title": "Catch-Only-One: Non-Transferable Examples for Model-Specific\n  Authorization",
    "summary": "Recent AI regulations call for data that remain useful for innovation while\nresistant to misuse, balancing utility with protection at the model level.\nExisting approaches either perturb data to make it unlearnable or retrain\nmodels to suppress transfer, but neither governs inference by unknown models,\nand both typically require control over training. We propose non-transferable\nexamples (NEs), a training-free and data-agnostic input-side usage-control\nmechanism. We recode inputs within a model-specific low-sensitivity subspace,\npreserving outputs for the authorized model while reducing performance on\nunauthorized models through subspace misalignment. We establish formal bounds\nthat guarantee utility for the authorized model and quantify deviation for\nunauthorized ones, with the Hoffman-Wielandt inequality linking degradation to\nspectral differences. Empirically, NEs retain performance on diverse vision\nbackbones and state-of-the-art vision-language models under common\npreprocessing, whereas non-target models collapse even with reconstruction\nattempts. These results establish NEs as a practical means to preserve intended\ndata utility while preventing unauthorized exploitation. Our project is\navailable at https://trusted-system-lab.github.io/model-specificity",
    "published": "2025-10-13T03:43:11Z",
    "updated": "2025-10-13T03:43:11Z",
    "link": "http://arxiv.org/pdf/2510.10982v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zihan Wang",
      "Zhiyong Ma",
      "Zhongkui Ma",
      "Shuofeng Liu",
      "Akide Liu",
      "Derui Wang",
      "Minhui Xue",
      "Guangdong Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10981v1",
    "title": "In-Context Learning Is Provably Bayesian Inference: A Generalization\n  Theory for Meta-Learning",
    "summary": "This paper develops a finite-sample statistical theory for in-context\nlearning (ICL), analyzed within a meta-learning framework that accommodates\nmixtures of diverse task types. We introduce a principled risk decomposition\nthat separates the total ICL risk into two orthogonal components: Bayes Gap and\nPosterior Variance. The Bayes Gap quantifies how well the trained model\napproximates the Bayes-optimal in-context predictor. For a uniform-attention\nTransformer, we derive a non-asymptotic upper bound on this gap, which\nexplicitly clarifies the dependence on the number of pretraining prompts and\ntheir context length. The Posterior Variance is a model-independent risk\nrepresenting the intrinsic task uncertainty. Our key finding is that this term\nis determined solely by the difficulty of the true underlying task, while the\nuncertainty arising from the task mixture vanishes exponentially fast with only\na few in-context examples. Together, these results provide a unified view of\nICL: the Transformer selects the optimal meta-algorithm during pretraining and\nrapidly converges to the optimal algorithm for the true task at test time.",
    "published": "2025-10-13T03:42:31Z",
    "updated": "2025-10-13T03:42:31Z",
    "link": "http://arxiv.org/pdf/2510.10981v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Tomoya Wakayama",
      "Taiji Suzuki"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12988v4",
    "title": "Why Ask One When You Can Ask $k$? Learning-to-Defer to the Top-$k$\n  Experts",
    "summary": "Existing Learning-to-Defer (L2D) frameworks are limited to single-expert\ndeferral, forcing each query to rely on only one expert and preventing the use\nof collective expertise. We introduce the first framework for Top-$k$\nLearning-to-Defer, which allocates queries to the $k$ most cost-effective\nentities. Our formulation unifies and strictly generalizes prior approaches,\nincluding the one-stage and two-stage regimes, selective prediction, and\nclassical cascades. In particular, it recovers the usual Top-1 deferral rule as\na special case while enabling principled collaboration with multiple experts\nwhen $k>1$. We further propose Top-$k(x)$ Learning-to-Defer, an adaptive\nvariant that learns the optimal number of experts per query based on input\ndifficulty, expert quality, and consultation cost. To enable practical\nlearning, we develop a novel surrogate loss that is Bayes-consistent,\n$\\mathcal{H}_h$-consistent in the one-stage setting, and\n$(\\mathcal{H}_r,\\mathcal{H}_g)$-consistent in the two-stage setting. Crucially,\nthis surrogate is independent of $k$, allowing a single policy to be learned\nonce and deployed flexibly across $k$. Experiments across both regimes show\nthat Top-$k$ and Top-$k(x)$ deliver superior accuracy-cost trade-offs, opening\na new direction for multi-expert deferral in L2D.",
    "published": "2025-04-17T14:50:40Z",
    "updated": "2025-10-13T03:36:20Z",
    "link": "http://arxiv.org/pdf/2504.12988v4.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Yannis Montreuil",
      "Axel Carlier",
      "Lai Xing Ng",
      "Wei Tsang Ooi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09222v2",
    "title": "FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in\n  Reinforcement Learning",
    "summary": "Flow Matching (FM) has shown remarkable ability in modeling complex\ndistributions and achieves strong performance in offline imitation learning for\ncloning expert behaviors. However, despite its behavioral cloning\nexpressiveness, FM-based policies are inherently limited by their lack of\nenvironmental interaction and exploration. This leads to poor generalization in\nunseen scenarios beyond the expert demonstrations, underscoring the necessity\nof online interaction with environment. Unfortunately, optimizing FM policies\nvia online interaction is challenging and inefficient due to instability in\ngradient computation and high inference costs. To address these issues, we\npropose to let a student policy with simple MLP structure explore the\nenvironment and be online updated via RL algorithm with a reward model. This\nreward model is associated with a teacher FM model, containing rich information\nof expert data distribution. Furthermore, the same teacher FM model is utilized\nto regularize the student policy's behavior to stabilize policy learning. Due\nto the student's simple architecture, we avoid the gradient instability of FM\npolicies and enable efficient online exploration, while still leveraging the\nexpressiveness of the teacher FM model. Extensive experiments show that our\napproach significantly enhances learning efficiency, generalization, and\nrobustness, especially when learning from suboptimal expert data.",
    "published": "2025-10-10T10:08:10Z",
    "updated": "2025-10-13T03:31:50Z",
    "link": "http://arxiv.org/pdf/2510.09222v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zhenglin Wan",
      "Jingxuan Wu",
      "Xingrui Yu",
      "Chubin Zhang",
      "Mingcong Lei",
      "Bo An",
      "Ivor Tsang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10968v1",
    "title": "Blade: A Derivative-free Bayesian Inversion Method using Diffusion\n  Priors",
    "summary": "Derivative-free Bayesian inversion is an important task in many science and\nengineering applications, particularly when computing the forward model\nderivative is computationally and practically challenging. In this paper, we\nintroduce Blade, which can produce accurate and well-calibrated posteriors for\nBayesian inversion using an ensemble of interacting particles. Blade leverages\npowerful data-driven priors based on diffusion models, and can handle nonlinear\nforward models that permit only black-box access (i.e., derivative-free).\nTheoretically, we establish a non-asymptotic convergence analysis to\ncharacterize the effects of forward model and prior estimation errors.\nEmpirically, Blade achieves superior performance compared to existing\nderivative-free Bayesian inversion methods on various inverse problems,\nincluding challenging highly nonlinear fluid dynamics.",
    "published": "2025-10-13T03:19:44Z",
    "updated": "2025-10-13T03:19:44Z",
    "link": "http://arxiv.org/pdf/2510.10968v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Hongkai Zheng",
      "Austin Wang",
      "Zihui Wu",
      "Zhengyu Huang",
      "Ricardo Baptista",
      "Yisong Yue"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10964v1",
    "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
    "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
    "published": "2025-10-13T03:14:28Z",
    "updated": "2025-10-13T03:14:28Z",
    "link": "http://arxiv.org/pdf/2510.10964v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Junhyuck Kim",
      "Ethan Ewer",
      "Taehong Moon",
      "Jongho Park",
      "Dimitris Papailiopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10963v1",
    "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with\n  Optimal Transport",
    "summary": "The reward model (RM) plays a crucial role in aligning Large Language Models\n(LLMs) with human preferences through Reinforcement Learning, where the\nBradley-Terry (BT) objective has been recognized as simple yet powerful,\nspecifically for pairwise preference learning. However, BT-based RMs often\nstruggle to effectively distinguish between similar preference responses,\nleading to insufficient separation between preferred and non-preferred outputs.\nConsequently, they may easily overfit easy samples and cannot generalize well\nto Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To\naddress these challenges, this paper introduces an effective enhancement to\nBT-based RMs through an adaptive margin mechanism. Specifically, we design to\ndynamically adjust the RM focus on more challenging samples through margins,\nbased on both semantic similarity and model-predicted reward differences, which\nis approached from a distributional perspective solvable with Optimal Transport\n(OT). By incorporating these factors into a principled OT cost matrix design,\nour adaptive margin enables the RM to better capture distributional differences\nbetween chosen and rejected responses, yielding significant improvements in\nperformance, convergence speed, and generalization capabilities. Experimental\nresults across multiple benchmarks demonstrate that our method outperforms\nseveral existing RM techniques, showcasing enhanced performance in both\nIn-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our\npractical effectiveness in better aligning LLMs with human preferences. Our\ncode is available at https://github.com/BIRlz/APLOT",
    "published": "2025-10-13T03:13:28Z",
    "updated": "2025-10-13T03:13:28Z",
    "link": "http://arxiv.org/pdf/2510.10963v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhuo Li",
      "Yuege Feng",
      "Dandan Guo",
      "Jinpeng Hu",
      "Anningzhe Gao",
      "Xiang Wan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10962v1",
    "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models",
    "summary": "Mixture-of-Experts (MoE) effectively scales large language models (LLMs) and\nvision-language models (VLMs) by increasing capacity through sparse activation.\nHowever, preloading all experts into memory and activating multiple experts per\ninput introduces significant computational and memory overhead, making the\nexpert module a major contributor to model size and inference cost. To address\nthis, we propose MC# (Mixture-Compressor-sharp), a framework that combines\nstatic quantization and dynamic expert pruning by leveraging the significance\nof experts and tokens for aggressive compression of MoE-LLMs/VLMs. To reduce\nstorage and loading costs, we introduce Pre-Loading Mixed-Precision\nQuantization (PMQ), which optimizes bit allocation via linear programming,\nbalancing expert importance and quantization error for a Pareto-optimal\ntrade-off between size and performance. To reduce runtime computation, Online\nTop-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a\nsubset of experts per token, enabling fine-grained control over activation. By\ncombining PMQ's static bit-width optimization with OTP's dynamic routing, MC#\nachieves extreme compression with minimal accuracy loss. On DeepSeek-VL2, MC#\nachieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%\naccuracy drop across five multimodal benchmarks. Additionally, OTP reduces\nexpert activation over 20% with less than 1% performance degradation,\ndemonstrating strong potential for efficient MoE-based model deployment.",
    "published": "2025-10-13T03:12:46Z",
    "updated": "2025-10-13T03:12:46Z",
    "link": "http://arxiv.org/pdf/2510.10962v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wei Huang",
      "Yue Liao",
      "Yukang Chen",
      "Jianhui Liu",
      "Haoru Tan",
      "Si Liu",
      "Shiming Zhang",
      "Shuicheng Yan",
      "Xiaojuan Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.07631v2",
    "title": "Efficient Approximate Posterior Sampling with Annealed Langevin Monte\n  Carlo",
    "summary": "We study the problem of posterior sampling in the context of score based\ngenerative models. We have a trained score network for a prior $p(x)$, a\nmeasurement model $p(y|x)$, and are tasked with sampling from the posterior\n$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)\nunder well-accepted computational hardness assumptions. Despite this, popular\nalgorithms for tasks such as image super-resolution, stylization, and\nreconstruction enjoy empirical success. Rather than establishing distributional\nassumptions or restricted settings under which exact posterior sampling is\ntractable, we view this as a more general \"tilting\" problem of biasing a\ndistribution towards a measurement. Under minimal assumptions, we show that one\ncan tractably sample from a distribution that is simultaneously close to the\nposterior of a noised prior in KL divergence and the true posterior in Fisher\ndivergence. Intuitively, this combination ensures that the resulting sample is\nconsistent with both the measurement and the prior. To the best of our\nknowledge these are the first formal results for (approximate) posterior\nsampling in polynomial time.",
    "published": "2025-08-11T05:25:24Z",
    "updated": "2025-10-13T03:05:04Z",
    "link": "http://arxiv.org/pdf/2508.07631v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Advait Parulekar",
      "Litu Rout",
      "Karthikeyan Shanmugam",
      "Sanjay Shakkottai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10952v1",
    "title": "Interpretable Machine Learning for Cognitive Aging: Handling Missing\n  Data and Uncovering Social Determinant",
    "summary": "Early detection of Alzheimer's disease (AD) is crucial because its\nneurodegenerative effects are irreversible, and neuropathologic and\nsocial-behavioral risk factors accumulate years before diagnosis. Identifying\nhigher-risk individuals earlier enables prevention, timely care, and equitable\nresource allocation. We predict cognitive performance from social determinants\nof health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset\nderived from the nationally representative Mex-Cog cohort of the 2003 and 2012\nMexican Health and Aging Study (MHAS).\n  Data: The target is a validated composite cognitive score across seven\ndomains-orientation, memory, attention, language, constructional praxis, and\nexecutive function-derived from the 2016 and 2021 MHAS waves. Predictors span\ndemographic, socioeconomic, health, lifestyle, psychosocial, and healthcare\naccess factors.\n  Methodology: Missingness was addressed with a singular value decomposition\n(SVD)-based imputation pipeline treating continuous and categorical variables\nseparately. This approach leverages latent feature correlations to recover\nmissing values while balancing reliability and scalability. After evaluating\nmultiple methods, XGBoost was chosen for its superior predictive performance.\n  Results and Discussion: The framework outperformed existing methods and the\ndata challenge leaderboard, demonstrating high accuracy, robustness, and\ninterpretability. SHAP-based post hoc analysis identified top contributing SDOH\nfactors and age-specific feature patterns. Notably, flooring material emerged\nas a strong predictor, reflecting socioeconomic and environmental disparities.\nOther influential factors, age, SES, lifestyle, social interaction, sleep,\nstress, and BMI, underscore the multifactorial nature of cognitive aging and\nthe value of interpretable, data-driven SDOH modeling.",
    "published": "2025-10-13T03:04:10Z",
    "updated": "2025-10-13T03:04:10Z",
    "link": "http://arxiv.org/pdf/2510.10952v1.pdf",
    "category": [
      "cs.LG",
      "stat.AP"
    ],
    "authors": [
      "Xi Mao",
      "Zhendong Wang",
      "Jingyu Li",
      "Lingchao Mao",
      "Utibe Essien",
      "Hairong Wang",
      "Xuelei Sherry Ni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10938v1",
    "title": "Redundancy as a Structural Information Principle for Learning and\n  Generalization",
    "summary": "We present a theoretical framework that extends classical information theory\nto finite and structured systems by redefining redundancy as a fundamental\nproperty of information organization rather than inefficiency. In this\nframework, redundancy is expressed as a general family of informational\ndivergences that unifies multiple classical measures, such as mutual\ninformation, chi-squared dependence, and spectral redundancy, under a single\ngeometric principle. This reveals that these traditional quantities are not\nisolated heuristics but projections of a shared redundancy geometry. The theory\nfurther predicts that redundancy is bounded both above and below, giving rise\nto an optimal equilibrium that balances over-compression (loss of structure)\nand over-coupling (collapse). While classical communication theory favors\nminimal redundancy for transmission efficiency, finite and structured systems,\nsuch as those underlying real-world learning, achieve maximal stability and\ngeneralization near this equilibrium. Experiments with masked autoencoders are\nused to illustrate and verify this principle: the model exhibits a stable\nredundancy level where generalization peaks. Together, these results establish\nredundancy as a measurable and tunable quantity that bridges the asymptotic\nworld of communication and the finite world of learning.",
    "published": "2025-10-13T02:55:37Z",
    "updated": "2025-10-13T02:55:37Z",
    "link": "http://arxiv.org/pdf/2510.10938v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "authors": [
      "Yuda Bi",
      "Ying Zhu",
      "Vince D Calhoun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01977v2",
    "title": "Towards Unsupervised Training of Matching-based Graph Edit Distance\n  Solver via Preference-aware GAN",
    "summary": "Graph Edit Distance (GED) is a fundamental graph similarity metric widely\nused in various applications. However, computing GED is an NP-hard problem.\nRecent state-of-the-art hybrid GED solver has shown promising performance by\nformulating GED as a bipartite graph matching problem, then leveraging a\ngenerative diffusion model to predict node matching between two graphs, from\nwhich both the GED and its corresponding edit path can be extracted using a\ntraditional algorithm. However, such methods typically rely heavily on\nground-truth supervision, where the ground-truth node matchings are often\ncostly to obtain in real-world scenarios. In this paper, we propose GEDRanker,\na novel unsupervised GAN-based framework for GED computation. Specifically,\nGEDRanker consists of a matching-based GED solver and introduces an\ninterpretable preference-aware discriminator. By leveraging preference signals\nover different node matchings derived from edit path lengths, the discriminator\ncan guide the matching-based solver toward generating high-quality node\nmatching without the need for ground-truth supervision. Extensive experiments\non benchmark datasets demonstrate that our GEDRanker enables the matching-based\nGED solver to achieve near-optimal solution quality without any ground-truth\nsupervision.",
    "published": "2025-05-16T03:45:31Z",
    "updated": "2025-10-13T02:53:27Z",
    "link": "http://arxiv.org/pdf/2506.01977v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wei Huang",
      "Hanchen Wang",
      "Dong Wen",
      "Shaozhen Ma",
      "Wenjie Zhang",
      "Xuemin Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10937v1",
    "title": "Neutral Agent-based Adversarial Policy Learning against Deep\n  Reinforcement Learning in Multi-party Open Systems",
    "summary": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems.",
    "published": "2025-10-13T02:53:22Z",
    "updated": "2025-10-13T02:53:22Z",
    "link": "http://arxiv.org/pdf/2510.10937v1.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Qizhou Peng",
      "Yang Zheng",
      "Yu Wen",
      "Yanna Wu",
      "Yingying Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08141v2",
    "title": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in\n  Reinforcement Fine-tuning",
    "summary": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers.",
    "published": "2025-10-09T12:24:08Z",
    "updated": "2025-10-13T02:52:25Z",
    "link": "http://arxiv.org/pdf/2510.08141v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Chen Wang",
      "Zhaochun Li",
      "Jionghao Bai",
      "Yuzhi Zhang",
      "Shisheng Cui",
      "Zhou Zhao",
      "Yue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.06145v3",
    "title": "Adaptive UAV-Assisted Hierarchical Federated Learning: Optimizing\n  Energy, Latency, and Resilience for Dynamic Smart IoT",
    "summary": "Hierarchical Federated Learning (HFL) extends conventional Federated Learning\n(FL) by introducing intermediate aggregation layers, enabling distributed\nlearning in geographically dispersed environments, particularly relevant for\nsmart IoT systems, such as remote monitoring and battlefield operations, where\ncellular connectivity is limited. In these scenarios, UAVs serve as mobile\naggregators, dynamically connecting terrestrial IoT devices. This paper\ninvestigates an HFL architecture with energy-constrained, dynamically deployed\nUAVs prone to communication disruptions. We propose a novel approach to\nminimize global training costs by formulating a joint optimization problem that\nintegrates learning configuration, bandwidth allocation, and device-to-UAV\nassociation, ensuring timely global aggregation before UAV disconnections and\nredeployments. The problem accounts for dynamic IoT devices and intermittent\nUAV connectivity and is NP-hard. To tackle this, we decompose it into three\nsubproblems: \\textit{(i)} optimizing learning configuration and bandwidth\nallocation via an augmented Lagrangian to reduce training costs; \\textit{(ii)}\nintroducing a device fitness score based on data heterogeneity (via\nKullback-Leibler divergence), device-to-UAV proximity, and computational\nresources, using a TD3-based algorithm for adaptive device-to-UAV assignment;\n\\textit{(iii)} developing a low-complexity two-stage greedy strategy for UAV\nredeployment and global aggregator selection, ensuring efficient aggregation\ndespite UAV disconnections. Experiments on diverse real-world datasets validate\nthe approach, demonstrating cost reduction and robust performance under\ncommunication disruptions.",
    "published": "2025-03-08T10:06:29Z",
    "updated": "2025-10-13T02:50:18Z",
    "link": "http://arxiv.org/pdf/2503.06145v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xiaohong Yang",
      "Minghui Liwang",
      "Liqun Fu",
      "Yuhan Su",
      "Seyyedali Hosseinalipour",
      "Xianbin Wang",
      "Yiguang Hong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10160v2",
    "title": "One-Stage Top-$k$ Learning-to-Defer: Score-Based Surrogates with\n  Theoretical Guarantees",
    "summary": "We introduce the first one-stage Top-$k$ Learning-to-Defer framework, which\nunifies prediction and deferral by learning a shared score-based model that\nselects the $k$ most cost-effective entities-labels or experts-per input. While\nexisting one-stage L2D methods are limited to deferring to a single expert, our\napproach jointly optimizes prediction and deferral across multiple entities\nthrough a single end-to-end objective. We define a cost-sensitive loss and\nderive a novel convex surrogate that is independent of the cardinality\nparameter $k$, enabling generalization across Top-$k$ regimes without\nretraining. Our formulation recovers the Top-1 deferral policy of prior\nscore-based methods as a special case, and we prove that our surrogate is both\nBayes-consistent and $\\mathcal{H}$-consistent under mild assumptions. We\nfurther introduce an adaptive variant, Top-$k(x)$, which dynamically selects\nthe number of consulted entities per input to balance predictive accuracy and\nconsultation cost. Experiments on CIFAR-10 and SVHN confirm that our one-stage\nTop-$k$ method strictly outperforms Top-1 deferral, while Top-$k(x)$ achieves\nsuperior accuracy-cost trade-offs by tailoring allocations to input complexity.",
    "published": "2025-05-15T10:41:16Z",
    "updated": "2025-10-13T02:35:17Z",
    "link": "http://arxiv.org/pdf/2505.10160v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Yannis Montreuil",
      "Axel Carlier",
      "Lai Xing Ng",
      "Wei Tsang Ooi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10915v1",
    "title": "LPCVAE: A Conditional VAE with Long-Term Dependency and Probabilistic\n  Time-Frequency Fusion for Time Series Anomaly Detection",
    "summary": "Time series anomaly detection(TSAD) is a critical task in signal processing\nfield, ensuring the reliability of complex systems. Reconstruction-based\nmethods dominate in TSAD. Among these methods, VAE-based methods have achieved\npromising results. Existing VAE-based methods suffer from the limitation of\nsingle-window feature and insufficient leveraging of long-term time and\nfrequency information. We propose a Conditional Variational AutoEncoder with\nLong-term dependency and Probabilistic time-frequency fusion, named LPCVAE.\nLPCVAE introduces LSTM to capture long-term dependencies beyond windows. It\nfurther incorporates a Product-of-Experts (PoE) mechanism for adaptive and\ndistribution-level probabilistic fusion. This design effectively mitigates\ntime-frequency information loss. Extensive experiments on four public datasets\ndemonstrate it outperforms state-of-the-art methods. The results confirm that\nintegrating long-term time and frequency representations with adaptive fusion\nyields a robust and efficient solution for TSAD.",
    "published": "2025-10-13T02:27:04Z",
    "updated": "2025-10-13T02:27:04Z",
    "link": "http://arxiv.org/pdf/2510.10915v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Hanchang Cheng",
      "Weimin Mu",
      "Fan Liu",
      "Weilin Zhu",
      "Can Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.13811v4",
    "title": "Can LLMs Handle WebShell Detection? Overcoming Detection Challenges with\n  Behavioral Function-Aware Framework",
    "summary": "WebShell attacks, where malicious scripts are injected into web servers, pose\na significant cybersecurity threat. Traditional ML and DL methods are often\nhampered by challenges such as the need for extensive training data,\ncatastrophic forgetting, and poor generalization. Recently, Large Language\nModels have emerged as powerful alternatives for code-related tasks, but their\npotential in WebShell detection remains underexplored. In this paper, we make\ntwo contributions: (1) a comprehensive evaluation of seven LLMs, including\nGPT-4, LLaMA 3.1 70B, and Qwen 2.5 variants, benchmarked against traditional\nsequence- and graph-based methods using a dataset of 26.59K PHP scripts, and\n(2) the Behavioral Function-Aware Detection (BFAD) framework, designed to\naddress the specific challenges of applying LLMs to this domain. Our framework\nintegrates three components: a Critical Function Filter that isolates malicious\nPHP function calls, a Context-Aware Code Extraction strategy that captures the\nmost behaviorally indicative code segments, and Weighted Behavioral Function\nProfiling that enhances in-context learning by prioritizing the most relevant\ndemonstrations based on discriminative function-level profiles. Our results\nshow that, stemming from their distinct analytical strategies, larger LLMs\nachieve near-perfect precision but lower recall, while smaller models exhibit\nthe opposite trade-off. However, all baseline models lag behind previous SOTA\nmethods. With the application of BFAD, the performance of all LLMs improves\nsignificantly, yielding an average F1 score increase of 13.82%. Notably, larger\nmodels now outperform SOTA benchmarks, while smaller models such as\nQwen-2.5-Coder-3B achieve performance competitive with traditional methods.\nThis work is the first to explore the feasibility and limitations of LLMs for\nWebShell detection and provides solutions to address the challenges in this\ntask.",
    "published": "2025-04-14T21:09:37Z",
    "updated": "2025-10-13T01:58:10Z",
    "link": "http://arxiv.org/pdf/2504.13811v4.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Feijiang Han",
      "Jiaming Zhang",
      "Chuyi Deng",
      "Jianheng Tang",
      "Yunhuai Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10902v1",
    "title": "Quantifying Information Disclosure During Gradient Descent Using\n  Gradient Uniqueness",
    "summary": "Disclosing private information via publication of a machine learning model is\noften a concern. Intuitively, publishing a learned model should be less risky\nthan publishing a dataset. But how much risk is there? In this paper, we\npresent a principled disclosure metric called \\emph{gradient uniqueness} that\nis derived from an upper bound on the amount of information disclosure from\npublishing a learned model. Gradient uniqueness provides an intuitive way to\nperform privacy auditing. The mathematical derivation of gradient uniqueness is\ngeneral, and does not make any assumption on the model architecture, dataset\ntype, or the strategy of an attacker. We examine a simple defense based on\nmonitoring gradient uniqueness, and find that it achieves privacy comparable to\nclassical methods such as DP-SGD, while being substantially better in terms of\n(utility) testing accuracy.",
    "published": "2025-10-13T01:57:27Z",
    "updated": "2025-10-13T01:57:27Z",
    "link": "http://arxiv.org/pdf/2510.10902v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Mahmoud Abdelghafar",
      "Maryam Aliakbarpour",
      "Chris Jermaine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07680v3",
    "title": "Hierarchical Balance Packing: Towards Efficient Supervised Fine-tuning\n  for Long-Context LLM",
    "summary": "Training Long-Context Large Language Models (LLMs) is challenging, as hybrid\ntraining with long-context and short-context data often leads to workload\nimbalances. Existing works mainly use data packing to alleviate this issue, but\nfail to consider imbalanced attention computation and wasted communication\noverhead. This paper proposes Hierarchical Balance Packing (HBP), which designs\na novel batch-construction method and training recipe to address those\ninefficiencies. In particular, the HBP constructs multi-level data packing\ngroups, each optimized with a distinct packing length. It assigns training\nsamples to their optimal groups and configures each group with the most\neffective settings, including sequential parallelism degree and gradient\ncheckpointing configuration. To effectively utilize multi-level groups of data,\nwe design a dynamic training pipeline specifically tailored to HBP, including\ncurriculum learning, adaptive sequential parallelism, and stable loss. Our\nextensive experiments demonstrate that our method significantly reduces\ntraining time over multiple datasets and open-source models while maintaining\nstrong performance. For the largest DeepSeek-V2 (236B) MoE model, our method\nspeeds up the training by 2.4$\\times$ with competitive performance. Codes will\nbe released at https://github.com/ModelTC/HBP.",
    "published": "2025-03-10T10:52:50Z",
    "updated": "2025-10-13T01:38:44Z",
    "link": "http://arxiv.org/pdf/2503.07680v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yongqiang Yao",
      "Jingru Tan",
      "Kaihuan Liang",
      "Feizhao Zhang",
      "Jiahao Hu",
      "Shuo Wu",
      "Yazhe Niu",
      "Ruihao Gong",
      "Dahua Lin",
      "Ningyi Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17370v4",
    "title": "FRIREN: Beyond Trajectories -- A Spectral Lens on Time",
    "summary": "Long-term time-series forecasting (LTSF) models are often presented as\ngeneral-purpose solutions that can be applied across domains, implicitly\nassuming that all data is pointwise predictable. Using chaotic systems such as\nLorenz-63 as a case study, we argue that geometric structure - not pointwise\nprediction - is the right abstraction for a dynamic-agnostic foundational\nmodel. Minimizing the Wasserstein-2 distance (W2), which captures geometric\nchanges, and providing a spectral view of dynamics are essential for\nlong-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via\nInterpretable Eigen-networks), implements an augmented normalizing-flow block\nthat embeds data into a normally distributed latent representation. It then\ngenerates a W2-efficient optimal path that can be decomposed into rotation,\nscaling, inverse rotation, and translation. This architecture yields locally\ngenerated, geometry-preserving predictions that are independent of the\nunderlying dynamics, and a global spectral representation that functions as a\nfinite Koopman operator with a small modification. This enables practitioners\nto identify which modes grow, decay, or oscillate, both locally and\nsystem-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on\nLorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE\n27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out\nof 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),\nFRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,\noutperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.\nFRIREN is also competitive on standard LTSF datasets such as ETT and Weather.\nBy connecting modern generative flows with classical spectral analysis, FRIREN\nmakes long-term forecasting both accurate and interpretable, setting a new\nbenchmark for LTSF model design.",
    "published": "2025-05-23T00:52:13Z",
    "updated": "2025-10-13T01:18:53Z",
    "link": "http://arxiv.org/pdf/2505.17370v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Qilin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2405.07038v2",
    "title": "Online Auction Design Using Distribution-Free Uncertainty Quantification\n  with Applications to E-Commerce",
    "summary": "Online auction is a cornerstone of e-commerce, and a key challenge is\ndesigning incentive-compatible mechanisms that maximize expected revenue.\nExisting approaches often assume known bidder value distributions and fixed\nsets of bidders and items, but these assumptions rarely hold in real-world\nsettings where bidder values are unknown, and the number of future participants\nis uncertain. In this paper, we introduce the Conformal Online Auction Design\n(COAD), a novel mechanism that maximizes revenue by quantifying uncertainty in\nbidder values without relying on known distributions. COAD incorporates both\nbidder and item features, using historical data to design an\nincentive-compatible mechanism for online auctions. Unlike traditional methods,\nCOAD leverages distribution-free uncertainty quantification techniques and\nintegrates machine learning methods, such as random forests, kernel methods,\nand deep neural networks, to predict bidder values while ensuring revenue\nguarantees. Moreover, COAD introduces bidder-specific reserve prices, based on\nthe lower confidence bounds of bidder valuations, contrasting with the single\nreserve prices commonly used in the literature. We demonstrate the practical\neffectiveness of COAD through an application to real-world eBay auction data.\nTheoretical results and extensive simulation studies further validate the\nproperties of our approach.",
    "published": "2024-05-11T15:28:25Z",
    "updated": "2025-10-13T01:01:55Z",
    "link": "http://arxiv.org/pdf/2405.07038v2.pdf",
    "category": [
      "cs.GT",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Jiale Han",
      "Xiaowu Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10870v1",
    "title": "Transfer Learning with Distance Covariance for Random Forest: Error\n  Bounds and an EHR Application",
    "summary": "Random forest is an important method for ML applications due to its broad\noutperformance over competing methods for structured tabular data. We propose a\nmethod for transfer learning in nonparametric regression using a centered\nrandom forest (CRF) with distance covariance-based feature weights, assuming\nthe unknown source and target regression functions are different for a few\nfeatures (sparsely different). Our method first obtains residuals from\npredicting the response in the target domain using a source domain-trained CRF.\nThen, we fit another CRF to the residuals, but with feature splitting\nprobabilities proportional to the sample distance covariance between the\nfeatures and the residuals in an independent sample. We derive an upper bound\non the mean square error rate of the procedure as a function of sample sizes\nand difference dimension, theoretically demonstrating transfer learning\nbenefits in random forests. In simulations, we show that the results obtained\nfor the CRFs also hold numerically for the standard random forest (SRF) method\nwith data-driven feature split selection. Beyond transfer learning, our results\nalso show the benefit of distance-covariance-based weights on the performance\nof RF in some situations. Our method shows significant gains in predicting the\nmortality of ICU patients in smaller-bed target hospitals using a large\nmulti-hospital dataset of electronic health records for 200,000 ICU patients.",
    "published": "2025-10-13T00:31:56Z",
    "updated": "2025-10-13T00:31:56Z",
    "link": "http://arxiv.org/pdf/2510.10870v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "authors": [
      "Chenze Li",
      "Subhadeep Paul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17417v2",
    "title": "A Comprehensive Evaluation on Quantization Techniques for Large Language\n  Models",
    "summary": "For large language models (LLMs), post-training quantization (PTQ) can\nsignificantly reduce memory footprint and computational overhead. Model\nquantization is rapidly evolving. Though many papers report breakthrough\nresults, they are often evaluated under different settings because a method\ntypically contains multiple components. Analyzing connections among existing\nmethods is important for deeper understanding. To bridge these gaps, we conduct\nan extensive review of state-of-the-art methods and perform comprehensive\nevaluations under the same conditions for fair comparison. To our knowledge,\nsuch a fair and extensive investigation remains critically underexplored. To\nbetter understand connections, first, we decouple published quantization\nmethods into two steps: pre-quantization transformation and quantization error\nmitigation. The former is a preprocessing step that reduces outlier impact by\nflattening the data distribution; the latter offsets quantization errors to\nimprove performance. Second, we evaluate and analyze the impact of different\nsettings, including granularity and symmetry. Third, we analyze and evaluate\nthe latest MXFP4 and NVFP4 data formats and their performance. Our experiments\nfirst demonstrate that optimized rotation and scaling yield the best\npre-quantization performance, and that combining low-rank compensation with\nGPTQ can occasionally outperform GPTQ alone for error mitigation. Second, finer\ngranularity improves performance but increases storage overhead. Third, we find\nthat scaling-factor format and precision greatly affect FP4 performance, and\nthat rotation-based strategies effective for INT4 offer limited gains for MXFP4\nand NVFP4, motivating further study.",
    "published": "2025-07-23T11:21:21Z",
    "updated": "2025-10-13T00:29:25Z",
    "link": "http://arxiv.org/pdf/2507.17417v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yutong Liu",
      "Cairong Zhao",
      "Guosheng Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10866v1",
    "title": "Quantifying Dataset Similarity to Guide Transfer Learning",
    "summary": "Transfer learning has become a cornerstone of modern machine learning, as it\ncan empower models by leveraging knowledge from related domains to improve\nlearning effectiveness. However, transferring from poorly aligned data can harm\nrather than help performance, making it crucial to determine whether the\ntransfer will be beneficial before implementation. This work aims to address\nthis challenge by proposing an innovative metric to measure dataset similarity\nand provide quantitative guidance on transferability. In the literature,\nexisting methods largely focus on feature distributions while overlooking label\ninformation and predictive relationships, potentially missing critical\ntransferability insights. In contrast, our proposed metric, the Cross-Learning\nScore (CLS), measures dataset similarity through bidirectional generalization\nperformance between domains. We provide a theoretical justification for CLS by\nestablishing its connection to the cosine similarity between the decision\nboundaries for the target and source datasets. Computationally, CLS is\nefficient and fast to compute as it bypasses the problem of expensive\ndistribution estimation for high-dimensional problems. We further introduce a\ngeneral framework that categorizes source datasets into positive, ambiguous, or\nnegative transfer zones based on their CLS relative to the baseline error,\nenabling informed decisions. Additionally, we extend this approach to\nencoder-head architectures in deep learning to better reflect modern transfer\npipelines. Extensive experiments on diverse synthetic and real-world tasks\ndemonstrate that CLS can reliably predict whether transfer will improve or\ndegrade performance, offering a principled tool for guiding data selection in\ntransfer learning.",
    "published": "2025-10-13T00:18:35Z",
    "updated": "2025-10-13T00:18:35Z",
    "link": "http://arxiv.org/pdf/2510.10866v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Shudong Sun",
      "Hao Helen Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01565v2",
    "title": "TetriServe: Efficient DiT Serving for Heterogeneous Image Generation",
    "summary": "Diffusion Transformer (DiT) models excel at generating highquality images\nthrough iterative denoising steps, but serving them under strict Service Level\nObjectives (SLOs) is challenging due to their high computational cost,\nparticularly at large resolutions. Existing serving systems use fixed degree\nsequence parallelism, which is inefficient for heterogeneous workloads with\nmixed resolutions and deadlines, leading to poor GPU utilization and low SLO\nattainment.\n  In this paper, we propose step-level sequence parallelism to dynamically\nadjust the parallel degree of individual requests according to their deadlines.\nWe present TetriServe, a DiT serving system that implements this strategy for\nhighly efficient image generation. Specifically, TetriServe introduces a novel\nround-based scheduling mechanism that improves SLO attainment: (1) discretizing\ntime into fixed rounds to make deadline-aware scheduling tractable, (2)\nadapting parallelism at the step level and minimize GPU hour consumption, and\n(3) jointly packing requests to minimize late completions. Extensive evaluation\non state-of-the-art DiT models shows that TetriServe achieves up to 32% higher\nSLO attainment compared to existing solutions without degrading image quality.",
    "published": "2025-10-02T01:23:32Z",
    "updated": "2025-10-13T00:16:25Z",
    "link": "http://arxiv.org/pdf/2510.01565v2.pdf",
    "category": [
      "cs.LG",
      "cs.DC"
    ],
    "authors": [
      "Runyu Lu",
      "Shiqi He",
      "Wenxuan Tan",
      "Shenggui Li",
      "Ruofan Wu",
      "Jeff J. Ma",
      "Ang Chen",
      "Mosharaf Chowdhury"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10864v1",
    "title": "HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic\n  Relations",
    "summary": "Graph heterophily, where connected nodes have different labels, has attracted\nsignificant interest recently. Most existing works adopt a simplified approach\n- using low-pass filters for homophilic graphs and high-pass filters for\nheterophilic graphs. However, we discover that the relationship between graph\nheterophily and spectral filters is more complex - the optimal filter response\nvaries across frequency components and does not follow a strict monotonic\ncorrelation with heterophily degree. This finding challenges conventional fixed\nfilter designs and suggests the need for adaptive filtering to preserve\nexpressiveness in graph embeddings. Formally, natural questions arise: Given a\nheterophilic graph G, how and to what extent will the varying heterophily\ndegree of G affect the performance of GNNs? How can we design adaptive filters\nto fit those varying heterophilic connections? Our theoretical analysis reveals\nthat the average frequency response of GNNs and graph heterophily degree do not\nfollow a strict monotonic correlation, necessitating adaptive graph filters to\nguarantee good generalization performance. Hence, we propose [METHOD NAME], a\nsimple yet powerful GNN, which extracts information across the heterophily\nspectrum and combines salient representations through adaptive mixing. [METHOD\nNAME]'s superior performance achieves up to 9.2% accuracy improvement over\nleading baselines across homophilic and heterophilic graphs.",
    "published": "2025-10-13T00:12:40Z",
    "updated": "2025-10-13T00:12:40Z",
    "link": "http://arxiv.org/pdf/2510.10864v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "authors": [
      "Shuaicheng Zhang",
      "Haohui Wang",
      "Junhong Lin",
      "Xiaojie Guo",
      "Yada Zhu",
      "Si Zhang",
      "Dongqi Fu",
      "Dawei Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10862v1",
    "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
    "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
    "published": "2025-10-13T00:11:02Z",
    "updated": "2025-10-13T00:11:02Z",
    "link": "http://arxiv.org/pdf/2510.10862v1.pdf",
    "category": [
      "cs.LG",
      "cs.AR"
    ],
    "authors": [
      "Samuel Yuan",
      "Divyanshu Saxena",
      "Jiayi Chen",
      "Nihal Sharma",
      "Aditya Akella"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10854v1",
    "title": "Discrete State Diffusion Models: A Sample Complexity Perspective",
    "summary": "Diffusion models have demonstrated remarkable performance in generating\nhigh-dimensional samples across domains such as vision, language, and the\nsciences. Although continuous-state diffusion models have been extensively\nstudied both empirically and theoretically, discrete-state diffusion models,\nessential for applications involving text, sequences, and combinatorial\nstructures, remain significantly less understood from a theoretical standpoint.\nIn particular, all existing analyses of discrete-state models assume score\nestimation error bounds without studying sample complexity results. In this\nwork, we present a principled theoretical framework for discrete-state\ndiffusion, providing the first sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$. Our structured decomposition of the\nscore estimation error into statistical, approximation, optimization, and\nclipping components offers critical insights into how discrete-state models can\nbe trained efficiently. This analysis addresses a fundamental gap in the\nliterature and establishes the theoretical tractability and practical relevance\nof discrete-state diffusion models.",
    "published": "2025-10-12T23:33:46Z",
    "updated": "2025-10-12T23:33:46Z",
    "link": "http://arxiv.org/pdf/2510.10854v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Aadithya Srikanth",
      "Mudit Gaur",
      "Vaneet Aggarwal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10849v1",
    "title": "Glance for Context: Learning When to Leverage LLMs for Node-Aware\n  GNN-LLM Fusion",
    "summary": "Learning on text-attributed graphs has motivated the use of Large Language\nModels (LLMs) for graph learning. However, most fusion strategies are applied\nuniformly across all nodes and attain only small overall performance gains. We\nargue this result stems from aggregate metrics that obscure when LLMs provide\nbenefit, inhibiting actionable signals for new strategies. In this work, we\nreframe LLM-GNN fusion around nodes where GNNs typically falter. We first show\nthat performance can significantly differ between GNNs and LLMs, with each\nexcelling on distinct structural patterns, such as local homophily. To leverage\nthis finding, we propose GLANCE (GNN with LLM Assistance for Neighbor- and\nContext-aware Embeddings), a framework that invokes an LLM to refine a GNN's\nprediction. GLANCE employs a lightweight router that, given inexpensive\nper-node signals, decides whether to query the LLM. Since the LLM calls are\nnon-differentiable, the router is trained with an advantage-based objective\nthat compares the utility of querying the LLM against relying solely on the\nGNN. Across multiple benchmarks, GLANCE achieves the best performance balance\nacross node subgroups, achieving significant gains on heterophilous nodes (up\nto $+13\\%$) while simultaneously achieving top overall performance. Our\nfindings highlight the value of adaptive, node-aware GNN-LLM architectures,\nwhere selectively invoking the LLM enables scalable deployment on large graphs\nwithout incurring high computational costs.",
    "published": "2025-10-12T23:25:16Z",
    "updated": "2025-10-12T23:25:16Z",
    "link": "http://arxiv.org/pdf/2510.10849v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Donald Loveland",
      "Yao-An Yang",
      "Danai Koutra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10830v1",
    "title": "Fast and the Furious: Hot Starts in Pursuit-Evasion Games",
    "summary": "Effectively positioning pursuers in pursuit-evasion games without prior\nknowledge of evader locations remains a significant challenge. A novel approach\nthat combines game-theoretic control theory with Graph Neural Networks is\nintroduced in this work. By conceptualizing pursuer configurations as strategic\narrangements and representing them as graphs, a Graph Characteristic Space is\nconstructed via multi-objective optimization to identify Pareto-optimal\nconfigurations. A Graph Convolutional Network (GCN) is trained on these\nPareto-optimal graphs to generate strategically effective initial\nconfigurations, termed \"hot starts\". Empirical evaluations demonstrate that the\nGCN-generated hot starts provide a significant advantage over random\nconfigurations. In scenarios considering multiple pursuers and evaders, this\nmethod hastens the decline in evader survival rates, reduces pursuer travel\ndistances, and enhances containment, showcasing clear strategic benefits.",
    "published": "2025-10-12T22:46:50Z",
    "updated": "2025-10-12T22:46:50Z",
    "link": "http://arxiv.org/pdf/2510.10830v1.pdf",
    "category": [
      "cs.MA",
      "cs.GT",
      "cs.LG"
    ],
    "authors": [
      "Gabriel Smithline",
      "Scott Nivison"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25136v2",
    "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression",
    "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.",
    "published": "2025-09-29T17:50:29Z",
    "updated": "2025-10-12T21:46:15Z",
    "link": "http://arxiv.org/pdf/2509.25136v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "David González-Martínez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10810v1",
    "title": "Aegis: A Correlation-Based Data Masking Advisor for Data Sharing\n  Ecosystems",
    "summary": "Data-sharing ecosystems enable entities -- such as providers, consumers, and\nintermediaries -- to access, exchange, and utilize data for various downstream\ntasks and applications. Due to privacy concerns, data providers typically\nanonymize datasets before sharing them; however, the existence of multiple\nmasking configurations results in masked datasets with varying utility.\nConsequently, a key challenge lies in efficiently determining the optimal\nmasking configuration that maximizes a dataset's utility. This paper presents\nAEGIS, a middleware framework for identifying the optimal masking configuration\nfor machine learning datasets that consist of features and a class label. We\nintroduce a utility optimizer that minimizes predictive utility deviation -- a\nmetric based on the changes in feature-label correlations before and after\nmasking. Our framework leverages limited data summaries (such as 1D histograms)\nor none to estimate the feature-label joint distribution, making it suitable\nfor scenarios where raw data is inaccessible due to privacy restrictions. To\nachieve this, we propose a joint distribution estimator based on iterative\nproportional fitting, which allows supporting various feature-label correlation\nquantification methods such as g3, mutual information, or chi-square. Our\nexperimental evaluation on real-world datasets shows that AEGIS identifies\noptimal masking configurations over an order of magnitude faster, while the\nresulting masked datasets achieve predictive performance on downstream ML tasks\nthat is on par with baseline approaches.",
    "published": "2025-10-12T21:16:43Z",
    "updated": "2025-10-12T21:16:43Z",
    "link": "http://arxiv.org/pdf/2510.10810v1.pdf",
    "category": [
      "cs.LG",
      "cs.DB"
    ],
    "authors": [
      "Omar Islam Laskar",
      "Fatemeh Ramezani Khozestani",
      "Ishika Nankani",
      "Sohrab Namazi Nia",
      "Senjuti Basu Roy",
      "Kaustubh Beedkar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10807v1",
    "title": "Crisis-Aware Regime-Conditioned Diffusion with CVaR Allocation",
    "summary": "We study whether regime-conditioned generative scenarios, coupled with a\nconvex CVaR allocator, improve portfolio decisions under regime shifts. We\nintroduce Multi-Agent Regime-Conditioned Diffusion (MARCD), which (i) infers\nlatent regimes via a Gaussian HMM, (ii) trains a diffusion model with a\ntail-weighted objective and a regime-specialized mixture-of-experts (MoE)\ndenoiser to enrich crisis co-movements, and (iii) feeds the generated scenarios\ninto a turnover-aware CVaR epigraph quadratic program with explicit governance.\nIn strict walk-forward tests on liquid multi-asset ETFs (2005-2025), MARCD\noutperforms standard allocators and improves calibration relative to popular\ngenerators. Over 2020-2025 out-of-sample (monthly; 10 bps), MARCD attains\nSharpe 1.23 (BL 1.02) and MaxDD 9.3 percent (BL 14.1 percent), a 34 percent\nreduction, at comparable turnover; stationary block-bootstrap intervals\nindicate the Sharpe uplift is significant at 5 percent. We provide theory\nlinking tail-weighted diffusion to spectral-risk control of the\ndecision-relevant CVaR gap, oracle/consistency results for the regime-MoE\ndenoiser, and Lipschitz/regret guarantees for the allocator. Together, MARCD\noffers a reproducible bridge from tail-faithful scenario modeling to governed\nportfolio decisions with materially improved drawdown control.",
    "published": "2025-10-12T20:56:10Z",
    "updated": "2025-10-12T20:56:10Z",
    "link": "http://arxiv.org/pdf/2510.10807v1.pdf",
    "category": [
      "cs.LG",
      "q-fin.CP"
    ],
    "authors": [
      "Ali Atiah Alzahrani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10803v1",
    "title": "PruneGCRN: Minimizing and explaining spatio-temporal problems through\n  node pruning",
    "summary": "This work addresses the challenge of using a deep learning model to prune\ngraphs and the ability of this method to integrate explainability into\nspatio-temporal problems through a new approach. Instead of applying\nexplainability to the model's behavior, we seek to gain a better understanding\nof the problem itself. To this end, we propose a novel model that integrates an\noptimized pruning mechanism capable of removing nodes from the graph during the\ntraining process, rather than doing so as a separate procedure. This\nintegration allows the architecture to learn how to minimize prediction error\nwhile selecting the most relevant nodes. Thus, during training, the model\nsearches for the most relevant subset of nodes, obtaining the most important\nelements of the problem, facilitating its analysis. To evaluate the proposed\napproach, we used several widely used traffic datasets, comparing the accuracy\nobtained by pruning with the model and with other methods. The experiments\ndemonstrate that our method is capable of retaining a greater amount of\ninformation as the graph reduces in size compared to the other methods used.\nThese results highlight the potential of pruning as a tool for developing\nmodels capable of simplifying spatio-temporal problems, thereby obtaining their\nmost important elements.",
    "published": "2025-10-12T20:40:22Z",
    "updated": "2025-10-12T20:40:22Z",
    "link": "http://arxiv.org/pdf/2510.10803v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Javier García-Sigüenza",
      "Mirco Nanni",
      "Faraón Llorens-Largo",
      "José F. Vicent"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10799v1",
    "title": "Rethinking deep learning: linear regression remains a key benchmark in\n  predicting terrestrial water storage",
    "summary": "Recent advances in machine learning such as Long Short-Term Memory (LSTM)\nmodels and Transformers have been widely adopted in hydrological applications,\ndemonstrating impressive performance amongst deep learning models and\noutperforming physical models in various tasks. However, their superiority in\npredicting land surface states such as terrestrial water storage (TWS) that are\ndominated by many factors such as natural variability and human driven\nmodifications remains unclear. Here, using the open-access, globally\nrepresentative HydroGlobe dataset - comprising a baseline version derived\nsolely from a land surface model simulation and an advanced version\nincorporating multi-source remote sensing data assimilation - we show that\nlinear regression is a robust benchmark, outperforming the more complex LSTM\nand Temporal Fusion Transformer for TWS prediction. Our findings highlight the\nimportance of including traditional statistical models as benchmarks when\ndeveloping and evaluating deep learning models. Additionally, we emphasize the\ncritical need to establish globally representative benchmark datasets that\ncapture the combined impact of natural variability and human interventions.",
    "published": "2025-10-12T20:34:45Z",
    "updated": "2025-10-12T20:34:45Z",
    "link": "http://arxiv.org/pdf/2510.10799v1.pdf",
    "category": [
      "cs.LG",
      "physics.ao-ph",
      "physics.geo-ph"
    ],
    "authors": [
      "Wanshu Nie",
      "Sujay V. Kumar",
      "Junyu Chen",
      "Long Zhao",
      "Olya Skulovich",
      "Jinwoong Yoo",
      "Justin Pflug",
      "Shahryar Khalique Ahmad",
      "Goutam Konapala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10790v1",
    "title": "BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal\n  Dynamics",
    "summary": "Today's deep learning architectures are primarily based on perceptron models,\nwhich do not capture the oscillatory dynamics characteristic of biological\nneurons. Although oscillatory systems have recently gained attention for their\ncloser resemblance to neural behavior, they still fall short of modeling the\nintricate spatio-temporal interactions observed in natural neural circuits. In\nthis paper, we propose a bio-inspired oscillatory state system (BioOSS)\ndesigned to emulate the wave-like propagation dynamics critical to neural\nprocessing, particularly in the prefrontal cortex (PFC), where complex activity\npatterns emerge. BioOSS comprises two interacting populations of neurons: p\nneurons, which represent simplified membrane-potential-like units inspired by\npyramidal cells in cortical columns, and o neurons, which govern propagation\nvelocities and modulate the lateral spread of activity. Through local\ninteractions, these neurons produce wave-like propagation patterns. The model\nincorporates trainable parameters for damping and propagation speed, enabling\nflexible adaptation to task-specific spatio-temporal structures. We evaluate\nBioOSS on both synthetic and real-world tasks, demonstrating superior\nperformance and enhanced interpretability compared to alternative\narchitectures.",
    "published": "2025-10-12T20:12:33Z",
    "updated": "2025-10-12T20:12:33Z",
    "link": "http://arxiv.org/pdf/2510.10790v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhongju Yuan",
      "Geraint Wiggins",
      "Dick Botteldooren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10777v1",
    "title": "Preconditioned Norms: A Unified Framework for Steepest Descent,\n  Quasi-Newton and Adaptive Methods",
    "summary": "Optimization lies at the core of modern deep learning, yet existing methods\noften face a fundamental trade-off between adapting to problem geometry and\nleveraging curvature utilization. Steepest descent algorithms adapt to\ndifferent geometries through norm choices but remain strictly first-order,\nwhereas quasi-Newton and adaptive optimizers incorporate curvature information\nbut are restricted to Frobenius geometry, limiting their applicability across\ndiverse architectures. In this work, we propose a unified framework\ngeneralizing steepest descent, quasi-Newton methods, and adaptive methods\nthrough the novel notion of preconditioned matrix norms. This abstraction\nreveals that widely used optimizers such as SGD and Adam, as well as more\nadvanced approaches like Muon and KL-Shampoo, and recent hybrids including SOAP\nand SPlus, all emerge as special cases of the same principle. Within this\nframework, we provide the first systematic treatment of affine and scale\ninvariance in the matrix-parameterized setting, establishing necessary and\nsufficient conditions under generalized norms. Building on this foundation, we\nintroduce two new methods, $\\texttt{MuAdam}$ and $\\texttt{MuAdam-SANIA}$, which\ncombine the spectral geometry of Muon with Adam-style preconditioning. Our\nexperiments demonstrate that these optimizers are competitive with, and in some\ncases outperform, existing state-of-the-art methods. Our code is available at\nhttps://github.com/brain-lab-research/LIB/tree/quasi_descent",
    "published": "2025-10-12T19:39:41Z",
    "updated": "2025-10-12T19:39:41Z",
    "link": "http://arxiv.org/pdf/2510.10777v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Andrey Veprikov",
      "Arman Bolatov",
      "Samuel Horváth",
      "Aleksandr Beznosikov",
      "Martin Takáč",
      "Slavomir Hanzely"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10775v1",
    "title": "Structure Over Signal: A Globalized Approach to Multi-relational GNNs\n  for Stock Prediction",
    "summary": "In financial markets, Graph Neural Networks have been successfully applied to\nmodeling relational data, effectively capturing nonlinear inter-stock\ndependencies. Yet, existing models often fail to efficiently propagate messages\nduring macroeconomic shocks. In this paper, we propose OmniGNN, an\nattention-based multi-relational dynamic GNN that integrates macroeconomic\ncontext via heterogeneous node and edge types for robust message passing.\nCentral to OmniGNN is a sector node acting as a global intermediary, enabling\nrapid shock propagation across the graph without relying on long-range\nmulti-hop diffusion. The model leverages Graph Attention Networks (GAT) to\nweigh neighbor contributions and employs Transformers to capture temporal\ndynamics across multiplex relations. Experiments show that OmniGNN outperforms\nexisting stock prediction models on public datasets, particularly demonstrating\nstrong robustness during the COVID-19 period.",
    "published": "2025-10-12T19:33:16Z",
    "updated": "2025-10-12T19:33:16Z",
    "link": "http://arxiv.org/pdf/2510.10775v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Amber Li",
      "Aruzhan Abil",
      "Juno Marques Oda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10774v1",
    "title": "ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for\n  Text-to-Speech Synthesis",
    "summary": "Persian Language, despite being spoken by over 100 million people worldwide,\nremains severely underrepresented in high-quality speech corpora, particularly\nfor text-to-speech (TTS) synthesis applications. Existing Persian speech\ndatasets are typically smaller than their English counterparts, which creates a\nkey limitation for developing Persian speech technologies. We address this gap\nby introducing ParsVoice, the largest Persian speech corpus designed\nspecifically for TTS applications. We created an automated pipeline that\ntransforms raw audiobook content into TTS-ready data, incorporating components\nsuch as a BERT-based sentence completion detector, a binary search boundary\noptimization method for precise audio-text alignment, and multi-dimensional\nquality assessment frameworks tailored to Persian. The pipeline processes 2,000\naudiobooks, yielding 3,526 hours of clean speech, which was further filtered\ninto a 1,804-hour high-quality subset suitable for TTS, featuring more than 470\nspeakers. ParsVoice is the largest high-quality Persian speech dataset,\noffering speaker diversity and audio quality comparable to major English\ncorpora. The complete dataset has been made publicly available to accelerate\nthe development of Persian speech technologies and to serve as a template for\nother low-resource languages. The ParsVoice dataset is publicly available at\nParsVoice (https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice).",
    "published": "2025-10-12T19:33:11Z",
    "updated": "2025-10-12T19:33:11Z",
    "link": "http://arxiv.org/pdf/2510.10774v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Mohammad Javad Ranjbar Kalahroodi",
      "Heshaam Faili",
      "Azadeh Shakery"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01649v2",
    "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients",
    "summary": "Gradients of neural networks encode valuable information for optimization,\nediting, and analysis of models. Therefore, practitioners often treat gradients\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\nworks explore learning algorithms that operate directly on gradients but use\narchitectures that are not specifically designed for gradient processing,\nlimiting their applicability. In this paper, we present a principled approach\nfor designing architectures that process gradients. Our approach is guided by\nthree principles: (1) equivariant design that preserves neuron permutation\nsymmetries, (2) processing sets of gradients across multiple data points to\ncapture curvature information, and (3) efficient gradient representation\nthrough rank-1 decomposition. Based on these principles, we introduce\nGradMetaNet, a novel architecture for learning on gradients, constructed from\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\nshow that previous approaches cannot approximate natural gradient-based\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\non a diverse set of gradient-based tasks on MLPs and transformers, such as\nlearned optimization, INR editing, and estimating loss landscape curvature.",
    "published": "2025-07-02T12:22:39Z",
    "updated": "2025-10-12T19:11:30Z",
    "link": "http://arxiv.org/pdf/2507.01649v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yoav Gelberg",
      "Yam Eitan",
      "Aviv Navon",
      "Aviv Shamsian",
      " Theo",
      " Putterman",
      "Michael Bronstein",
      "Haggai Maron"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10767v1",
    "title": "Understanding Sampler Stochasticity in Training Diffusion Models for\n  RLHF",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nfine-tune diffusion models, but a key challenge arises from the mismatch\nbetween stochastic samplers used during training and deterministic samplers\nused during inference. In practice, models are fine-tuned using stochastic SDE\nsamplers to encourage exploration, while inference typically relies on\ndeterministic ODE samplers for efficiency and stability. This discrepancy\ninduces a reward gap, raising concerns about whether high-quality outputs can\nbe expected during inference. In this paper, we theoretically characterize this\nreward gap and provide non-vacuous bounds for general diffusion models, along\nwith sharper convergence rates for Variance Exploding (VE) and Variance\nPreserving (VP) Gaussian models. Methodologically, we adopt the generalized\ndenoising diffusion implicit models (gDDIM) framework to support arbitrarily\nhigh levels of stochasticity, preserving data marginals throughout.\nEmpirically, our findings through large-scale experiments on text-to-image\nmodels using denoising diffusion policy optimization (DDPO) and mixed group\nrelative policy optimization (MixGRPO) validate that reward gaps consistently\nnarrow over training, and ODE sampling quality improves when models are updated\nusing higher-stochasticity SDE training.",
    "published": "2025-10-12T19:08:38Z",
    "updated": "2025-10-12T19:08:38Z",
    "link": "http://arxiv.org/pdf/2510.10767v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Jiayuan Sheng",
      "Hanyang Zhao",
      "Haoxian Chen",
      "David D. Yao",
      "Wenpin Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.02242v2",
    "title": "Methods to improve run time of hydrologic models: opportunities and\n  challenges in the machine learning era",
    "summary": "The application of Machine Learning (ML) to hydrologic modeling is fledgling.\nIts applicability to capture the dependencies on watersheds to forecast better\nwithin a short period is fascinating. One of the key reasons to adopt ML\nalgorithms over physics-based models is its computational efficiency advantage\nand flexibility to work with various data sets. The diverse applications,\nparticularly in emergency response and expanding over a large scale, demand the\nhydrological model in a short time and make researchers adopt data-driven\nmodeling approaches unhesitatingly. In this work, in the era of ML and deep\nlearning (DL), how it can help to improve the overall run time of physics-based\nmodel and potential constraints that should be addressed while modeling. This\npaper covers the opportunities and challenges of adopting ML for hydrological\nmodeling and subsequently how it can help to improve the simulation time of\nphysics-based models and future works that should be addressed.",
    "published": "2024-08-05T05:27:19Z",
    "updated": "2025-10-12T18:53:04Z",
    "link": "http://arxiv.org/pdf/2408.02242v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Supath Dhital"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.02930v2",
    "title": "VendiRL: A Framework for Self-Supervised Reinforcement Learning of\n  Diversely Diverse Skills",
    "summary": "In self-supervised reinforcement learning (RL), one of the key challenges is\nlearning a diverse set of skills to prepare agents for unknown future tasks.\nDespite impressive advances, scalability and evaluation remain prevalent\nissues. Regarding scalability, the search for meaningful skills can be obscured\nby high-dimensional feature spaces, where relevant features may vary across\ndownstream task domains. For evaluating skill diversity, defining what\nconstitutes \"diversity\" typically requires a hard commitment to a specific\nnotion of what it means for skills to be diverse, potentially leading to\ninconsistencies in how skill diversity is understood, making results across\ndifferent approaches hard to compare, and leaving many forms of diversity\nunexplored. To address these issues, we adopt a measure of sample diversity\nthat translates ideas from ecology to machine learning -- the Vendi Score --\nallowing the user to specify and evaluate any desired form of diversity. We\ndemonstrate how this metric facilitates skill evaluation and introduce VendiRL,\na unified framework for learning diversely diverse sets of skills. Given\ndistinct similarity functions, VendiRL motivates distinct forms of diversity,\nwhich could support skill-diversity pretraining in new and richly interactive\nenvironments where optimising for various forms of diversity may be desirable.",
    "published": "2025-09-03T01:53:29Z",
    "updated": "2025-10-12T18:35:39Z",
    "link": "http://arxiv.org/pdf/2509.02930v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Erik M. Lintunen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10744v1",
    "title": "How Patterns Dictate Learnability in Sequential Data",
    "summary": "Sequential data - ranging from financial time series to natural language -\nhas driven the growing adoption of autoregressive models. However, these\nalgorithms rely on the presence of underlying patterns in the data, and their\nidentification often depends heavily on human expertise. Misinterpreting these\npatterns can lead to model misspecification, resulting in increased\ngeneralization error and degraded performance. The recently proposed evolving\npattern (EvoRate) metric addresses this by using the mutual information between\nthe next data point and its past to guide regression order estimation and\nfeature selection. Building on this idea, we introduce a general framework\nbased on predictive information, defined as the mutual information between the\npast and the future, $I(X_{past}; X_{future})$. This quantity naturally defines\nan information-theoretic learning curve, which quantifies the amount of\npredictive information available as the observation window grows. Using this\nformalism, we show that the presence or absence of temporal patterns\nfundamentally constrains the learnability of sequential models: even an optimal\npredictor cannot outperform the intrinsic information limit imposed by the\ndata. We validate our framework through experiments on synthetic data,\ndemonstrating its ability to assess model adequacy, quantify the inherent\ncomplexity of a dataset, and reveal interpretable structure in sequential data.",
    "published": "2025-10-12T18:31:39Z",
    "updated": "2025-10-12T18:31:39Z",
    "link": "http://arxiv.org/pdf/2510.10744v1.pdf",
    "category": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "authors": [
      "Mario Morawski",
      "Anais Despres",
      "Rémi Rehm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10739v1",
    "title": "A Stochastic Differential Equation Framework for Multi-Objective LLM\n  Interactions: Dynamical Systems Analysis with Code Generation Applications",
    "summary": "We introduce a general stochastic differential equation framework for\nmodelling multiobjective optimization dynamics in iterative Large Language\nModel (LLM) interactions. Our framework captures the inherent stochasticity of\nLLM responses through explicit diffusion terms and reveals systematic\ninterference patterns between competing objectives via an interference matrix\nformulation. We validate our theoretical framework using iterative code\ngeneration as a proof-of-concept application, analyzing 400 sessions across\nsecurity, efficiency, and functionality objectives. Our results demonstrate\nstrategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29,\nand predictive accuracy achieving R2 = 0.74 for balanced approaches. This work\nproposes the feasibility of dynamical systems analysis for multi-objective LLM\ninteractions, with code generation serving as an initial validation domain.",
    "published": "2025-10-12T18:25:12Z",
    "updated": "2025-10-12T18:25:12Z",
    "link": "http://arxiv.org/pdf/2510.10739v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Shivani Shukla",
      "Himanshu Joshi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07648v2",
    "title": "Continual Learning for Adaptive AI Systems",
    "summary": "Continual learning the ability of a neural network to learn multiple\nsequential tasks without catastrophic forgetting remains a central challenge in\ndeveloping adaptive artificial intelligence systems. While deep learning models\nachieve state-of-the-art performance across domains, they remain limited by\noverfitting and forgetting. This paper introduces Cluster-Aware Replay (CAR), a\nhybrid continual learning framework that integrates a small, class-balanced\nreplay buffer with a regularization term based on Inter-Cluster Fitness (ICF)\nin the feature space. The ICF loss penalizes overlapping feature\nrepresentations between new and previously learned tasks, encouraging geometric\nseparation in the latent space and reducing interference. Using the standard\nfive-task Split CIFAR-10 benchmark with a ResNet-18 backbone, initial\nexperiments demonstrate that CAR better preserves earlier task performance\ncompared to fine-tuning alone. These findings are preliminary but highlight\nfeature-space regularization as a promising direction for mitigating\ncatastrophic forgetting.",
    "published": "2025-10-09T00:44:32Z",
    "updated": "2025-10-12T18:23:57Z",
    "link": "http://arxiv.org/pdf/2510.07648v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Md Hasibul Amin",
      "Tamzid Tanvi Alam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10650v2",
    "title": "On a Geometry of Interbrain Networks",
    "summary": "Effective analysis in neuroscience benefits significantly from robust\nconceptual frameworks. Traditional metrics of interbrain synchrony in social\nneuroscience typically depend on fixed, correlation-based approaches,\nrestricting their explanatory capacity to descriptive observations. Inspired by\nthe successful integration of geometric insights in network science, we propose\nleveraging discrete geometry to examine the dynamic reconfigurations in neural\ninteractions during social exchanges. Unlike conventional synchrony approaches,\nour method interprets inter-brain connectivity changes through the evolving\ngeometric structures of neural networks. This geometric framework is realized\nthrough a pipeline that identifies critical transitions in network connectivity\nusing entropy metrics derived from curvature distributions. By doing so, we\nsignificantly enhance the capacity of hyperscanning methodologies to uncover\nunderlying neural mechanisms in interactive social behavior.",
    "published": "2025-09-12T19:26:27Z",
    "updated": "2025-10-12T18:20:20Z",
    "link": "http://arxiv.org/pdf/2509.10650v2.pdf",
    "category": [
      "q-bio.NC",
      "cs.LG"
    ],
    "authors": [
      "Nicolás Hinrichs",
      "Noah Guzmán",
      "Melanie Weber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10731v1",
    "title": "Controllable Generative Trajectory Prediction via Weak Preference\n  Alignment",
    "summary": "Deep generative models such as conditional variational autoencoders (CVAEs)\nhave shown great promise for predicting trajectories of surrounding agents in\nautonomous vehicle planning. State-of-the-art models have achieved remarkable\naccuracy in such prediction tasks. Besides accuracy, diversity is also crucial\nfor safe planning because human behaviors are inherently uncertain and\nmultimodal. However, existing methods generally lack a scheme to generate\ncontrollably diverse trajectories, which is arguably more useful than randomly\ndiversified trajectories, to the end of safe planning. To address this, we\npropose PrefCVAE, an augmented CVAE framework that uses weakly labeled\npreference pairs to imbue latent variables with semantic attributes. Using\naverage velocity as an example attribute, we demonstrate that PrefCVAE enables\ncontrollable, semantically meaningful predictions without degrading baseline\naccuracy. Our results show the effectiveness of preference supervision as a\ncost-effective way to enhance sampling-based generative models.",
    "published": "2025-10-12T18:06:39Z",
    "updated": "2025-10-12T18:06:39Z",
    "link": "http://arxiv.org/pdf/2510.10731v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Yongxi Cao",
      "Julian F. Schumann",
      "Jens Kober",
      "Joni Pajarinen",
      "Arkady Zgonnikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10730v1",
    "title": "Provable Anytime Ensemble Sampling Algorithms in Nonlinear Contextual\n  Bandits",
    "summary": "We provide a unified algorithmic framework for ensemble sampling in nonlinear\ncontextual bandits and develop corresponding regret bounds for two most common\nnonlinear contextual bandit settings: Generalized Linear Ensemble Sampling\n(\\texttt{GLM-ES}) for generalized linear bandits and Neural Ensemble Sampling\n(\\texttt{Neural-ES}) for neural contextual bandits. Both methods maintain\nmultiple estimators for the reward model parameters via maximum likelihood\nestimation on randomly perturbed data. We prove high-probability frequentist\nregret bounds of $\\mathcal{O}(d^{3/2} \\sqrt{T} + d^{9/2})$ for \\texttt{GLM-ES}\nand $\\mathcal{O}(\\widetilde{d} \\sqrt{T})$ for \\texttt{Neural-ES}, where $d$ is\nthe dimension of feature vectors, $\\widetilde{d}$ is the effective dimension of\na neural tangent kernel matrix, and $T$ is the number of rounds. These regret\nbounds match the state-of-the-art results of randomized exploration algorithms\nin nonlinear contextual bandit settings. In the theoretical analysis, we\nintroduce techniques that address challenges specific to nonlinear models.\nPractically, we remove fixed-time horizon assumptions by developing anytime\nversions of our algorithms, suitable when $T$ is unknown. Finally, we\nempirically evaluate \\texttt{GLM-ES}, \\texttt{Neural-ES}, and their anytime\nvariants, demonstrating strong performance. Overall, our results establish\nensemble sampling as a provable and practical randomized exploration approach\nfor nonlinear contextual bandits.",
    "published": "2025-10-12T18:05:53Z",
    "updated": "2025-10-12T18:05:53Z",
    "link": "http://arxiv.org/pdf/2510.10730v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jiazheng Sun",
      "Weixin Wang",
      "Pan Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10728v1",
    "title": "Deep Signature and Neural RDE Methods for Path-Dependent Portfolio\n  Optimization",
    "summary": "We present a deep BSDE and 2BSDE solver that combines truncated log\nsignatures with a neural rough differential equation backbone for high\ndimensional, path dependent valuation and control. The design aligns stochastic\nanalysis with sequence to path learning, using a CVaR tilted objective to\nemphasize left tail risk and an optional second order head for risk sensitive\ncontrol. Under equal compute and parameter budgets, the method improves\naccuracy, tail fidelity, and training stability across Asian and barrier option\npricing and portfolio control tasks. At 200 dimensions, it achieves CVaR(0.99)\n= 9.8 percent compared to 12.0-13.1 percent for strong baselines, while\nattaining low HJB residuals and small RMSE for Z and Gamma. Ablations confirm\ncomplementary gains from the sequence to path representation and the second\norder structure. Overall, the results show that combining stochastic analysis\nwith modern deep learning expands the class of solvable path dependent\nfinancial models at scale.",
    "published": "2025-10-12T18:02:12Z",
    "updated": "2025-10-12T18:02:12Z",
    "link": "http://arxiv.org/pdf/2510.10728v1.pdf",
    "category": [
      "q-fin.MF",
      "cs.LG"
    ],
    "authors": [
      "Ali Atiah Alzahrani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11447v1",
    "title": "Building and Evaluating a Realistic Virtual World for Large Scale Urban\n  Exploration from 360° Videos",
    "summary": "We propose to build realistic virtual worlds, called 360RVW, for large urban\nenvironments directly from 360{\\deg} videos. We provide an interface for\ninteractive exploration, where users can freely navigate via their own avatars.\n360{\\deg} videos record the entire environment of the shooting location\nsimultaneously leading to highly realistic and immersive representations. Our\nsystem uses 360{\\deg} videos recorded along streets and builds a 360RVW through\nfour main operations: video segmentation by intersection detection, video\ncompletion to remove the videographer, semantic segmentation for virtual\ncollision detection with the avatar, and projection onto a distorted sphere\nthat moves along the camera trajectory following the avatar's movements. Our\ninterface allows users to explore large urban environments by changing their\nwalking direction at intersections or choosing a new location by clicking on a\nmap. Even without a 3D model, the users can experience collision with buildings\nusing metadata produced by semantic segmentation. Furthermore, we stream the\n360{\\deg} videos so users can directly access 360RVW via their web browser. We\nfully evaluate our system, including a perceptual experiment comparing our\napproach to previous exploratory interfaces. The results confirm the quality of\nour system, especially regarding the presence of users and the interactive\nexploration, making it most suitable for a virtual tour of urban environments.",
    "published": "2025-10-13T14:17:06Z",
    "updated": "2025-10-13T14:17:06Z",
    "link": "http://arxiv.org/pdf/2510.11447v1.pdf",
    "category": [
      "cs.MM"
    ],
    "authors": [
      "Mizuki Takenawa",
      "Naoki Sugimoto",
      "Leslie Wöhler",
      "Satoshi Ikehata",
      "Kiyoharu Aizawa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2409.10762v3",
    "title": "Stimulus Modality Matters: Impact of Perceptual Evaluations from\n  Different Modalities on Speech Emotion Recognition System Performance",
    "summary": "Speech Emotion Recognition (SER) systems rely on speech input and emotional\nlabels annotated by humans. However, various emotion databases collect\nperceptional evaluations in different ways. For instance, the IEMOCAP dataset\nuses video clips with sounds for annotators to provide their emotional\nperceptions. However, the most significant English emotion dataset, the\nMSP-PODCAST, only provides speech for raters to choose the emotional ratings.\nNevertheless, using speech as input is the standard approach to training SER\nsystems. Therefore, the open question is the emotional labels elicited by which\nscenarios are the most effective for training SER systems. We comprehensively\ncompare the effectiveness of SER systems trained with labels elicited by\ndifferent modality stimuli and evaluate the SER systems on various testing\nconditions. Also, we introduce an all-inclusive label that combines all labels\nelicited by various modalities. We show that using labels elicited by\nvoice-only stimuli for training yields better performance on the test set,\nwhereas labels elicited by voice-only stimuli.",
    "published": "2024-09-16T22:32:22Z",
    "updated": "2025-10-13T04:57:37Z",
    "link": "http://arxiv.org/pdf/2409.10762v3.pdf",
    "category": [
      "eess.AS",
      "cs.MM",
      "cs.SD",
      "eess.SP"
    ],
    "authors": [
      "Huang-Cheng Chou",
      "Haibin Wu",
      "Chi-Chun Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10841v1",
    "title": "The Fire We Share",
    "summary": "The Fire We Share proposes a care-centered, consequence-aware visualization\nframework for engaging with wildfire data not as static metrics, but as living\narchives of ecological and social entanglement. By combining plants-inspired\ndata forms, event-based mapping, and narrative layering, the project\nforegrounds fire as a shared temporal condition-one that cuts across natural\ncycles and human systems. Rather than simplifying wildfire data into digestible\nvisuals, The Fire We Share reimagines it as a textured, wounded\narchive-embodied, relational, and radically ethical.",
    "published": "2025-10-12T23:07:48Z",
    "updated": "2025-10-12T23:07:48Z",
    "link": "http://arxiv.org/pdf/2510.10841v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Chen Wang",
      "Mengtan Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10751v1",
    "title": "MATStruct: High-Quality Medial Mesh Computation via Structure-aware\n  Variational Optimization",
    "summary": "We propose a novel optimization framework for computing the medial axis\ntransform that simultaneously preserves the medial structure and ensures high\nmedial mesh quality. The medial structure, consisting of interconnected sheets,\nseams, and junctions, provides a natural volumetric decomposition of a 3D\nshape. Our method introduces a structure-aware, particle-based optimization\npipeline guided by the restricted power diagram (RPD), which partitions the\ninput volume into convex cells whose dual encodes the connectivity of the\nmedial mesh. Structure-awareness is enforced through a spherical quadratic\nerror metric (SQEM) projection that constrains the movement of medial spheres,\nwhile a Gaussian kernel energy encourages an even spatial distribution.\nCompared to feature-preserving methods such as MATFP and MATTopo, our approach\nproduces cleaner and more accurate medial structures with significantly\nimproved mesh quality. In contrast to voxel-based, point-cloud-based, and\nvariational methods, our framework is the first to integrate structural\nawareness into the optimization process, yielding medial meshes with superior\ngeometric fidelity, topological correctness, and explicit structural\ndecomposition.",
    "published": "2025-10-12T18:39:36Z",
    "updated": "2025-10-12T18:39:36Z",
    "link": "http://arxiv.org/pdf/2510.10751v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Ningna Wang",
      "Rui Xu",
      "Yibo Yin",
      "Zichun Zhong",
      "Taku Komura",
      "Wenping Wang",
      "Xiaohu Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07343v2",
    "title": "Local MAP Sampling for Diffusion Models",
    "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to\ninverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the\ngoal of inverse problem solving is not to cover the posterior but to recover\nthe most accurate reconstruction, where optimization-based diffusion solvers\noften excel despite lacking a clear probabilistic foundation. We introduce\nLocal MAP Sampling (LMAPS), a new inference framework that iteratively solving\nlocal MAP subproblems along the diffusion trajectory. This perspective\nclarifies their connection to global MAP estimation and DPS, offering a unified\nprobabilistic interpretation for optimization-based methods. Building on this\nfoundation, we develop practical algorithms with a probabilistically\ninterpretable covariance approximation, a reformulated objective for stability\nand interpretability, and a gradient approximation for non-differentiable\noperators. Across a broad set of image restoration and scientific tasks, LMAPS\nachieves state-of-the-art performance, including $\\geq 2$ dB gains on motion\ndeblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on\ninverse scattering benchmarks.",
    "published": "2025-10-07T19:02:32Z",
    "updated": "2025-10-12T18:18:02Z",
    "link": "http://arxiv.org/pdf/2510.07343v2.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Shaorong Zhang",
      "Rob Brekelmans",
      "Greg Ver Steeg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11583v1",
    "title": "Smooth Spatiotemporal Tube Synthesis for Prescribed-Time\n  Reach-Avoid-Stay Control",
    "summary": "In this work, we address the issue of controller synthesis for a\ncontrol-affine nonlinear system to meet prescribed time reach-avoid-stay\nspecifications. Our goal is to improve upon previous methods based on\nspatiotemporal tubes (STTs) by eliminating the need for circumvent functions,\nwhich often lead to abrupt tube modifications and high control effort. We\npropose an adaptive framework that constructs smooth STTs around static unsafe\nsets, enabling continuous avoidance while guiding the system toward the target\nwithin the prescribed time. A closed-form, approximation-free control law is\nderived to ensure the system trajectory remains within the tube and satisfies\nthe RAS task. The effectiveness of the proposed approach is demonstrated\nthrough a case study, showing a significant reduction in control effort\ncompared to prior methods.",
    "published": "2025-10-13T16:27:11Z",
    "updated": "2025-10-13T16:27:11Z",
    "link": "http://arxiv.org/pdf/2510.11583v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "authors": [
      "Siddhartha Upadhyay",
      "Ratnangshu Das",
      "Pushpak Jagtap"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11574v1",
    "title": "Calibrated Dynamic Modeling for Force and Payload Estimation in\n  Hydraulic Machinery",
    "summary": "Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision.",
    "published": "2025-10-13T16:20:48Z",
    "updated": "2025-10-13T16:20:48Z",
    "link": "http://arxiv.org/pdf/2510.11574v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Lennart Werner",
      "Pol Eyschen",
      "Sean Costello",
      "Pierluigi Micarelli",
      "Marco Hutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11552v1",
    "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education",
    "summary": "Recent developments of low cost off-the-shelf programmable components, their\nmodularity, and also rapid prototyping made educational robotics flourish, as\nit is accessible in most schools today. They allow to illustrate and embody\ntheoretical problems in practical and tangible applications, and gather\nmultidisciplinary skills. They also give a rich natural context for\nproject-oriented pedagogy. However, most current robot kits all are limited to\negocentric aspect of the robots perception. This makes it difficult to access\nmore high-level problems involving e.g. coordinates or navigation. In this\npaper we introduce an educational holonomous robot kit that comes with an\nexternal tracking system, which lightens the constraint on embedded systems,\nbut allows in the same time to discover high-level aspects of robotics,\notherwise unreachable.",
    "published": "2025-10-13T15:53:51Z",
    "updated": "2025-10-13T15:53:51Z",
    "link": "http://arxiv.org/pdf/2510.11552v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Gregoire Passault",
      "Clement Gaspard",
      "Olivier Ly"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.09543v2",
    "title": "Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards",
    "summary": "Animals achieve energy-efficient locomotion by their implicit passive\ndynamics, a marvel that has captivated roboticists for decades.Recently,\nmethods incorporated Adversarial Motion Prior (AMP) and Reinforcement learning\n(RL) shows promising progress to replicate Animals' naturalistic motion.\nHowever, such imitation learning approaches predominantly capture explicit\nkinematic patterns, so-called gaits, while overlooking the implicit passive\ndynamics. This work bridges this gap by incorporating a reward term guided by\nImpact Mitigation Factor (IMF), a physics-informed metric that quantifies a\nrobot's ability to passively mitigate impacts. By integrating IMF with AMP, our\napproach enables RL policies to learn both explicit motion trajectories from\nanimal reference motion and the implicit passive dynamic. We demonstrate energy\nefficiency improvements of up to 32%, as measured by the Cost of Transport\n(CoT), across both AMP and handcrafted reward structure.",
    "published": "2025-10-10T16:56:35Z",
    "updated": "2025-10-13T15:49:30Z",
    "link": "http://arxiv.org/pdf/2510.09543v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Chenghao Wang",
      "Arjun Viswanathan",
      "Eric Sihite",
      "Alireza Ramezani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11542v1",
    "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep\n  Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust\ncontrol policies for bipedal locomotion. Yet, it can be difficult to tune\ndesired robot behaviors due to unintuitive and complex reward design. In\ncomparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,\noffer more tuneable, interpretable, and mathematically grounded motion plans\nfor high-dimensional legged systems. However, these methods often remain\nbrittle to real-world disturbances like external perturbations.\n  In this work, we present NaviGait, a hierarchical framework that combines the\nstructure of trajectory optimization with the adaptability of RL for robust and\nintuitive locomotion control. NaviGait leverages a library of offline-optimized\ngaits and smoothly interpolates between them to produce continuous reference\nmotions in response to high-level commands. The policy provides both\njoint-level and velocity command residual corrections to modulate and stabilize\nthe reference trajectories in the gait library. One notable advantage of\nNaviGait is that it dramatically simplifies reward design by encoding rich\nmotion priors from trajectory optimization, reducing the need for finely tuned\nshaping terms and enabling more stable and interpretable learning. Our\nexperimental results demonstrate that NaviGait enables faster training compared\nto conventional and imitation-based RL, and produces motions that remain\nclosest to the original reference. Overall, by decoupling high-level motion\ngeneration from low-level correction, NaviGait offers a more scalable and\ngeneralizable approach for achieving dynamic and robust locomotion.",
    "published": "2025-10-13T15:41:38Z",
    "updated": "2025-10-13T15:41:38Z",
    "link": "http://arxiv.org/pdf/2510.11542v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Neil C. Janwani",
      "Varun Madabushi",
      "Maegan Tucker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11539v1",
    "title": "Simultaneous Calibration of Noise Covariance and Kinematics for State\n  Estimation of Legged Robots via Bi-level Optimization",
    "summary": "Accurate state estimation is critical for legged and aerial robots operating\nin dynamic, uncertain environments. A key challenge lies in specifying process\nand measurement noise covariances, which are typically unknown or manually\ntuned. In this work, we introduce a bi-level optimization framework that\njointly calibrates covariance matrices and kinematic parameters in an\nestimator-in-the-loop manner. The upper level treats noise covariances and\nmodel parameters as optimization variables, while the lower level executes a\nfull-information estimator. Differentiating through the estimator allows direct\noptimization of trajectory-level objectives, resulting in accurate and\nconsistent state estimates. We validate our approach on quadrupedal and\nhumanoid robots, demonstrating significantly improved estimation accuracy and\nuncertainty calibration compared to hand-tuned baselines. Our method unifies\nstate estimation, sensor, and kinematics calibration into a principled,\ndata-driven framework applicable across diverse robotic platforms.",
    "published": "2025-10-13T15:39:21Z",
    "updated": "2025-10-13T15:39:21Z",
    "link": "http://arxiv.org/pdf/2510.11539v1.pdf",
    "category": [
      "cs.RO",
      "math.OC"
    ],
    "authors": [
      "Denglin Cheng",
      "Jiarong Kang",
      "Xiaobin Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11534v1",
    "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on\n  Interaction Decoupling Strategy",
    "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics.",
    "published": "2025-10-13T15:38:05Z",
    "updated": "2025-10-13T15:38:05Z",
    "link": "http://arxiv.org/pdf/2510.11534v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Enli Lin",
      "Ziyuan Yang",
      "Qiujing Lu",
      "Jianming Hu",
      "Shuo Feng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11525v1",
    "title": "DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight",
    "summary": "MAVs have great potential to assist humans in complex tasks, with\napplications ranging from logistics to emergency response. Their agility makes\nthem ideal for operations in complex and dynamic environments. However,\nachieving precise control in agile flights remains a significant challenge,\nparticularly due to the underactuated nature of quadrotors and the strong\ncoupling between their translational and rotational dynamics. In this work, we\npropose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for\nquadrotor flight. By representing both quadrotor dynamics and the pose error\ndirectly on the dual-quaternion manifold, our approach enables a compact and\nglobally non-singular formulation that captures the quadrotor coupled dynamics.\nWe validate our approach through simulations and real-world experiments,\ndemonstrating better numerical conditioning and significantly improved tracking\nperformance, with reductions in position and orientation errors of up to 56.11%\nand 56.77%, compared to a conventional baseline NMPC method. Furthermore, our\ncontroller successfully handles aggressive trajectories, reaching maximum\nspeeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space\nconditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller\nfails.",
    "published": "2025-10-13T15:30:34Z",
    "updated": "2025-10-13T15:30:34Z",
    "link": "http://arxiv.org/pdf/2510.11525v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Luis F. Recalde",
      "Dhruv Agrawal",
      "Jon Arrizabalaga",
      "Guanrui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11448v1",
    "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems",
    "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m).",
    "published": "2025-10-13T14:17:14Z",
    "updated": "2025-10-13T14:17:14Z",
    "link": "http://arxiv.org/pdf/2510.11448v1.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "C.3; D.4.1; D.4.4; D.4.8; I.2.9"
    ],
    "authors": [
      "Yuankai He",
      "Hanlin Chen",
      "Weisong Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11421v1",
    "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation\n  in Smart Cities",
    "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability.",
    "published": "2025-10-13T13:56:15Z",
    "updated": "2025-10-13T13:56:15Z",
    "link": "http://arxiv.org/pdf/2510.11421v1.pdf",
    "category": [
      "cs.RO",
      "cs.HC"
    ],
    "authors": [
      "Shih-Chieh Sun",
      "Yun-Cheng Tsai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.11401v1",
    "title": "Path and Motion Optimization for Efficient Multi-Location Inspection\n  with Humanoid Robots",
    "summary": "This paper proposes a novel framework for humanoid robots to execute\ninspection tasks with high efficiency and millimeter-level precision. The\napproach combines hierarchical planning, time-optimal standing position\ngeneration, and integrated \\ac{mpc} to achieve high speed and precision. A\nhierarchical planning strategy, leveraging \\ac{ik} and \\ac{mip}, reduces\ncomputational complexity by decoupling the high-dimensional planning problem. A\nnovel MIP formulation optimizes standing position selection and trajectory\nlength, minimizing task completion time. Furthermore, an MPC system with\nsimplified kinematics and single-step position correction ensures\nmillimeter-level end-effector tracking accuracy. Validated through simulations\nand experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates\nlow time cost and a high success rate in multi-location tasks, enabling\nefficient and precise execution of complex industrial operations.",
    "published": "2025-10-13T13:44:08Z",
    "updated": "2025-10-13T13:44:08Z",
    "link": "http://arxiv.org/pdf/2510.11401v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiayang Wu",
      "Jiongye Li",
      "Shibowen Zhang",
      "Zhicheng He",
      "Zaijin Wang",
      "Xiaokun Leng",
      "Hangxin Liu",
      "Jingwen Zhang",
      "Jiayi Wang",
      "Song-Chun Zhu",
      "Yao Su"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.02167v2",
    "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle\n  Batteries Utilizing Product-Process-Resource Asset Network",
    "summary": "In a circular economy, products in their end-of-life phase should be either\nremanufactured or recycled. Both of these processes are crucial for\nsustainability and environmental conservation. However, manufacturers\nfrequently do not support these processes enough in terms of not sharing\nrelevant data about the products nor their (re-)manufacturing processes. This\npaper proposes to accompany each product with a digital twin technology,\nspecifically the Product Digital Twin (PDT), which can carry information for\nfacilitating and optimizing production and remanufacturing processes. This\npaper introduces a knowledge representation called Bi-Flow\nProduct-Process-Resource Asset Network (Bi-PAN). Bi-PAN extends a well-proven\nProduct-Process-Resource Asset Network (PAN) paradigm by integrating both\nassembly and disassembly workflows into a single information model. Such\nnetworks enable capturing relevant relationships across products, production\nresources, manufacturing processes, and specific production operations that\nhave to be done in the manufacturing phase of a product. The proposed approach\nis demonstrated in a use-case of disassembling electric vehicle (EV) batteries.\nBy utilizing PDTs with Bi-PAN knowledge models, challenges associated with\ndisassembling of EV batteries can be solved flexibly and efficiently for\nvarious battery types, enhancing the sustainability of the EV battery\nlife-cycle management.",
    "published": "2025-10-02T16:14:24Z",
    "updated": "2025-10-13T13:23:34Z",
    "link": "http://arxiv.org/pdf/2510.02167v2.pdf",
    "category": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "authors": [
      "Sara Strakosova",
      "Petr Novak",
      "Petr Kadera"
    ]
  }
]