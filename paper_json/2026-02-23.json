[
  {
    "id": "http://arxiv.org/abs/2602.20159v1",
    "title": "A Very Big Video Reasoning Suite",
    "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
    "published": "2026-02-23T18:59:41Z",
    "updated": "2026-02-23T18:59:41Z",
    "link": "http://arxiv.org/pdf/2602.20159v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "authors": [
      "Maijunxian Wang",
      "Ruisi Wang",
      "Juyi Lin",
      "Ran Ji",
      "Thaddäus Wiedemer",
      "Qingying Gao",
      "Dezhi Luo",
      "Yaoyao Qian",
      "Lianyu Huang",
      "Zelong Hong",
      "Jiahui Ge",
      "Qianli Ma",
      "Hang He",
      "Yifan Zhou",
      "Lingzi Guo",
      "Lantao Mei",
      "Jiachen Li",
      "Hanwen Xing",
      "Tianqi Zhao",
      "Fengyuan Yu",
      "Weihang Xiao",
      "Yizheng Jiao",
      "Jianheng Hou",
      "Danyang Zhang",
      "Pengcheng Xu",
      "Boyang Zhong",
      "Zehong Zhao",
      "Gaoyun Fang",
      "John Kitaoka",
      "Yile Xu",
      "Hua Xu",
      "Kenton Blacutt",
      "Tin Nguyen",
      "Siyuan Song",
      "Haoran Sun",
      "Shaoyue Wen",
      "Linyang He",
      "Runming Wang",
      "Yanzhi Wang",
      "Mengyue Yang",
      "Ziqiao Ma",
      "Raphaël Millière",
      "Freda Shi",
      "Nuno Vasconcelos",
      "Daniel Khashabi",
      "Alan Yuille",
      "Yilun Du",
      "Ziming Liu",
      "Bo Li",
      "Dahua Lin",
      "Ziwei Liu",
      "Vikash Kumar",
      "Yijiang Li",
      "Lei Yang",
      "Zhongang Cai",
      "Hokin Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20152v1",
    "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
    "summary": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.",
    "published": "2026-02-23T18:59:04Z",
    "updated": "2026-02-23T18:59:04Z",
    "link": "http://arxiv.org/pdf/2602.20152v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Zhenyao Ma",
      "Yue Liang",
      "Dongxu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20144v1",
    "title": "Agentic AI for Scalable and Robust Optical Systems Control",
    "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
    "published": "2026-02-23T18:54:32Z",
    "updated": "2026-02-23T18:54:32Z",
    "link": "http://arxiv.org/pdf/2602.20144v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.NI"
    ],
    "authors": [
      "Zehao Wang",
      "Mingzhe Han",
      "Wei Cheng",
      "Yue-Kai Huang",
      "Philip Ji",
      "Denton Wu",
      "Mahdi Safari",
      "Flemming Holtorf",
      "Kenaish AlQubaisi",
      "Norbert M. Linke",
      "Danyang Zhuo",
      "Yiran Chen",
      "Ting Wang",
      "Dirk Englund",
      "Tingjun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20141v1",
    "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
    "summary": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.",
    "published": "2026-02-23T18:53:09Z",
    "updated": "2026-02-23T18:53:09Z",
    "link": "http://arxiv.org/pdf/2602.20141v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Clarisse Wibault",
      "Johannes Forkel",
      "Sebastian Towers",
      "Tiphaine Wibault",
      "Juan Duque",
      "George Whittle",
      "Andreas Schaab",
      "Yucheng Yang",
      "Chiyuan Wang",
      "Michael Osborne",
      "Benjamin Moll",
      "Jakob Foerster"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.16666v2",
    "title": "Towards a Science of AI Agent Reliability",
    "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
    "published": "2026-02-18T18:05:44Z",
    "updated": "2026-02-23T18:49:07Z",
    "link": "http://arxiv.org/pdf/2602.16666v2.pdf",
    "category": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "authors": [
      "Stephan Rabanser",
      "Sayash Kapoor",
      "Peter Kirgis",
      "Kangheng Liu",
      "Saiteja Utpala",
      "Arvind Narayanan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.01149v2",
    "title": "A Benchmark of Causal vs. Correlation AI for Predictive Maintenance",
    "summary": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study benchmarks eight predictive models, ranging from baseline statistical approaches to Bayesian structural causal methods, on a dataset of 10,000 CNC machines with a 3.3 percent failure prevalence. While ensemble correlation-based models such as Random Forest (L4) achieve the highest raw cost savings (70.8 percent reduction), the Bayesian Structural Causal Model (L7) delivers competitive financial performance (66.4 percent cost reduction) with an inherent ability of failure attribution, which correlation-based models do not readily provide. The model achieves perfect attribution for HDF, PWF, and OSF failure types. These results suggest that causal methods, when combined with domain knowledge and Bayesian inference, offer a potentially favorable trade-off between predictive performance and operational interpretability in predictive maintenance applications.",
    "published": "2025-11-30T23:59:37Z",
    "updated": "2026-02-23T18:46:56Z",
    "link": "http://arxiv.org/pdf/2512.01149v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shaunak Dhande",
      "Chutian Ma",
      "Giacinto Paolo Saggese",
      "Paul Smith",
      "Krishna Taduri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.16547v3",
    "title": "Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation",
    "summary": "Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.",
    "published": "2025-05-22T11:37:39Z",
    "updated": "2026-02-23T18:46:55Z",
    "link": "http://arxiv.org/pdf/2505.16547v3.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Nitesh Subedi",
      "Hsin-Jung Yang",
      "Devesh K. Jha",
      "Soumik Sarkar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20135v1",
    "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
    "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
    "published": "2026-02-23T18:46:27Z",
    "updated": "2026-02-23T18:46:27Z",
    "link": "http://arxiv.org/pdf/2602.20135v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Mohammad Amanlou",
      "Erfan Shafiee Moghaddam",
      "Yasaman Amou Jafari",
      "Mahdi Noori",
      "Farhan Farsi",
      "Behnam Bahrak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20134v1",
    "title": "Modeling Epidemiological Dynamics Under Adversarial Data and User Deception",
    "summary": "Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.",
    "published": "2026-02-23T18:45:55Z",
    "updated": "2026-02-23T18:45:55Z",
    "link": "http://arxiv.org/pdf/2602.20134v1.pdf",
    "category": [
      "cs.GT",
      "cs.AI"
    ],
    "authors": [
      "Yiqi Su",
      "Christo Kurisummoottil Thomas",
      "Walid Saad",
      "Bud Mishra",
      "Naren Ramakrishnan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20133v1",
    "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
    "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
    "published": "2026-02-23T18:45:31Z",
    "updated": "2026-02-23T18:45:31Z",
    "link": "http://arxiv.org/pdf/2602.20133v1.pdf",
    "category": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Mert Cemri",
      "Shubham Agrawal",
      "Akshat Gupta",
      "Shu Liu",
      "Audrey Cheng",
      "Qiuyang Mang",
      "Ashwin Naren",
      "Lutfi Eren Erdogan",
      "Koushik Sen",
      "Matei Zaharia",
      "Alex Dimakis",
      "Ion Stoica"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20130v1",
    "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
    "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.\n  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.\n  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.\n  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.\n  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
    "published": "2026-02-23T18:42:50Z",
    "updated": "2026-02-23T18:42:50Z",
    "link": "http://arxiv.org/pdf/2602.20130v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zaifu Zhan",
      "Min Zeng",
      "Shuang Zhou",
      "Yiran Song",
      "Xiaoyi Chen",
      "Yu Hou",
      "Yifan Wu",
      "Yang Ruan",
      "Rui Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20122v1",
    "title": "NanoKnow: How to Know What Your Language Model Knows",
    "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
    "published": "2026-02-23T18:37:49Z",
    "updated": "2026-02-23T18:37:49Z",
    "link": "http://arxiv.org/pdf/2602.20122v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Lingwei Gu",
      "Nour Jedidi",
      "Jimmy Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.13174v2",
    "title": "Towards Unifying Perceptual Reasoning and Logical Reasoning",
    "summary": "An increasing number of scientific experiments support the view of perception as Bayesian inference, which is rooted in Helmholtz's view of perception as unconscious inference. Recent study of logic presents a view of logical reasoning as Bayesian inference. In this paper, we give a simple probabilistic model that is applicable to both perceptual reasoning and logical reasoning. We show that the model unifies the two essential processes common in perceptual and logical systems: on the one hand, the process by which perceptual and logical knowledge is derived from another knowledge, and on the other hand, the process by which such knowledge is derived from data. We fully characterise the model in terms of logical consequence relations.",
    "published": "2022-06-27T10:32:47Z",
    "updated": "2026-02-23T18:36:24Z",
    "link": "http://arxiv.org/pdf/2206.13174v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hiroyuki Kido"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20119v1",
    "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
    "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
    "published": "2026-02-23T18:35:18Z",
    "updated": "2026-02-23T18:35:18Z",
    "link": "http://arxiv.org/pdf/2602.20119v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Jiahui Fu",
      "Junyu Nan",
      "Lingfeng Sun",
      "Hongyu Li",
      "Jianing Qian",
      "Jennifer L. Barry",
      "Kris Kitani",
      "George Konidaris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20117v1",
    "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs",
    "published": "2026-02-23T18:34:29Z",
    "updated": "2026-02-23T18:34:29Z",
    "link": "http://arxiv.org/pdf/2602.20117v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Andre He",
      "Nathaniel Weir",
      "Kaj Bostrom",
      "Allen Nie",
      "Darion Cassel",
      "Sam Bayless",
      "Huzefa Rangwala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20114v1",
    "title": "Benchmarking Unlearning for Vision Transformers",
    "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
    "published": "2026-02-23T18:33:16Z",
    "updated": "2026-02-23T18:33:16Z",
    "link": "http://arxiv.org/pdf/2602.20114v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kairan Zhao",
      "Iurie Luca",
      "Peter Triantafillou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20113v1",
    "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion",
    "summary": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.",
    "published": "2026-02-23T18:32:59Z",
    "updated": "2026-02-23T18:32:59Z",
    "link": "http://arxiv.org/pdf/2602.20113v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Yisi Liu",
      "Nicholas Lee",
      "Gopala Anumanchipalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07751v4",
    "title": "AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
    "summary": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in grade school math (GSM) reasoning. In particular, they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In this work, we instead focus on the strategy of \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. Focusing on GSM, we find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks. Besides, improving GSM robustness via AbstRaL is shown to also implicitly benefit LLMs' capabilities on OOD mathematical and general reasoning tasks, indicating that abstract thinking broadly enables better generalizability.",
    "published": "2025-06-09T13:34:50Z",
    "updated": "2026-02-23T18:25:13Z",
    "link": "http://arxiv.org/pdf/2506.07751v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.SC"
    ],
    "authors": [
      "Silin Gao",
      "Antoine Bosselut",
      "Samy Bengio",
      "Emmanuel Abbe"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05165v3",
    "title": "EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.",
    "published": "2026-02-05T00:33:02Z",
    "updated": "2026-02-23T18:23:57Z",
    "link": "http://arxiv.org/pdf/2602.05165v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Kevin Han",
      "Yuhang Zhou",
      "Mingze Gao",
      "Gedi Zhou",
      "Serena Li",
      "Abhishek Kumar",
      "Xiangjun Fan",
      "Weiwei Li",
      "Lizhu Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20104v1",
    "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration",
    "summary": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.",
    "published": "2026-02-23T18:22:58Z",
    "updated": "2026-02-23T18:22:58Z",
    "link": "http://arxiv.org/pdf/2602.20104v1.pdf",
    "category": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Hasan Amin",
      "Ming Yin",
      "Rajiv Khanna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20102v1",
    "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
    "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
    "published": "2026-02-23T18:19:46Z",
    "updated": "2026-02-23T18:19:46Z",
    "link": "http://arxiv.org/pdf/2602.20102v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Thanh Q. Tran",
      "Arun Verma",
      "Kiwan Wong",
      "Bryan Kian Hsiang Low",
      "Daniela Rus",
      "Wei Xiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.05500v3",
    "title": "The Illusion of Human AI Parity Under Uncertainty: Navigating Elusive Ground Truth via a Probabilistic Paradigm",
    "summary": "Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is not just limited to human preferences, but is also consequential even in safety critical domains such as medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how - high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability. Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80\\%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.",
    "published": "2026-01-09T03:19:37Z",
    "updated": "2026-02-23T18:16:48Z",
    "link": "http://arxiv.org/pdf/2601.05500v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Aparna Elangovan",
      "Lei Xu",
      "Mahsa Elyasi",
      "Ismail Akdulum",
      "Mehmet Aksakal",
      "Enes Gurun",
      "Brian Hur",
      "Saab Mansour",
      "Ravid Shwartz Ziv",
      "Karin Verspoor",
      "Dan Roth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20100v1",
    "title": "Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine",
    "summary": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.",
    "published": "2026-02-23T18:15:30Z",
    "updated": "2026-02-23T18:15:30Z",
    "link": "http://arxiv.org/pdf/2602.20100v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "authors": [
      "Soumick Chatterjee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.14370v2",
    "title": "Competition for attention predicts good-to-bad tipping in AI",
    "summary": "More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.",
    "published": "2026-02-16T00:43:56Z",
    "updated": "2026-02-23T18:12:05Z",
    "link": "http://arxiv.org/pdf/2602.14370v2.pdf",
    "category": [
      "cs.AI",
      "physics.app-ph",
      "physics.soc-ph"
    ],
    "authors": [
      "Neil F. Johnson",
      "Frank Y. Huo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20094v1",
    "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
    "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
    "published": "2026-02-23T18:06:15Z",
    "updated": "2026-02-23T18:06:15Z",
    "link": "http://arxiv.org/pdf/2602.20094v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuzhe Wang",
      "Yaochen Zhu",
      "Jundong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13632v2",
    "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
    "summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities to speech inputs. However, these speech-adapted LLMs consistently underperform their text-based counterparts--and even cascaded pipelines--on language understanding tasks. We term this shortfall the text-speech understanding gap: the performance drop observed when a speech-adapted LLM processes spoken inputs relative to when the original text-based LLM processes the equivalent text. Recent approaches to narrowing this gap either rely on large-scale speech synthesis of text corpora, which is costly and heavily dependent on synthetic data, or on large-scale proprietary speech datasets, which are not reproducible. As a result, there remains a need for more data-efficient alternatives for closing the text-speech understanding gap. In this work, we analyze the gap as driven by two factors: (i) forgetting of text capabilities during adaptation, and (ii) cross-modal misalignment between speech and text. Based on this analysis, we introduce SALAD--Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation--which combines cross-modal distillation with targeted synthetic data to improve alignment while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves competitive performance with a strong open-weight model across broad-domain benchmarks in knowledge, language understanding, and reasoning, while training on over an order of magnitude less speech data from public corpora.",
    "published": "2025-10-15T14:57:16Z",
    "updated": "2026-02-23T18:05:51Z",
    "link": "http://arxiv.org/pdf/2510.13632v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "authors": [
      "Santiago Cuervo",
      "Skyler Seto",
      "Maureen de Seyssel",
      "Richard He Bai",
      "Zijin Gu",
      "Tatiana Likhomanenko",
      "Navdeep Jaitly",
      "Zakaria Aldeneh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.16210v2",
    "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
    "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.",
    "published": "2026-01-22T18:58:55Z",
    "updated": "2026-02-23T18:05:24Z",
    "link": "http://arxiv.org/pdf/2601.16210v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Onkar Susladkar",
      "Tushar Prakash",
      "Adheesh Juvekar",
      "Kiet A. Nguyen",
      "Dong-Hwan Jang",
      "Inderjit S Dhillon",
      "Ismini Lourentzou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.17898v2",
    "title": "Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally",
    "summary": "Over a billion users globally interact with AI systems engineered to mimic human traits. This development raises concerns that anthropomorphism, the attribution of human characteristics to AI, may foster over-reliance and misplaced trust. Yet, causal effects of humanlike AI design on users remain untested in ecologically valid, cross-cultural settings, leaving policy discussions to rely on theoretical assumptions derived largely from Western populations. Here we conducted two experiments (N=3,500) across ten countries representing a wide cultural spectrum, involving real-time, open-ended interactions with a state-of-the-art chatbot. We found users evaluate human-likeness based on pragmatic interactional cues (conversation flow, response speed, perspective-taking) rather than abstract theory-driven attributes emphasized in academic discourse (e.g., sentience, consciousness). Furthermore, while experimentally increasing chatbot's human-likeness reliably increased anthropomorphism across all sampled countries, it did not universally increase trust or engagement. Instead, effects were culturally contingent; design choices fostering engagement or trust in one country may reduce them in another. These findings challenge prevailing assumptions that humanlike AI poses uniform psychological risks and necessarily increases trust. Risk is not inherent to humanlike design but emerges from its interplay with cultural context. Consequently, governance frameworks must move beyond universalist approaches to account for this global heterogeneity.",
    "published": "2025-12-19T18:57:53Z",
    "updated": "2026-02-23T18:02:49Z",
    "link": "http://arxiv.org/pdf/2512.17898v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Robin Schimmelpfennig",
      "Mark Díaz",
      "Vinodkumar Prabhakaran",
      "Aida Davani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20089v1",
    "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
    "summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
    "published": "2026-02-23T17:57:37Z",
    "updated": "2026-02-23T17:57:37Z",
    "link": "http://arxiv.org/pdf/2602.20089v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zanxi Ruan",
      "Qiuyu Kong",
      "Songqun Gao",
      "Yiming Wang",
      "Marco Cristani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14242v3",
    "title": "APEX-Agents",
    "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open source Archipelago, our infrastructure for agent execution and evaluation.",
    "published": "2026-01-20T18:53:44Z",
    "updated": "2026-02-23T17:49:38Z",
    "link": "http://arxiv.org/pdf/2601.14242v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Bertie Vidgen",
      "Austin Mann",
      "Abby Fennelly",
      "John Wright Stanly",
      "Lucas Rothman",
      "Marco Burstein",
      "Julien Benchek",
      "David Ostrofsky",
      "Anirudh Ravichandran",
      "Debnil Sur",
      "Neel Venugopal",
      "Alannah Hsia",
      "Isaac Robinson",
      "Calix Huang",
      "Olivia Varones",
      "Daniyal Khan",
      "Michael Haines",
      "Austin Bridges",
      "Jesse Boyle",
      "Koby Twist",
      "Zach Richards",
      "Chirag Mahapatra",
      "Brendan Foody",
      "Osvald Nitski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20078v1",
    "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
    "summary": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.",
    "published": "2026-02-23T17:45:08Z",
    "updated": "2026-02-23T17:45:08Z",
    "link": "http://arxiv.org/pdf/2602.20078v1.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Shan Yang",
      "Yang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20076v1",
    "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems",
    "summary": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.",
    "published": "2026-02-23T17:40:05Z",
    "updated": "2026-02-23T17:40:05Z",
    "link": "http://arxiv.org/pdf/2602.20076v1.pdf",
    "category": [
      "eess.SY",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Wei Xiao",
      "Christos Cassandras",
      "Anni Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.17385v2",
    "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature",
    "summary": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.",
    "published": "2026-02-19T14:10:45Z",
    "updated": "2026-02-23T17:36:15Z",
    "link": "http://arxiv.org/pdf/2602.17385v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Angelo Porrello",
      "Pietro Buzzega",
      "Felix Dangel",
      "Thomas Sommariva",
      "Riccardo Salami",
      "Lorenzo Bonicelli",
      "Simone Calderara"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.14637v2",
    "title": "KINESIS: Motion Imitation for Human Musculoskeletal Locomotion",
    "summary": "How do humans move? Advances in reinforcement learning (RL) have produced impressive results in capturing human motion using physics-based humanoid control. However, torque-controlled humanoids fail to model key aspects of human motor control such as biomechanical joint constraints \\& non-linear and overactuated musculotendon control. We present KINESIS, a model-free motion imitation framework that tackles these challenges. KINESIS is trained on 1.8 hours of locomotion data and achieves strong motion imitation performance on unseen trajectories. Through a negative mining approach, KINESIS learns robust locomotion priors that we leverage to deploy the policy on several downstream tasks such as text-to-control, target point reaching, and football penalty kicks. Importantly, KINESIS learns to generate muscle activity patterns that correlate well with human EMG activity. We show that these results scale seamlessly across biomechanical model complexity, demonstrating control of up to 290 muscles. Overall, the physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control. Code, videos and benchmarks are available at https://github.com/amathislab/Kinesis.",
    "published": "2025-03-18T18:37:49Z",
    "updated": "2026-02-23T17:30:07Z",
    "link": "http://arxiv.org/pdf/2503.14637v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "authors": [
      "Merkourios Simos",
      "Alberto Silvio Chiappa",
      "Alexander Mathis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20066v1",
    "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
    "summary": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.",
    "published": "2026-02-23T17:22:54Z",
    "updated": "2026-02-23T17:22:54Z",
    "link": "http://arxiv.org/pdf/2602.20066v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kundan Thota",
      "Xuanhao Mu",
      "Thorsten Schlachter",
      "Veit Hagenmeyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20065v1",
    "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
    "summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
    "published": "2026-02-23T17:22:46Z",
    "updated": "2026-02-23T17:22:46Z",
    "link": "http://arxiv.org/pdf/2602.20065v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Natalia Moskvina",
      "Raquel Montero",
      "Masaya Yoshida",
      "Ferdy Hubers",
      "Paolo Morosi",
      "Walid Irhaymi",
      "Jin Yan",
      "Tamara Serrano",
      "Elena Pagliarini",
      "Fritz Günther",
      "Evelina Leivada"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20064v1",
    "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
    "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.",
    "published": "2026-02-23T17:22:35Z",
    "updated": "2026-02-23T17:22:35Z",
    "link": "http://arxiv.org/pdf/2602.20064v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI",
      "cs.CR"
    ],
    "authors": [
      "Zac Garby",
      "Andrew D. Gordon",
      "David Sands"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20059v1",
    "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
    "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.",
    "published": "2026-02-23T17:14:29Z",
    "updated": "2026-02-23T17:14:29Z",
    "link": "http://arxiv.org/pdf/2602.20059v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Sarath Shekkizhar",
      "Adam Earle"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20057v1",
    "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
    "summary": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.",
    "published": "2026-02-23T17:12:25Z",
    "updated": "2026-02-23T17:12:25Z",
    "link": "http://arxiv.org/pdf/2602.20057v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Ge Yuan",
      "Qiyuan Qiao",
      "Jing Zhang",
      "Dong Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20055v1",
    "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
    "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
    "published": "2026-02-23T17:10:00Z",
    "updated": "2026-02-23T17:10:00Z",
    "link": "http://arxiv.org/pdf/2602.20055v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Apoorva Vashisth",
      "Manav Kulshrestha",
      "Pranav Bakshi",
      "Damon Conover",
      "Guillaume Sartoretti",
      "Aniket Bera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08011v6",
    "title": "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
    "summary": "There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely.",
    "published": "2025-02-11T23:14:39Z",
    "updated": "2026-02-23T17:09:14Z",
    "link": "http://arxiv.org/pdf/2502.08011v6.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Mingyu Kim",
      "Dongjun Kim",
      "Amman Yusuf",
      "Stefano Ermon",
      "Mijung Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08119v2",
    "title": "SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents",
    "summary": "LLM-based agents struggle to execute complex, multi-step Standard Operating Procedures (SOPs) that are fundamental to industrial automation. Existing benchmarks fail to capture the procedural complexity and tool orchestration demands of real-world workflows. We introduce SOP-Bench, a benchmark of 2,000+ tasks from human expert-authored SOPs across 12 business domains (healthcare, logistics, finance, content moderation, etc.). Using a human-AI collaborative framework, experts crafted authentic SOPs while AI generated artifacts (tools, APIs, datasets), all human-validated, yielding realistic tasks with executable interfaces and ground-truth outputs.\n  SOP-Bench serves as a research enabler for systematically investigating agent architectures, model capabilities, and deployment considerations across diverse procedural tasks. We demonstrate its utility through illustrative experiments with a subset of frontier models across Function-Calling (FC) and ReAct agents, revealing critical insights. For example, (1) newer models do not guarantee better performance - Claude 4 family outperforms Claude 4.5 family on ReAct tasks (Claude 4 Opus: 72.4% vs. Claude 4.5 Sonnet: 63.3% task success rate), demonstrating that production upgrades require validation; (2) no single model-agent combination dominates: best performances range from 57% to 100% depending on domain. These examples illustrate how SOP-Bench enables isolating and studying specific dimensions of agent performance without costly production experiments. Our goal is not to rank model capabilities or build optimal agents, but to provide a rigorous evaluation framework that enables the researchers and practitioners to systematically investigate agent design choices, model selection, and deployment strategies. We release the benchmark at https://github.com/amazon-science/sop-bench.",
    "published": "2025-06-09T18:20:12Z",
    "updated": "2026-02-23T17:05:34Z",
    "link": "http://arxiv.org/pdf/2506.08119v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Subhrangshu Nandi",
      "Arghya Datta",
      "Rohith Nama",
      "Udita Patel",
      "Nikhil Vichare",
      "Indranil Bhattacharya",
      "Prince Grover",
      "Shivam Asija",
      "Giuseppe Carenini",
      "Wei Zhang",
      "Arushi Gupta",
      "Sreyoshi Bhaduri",
      "Jing Xu",
      "Huzefa Raja",
      "Shayan Ray",
      "Aaron Chan",
      "Esther Xu Fei",
      "Gaoyuan Du",
      "Zuhaib Akhtar",
      "Harshita Asnani",
      "Weian Chan",
      "Ming Xiong",
      "Francesco Carbone",
      "Jeetu Mirchandani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20051v1",
    "title": "SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency",
    "summary": "3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.",
    "published": "2026-02-23T17:00:35Z",
    "updated": "2026-02-23T17:00:35Z",
    "link": "http://arxiv.org/pdf/2602.20051v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yeonsung Kim",
      "Junggeun Do",
      "Seunguk Do",
      "Sangmin Kim",
      "Jaesik Park",
      "Jay-Yoon Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19800v3",
    "title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function",
    "summary": "There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear simple, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.",
    "published": "2025-09-24T06:36:11Z",
    "updated": "2026-02-23T16:58:45Z",
    "link": "http://arxiv.org/pdf/2509.19800v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Donghwan Lee",
      "Hyukjun Yang",
      "Bum Geun Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20048v1",
    "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence",
    "summary": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.",
    "published": "2026-02-23T16:58:37Z",
    "updated": "2026-02-23T16:58:37Z",
    "link": "http://arxiv.org/pdf/2602.20048v1.pdf",
    "category": [
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Tarakanath Paipuru"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06326v2",
    "title": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs",
    "summary": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.",
    "published": "2025-09-08T04:17:02Z",
    "updated": "2026-02-23T16:56:50Z",
    "link": "http://arxiv.org/pdf/2509.06326v2.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Ruisi Zhang",
      "Yifei Zhao",
      "Neusha Javidnia",
      "Mengxin Zheng",
      "Farinaz Koushanfar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20040v1",
    "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
    "summary": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.",
    "published": "2026-02-23T16:49:37Z",
    "updated": "2026-02-23T16:49:37Z",
    "link": "http://arxiv.org/pdf/2602.20040v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Fahmida Liza Piya",
      "Rahmatollah Beheshti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20031v1",
    "title": "Latent Introspection: Models Can Detect Prior Concept Injections",
    "summary": "We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.",
    "published": "2026-02-23T16:39:42Z",
    "updated": "2026-02-23T16:39:42Z",
    "link": "http://arxiv.org/pdf/2602.20031v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Theia Pearson-Vogel",
      "Martin Vanek",
      "Raymond Douglas",
      "Jan Kulveit"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06932v3",
    "title": "Symphonym: Universal Phonetic Embeddings for Cross-Script Name Matching",
    "summary": "Linking names across historical sources, languages, and writing systems remains a fundamental challenge in digital humanities and geographic information retrieval. Existing approaches require language-specific phonetic algorithms or fail to capture phonetic relationships across different scripts. This paper presents Symphonym, a neural embedding system that maps names from any script into a unified 128-dimensional phonetic space, enabling direct similarity comparison without runtime phonetic conversion. Symphonym uses a Teacher-Student architecture where a Teacher network trained on articulatory phonetic features produces target embeddings, while a Student network learns to approximate these embeddings directly from characters. The Teacher combines Epitran (extended with 100 new language-script mappings), Phonikud for Hebrew, and CharsiuG2P for Chinese, Japanese, and Korean. Training used 32.7 million triplet samples of toponyms spanning 20 writing systems from GeoNames, Wikidata, and Getty Thesaurus of Geographic Names. On the MEHDIE Hebrew-Arabic historical toponym benchmark, Symphonym achieves Recall@10 of 97.6% and MRR of 90.3%, outperforming Levenshtein and Jaro-Winkler baselines (Recall@1: 86.7% vs 81.5% and 78.5%). Evaluation on 12,947 real cross-script training pairs shows 82.6% achieve greater than 0.75 cosine similarity, with best performance on Arabic-Cyrillic (94--100%) and Cyrillic-Latin (94.3%) combinations. The fixed-length embeddings enable efficient retrieval in digital humanities workflows, with a case study on medieval personal names demonstrating effective transfer from modern place names to historical orthographic variation.",
    "published": "2026-01-11T14:36:36Z",
    "updated": "2026-02-23T16:39:36Z",
    "link": "http://arxiv.org/pdf/2601.06932v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Stephen Gadd"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20021v1",
    "title": "Agents of Chaos",
    "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.",
    "published": "2026-02-23T16:28:48Z",
    "updated": "2026-02-23T16:28:48Z",
    "link": "http://arxiv.org/pdf/2602.20021v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Natalie Shapira",
      "Chris Wendler",
      "Avery Yen",
      "Gabriele Sarti",
      "Koyena Pal",
      "Olivia Floody",
      "Adam Belfki",
      "Alex Loftus",
      "Aditya Ratan Jannali",
      "Nikhil Prakash",
      "Jasmine Cui",
      "Giordano Rogers",
      "Jannik Brinkmann",
      "Can Rager",
      "Amir Zur",
      "Michael Ripa",
      "Aruna Sankaranarayanan",
      "David Atkinson",
      "Rohit Gandikota",
      "Jaden Fiotto-Kaufman",
      "EunJeong Hwang",
      "Hadas Orgad",
      "P Sam Sahil",
      "Negev Taglicht",
      "Tomer Shabtay",
      "Atai Ambus",
      "Nitay Alon",
      "Shiri Oron",
      "Ayelet Gordon-Tapiero",
      "Yotam Kaplan",
      "Vered Shwartz",
      "Tamar Rott Shaham",
      "Christoph Riedl",
      "Reuth Mirsky",
      "Maarten Sap",
      "David Manheim",
      "Tomer Ullman",
      "David Bau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20019v1",
    "title": "Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision",
    "summary": "Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \\textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.",
    "published": "2026-02-23T16:25:35Z",
    "updated": "2026-02-23T16:25:35Z",
    "link": "http://arxiv.org/pdf/2602.20019v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yuxing Tian",
      "Yiyan Qi",
      "Fengran Mo",
      "Weixu Zhang",
      "Jian Guo",
      "Jian-Yun Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20003v1",
    "title": "A Secure and Private Distributed Bayesian Federated Learning Design",
    "summary": "Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.",
    "published": "2026-02-23T16:12:02Z",
    "updated": "2026-02-23T16:12:02Z",
    "link": "http://arxiv.org/pdf/2602.20003v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nuocheng Yang",
      "Sihua Wang",
      "Zhaohui Yang",
      "Mingzhe Chen",
      "Changchuan Yin",
      "Kaibin Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03646v4",
    "title": "GRILL: Restoring Gradient Signal in Ill-Conditioned Layers for More Effective Adversarial Attacks on Autoencoders",
    "summary": "Adversarial robustness of deep autoencoders (AEs) has received less attention than that of discriminative models, although their compressed latent representations induce ill-conditioned mappings that can amplify small input perturbations and destabilize reconstructions. Existing white-box attacks for AEs, which optimize norm-bounded adversarial perturbations to maximize output damage, often stop at suboptimal attacks. We observe that this limitation stems from vanishing adversarial loss gradients during backpropagation through ill-conditioned layers, caused by near-zero singular values in their Jacobians. To address this issue, we introduce GRILL, a technique that locally restores gradient signals in ill-conditioned layers, enabling more effective norm-bounded attacks. Through extensive experiments across multiple AE architectures, considering both sample-specific and universal attacks under both standard and adaptive attack settings, we show that GRILL significantly increases attack effectiveness, leading to a more rigorous evaluation of AE robustness. Beyond AEs, we provide empirical evidence that modern multimodal architectures with encoder-decoder structures exhibit similar vulnerabilities under GRILL.",
    "published": "2025-05-06T15:52:14Z",
    "updated": "2026-02-23T16:09:40Z",
    "link": "http://arxiv.org/pdf/2505.03646v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Chethan Krishnamurthy Ramanaik",
      "Arjun Roy",
      "Tobias Callies",
      "Eirini Ntoutsi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.10604v2",
    "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
    "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
    "published": "2026-02-11T07:53:51Z",
    "updated": "2026-02-23T16:07:40Z",
    "link": "http://arxiv.org/pdf/2602.10604v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Ailin Huang",
      "Ang Li",
      "Aobo Kong",
      "Bin Wang",
      "Binxing Jiao",
      "Bo Dong",
      "Bojun Wang",
      "Boyu Chen",
      "Brian Li",
      "Buyun Ma",
      "Chang Su",
      "Changxin Miao",
      "Changyi Wan",
      "Chao Lou",
      "Chen Hu",
      "Chen Xu",
      "Chenfeng Yu",
      "Chengting Feng",
      "Chengyuan Yao",
      "Chunrui Han",
      "Dan Ma",
      "Dapeng Shi",
      "Daxin Jiang",
      "Dehua Ma",
      "Deshan Sun",
      "Di Qi",
      "Enle Liu",
      "Fajie Zhang",
      "Fanqi Wan",
      "Guanzhe Huang",
      "Gulin Yan",
      "Guoliang Cao",
      "Guopeng Li",
      "Han Cheng",
      "Hangyu Guo",
      "Hanshan Zhang",
      "Hao Nie",
      "Haonan Jia",
      "Haoran Lv",
      "Hebin Zhou",
      "Hekun Lv",
      "Heng Wang",
      "Heung-Yeung Shum",
      "Hongbo Huang",
      "Hongbo Peng",
      "Hongyu Zhou",
      "Hongyuan Wang",
      "Houyong Chen",
      "Huangxi Zhu",
      "Huimin Wu",
      "Huiyong Guo",
      "Jia Wang",
      "Jian Zhou",
      "Jianjian Sun",
      "Jiaoren Wu",
      "Jiaran Zhang",
      "Jiashu Lv",
      "Jiashuo Liu",
      "Jiayi Fu",
      "Jiayu Liu",
      "Jie Cheng",
      "Jie Luo",
      "Jie Yang",
      "Jie Zhou",
      "Jieyi Hou",
      "Jing Bai",
      "Jingcheng Hu",
      "Jingjing Xie",
      "Jingwei Wu",
      "Jingyang Zhang",
      "Jishi Zhou",
      "Junfeng Liu",
      "Junzhe Lin",
      "Ka Man Lo",
      "Kai Liang",
      "Kaibo Liu",
      "Kaijun Tan",
      "Kaiwen Yan",
      "Kaixiang Li",
      "Kang An",
      "Kangheng Lin",
      "Lei Yang",
      "Liang Lv",
      "Liang Zhao",
      "Liangyu Chen",
      "Lieyu Shi",
      "Liguo Tan",
      "Lin Lin",
      "Lina Chen",
      "Luck Ma",
      "Mengqiang Ren",
      "Michael Li",
      "Ming Li",
      "Mingliang Li",
      "Mingming Zhang",
      "Mingrui Chen",
      "Mitt Huang",
      "Na Wang",
      "Peng Liu",
      "Qi Han",
      "Qian Zhao",
      "Qinglin He",
      "Qinxin Du",
      "Qiuping Wu",
      "Quan Sun",
      "Rongqiu Yang",
      "Ruihang Miao",
      "Ruixin Han",
      "Ruosi Wan",
      "Ruyan Guo",
      "Shan Wang",
      "Shaoliang Pang",
      "Shaowen Yang",
      "Shengjie Fan",
      "Shijie Shang",
      "Shiliang Yang",
      "Shiwei Li",
      "Shuangshuang Tian",
      "Siqi Liu",
      "Siye Wu",
      "Siyu Chen",
      "Song Yuan",
      "Tiancheng Cao",
      "Tianchi Yue",
      "Tianhao Cheng",
      "Tianning Li",
      "Tingdan Luo",
      "Wang You",
      "Wei Ji",
      "Wei Yuan",
      "Wei Zhang",
      "Weibo Wu",
      "Weihao Xie",
      "Wen Sun",
      "Wenjin Deng",
      "Wenzhen Zheng",
      "Wuxun Xie",
      "Xiangfeng Wang",
      "Xiangwen Kong",
      "Xiangyu Liu",
      "Xiangyu Zhang",
      "Xiaobo Yang",
      "Xiaojia Liu",
      "Xiaolan Yuan",
      "Xiaoran Jiao",
      "Xiaoxiao Ren",
      "Xiaoyun Zhang",
      "Xin Li",
      "Xin Liu",
      "Xin Wu",
      "Xing Chen",
      "Xingping Yang",
      "Xinran Wang",
      "Xu Zhao",
      "Xuan He",
      "Xuanti Feng",
      "Xuedan Cai",
      "Xuqiang Zhou",
      "Yanbo Yu",
      "Yang Li",
      "Yang Xu",
      "Yanlin Lai",
      "Yanming Xu",
      "Yaoyu Wang",
      "Yeqing Shen",
      "Yibo Zhu",
      "Yichen Lv",
      "Yicheng Cao",
      "Yifeng Gong",
      "Yijing Yang",
      "Yikun Yang",
      "Yin Zhao",
      "Yingxiu Zhao",
      "Yinmin Zhang",
      "Yitong Zhang",
      "Yixuan Zhang",
      "Yiyang Chen",
      "Yongchi Zhao",
      "Yongshen Long",
      "Yongyao Wang",
      "Yousong Guan",
      "Yu Zhou",
      "Yuang Peng",
      "Yuanhao Ding",
      "Yuantao Fan",
      "Yuanwei Lu",
      "Yuanzhen Yang",
      "Yuchu Luo",
      "Yudi Zhao",
      "Yue Peng",
      "Yueqiang Lin",
      "Yufan Lu",
      "Yuling Zhao",
      "Yunzhou Ju",
      "Yurong Zhang",
      "Yusheng Li",
      "Yuxiang Yang",
      "Yuyang Chen",
      "Yuzhu Cai",
      "Zejia Weng",
      "Zetao Hong",
      "Zexi Li",
      "Zhe Xie",
      "Zheng Ge",
      "Zheng Gong",
      "Zheng Zeng",
      "Zhenyi Lu",
      "Zhewei Huang",
      "Zhichao Chang",
      "Zhiguo Huang",
      "Zhiheng Hu",
      "Zidong Yang",
      "Zili Wang",
      "Ziqi Ren",
      "Zixin Zhang",
      "Zixuan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07805v4",
    "title": "Group Representational Position Encoding",
    "summary": "We present GRAPE (Group Representational Position Encoding), a unified framework for positional encoding based on group actions. GRAPE unifies two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\operatorname{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n) = \\exp(n \\, ω\\, \\mathbf{L})$ with a rank-2 skew-symmetric generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes correspond to canonical coordinate pairs with a log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise from rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Overall, GRAPE provides a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project page: https://github.com/model-architectures/GRAPE.",
    "published": "2025-12-08T18:39:13Z",
    "updated": "2026-02-23T15:57:05Z",
    "link": "http://arxiv.org/pdf/2512.07805v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yifan Zhang",
      "Zixiang Chen",
      "Yifeng Liu",
      "Zhen Qin",
      "Huizhuo Yuan",
      "Kangping Xu",
      "Yang Yuan",
      "Quanquan Gu",
      "Andrew Chi-Chih Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19983v1",
    "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
    "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
    "published": "2026-02-23T15:51:23Z",
    "updated": "2026-02-23T15:51:23Z",
    "link": "http://arxiv.org/pdf/2602.19983v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Zachary Ravichadran",
      "David Snyder",
      "Alexander Robey",
      "Hamed Hassani",
      "Vijay Kumar",
      "George J. Pappas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19969v1",
    "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
    "summary": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",
    "published": "2026-02-23T15:30:52Z",
    "updated": "2026-02-23T15:30:52Z",
    "link": "http://arxiv.org/pdf/2602.19969v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yuxing Tian",
      "Fengran Mo",
      "Weixu Zhang",
      "Yiyan Qi",
      "Jian-Yun Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19964v1",
    "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
    "summary": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.",
    "published": "2026-02-23T15:28:27Z",
    "updated": "2026-02-23T15:28:27Z",
    "link": "http://arxiv.org/pdf/2602.19964v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "authors": [
      "Moritz A. Zanger",
      "Yijun Wu",
      "Pascal R. Van der Vaart",
      "Wendelin Böhmer",
      "Matthijs T. J. Spaan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19948v1",
    "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
    "summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.\n  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
    "published": "2026-02-23T15:17:18Z",
    "updated": "2026-02-23T15:17:18Z",
    "link": "http://arxiv.org/pdf/2602.19948v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.MA"
    ],
    "authors": [
      "Ian Steenstra",
      "Paola Pedrelli",
      "Weiyan Shi",
      "Stacy Marsella",
      "Timothy W. Bickmore"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19946v1",
    "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators",
    "summary": "Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.",
    "published": "2026-02-23T15:15:53Z",
    "updated": "2026-02-23T15:15:53Z",
    "link": "http://arxiv.org/pdf/2602.19946v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Krzysztof Adamkiewicz",
      "Brian Moser",
      "Stanislav Frolov",
      "Tobias Christian Nauen",
      "Federico Raue",
      "Andreas Dengel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19945v1",
    "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models",
    "summary": "Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\\varepsilon,δ)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\\%. The code is available in Appendix.",
    "published": "2026-02-23T15:15:47Z",
    "updated": "2026-02-23T15:15:47Z",
    "link": "http://arxiv.org/pdf/2602.19945v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jin Liu",
      "Yinbin Miao",
      "Ning Xi",
      "Junkang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.08550v2",
    "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing",
    "summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.",
    "published": "2026-02-09T11:50:29Z",
    "updated": "2026-02-23T15:12:01Z",
    "link": "http://arxiv.org/pdf/2602.08550v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "authors": [
      "Shih-Fang Chen",
      "Jun-Cheng Chen",
      "I-Hong Jhuo",
      "Yen-Yu Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19930v1",
    "title": "Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning",
    "summary": "Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.",
    "published": "2026-02-23T15:06:33Z",
    "updated": "2026-02-23T15:06:33Z",
    "link": "http://arxiv.org/pdf/2602.19930v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Nathan Gavenski",
      "Felipe Meneguzzi",
      "Odinaldo Rodrigues"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19926v1",
    "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models",
    "summary": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.",
    "published": "2026-02-23T15:05:28Z",
    "updated": "2026-02-23T15:05:28Z",
    "link": "http://arxiv.org/pdf/2602.19926v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jin Liu",
      "Yinbin Miao",
      "Ning Xi",
      "Junkang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19914v1",
    "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning",
    "summary": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.",
    "published": "2026-02-23T14:54:38Z",
    "updated": "2026-02-23T14:54:38Z",
    "link": "http://arxiv.org/pdf/2602.19914v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Thatchawin Leelawat",
      "Lewis D Griffin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.06199v4",
    "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning",
    "summary": "Pretrained neural networks have attracted significant interest in chemistry and small molecule drug design. Embeddings from these models are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry. This study presents the most extensive comparison of such models to date, evaluating 25 models across 25 datasets. Under a fair comparison framework, we assess models spanning various modalities, architectures, and pretraining strategies. Using a dedicated hierarchical Bayesian statistical testing model, we arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model, which is also based on molecular fingerprints, performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies. We discuss potential causes, propose solutions, and offer practical recommendations.",
    "published": "2025-08-08T10:29:24Z",
    "updated": "2026-02-23T14:41:11Z",
    "link": "http://arxiv.org/pdf/2508.06199v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Mateusz Praski",
      "Jakub Adamczyk",
      "Wojciech Czech"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.16890v2",
    "title": "Layer Collapse Can be Induced by Unstructured Pruning",
    "summary": "Unstructured pruning is a popular compression method for efficiently reducing model parameters. However, while it effectively decreases the number of parameters, it is commonly believed that unstructured pruning cannot shorten the computational critical path, i.e., the maximum number of layers traversed during forward propagation.\n  In this paper, we study when and how unstructured pruning can yield structural effects. For rectifier-activated networks, we introduce the notion of neuron entropy, which quantifies the degree of nonlinearity utilization. We show that magnitude-based pruning naturally lowers this entropy, sometimes down to zero-entropy layers that become linearizable and can thus be removed. Building on this insight, we propose a method that leverages \"unstructured\" pruning to favor sparsity in low-entropy layers, enabling their complete removal. We validate the phenomenon across CNNs, Vision Transformers, and NLP models: unstructured pruning can induce effective layer removal with little or no performance degradation in over-parameterized networks.",
    "published": "2024-04-24T09:12:04Z",
    "updated": "2026-02-23T14:40:05Z",
    "link": "http://arxiv.org/pdf/2404.16890v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhu Liao",
      "Victor Quétu",
      "Van-Tam Nguyen",
      "Enzo Tartaglione"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.00017v3",
    "title": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation",
    "summary": "We present Generative Logic (GL), a deterministic architecture that starts from user-supplied axiomatic definitions, written in a minimalist Mathematical Programming Language (MPL), and systematically explores a configurable region of their deductive neighborhood. A defining feature of the architecture is its unified hash-based inference engine, which executes both algebraic manipulations and deterministic logical transformations. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; whenever the premises of an inference rule unify, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs. Experimental validation is performed on Elementary Number Theory (ENT) utilizing a batched execution strategy. Starting from foundational axioms and definitions, the system first develops first-order Peano arithmetic, which is subsequently applied to autonomously derive and prove Gauss's summation formula as a main result. To manage combinatorial explosion, GL algorithmically enumerates conjectures and applies normalization, type constraints, and counterexample (CE) filtering. On commodity hardware, an end-to-end run completes in under 7 minutes. Generated proofs export as navigable HTML so that every inference step can be inspected independently. We outline a hardware-software co-design path toward massively parallel realizations and describe future integration with large language models (LLMs) for auto-formalization and conjecture seeding. The Python, C++, and MPL code to reproduce these experiments, along with the full proof graphs in HTML as well as machine-readable text format, are available in the project's GitHub repository at github.com/Generative-Logic/GL commit 1771330 and are permanently archived at doi:10.5281/zenodo.17206386.",
    "published": "2025-07-25T17:29:19Z",
    "updated": "2026-02-23T14:37:20Z",
    "link": "http://arxiv.org/pdf/2508.00017v3.pdf",
    "category": [
      "cs.LO",
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Nikolai Sergeev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19881v1",
    "title": "Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations",
    "summary": "Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd",
    "published": "2026-02-23T14:27:36Z",
    "updated": "2026-02-23T14:27:36Z",
    "link": "http://arxiv.org/pdf/2602.19881v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Blaž Rolih",
      "Matic Fučka",
      "Filip Wolf",
      "Luka Čehovin Zajc"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.14674v3",
    "title": "From User Preferences to Base Score Extraction Functions in Gradual Argumentation (with Appendix)",
    "summary": "Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \\emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \\emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \\emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.",
    "published": "2026-02-16T12:01:58Z",
    "updated": "2026-02-23T14:16:18Z",
    "link": "http://arxiv.org/pdf/2602.14674v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Aniol Civit",
      "Antonio Rago",
      "Antonio Andriella",
      "Guillem Alenyà",
      "Francesca Toni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19872v1",
    "title": "GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery",
    "summary": "Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.",
    "published": "2026-02-23T14:15:56Z",
    "updated": "2026-02-23T14:15:56Z",
    "link": "http://arxiv.org/pdf/2602.19872v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jizhou Han",
      "Chenhao Ding",
      "SongLin Dong",
      "Yuhang He",
      "Shaokun Wang",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.19657v4",
    "title": "One Token Is Enough: Improving Diffusion Language Models with a Sink Token",
    "summary": "Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.",
    "published": "2026-01-27T14:32:36Z",
    "updated": "2026-02-23T14:06:12Z",
    "link": "http://arxiv.org/pdf/2601.19657v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Zihou Zhang",
      "Zheyong Xie",
      "Li Zhong",
      "Haifeng Liu",
      "Yao Hu",
      "Shaosheng Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.16874v2",
    "title": "Budget Allocation Policies for Real-Time Multi-Agent Path Finding",
    "summary": "Multi-Agent Path finding (MAPF) is the problem of finding paths for a set of agents such that each agent reaches its desired destination while avoiding collisions with the other agents. This problem arises in many robotics applications, such as automated warehouses and swarms of drones. Many MAPF solvers are designed to run offline, that is, first generate paths for all agents and then execute them. In real-world scenarios, waiting for a complete solution before allowing any robot to move is often impractical. Real-time MAPF (RT-MAPF) captures this setting by assuming that agents must begin execution after a fixed planning period, referred to as the planning budget, and execute a fixed number of actions, referred to as the execution window. This results in an iterative process in which a short plan is executed, while the next execution window is planned concurrently. Existing solutions to RT-MAPF iteratively call windowed versions of MAPF algorithms in every planning period, without explicitly considering the size of the planning budget. We address this gap and explore different policies for allocating the planning budget in windowed versions of MAPF-LNS2, a state-of-the-art MAPF algorithm. Our exploration shows that the baseline approach in which all agents draw from a shared planning budget pool is ineffective in challenging scenarios. Instead, policies that intelligently distribute the planning budget among agents are able to solve more problem instances in less time.",
    "published": "2025-07-22T08:32:55Z",
    "updated": "2026-02-23T13:53:21Z",
    "link": "http://arxiv.org/pdf/2507.16874v2.pdf",
    "category": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Raz Beck",
      "Roni Stern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22147v3",
    "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions",
    "summary": "When allowing concurrent actions in Markov Decision Processes, whose state and action spaces grow exponentially in the number of objects, computing a policy becomes highly inefficient, as it requires enumerating the joint of the two spaces. For the case of indistinguishable objects, we present a first-order representation to tackle the exponential blow-up in the action and state spaces. We propose Foreplan, an efficient relational forward planner, which uses the first-order representation allowing to compute policies in space and time polynomially in the number of objects. Thus, Foreplan significantly increases the number of planning problems solvable in an exact manner in reasonable time, which we underscore with a theoretical analysis. To speed up computations even further, we also introduce an approximate version of Foreplan, including guarantees on the error. Further, we provide an empirical evaluation of both Foreplan versions, demonstrating a speedup of several orders of magnitude. For the approximate version of Foreplan, we also empirically show that the induced error is often negligible.",
    "published": "2025-05-28T09:08:27Z",
    "updated": "2026-02-23T13:51:27Z",
    "link": "http://arxiv.org/pdf/2505.22147v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Florian Andreas Marwitz",
      "Tanya Braun",
      "Ralf Möller",
      "Marcel Gehrke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19844v1",
    "title": "LLM-enabled Applications Require System-Level Threat Monitoring",
    "summary": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.",
    "published": "2026-02-23T13:48:36Z",
    "updated": "2026-02-23T13:48:36Z",
    "link": "http://arxiv.org/pdf/2602.19844v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Yedi Zhang",
      "Haoyu Wang",
      "Xianglin Yang",
      "Jin Song Dong",
      "Jun Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19843v1",
    "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems",
    "summary": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.",
    "published": "2026-02-23T13:47:43Z",
    "updated": "2026-02-23T13:47:43Z",
    "link": "http://arxiv.org/pdf/2602.19843v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Jin Jia",
      "Zhiling Deng",
      "Zhuangbin Chen",
      "Yingqi Wang",
      "Zibin Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.24943v2",
    "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
    "summary": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.",
    "published": "2025-12-31T16:09:08Z",
    "updated": "2026-02-23T13:41:40Z",
    "link": "http://arxiv.org/pdf/2512.24943v2.pdf",
    "category": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Chenji Lu",
      "Zhuo Chen",
      "Hui Zhao",
      "Zhenyi Wang",
      "Pengjie Wang",
      "Chuan Yu",
      "Jian Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19837v1",
    "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent",
    "summary": "Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.",
    "published": "2026-02-23T13:39:58Z",
    "updated": "2026-02-23T13:39:58Z",
    "link": "http://arxiv.org/pdf/2602.19837v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Björn Hoppmann",
      "Christoph Scholz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19822v1",
    "title": "Efficient endometrial carcinoma screening via cross-modal synthesis and gradient distillation",
    "summary": "Early detection of myometrial invasion is critical for the staging and life-saving management of endometrial carcinoma (EC), a prevalent global malignancy. Transvaginal ultrasound serves as the primary, accessible screening modality in resource-constrained primary care settings; however, its diagnostic reliability is severely hindered by low tissue contrast, high operator dependence, and a pronounced scarcity of positive pathological samples. Existing artificial intelligence solutions struggle to overcome this severe class imbalance and the subtle imaging features of invasion, particularly under the strict computational limits of primary care clinics. Here we present an automated, highly efficient two-stage deep learning framework that resolves both data and computational bottlenecks in EC screening. To mitigate pathological data scarcity, we develop a structure-guided cross-modal generation network that synthesizes diverse, high-fidelity ultrasound images from unpaired magnetic resonance imaging (MRI) data, strictly preserving clinically essential anatomical junctions. Furthermore, we introduce a lightweight screening network utilizing gradient distillation, which transfers discriminative knowledge from a high-capacity teacher model to dynamically guide sparse attention towards task-critical regions. Evaluated on a large, multicenter cohort of 7,951 participants, our model achieves a sensitivity of 99.5\\%, a specificity of 97.2\\%, and an area under the curve of 0.987 at a minimal computational cost (0.289 GFLOPs), substantially outperforming the average diagnostic accuracy of expert sonographers. Our approach demonstrates that combining cross-modal synthetic augmentation with knowledge-driven efficient modeling can democratize expert-level, real-time cancer screening for resource-constrained primary care settings.",
    "published": "2026-02-23T13:22:25Z",
    "updated": "2026-02-23T13:22:25Z",
    "link": "http://arxiv.org/pdf/2602.19822v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Dongjing Shan",
      "Yamei Luo",
      "Jiqing Xuan",
      "Lu Huang",
      "Jin Li",
      "Mengchu Yang",
      "Zeyu Chen",
      "Fajin Lv",
      "Yong Tang",
      "Chunxiang Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19818v1",
    "title": "SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models",
    "summary": "Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.",
    "published": "2026-02-23T13:19:43Z",
    "updated": "2026-02-23T13:19:43Z",
    "link": "http://arxiv.org/pdf/2602.19818v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Hillel Ohayon",
      "Daniel Gilkarov",
      "Ran Dubin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.02780v3",
    "title": "PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts",
    "summary": "Smart contracts operate in a highly adversarial environment, where vulnerabilities can lead to substantial financial losses. Thus, smart contracts are subject to security audits. In auditing, proof-of-concept (PoC) exploits play a critical role by demonstrating to the stakeholders that the reported vulnerabilities are genuine, reproducible, and actionable. However, manually creating PoCs is time-consuming, error-prone, and often constrained by tight audit schedules. We introduce PoCo, an agentic framework that automatically generates executable PoC exploits from natural-language vulnerability descriptions written by auditors. PoCo autonomously generates PoC exploits in an agentic manner by interacting with a set of codeexecution tools in a Reason-Act-Observe loop. It produces fully executable exploits compatible with the Foundry testing framework, ready for integration into audit reports and other security tools. We evaluate PoCo on a dataset of 23 real-world vulnerability reports. PoCo consistently outperforms the Zero-shot and Workflow baselines, generating well-formed and logically correct PoCs. Our results demonstrate that agentic frameworks can significantly reduce the effort required for high-quality PoCs in smart contract audits. Our contribution provides actionable knowledge for the smart contract security community.",
    "published": "2025-11-04T18:03:12Z",
    "updated": "2026-02-23T13:17:25Z",
    "link": "http://arxiv.org/pdf/2511.02780v3.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "authors": [
      "Vivi Andersson",
      "Sofia Bobadilla",
      "Harald Hobbelhagen",
      "Martin Monperrus"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19816v1",
    "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling",
    "summary": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.",
    "published": "2026-02-23T13:13:41Z",
    "updated": "2026-02-23T13:13:41Z",
    "link": "http://arxiv.org/pdf/2602.19816v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yungang Yi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.17596v4",
    "title": "Evaluating LLMs' Divergent Thinking Capabilities for Scientific Idea Generation with Minimal Context",
    "summary": "While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.",
    "published": "2024-12-23T14:13:44Z",
    "updated": "2026-02-23T13:11:55Z",
    "link": "http://arxiv.org/pdf/2412.17596v4.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Kai Ruan",
      "Xuan Wang",
      "Jixiang Hong",
      "Peng Wang",
      "Yang Liu",
      "Hao Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19810v1",
    "title": "OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research",
    "summary": "In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.",
    "published": "2026-02-23T13:10:01Z",
    "updated": "2026-02-23T13:10:01Z",
    "link": "http://arxiv.org/pdf/2602.19810v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Lukas Weidener",
      "Marko Brkić",
      "Mihailo Jovanović",
      "Ritvik Singh",
      "Emre Ulgac",
      "Aakaash Meduri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19805v1",
    "title": "Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing",
    "summary": "Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.",
    "published": "2026-02-23T13:03:48Z",
    "updated": "2026-02-23T13:03:48Z",
    "link": "http://arxiv.org/pdf/2602.19805v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Wall Kim",
      "Chaeyoung Song",
      "Hanul Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.15888v2",
    "title": "NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing",
    "summary": "Objective. Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. Approach. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Main results. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded sleep-cassette (SC) subset with single-channel EEG demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared to the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. Significance. By coupling neuromorphic event encoding with state-aware context modeling, NeuroSleep offers a deployment-oriented framework for single-channel sleep staging that reduces redundant high-rate processing and improves energy scalability for wearable and edge platforms.",
    "published": "2026-02-06T13:16:28Z",
    "updated": "2026-02-23T12:47:29Z",
    "link": "http://arxiv.org/pdf/2602.15888v2.pdf",
    "category": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Boyu Li",
      "Xingchun Zhu",
      "Yonghui Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19786v1",
    "title": "The Climate Change Knowledge Graph: Supporting Climate Services",
    "summary": "Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.",
    "published": "2026-02-23T12:42:05Z",
    "updated": "2026-02-23T12:42:05Z",
    "link": "http://arxiv.org/pdf/2602.19786v1.pdf",
    "category": [
      "cs.DB",
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Miguel Ceriani",
      "Fiorela Ciroku",
      "Alessandro Russo",
      "Massimiliano Schembri",
      "Fai Fung",
      "Neha Mittal",
      "Vito Trianni",
      "Andrea Giovanni Nuzzolese"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2305.11098v2",
    "title": "A Simple Generative Model of Logical Reasoning and Statistical Learning",
    "summary": "Statistical learning and logical reasoning are two major fields of AI expected to be unified for human-like machine intelligence. Most existing work considers how to combine existing logical and statistical systems. However, there is no theory of inference so far explaining how basic approaches to statistical learning and logical reasoning stem from a common principle. Inspired by the fact that much empirical work in neuroscience suggests Bayesian (or probabilistic generative) approaches to brain function including learning and reasoning, we here propose a simple Bayesian model of logical reasoning and statistical learning. The theory is statistically correct as it satisfies Kolmogorov's axioms, is consistent with both Fenstad's representation theorem and maximum likelihood estimation and performs exact Bayesian inference with a linear-time complexity. The theory is logically correct as it is a data-driven generalisation of uncertain reasoning from consistency, possibility, inconsistency and impossibility. The theory is correct in terms of machine learning as its solution to generation and prediction tasks on the MNIST dataset is not only empirically reasonable but also theoretically correct against the K nearest neighbour method. We simply model how data causes symbolic knowledge in terms of its satisfiability in formal logic. Symbolic reasoning emerges as a result of the process of going the causality forwards and backwards. The forward and backward processes correspond to an interpretation and inverse interpretation in formal logic, respectively. The inverse interpretation differentiates our work from the mainstream often referred to as inverse entailment, inverse deduction or inverse resolution. The perspective gives new insights into learning and reasoning towards human-like machine intelligence.",
    "published": "2023-05-18T16:34:51Z",
    "updated": "2026-02-23T12:33:45Z",
    "link": "http://arxiv.org/pdf/2305.11098v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hiroyuki Kido"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09257v4",
    "title": "From Contextual Combinatorial Semi-Bandits to Bandit List Classification: Improved Sample Complexity with Sparse Rewards",
    "summary": "We study the problem of contextual combinatorial semi-bandits, where input contexts are mapped into subsets of size $m$ of a collection of $K$ possible actions. In each round, the learner observes the realized reward of the predicted actions. Motivated by prototypical applications of contextual bandits, we focus on the $s$-sparse regime where we assume that the sum of rewards is bounded by some value $s\\ll K$. For example, in recommendation systems the number of products purchased by any customer is significantly smaller than the total number of available products. Our main result is for the $(ε,δ)$-PAC variant of the problem for which we design an algorithm that returns an $ε$-optimal policy with high probability using a sample complexity of $\\tilde{O}((poly(K/m)+sm/ε^2) \\log(|Π|/δ))$ where $Π$ is the underlying (finite) class and $s$ is the sparsity parameter. This bound improves upon known bounds for combinatorial semi-bandits whenever $s\\ll K$, and in the regime where $s=O(1)$, the leading term is independent of $K$. Our algorithm is also computationally efficient given access to an ERM oracle for $Π$. Our framework generalizes the list multiclass classification problem with bandit feedback, which can be seen as a special case with binary reward vectors. In the special case of single-label classification corresponding to $s=m=1$, we prove an $O((K^7+1/ε^2)\\log(|H|/δ))$ sample complexity bound, which improves upon recent results in this scenario. Additionally, we consider the regret minimization setting where data can be generated adversarially, and establish a regret bound of $\\tilde O(|Π|+\\sqrt{smT\\log |Π|})$, extending the result of Erez et al. (2024) who consider the simpler single label classification setting.",
    "published": "2025-02-13T12:13:25Z",
    "updated": "2026-02-23T12:20:57Z",
    "link": "http://arxiv.org/pdf/2502.09257v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Liad Erez",
      "Tomer Koren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19770v1",
    "title": "The Confusion is Real: GRAPHIC - A Network Science Approach to Confusion Matrices in Deep Learning",
    "summary": "Explainable artificial intelligence has emerged as a promising field of research to address reliability concerns in artificial intelligence. Despite significant progress in explainable artificial intelligence, few methods provide a systematic way to visualize and understand how classes are confused and how their relationships evolve as training progresses. In this work, we present GRAPHIC, an architecture-agnostic approach that analyzes neural networks on a class level. It leverages confusion matrices derived from intermediate layers using linear classifiers. We interpret these as adjacency matrices of directed graphs, allowing tools from network science to visualize and quantify learning dynamics across training epochs and intermediate layers. GRAPHIC provides insights into linear class separability, dataset issues, and architectural behavior, revealing, for example, similarities between flatfish and man and labeling ambiguities validated in a human study. In summary, by uncovering real confusions, GRAPHIC offers new perspectives on how neural networks learn. The code is available at https://github.com/Johanna-S-Froehlich/GRAPHIC.",
    "published": "2026-02-23T12:20:37Z",
    "updated": "2026-02-23T12:20:37Z",
    "link": "http://arxiv.org/pdf/2602.19770v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Johanna S. Fröhlich",
      "Bastian Heinlein",
      "Jan U. Claar",
      "Hans Rosenberger",
      "Vasileios Belagiannis",
      "Ralf R. Müller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19762v1",
    "title": "Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)",
    "summary": "In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort.",
    "published": "2026-02-23T12:12:39Z",
    "updated": "2026-02-23T12:12:39Z",
    "link": "http://arxiv.org/pdf/2602.19762v1.pdf",
    "category": [
      "cs.PL",
      "cs.AI"
    ],
    "authors": [
      "Mohammed Javed Absar",
      "Muthu Baskaran",
      "Abhikrant Sharma",
      "Abhilash Bhandari",
      "Ankit Aggarwal",
      "Arun Rangasamy",
      "Dibyendu Das",
      "Fateme Hosseini",
      "Franck Slama",
      "Iulian Brumar",
      "Jyotsna Verma",
      "Krishnaprasad Bindumadhavan",
      "Mitesh Kothari",
      "Mohit Gupta",
      "Ravishankar Kolachana",
      "Richard Lethin",
      "Samarth Narang",
      "Sanjay Motilal Ladwa",
      "Shalini Jain",
      "Snigdha Suresh Dalvi",
      "Tasmia Rahman",
      "Venkat Rasagna Reddy Komatireddy",
      "Vivek Vasudevbhai Pandya",
      "Xiyue Shi",
      "Zachary Zipper"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.08646v2",
    "title": "Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data",
    "summary": "Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.",
    "published": "2024-02-13T18:24:23Z",
    "updated": "2026-02-23T12:10:05Z",
    "link": "http://arxiv.org/pdf/2402.08646v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hiroyuki Kido"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19405v2",
    "title": "Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation",
    "summary": "Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance with respect to state-of-the-art approaches, reducing the median positioning error by up to 30% in the most sparsely sampled or structurally complex regions. We also observe region-dependent saturation effects, which emerge most rapidly in scenarios with high user density where the information gain from additional synthetic samples quickly diminishes. Overall, the framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces.",
    "published": "2025-09-23T09:09:45Z",
    "updated": "2026-02-23T11:57:41Z",
    "link": "http://arxiv.org/pdf/2509.19405v2.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Tony Chahoud",
      "Lorenzo Mario Amorosa",
      "Riccardo Marini",
      "Luca De Nardis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.04272v4",
    "title": "PoTable: Towards Systematic Thinking via Plan-then-Execute Stage Reasoning on Tables",
    "summary": "In recent years, table reasoning has garnered substantial research interest, particularly regarding its integration with Large Language Models (LLMs), which have revolutionized natural language applications. Existing LLM-based studies typically achieve step-by-step thinking for table reasoning guided by task semantics. While these approaches emphasize autonomous exploration and enhance fine-grained table understanding, they often overlook systematic thinking in the reasoning process. This oversight can lead to omitted steps, disorganized logic and misleading results, especially in complex scenarios. In this paper, we propose PoTable, a novel stage-oriented plan-then-execute approach that incorporates systematic thinking into table reasoning. Specifically, PoTable involves several distinct analytical stages with clear objectives to provide adequate guidance. To accomplish stage-specific goals, PoTable employs a plan-then-execute mechanism: it first plans the operation chain based on the stage objective, and then executes operations sequentially through code generation, real-time running and feedback processing. Consequently, PoTable produces reliable table reasoning results with highly accurate, step-wise commented and completely executable programs. It mirrors the workflow of a professional data analyst, offering advantages in both accuracy and explainability. Finally, we conduct extensive experiments on four datasets from the WikiTQ and TabFact benchmarks, where the results demonstrate the effectiveness, efficiency and explainability of PoTable. Our code is available at: https://github.com/Double680/PoTable.",
    "published": "2024-12-05T15:54:16Z",
    "updated": "2026-02-23T11:53:02Z",
    "link": "http://arxiv.org/pdf/2412.04272v4.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Qingyang Mao",
      "Qi Liu",
      "Zhi Li",
      "Mingyue Cheng",
      "Zheng Zhang",
      "Rui Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.17690v2",
    "title": "DesignAsCode: Bridging Structural Editability and Visual Fidelity in Graphic Design Generation",
    "summary": "Graphic design generation demands a delicate balance between high visual fidelity and fine-grained structural editability. However, existing approaches typically bifurcate into either non-editable raster image synthesis or abstract layout generation devoid of visual content. Recent combinations of these two approaches attempt to bridge this gap but often suffer from rigid composition schemas and unresolvable visual dissonances (e.g., text-background conflicts) due to their inexpressive representation and open-loop nature. To address these challenges, we propose DesignAsCode, a novel framework that reimagines graphic design as a programmatic synthesis task using HTML/CSS. Specifically, we introduce a Plan-Implement-Reflect pipeline, incorporating a Semantic Planner to construct dynamic, variable-depth element hierarchies and a Visual-Aware Reflection mechanism that iteratively optimizes the code to rectify rendering artifacts. Extensive experiments demonstrate that DesignAsCode significantly outperforms state-of-the-art baselines in both structural validity and aesthetic quality. Furthermore, our code-native representation unlocks advanced capabilities, including automatic layout retargeting, complex document generation (e.g., resumes), and CSS-based animation. Our project page is available at https://liuziyuan1109.github.io/design-as-code/.",
    "published": "2026-02-06T05:10:19Z",
    "updated": "2026-02-23T11:36:42Z",
    "link": "http://arxiv.org/pdf/2602.17690v2.pdf",
    "category": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Ziyuan Liu",
      "Shizhao Sun",
      "Danqing Huang",
      "Yingdong Shi",
      "Meisheng Zhang",
      "Ji Li",
      "Jingsong Yu",
      "Jiang Bian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.12313v2",
    "title": "Visible and Hyperspectral Imaging for Quality Assessment of Milk: Property Characterisation and Identification",
    "summary": "Rapid and non-destructive assessment of milk quality is crucial to ensuring both nutritional value and food safety. In this study, we investigated the potential of visible and hyperspectral imaging as cost-effective and quick-response alternatives to conventional chemical analyses for characterizing key properties of cowś milk. A total of 52 milk samples were analysed to determine their biochemical composition (polyphenols, antioxidant capacity, and fatty acids) using spectrophotometer methods and standard gas-liquid and high-performance liquid chromatography (GLC/HPLC). Concurrently, visible (RGB) images were captured using a standard smartphone, and hyperspectral data were acquired in the near-infrared range. A comprehensive analytical framework, including eleven different machine learning algorithms, was employed to correlate imaging features with biochemical measurements. Analysis of visible images accurately distinguished between fresh samples and those stored for 12 days (100 percent accuracy) and achieved perfect discrimination between antibiotic-treated and untreated groups (100 percent accuracy). Moreover, image-derived features enabled perfect prediction of the polyphenols content and the antioxidant capacity using an XGBoost model. Hyperspectral imaging further achieved classification accuracies exceeding 95 percent for several individual fatty acids and 94.8 percent for treatment groups using a Random Forest model. These findings demonstrate that both visible and hyperspectral imaging, when coupled with machine learning, are powerful, non-invasive tools for the rapid assessment of milkś chemical and nutritional profiles, highlighting the strong potential of imaging-based approaches for milk quality assessment.",
    "published": "2026-02-12T17:18:22Z",
    "updated": "2026-02-23T11:26:43Z",
    "link": "http://arxiv.org/pdf/2602.12313v2.pdf",
    "category": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Massimo Martinelli",
      "Elena Tomassi",
      "Nafiou Arouna",
      "Morena Gabriele",
      "Laryssa Perez Fabbri",
      "Luisa Pozzo",
      "Bianca Castiglioni",
      "Paola Cremonesi",
      "Giuseppe Conte",
      "Davide Moroni",
      "Laura Pucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19718v1",
    "title": "Carbon-Aware Governance Gates: An Architecture for Sustainable GenAI Development",
    "summary": "The rapid adoption of Generative AI (GenAI) in the software development life cycle (SDLC) increases computational demand, which can raise the carbon footprint of development activities. At the same time, organizations are increasingly embedding governance mechanisms into GenAI-assisted development to support trust, transparency, and accountability. However, these governance mechanisms introduce additional computational workloads, including repeated inference, regeneration cycles, and expanded validation pipelines, increasing energy use and the carbon footprint of GenAI-assisted development. This paper proposes Carbon-Aware Governance Gates (CAGG), an architectural extension that embeds carbon budgets, energy provenance, and sustainability-aware validation orchestration into human-AI governance layers. CAGG comprises three components: (i) an Energy and Carbon Provenance Ledger, (ii) a Carbon Budget Manager, and (iii) a Green Validation Orchestrator, operationalized through governance policies and reusable design patterns.",
    "published": "2026-02-23T11:11:56Z",
    "updated": "2026-02-23T11:11:56Z",
    "link": "http://arxiv.org/pdf/2602.19718v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Mateen A. Abbasi",
      "Tommi J. Mikkonen",
      "Petri J. Ihantola",
      "Muhammad Waseem",
      "Pekka Abrahamsson",
      "Niko K. Mäkitalo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.18949v2",
    "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach",
    "summary": "Bayesian networks (BN) are probabilistic graphical models that enable efficient knowledge representation and inference. These have proven effective across diverse domains, including healthcare, bioinformatics and economics. The structure and parameters of a BN can be obtained by domain experts or directly learned from available data. However, as privacy concerns escalate, it becomes increasingly critical for publicly released models to safeguard sensitive information in training data. Typically, released models do not prioritize privacy by design. In particular, tracing attacks from adversaries can combine the released BN with auxiliary data to determine whether specific individuals belong to the data from which the BN was learned. State-of-the-art protection tecniques involve introducing noise into the learned parameters. While this offers robust protection against tracing attacks, it significantly impacts the model's utility, in terms of both the significance and accuracy of the resulting inferences. Hence, high privacy may be attained at the cost of releasing a possibly ineffective model. This paper introduces credal networks (CN) as a novel solution for balancing the model's privacy and utility. After adapting the notion of tracing attacks, we demonstrate that a CN enables the masking of the learned BN, thereby reducing the probability of successful attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve meaningful inferences while safeguarding privacy. Moreover, we identify key learning information that must be concealed to prevent attackers from recovering the underlying BN. Finally, we conduct a set of numerical experiments to analyze how privacy gains can be modulated by tuning the CN hyperparameters. Our results confirm that CNs provide a principled, practical, and effective approach towards the development of privacy-aware probabilistic graphical models.",
    "published": "2025-09-23T12:58:32Z",
    "updated": "2026-02-23T11:07:21Z",
    "link": "http://arxiv.org/pdf/2509.18949v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Niccolò Rocchi",
      "Fabio Stella",
      "Cassio de Campos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.08941v3",
    "title": "Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation",
    "summary": "This paper analyzes multi-step temporal difference (TD)-learning algorithms within the ``deadly triad'' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that $n$-step TD-learning algorithms converge to a solution as the sampling horizon $n$ increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when $n$ is sufficiently large. Based on these findings, in the second part, two $n$-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.",
    "published": "2025-02-13T03:43:13Z",
    "updated": "2026-02-23T11:04:45Z",
    "link": "http://arxiv.org/pdf/2502.08941v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Han-Dong Lim",
      "Donghwan Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.16902v3",
    "title": "LLM-WikiRace Benchmark: How Far Can LLMs Plan over Real-World Knowledge Graphs?",
    "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.",
    "published": "2026-02-18T21:33:59Z",
    "updated": "2026-02-23T11:03:50Z",
    "link": "http://arxiv.org/pdf/2602.16902v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Juliusz Ziomek",
      "William Bankes",
      "Lorenz Wolf",
      "Shyam Sundhar Ramesh",
      "Xiaohang Tang",
      "Ilija Bogunovic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19702v1",
    "title": "DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework",
    "summary": "Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.",
    "published": "2026-02-23T10:52:20Z",
    "updated": "2026-02-23T10:52:20Z",
    "link": "http://arxiv.org/pdf/2602.19702v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Adamya Shyam",
      "Venkateswara Rao Kagita",
      "Bharti Rana",
      "Vikas Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19698v1",
    "title": "Iconographic Classification and Content-Based Recommendation for Digitized Artworks",
    "summary": "We present a proof-of-concept system that automates iconographic classification and content-based recommendation of digitized artworks using the Iconclass vocabulary and selected artificial intelligence methods. The prototype implements a four-stage workflow for classification and recommendation, which integrates YOLOv8 object detection with algorithmic mappings to Iconclass codes, rule-based inference for abstract meanings, and three complementary recommenders (hierarchical proximity, IDF-weighted overlap, and Jaccard similarity). Although more engineering is still needed, the evaluation demonstrates the potential of this solution: Iconclass-aware computer vision and recommendation methods can accelerate cataloging and enhance navigation in large heritage repositories. The key insight is to let computer vision propose visible elements and to use symbolic structures (Iconclass hierarchy) to reach meaning.",
    "published": "2026-02-23T10:44:27Z",
    "updated": "2026-02-23T10:44:27Z",
    "link": "http://arxiv.org/pdf/2602.19698v1.pdf",
    "category": [
      "cs.DL",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Krzysztof Kutt",
      "Maciej Baczyński"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.16703v2",
    "title": "On the Granularity of Causal Effect Identifiability",
    "summary": "The classical notion of causal effect identifiability is defined in terms of treatment and outcome variables. In this paper, we consider the identifiability of state-based causal effects: how an intervention on a particular state of treatment variables affects a particular state of outcome variables. We demonstrate that state-based causal effects may be identifiable even when variable-based causal effects may not. Moreover, we show that this separation occurs only when additional knowledge -- such as context-specific independencies -- is available. We further examine knowledge that constrains the states of variables, and show that such knowledge can improve both variable-based and state-based identifiability when combined with other knowledge such as context-specific independencies. We finally propose an approach for identifying causal effects under these additional constraints, and conduct empirical studies to further illustrate the separations between the two levels of identifiability.",
    "published": "2025-10-19T04:13:09Z",
    "updated": "2026-02-23T10:35:13Z",
    "link": "http://arxiv.org/pdf/2510.16703v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "authors": [
      "Yizuo Chen",
      "Adnan Darwiche"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.15082v2",
    "title": "S-PRESSO: Ultra Low Bitrate Sound Effect Compression With Diffusion Autoencoders And Offline Quantization",
    "summary": "Neural audio compression models have recently achieved extreme compression rates, enabling efficient latent generative modeling. Conversely, latent generative models have been applied to compression, pushing the limits of continuous and discrete approaches. However, existing methods remain constrained to low-resolution audio and degrade substantially at very low bitrates, where audible artifacts are prominent. In this paper, we present S-PRESSO, a 48kHz sound effect compression model that produces both continuous and discrete embeddings at ultra-low bitrates, down to 0.096 kbps, via offline quantization. Our model relies on a pretrained latent diffusion model to decode compressed audio embeddings learned by a latent encoder. Leveraging the generative priors of the diffusion decoder, we achieve extremely low frame rates, down to 1Hz (750x compression rate), producing convincing and realistic reconstructions at the cost of exact fidelity. Despite operating at high compression rates, we demonstrate that S-PRESSO outperforms both continuous and discrete baselines in audio quality, acoustic similarity and reconstruction metrics.",
    "published": "2026-02-16T10:28:38Z",
    "updated": "2026-02-23T10:34:04Z",
    "link": "http://arxiv.org/pdf/2602.15082v2.pdf",
    "category": [
      "cs.SD",
      "cs.AI",
      "cs.MM"
    ],
    "authors": [
      "Zineb Lahrichi",
      "Gaëtan Hadjeres",
      "Gaël Richard",
      "Geoffroy Peeters"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19685v1",
    "title": "PerturbDiff: Functional Diffusion for Single-Cell Perturbation Modeling",
    "summary": "Building Virtual Cells that can accurately simulate cellular responses to perturbations is a long-standing goal in systems biology. A fundamental challenge is that high-throughput single-cell sequencing is destructive: the same cell cannot be observed both before and after a perturbation. Thus, perturbation prediction requires mapping unpaired control and perturbed populations. Existing models address this by learning maps between distributions, but typically assume a single fixed response distribution when conditioned on observed cellular context (e.g., cell type) and the perturbation type. In reality, responses vary systematically due to unobservable latent factors such as microenvironmental fluctuations and complex batch effects, forming a manifold of possible distributions for the same observed conditions. To account for this variability, we introduce PerturbDiff, which shifts modeling from individual cells to entire distributions. By embedding distributions as points in a Hilbert space, we define a diffusion-based generative process operating directly over probability distributions. This allows PerturbDiff to capture population-level response shifts across hidden factors. Benchmarks on established datasets show that PerturbDiff achieves state-of-the-art performance in single-cell response prediction and generalizes substantially better to unseen perturbations. See our project page (https://katarinayuan.github.io/PerturbDiff-ProjectPage/), where code and data will be made publicly available (https://github.com/DeepGraphLearning/PerturbDiff).",
    "published": "2026-02-23T10:28:56Z",
    "updated": "2026-02-23T10:28:56Z",
    "link": "http://arxiv.org/pdf/2602.19685v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xinyu Yuan",
      "Xixian Liu",
      "Ya Shi Zhang",
      "Zuobai Zhang",
      "Hongyu Guo",
      "Jian Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19679v1",
    "title": "TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures",
    "summary": "Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.",
    "published": "2026-02-23T10:22:52Z",
    "updated": "2026-02-23T10:22:52Z",
    "link": "http://arxiv.org/pdf/2602.19679v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Hyeongjin Nam",
      "Daniel Sungho Jung",
      "Kyoung Mu Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.23465v3",
    "title": "Role-Aware Language Models for Secure and Contextualized Access Control in Organizations",
    "summary": "As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.",
    "published": "2025-07-31T11:41:04Z",
    "updated": "2026-02-23T10:22:06Z",
    "link": "http://arxiv.org/pdf/2507.23465v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Saeed Almheiri",
      "Yerulan Kongrat",
      "Adrian Santosh",
      "Ruslan Tasmukhanov",
      "Josemaria Loza Vera",
      "Muhammad Dehan Al Kautsar",
      "Fajri Koto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19674v1",
    "title": "Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics",
    "summary": "Remote monitoring of heart failure (HF) via speech signals provides a non-invasive and cost-effective solution for long-term patient management. However, substantial inter-individual heterogeneity in vocal characteristics often limits the accuracy of traditional cross-sectional classification models. To address this, we propose a Longitudinal Intra-Patient Tracking (LIPT) scheme designed to capture the trajectory of relative symptomatic changes within individuals. Central to this framework is a Personalised Sequential Encoder (PSE), which transforms longitudinal speech recordings into context-aware latent representations. By incorporating historical data at each timestamp, the PSE facilitates a holistic assessment of the clinical trajectory rather than modelling discrete visits independently. Experimental results from a cohort of 225 patients demonstrate that the LIPT paradigm significantly outperforms the classic cross-sectional approaches, achieving a recognition accuracy of 99.7% for clinical status transitions. The model's high sensitivity was further corroborated by additional follow-up data, confirming its efficacy in predicting HF deterioration and its potential to secure patient safety in remote, home-based settings. Furthermore, this work addresses the gap in existing literature by providing a comprehensive analysis of different speech task designs and acoustic features. Taken together, the superior performance of the LIPT framework and PSE architecture validates their readiness for integration into long-term telemonitoring systems, offering a scalable solution for remote heart failure management.",
    "published": "2026-02-23T10:19:17Z",
    "updated": "2026-02-23T10:19:17Z",
    "link": "http://arxiv.org/pdf/2602.19674v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Yue Pan",
      "Xingyao Wang",
      "Hanyue Zhang",
      "Liwei Liu",
      "Changxin Li",
      "Gang Yang",
      "Rong Sheng",
      "Yili Xia",
      "Ming Chu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19672v1",
    "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
    "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
    "published": "2026-02-23T10:17:25Z",
    "updated": "2026-02-23T10:17:25Z",
    "link": "http://arxiv.org/pdf/2602.19672v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jiayu Wang",
      "Yifei Ming",
      "Zixuan Ke",
      "Shafiq Joty",
      "Aws Albarghouthi",
      "Frederic Sala"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.07755v3",
    "title": "Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems",
    "summary": "Edge computing has emerged as a pivotal technology, offering significant advantages such as low latency, enhanced data security, and reduced reliance on centralized cloud infrastructure. These benefits are crucial for applications requiring real-time data processing or strict security measures. Despite these advantages, edge devices operating within edge clusters are often underutilized. This inefficiency is mainly due to the absence of a holistic performance profiling mechanism which can help dynamically adjust the desired system configuration for a given workload. Since edge computing environments involve a complex interplay between CPU frequency, power consumption, and application performance, a deeper understanding of these correlations is essential. By uncovering these relationships, it becomes possible to make informed decisions that enhance both computational efficiency and energy savings. To address this gap, this paper evaluates the power consumption and performance characteristics of a single processing node within an edge cluster using a synthetic microbenchmark by varying the workload size and CPU frequency. The results show how an optimal measure can lead to optimized usage of edge resources, given both performance and power consumption.",
    "published": "2025-05-12T17:02:02Z",
    "updated": "2026-02-23T10:14:07Z",
    "link": "http://arxiv.org/pdf/2505.07755v3.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Tomasz Szydlo",
      "Viacheslav Horbanov",
      "Devki Nandan Jha",
      "Shashikant Ilager",
      "Aleksander Slominski",
      "Rajiv Ranjan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.08549v3",
    "title": "Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures",
    "summary": "We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.",
    "published": "2026-01-13T13:36:38Z",
    "updated": "2026-02-23T10:11:13Z",
    "link": "http://arxiv.org/pdf/2601.08549v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Sucheta Ghosh",
      "Felix Dietrich",
      "Zahra Monfared"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22387v2",
    "title": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly",
    "summary": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.",
    "published": "2025-09-26T14:15:44Z",
    "updated": "2026-02-23T10:05:41Z",
    "link": "http://arxiv.org/pdf/2509.22387v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "authors": [
      "Narada Maugin",
      "Tristan Cazenave"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03734v2",
    "title": "Cost Efficient Fairness Audit Under Partial Feedback",
    "summary": "We study the problem of auditing the fairness of a given classifier under partial feedback, where true labels are available only for positively classified individuals, (e.g., loan repayment outcomes are observed only for approved applicants). We introduce a novel cost model for acquiring additional labeled data, designed to more accurately reflect real-world costs such as credit assessment, loan processing, and potential defaults. Our goal is to find optimal fairness audit algorithms that are more cost-effective than random exploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no assumptions on the data distribution, and a mixture model, where features and true labels follow a mixture of exponential family distributions. In the black-box setting, we propose a near-optimal auditing algorithm under mild assumptions and show that a natural baseline can be strictly suboptimal. In the mixture model setting, we design a novel algorithm that achieves significantly lower audit cost than the black-box case. Our approach leverages prior work on learning from truncated samples and maximum-a-posteriori oracles, and extends known results on spherical Gaussian mixtures to handle exponential family mixtures, which may be of independent interest. Moreover, our algorithms apply to popular fairness metrics including demographic parity, equal opportunity, and equalized odds. Empirically, we demonstrate strong performance of our algorithms on real-world fair classification datasets like Adult Income and Law School, consistently outperforming natural baselines by around 50% in terms of audit cost.",
    "published": "2025-10-04T08:38:03Z",
    "updated": "2026-02-23T10:04:26Z",
    "link": "http://arxiv.org/pdf/2510.03734v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "authors": [
      "Nirjhar Das",
      "Mohit Sharma",
      "Praharsh Nanavati",
      "Kirankumar Shiragur",
      "Amit Deshpande"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19655v1",
    "title": "Representation Stability in a Minimal Continual Learning Agent",
    "summary": "Continual learning systems are increasingly deployed in environments where retraining or reset is infeasible, yet many approaches emphasize task performance rather than the evolution of internal representations over time. In this work, we study a minimal continual learning agent designed to isolate representational dynamics from architectural complexity and optimization objectives. The agent maintains a persistent state vector across executions and incrementally updates it as new textual data is introduced. We quantify representational change using cosine similarity between successive normalized state vectors and define a stability metric over time intervals. Longitudinal experiments across eight executions reveal a transition from an initial plastic regime to a stable representational regime under consistent input. A deliberately introduced semantic perturbation produces a bounded decrease in similarity, followed by recovery and restabilization under subsequent coherent input. These results demonstrate that meaningful stability plasticity tradeoffs can emerge in a minimal, stateful learning system without explicit regularization, replay, or architectural complexity. The work establishes a transparent empirical baseline for studying representational accumulation and adaptation in continual learning systems.",
    "published": "2026-02-23T09:59:03Z",
    "updated": "2026-02-23T09:59:03Z",
    "link": "http://arxiv.org/pdf/2602.19655v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Vishnu Subramanian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19654v1",
    "title": "NEXUS : A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi Nationa Capital Region",
    "summary": "Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.",
    "published": "2026-02-23T09:56:22Z",
    "updated": "2026-02-23T09:56:22Z",
    "link": "http://arxiv.org/pdf/2602.19654v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Rampunit Kumar",
      "Aditya Maheshwari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19651v1",
    "title": "Denoising Particle Filters: Learning State Estimation with Single-Step Objectives",
    "summary": "Learning-based methods commonly treat state estimation in robotics as a sequence modeling problem. While this paradigm can be effective at maximizing end-to-end performance, models are often difficult to interpret and expensive to train, since training requires unrolling sequences of predictions in time. As an alternative to end-to-end trained state estimation, we propose a novel particle filtering algorithm in which models are trained from individual state transitions, fully exploiting the Markov property in robotic systems. In this framework, measurement models are learned implicitly by minimizing a denoising score matching objective. At inference, the learned denoiser is used alongside a (learned) dynamics model to approximately solve the Bayesian filtering equation at each time step, effectively guiding predicted states toward the data manifold informed by measurements. We evaluate the proposed method on challenging robotic state estimation tasks in simulation, demonstrating competitive performance compared to tuned end-to-end trained baselines. Importantly, our method offers the desirable composability of classical filtering algorithms, allowing prior information and external sensor models to be incorporated without retraining.",
    "published": "2026-02-23T09:53:23Z",
    "updated": "2026-02-23T09:53:23Z",
    "link": "http://arxiv.org/pdf/2602.19651v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Lennart Röstel",
      "Berthold Bäuml"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.05695v2",
    "title": "SweetSpot: An Analytical Model for Predicting Energy Efficiency of LLM Inference",
    "summary": "Large Language Models (LLMs) inference is central to modern AI applications, dominating worldwide datacenter workloads, making it critical to predict its energy footprint. Existing approaches estimate energy consumption as a simple linear function of input and output sequence. However, by analyzing the autoregressive structure of Transformers, which implies a fundamentally non-linear relationship between input and output sequence lengths and energy consumption, we demonstrate the existence of a generation energy minima. Peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs. Consequently, we propose SweetSpot, an analytical model derived from the computational and memory-access complexity of the Transformer architecture, which accurately characterizes the efficiency curve as a function of input and output lengths. To assess accuracy, we measure energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite. We test input and output lengths from 64 to 4096 tokens and achieve a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"sweet spots\" reduce energy usage, up to 33.41x, enabling informed truncation, summarization, and adaptive generation strategies in production systems.",
    "published": "2026-02-05T14:21:00Z",
    "updated": "2026-02-23T09:49:19Z",
    "link": "http://arxiv.org/pdf/2602.05695v2.pdf",
    "category": [
      "cs.AI",
      "cs.PF"
    ],
    "authors": [
      "Hiari Pizzini Cavagna",
      "Andrea Proia",
      "Giacomo Madella",
      "Giovanni B. Esposito",
      "Francesco Antici",
      "Daniele Cesarini",
      "Zeynep Kiziltan",
      "Andrea Bartolini"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.05612v5",
    "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
    "summary": "Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.",
    "published": "2025-08-07T17:53:47Z",
    "updated": "2026-02-23T09:33:32Z",
    "link": "http://arxiv.org/pdf/2508.05612v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Linghao Zhu",
      "Yiran Guan",
      "Dingkang Liang",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Bin Qin",
      "Jian Luan",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19634v1",
    "title": "Compositional Planning with Jumpy World Models",
    "summary": "The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.",
    "published": "2026-02-23T09:22:21Z",
    "updated": "2026-02-23T09:22:21Z",
    "link": "http://arxiv.org/pdf/2602.19634v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Jesse Farebrother",
      "Matteo Pirotta",
      "Andrea Tirinzoni",
      "Marc G. Bellemare",
      "Alessandro Lazaric",
      "Ahmed Touati"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19633v1",
    "title": "TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents",
    "summary": "Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.",
    "published": "2026-02-23T09:19:56Z",
    "updated": "2026-02-23T09:19:56Z",
    "link": "http://arxiv.org/pdf/2602.19633v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Jongwon Jeong",
      "Jungtaek Kim",
      "Kangwook Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19631v1",
    "title": "Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection",
    "summary": "Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.",
    "published": "2026-02-23T09:18:27Z",
    "updated": "2026-02-23T09:18:27Z",
    "link": "http://arxiv.org/pdf/2602.19631v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Uichan Lee",
      "Jeonghyeon Kim",
      "Sangheum Hwang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19629v1",
    "title": "Cooperation After the Algorithm: Designing Human-AI Coexistence Beyond the Illusion of Collaboration",
    "summary": "Generative artificial intelligence systems increasingly participate in research, law, education, media, and governance. Their fluent and adaptive outputs create an experience of collaboration. However, these systems do not bear responsibility, incur liability, or share stakes in downstream consequences. This structural asymmetry has already produced sanctions, professional errors, and governance failures in high-stakes contexts We argue that stable human-AI coexistence is an institutional achievement that depends on governance infrastructure capable of distributing residual risk. Drawing on institutional analysis and evolutionary cooperation theory, we introduce a formal inequality that specifies when reliance on AI yields positive expected cooperative value. The model makes explicit how governance conditions, system policy, and accountability regimes jointly determine whether cooperation is rational or structurally defective. From this formalization we derive a cooperation ecology framework with six design principles: reciprocity contracts, visible trust infrastructure, conditional cooperation modes, defection-mitigation mechanisms, narrative literacy against authority theatre, and an Earth-first sustainability constraint. We operationalize the framework through three policy artefacts: a Human-AI Cooperation Charter, a Defection Risk Register, and a Cooperation Readiness Audit. Together, these elements shift the unit of analysis from the user-AI dyad to the institutional environment that shapes incentives, signals, accountability, and repair. The paper provides a theoretical foundation and practical toolkit for designing human-AI systems that can sustain accountable, trustworthy cooperation over time.",
    "published": "2026-02-23T09:17:12Z",
    "updated": "2026-02-23T09:17:12Z",
    "link": "http://arxiv.org/pdf/2602.19629v1.pdf",
    "category": [
      "cs.HC",
      "cs.AI"
    ],
    "authors": [
      "Tatia Codreanu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19623v1",
    "title": "PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring",
    "summary": "While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer's Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional \"one-shot\" generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.",
    "published": "2026-02-23T09:12:13Z",
    "updated": "2026-02-23T09:12:13Z",
    "link": "http://arxiv.org/pdf/2602.19623v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Injun Baek",
      "Yearim Kim",
      "Nojun Kwak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19622v1",
    "title": "VecFormer: Towards Efficient and Generalizable Graph Transformer with Graph Token Attention",
    "summary": "Graph Transformer has demonstrated impressive capabilities in the field of graph representation learning. However, existing approaches face two critical challenges: (1) most models suffer from exponentially increasing computational complexity, making it difficult to scale to large graphs; (2) attention mechanisms based on node-level operations limit the flexibility of the model and result in poor generalization performance in out-of-distribution (OOD) scenarios. To address these issues, we propose \\textbf{VecFormer} (the \\textbf{Vec}tor Quantized Graph Trans\\textbf{former}), an efficient and highly generalizable model for node classification, particularly under OOD settings. VecFormer adopts a two-stage training paradigm. In the first stage, two codebooks are used to reconstruct the node features and the graph structure, aiming to learn the rich semantic \\texttt{Graph Codes}. In the second stage, attention mechanisms are performed at the \\texttt{Graph Token} level based on the transformed cross codebook, reducing computational complexity while enhancing the model's generalization capability. Extensive experiments on datasets of various sizes demonstrate that VecFormer outperforms the existing Graph Transformer in both performance and speed.",
    "published": "2026-02-23T09:10:39Z",
    "updated": "2026-02-23T09:10:39Z",
    "link": "http://arxiv.org/pdf/2602.19622v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jingbo Zhou",
      "Jun Xia",
      "Siyuan Li",
      "Yunfan Liu",
      "Wenjun Wang",
      "Yufei Huang",
      "Changxi Chi",
      "Mutian Hong",
      "Zhuoli Ouyang",
      "Shu Wang",
      "Zhongqi Wang",
      "Xingyu Wu",
      "Chang Yu",
      "Stan Z. Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19620v1",
    "title": "Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model",
    "summary": "Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques.",
    "published": "2026-02-23T09:07:16Z",
    "updated": "2026-02-23T09:07:16Z",
    "link": "http://arxiv.org/pdf/2602.19620v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Louth Bin Rawshan",
      "Zhuoyu Wang",
      "Brian Y Lim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.12691v2",
    "title": "ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training",
    "summary": "We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.",
    "published": "2026-02-13T07:46:37Z",
    "updated": "2026-02-23T08:56:56Z",
    "link": "http://arxiv.org/pdf/2602.12691v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Rushuai Yang",
      "Hecheng Wang",
      "Chiming Liu",
      "Xiaohan Yan",
      "Yunlong Wang",
      "Xuan Du",
      "Shuoyu Yue",
      "Yongcheng Liu",
      "Chuheng Zhang",
      "Lizhe Qi",
      "Yi Chen",
      "Wei Shan",
      "Maoqing Yao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.08604v4",
    "title": "Physics vs Distributions: Pareto Optimal Flow Matching with Physics Constraints",
    "summary": "Physics-constrained generative modeling aims to produce high-dimensional samples that are both physically consistent and distributionally accurate, a task that remains challenging due to often conflicting optimization objectives. Recent advances in flow matching and diffusion models have enabled efficient generative modeling, but integrating physical constraints often degrades generative fidelity or requires costly inference-time corrections. Our work is the first to recognize the trade-off between distributional and physical accuracy. Based on the insight of inherently conflicting objectives, we introduce Physics-Based Flow Matching (PBFM) a method that enforces physical constraints at training time using conflict-free gradient updates and unrolling to mitigate Jensen's gap. Our approach avoids manual loss balancing and enables simultaneous optimization of generative and physical objectives. As a consequence, physics constraints do not impede inference performance. We benchmark our method across three representative PDE benchmarks. PBFM achieves a Pareto-optimal trade-off, competitive inference speed, and generalizes to a wide range of physics-constrained generative tasks, providing a practical tool for scientific machine learning. Code and datasets available at https://github.com/tum-pbs/PBFM.",
    "published": "2025-06-10T09:13:37Z",
    "updated": "2026-02-23T08:51:45Z",
    "link": "http://arxiv.org/pdf/2506.08604v4.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "math.NA"
    ],
    "authors": [
      "Giacomo Baldan",
      "Qiang Liu",
      "Alberto Guardone",
      "Nils Thuerey"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.17439v2",
    "title": "InTAct: Interval-based Task Activation Consolidation for Continual Learning",
    "summary": "Continual learning is a fundamental challenge in artificial intelligence that requires networks to acquire new knowledge while preserving previously learned representations. Despite the success of various approaches, most existing paradigms do not provide rigorous mathematical guarantees against catastrophic forgetting. Current methods that offer such guarantees primarily focus on analyzing the parameter space using \\textit{interval arithmetic (IA)}, as seen in frameworks such as InterContiNet. However, restricting high-dimensional weight updates can be computationally expensive. In this work, we propose InTAct (Interval-based Task Activation Consolidation), a method that mitigates catastrophic forgetting by enforcing functional invariance at the neuron level. We identify specific activation intervals where previous tasks reside and constrain updates within these regions while allowing for flexible adaptation elsewhere. By ensuring that predictions remain stable within these nested activation intervals, we provide a tractable mathematical guarantee of functional invariance. We emphasize that regulating the activation space is significantly more efficient than parameter-based constraints, because the dimensionality of internal signals is much lower than that of the vast space of model weights. While our approach is architecture-agnostic and applicable to various continual learning settings, its integration with prompt-based methods enables it to achieve state-of-the-art performance on challenging benchmarks.",
    "published": "2025-11-21T17:36:12Z",
    "updated": "2026-02-23T08:51:09Z",
    "link": "http://arxiv.org/pdf/2511.17439v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Patryk Krukowski",
      "Jan Miksa",
      "Piotr Helm",
      "Jacek Tabor",
      "Paweł Wawrzyński",
      "Przemysław Spurek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19608v1",
    "title": "Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning",
    "summary": "Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.",
    "published": "2026-02-23T08:50:07Z",
    "updated": "2026-02-23T08:50:07Z",
    "link": "http://arxiv.org/pdf/2602.19608v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Girmaw Abebe Tadesse",
      "Titien Bartette",
      "Andrew Hassanali",
      "Allen Kim",
      "Jonathan Chemla",
      "Andrew Zolli",
      "Yves Ubelmann",
      "Caleb Robinson",
      "Inbal Becker-Reshef",
      "Juan Lavista Ferres"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.13033v2",
    "title": "Buy versus Build an LLM: A Decision Framework for Governments",
    "summary": "Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.\n  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, \"building\" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.",
    "published": "2026-02-13T15:39:31Z",
    "updated": "2026-02-23T08:47:46Z",
    "link": "http://arxiv.org/pdf/2602.13033v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.SI"
    ],
    "authors": [
      "Jiahao Lu",
      "Ziwei Xu",
      "William Tjhi",
      "Junnan Li",
      "Antoine Bosselut",
      "Pang Wei Koh",
      "Mohan Kankanhalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19605v1",
    "title": "CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning",
    "summary": "Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.",
    "published": "2026-02-23T08:47:19Z",
    "updated": "2026-02-23T08:47:19Z",
    "link": "http://arxiv.org/pdf/2602.19605v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "authors": [
      "Chunlei Meng",
      "Guanhong Huang",
      "Rong Fu",
      "Runmin Jian",
      "Zhongxue Gan",
      "Chun Ouyang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.16175v2",
    "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight",
    "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.",
    "published": "2025-11-20T09:30:23Z",
    "updated": "2026-02-23T08:44:17Z",
    "link": "http://arxiv.org/pdf/2511.16175v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yi Yang",
      "Xueqi Li",
      "Yiyang Chen",
      "Jin Song",
      "Yihan Wang",
      "Zipeng Xiao",
      "Jiadi Su",
      "You Qiaoben",
      "Pengfei Liu",
      "Zhijie Deng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.14697v2",
    "title": "Unifying Evolutionary Prompt Search and Reinforcement Learning for LLM Self-Improvement",
    "summary": "Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL samples trajectories under multiple system prompts in parallel. It applies RL updates to LLM weights conditioned on system prompts, and evolutionary updates to system prompts via mutation and crossover, two genetic operators based on LLM self-reflection. Each system prompt is assigned a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results demonstrate that RL and evolutionary prompt search are deeply synergistic, and unifying the two yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL",
    "published": "2026-02-16T12:34:27Z",
    "updated": "2026-02-23T08:43:06Z",
    "link": "http://arxiv.org/pdf/2602.14697v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Lunjun Zhang",
      "Ryan Chen",
      "Bradly C. Stadie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.01650v2",
    "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM",
    "summary": "Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Moreover, we show that $\\texttt{Elsa}$ remains stable even at extreme sparsity (e.g., 95\\%), yielding up to $\\times$3.98 inference speedup and $\\times$7.80 memory compression over its dense counterpart. We also present $\\texttt{Elsa}_{-L}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees.These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.",
    "published": "2025-10-02T04:10:17Z",
    "updated": "2026-02-23T08:37:18Z",
    "link": "http://arxiv.org/pdf/2510.01650v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kwanhee Lee",
      "Hyeondo Jang",
      "Dongyeop Lee",
      "Dan Alistarh",
      "Namhoon Lee"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19591v1",
    "title": "Detecting High-Potential SMEs with Heterogeneous Graph Neural Networks",
    "summary": "Small and Medium Enterprises (SMEs) constitute 99.9% of U.S. businesses and generate 44% of economic activity, yet systematically identifying high-potential SMEs remains an open challenge. We introduce SME-HGT, a Heterogeneous Graph Transformer framework that predicts which SBIR Phase I awardees will advance to Phase II funding using exclusively public data. We construct a heterogeneous graph with 32,268 company nodes, 124 research topic nodes, and 13 government agency nodes connected by approximately 99,000 edges across three semantic relation types. SME-HGT achieves an AUPRC of 0.621 0.003 on a temporally-split test set, outperforming an MLP baseline (0.590 0.002) and R-GCN (0.608 0.013) across five random seeds. At a screening depth of 100 companies, SME-HGT attains 89.6% precision with a 2.14 lift over random selection. Our temporal evaluation protocol prevents information leakage, and our reliance on public data ensures reproducibility. These results demonstrate that relational structure among firms, research topics, and funding agencies provides meaningful signal for SME potential assessment, with implications for policymakers and early-stage investors.",
    "published": "2026-02-23T08:35:55Z",
    "updated": "2026-02-23T08:35:55Z",
    "link": "http://arxiv.org/pdf/2602.19591v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yijiashun Qi",
      "Hanzhe Guo",
      "Yijiazhen Qi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19585v1",
    "title": "Tri-Subspaces Disentanglement for Multimodal Sentiment Analysis",
    "summary": "Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic modalities to infer human sentiment. Most existing methods either focus on globally shared representations or modality-specific features, while overlooking signals that are shared only by certain modality pairs. This limits the expressiveness and discriminative power of multimodal representations. To address this limitation, we propose a Tri-Subspace Disentanglement (TSD) framework that explicitly factorizes features into three complementary subspaces: a common subspace capturing global consistency, submodally-shared subspaces modeling pairwise cross-modal synergies, and private subspaces preserving modality-specific cues. To keep these subspaces pure and independent, we introduce a decoupling supervisor together with structured regularization losses. We further design a Subspace-Aware Cross-Attention (SACA) fusion module that adaptively models and integrates information from the three subspaces to obtain richer and more robust representations. Experiments on CMU-MOSI and CMU-MOSEI demonstrate that TSD achieves state-of-the-art performance across all key metrics, reaching 0.691 MAE on CMU-MOSI and 54.9% ACC-7 on CMU-MOSEI, and also transfers well to multimodal intent recognition tasks. Ablation studies confirm that tri-subspace disentanglement and SACA jointly enhance the modeling of multi-granular cross-modal sentiment cues.",
    "published": "2026-02-23T08:19:54Z",
    "updated": "2026-02-23T08:19:54Z",
    "link": "http://arxiv.org/pdf/2602.19585v1.pdf",
    "category": [
      "cs.MM",
      "cs.AI"
    ],
    "authors": [
      "Chunlei Meng",
      "Jiabin Luo",
      "Zhenglin Yan",
      "Zhenyu Yu",
      "Rong Fu",
      "Zhongxue Gan",
      "Chun Ouyang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19584v1",
    "title": "Interpolation-Driven Machine Learning Approaches for Plume Shine Dose Estimation: A Comparison of XGBoost, Random Forest, and TabNet",
    "summary": "Despite the success of machine learning (ML) in surrogate modeling, its use in radiation dose assessment is limited by safety-critical constraints, scarce training-ready data, and challenges in selecting suitable architectures for physics-dominated systems. Within this context, rapid and accurate plume shine dose estimation serves as a practical test case, as it is critical for nuclear facility safety assessment and radiological emergency response, while conventional photon-transport-based calculations remain computationally expensive. In this work, an interpolation-assisted ML framework was developed using discrete dose datasets generated with the pyDOSEIA suite for 17 gamma-emitting radionuclides across varying downwind distances, release heights, and atmospheric stability categories. The datasets were augmented using shape-preserving interpolation to construct dense, high-resolution training data. Two tree-based ML models (Random Forest and XGBoost) and one deep learning (DL) model (TabNet) were evaluated to examine predictive performance and sensitivity to dataset resolution. All models showed higher prediction accuracy with the interpolated high-resolution dataset than with the discrete data; however, XGBoost consistently achieved the highest accuracy. Interpretability analysis using permutation importance (tree-based models) and attention-based feature attribution (TabNet) revealed that performance differences stem from how the models utilize input features. Tree-based models focus mainly on dominant geometry-dispersion features (release height, stability category, and downwind distance), treating radionuclide identity as a secondary input, whereas TabNet distributes attention more broadly across multiple variables. For practical deployment, a web-based GUI was developed for interactive scenario evaluation and transparent comparison with photon-transport reference calculations.",
    "published": "2026-02-23T08:12:49Z",
    "updated": "2026-02-23T08:12:49Z",
    "link": "http://arxiv.org/pdf/2602.19584v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Biswajit Sadhu",
      "Kalpak Gupte",
      "Trijit Sadhu",
      "S. Anand"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2204.07520v4",
    "title": "Resource-Aware Distributed Submodular Maximization: A Paradigm for Multi-Robot Decision-Making",
    "summary": "Multi-robot decision-making is the process where multiple robots coordinate actions. In this paper, we aim for efficient and effective multi-robot decision-making despite the robots' limited on-board resources and the often resource-demanding complexity of their tasks. We introduce the first algorithm enabling the robots to choose with which few other robots to coordinate and provably balance the trade-off of centralized vs. decentralized coordination. Particularly, centralization favors globally near-optimal decision-making but at the cost of increased on-board resource requirements; whereas, decentralization favors minimal resource requirements but at a global suboptimality cost. All robots can thus afford our algorithm, irrespective of their resources. We are motivated by the future of autonomy that involves multiple robots coordinating actions to complete resource-demanding tasks, such as target tracking, area coverage, and monitoring. To provide closed-form guarantees, we focus on maximization problems involving monotone and 2nd-order submodular functions. To capture the cost of decentralization, we introduce the notion of Centralization Of Information among non-Neighbors (COIN). We validate our algorithm in simulated scenarios of image covering.",
    "published": "2022-04-15T15:47:05Z",
    "updated": "2026-02-23T08:03:56Z",
    "link": "http://arxiv.org/pdf/2204.07520v4.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "cs.MA",
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Zirui Xu",
      "Vasileios Tzoumas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05850v3",
    "title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models",
    "summary": "Reinforcement learning with verifiable reward (RLVR) has been instrumental in eliciting strong reasoning capabilities from large language models (LLMs) via long chains of thought (CoT). During RLVR training, we formalize and systemically study an empirical phenomenon whereby a multilingual model's CoT reverts to its dominant pre-training language (e.g., English) even when prompted in another language, which we term Cross-lingual Collapse. Because the long-CoT regime magnifies exposure to linguistic priors, the underlying trade-off between maximizing reasoning depth and preserving target-language fidelity has remained under-characterized. To examine this trade-off, we train LLMs with Group-Relative Policy Optimization (GRPO) on translated versions of math datasets widely used to elicit long-CoT reasoning. Throughout training, we track both task accuracy and the language consistency of reasoning chains. Our experiments yield three findings: (i) under RLVR, CoT in LLMs systematically drifts toward the pre-training dominant language as reasoning performance rises; (ii) English-centric priors, long-CoT GRPO optimization, task difficulty, and high-entropy decoding jointly amplify this drift, and the pattern persists beyond mathematics; and (iii) interventions that favor target-language traces--via a language-consistency reward, decoding-time controls, or more balanced backbones--mitigate collapse but reveal a persistent performance-fidelity trade-off.",
    "published": "2025-06-06T08:08:48Z",
    "updated": "2026-02-23T08:02:48Z",
    "link": "http://arxiv.org/pdf/2506.05850v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Cheonbok Park",
      "Jeonghoon Kim",
      "Joosung Lee",
      "Sanghwan Bae",
      "Jaegul Choo",
      "Kang Min Yoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.05806v2",
    "title": "Meta-Continual Learning of Neural Fields",
    "summary": "Neural Fields (NF) have gained prominence as a versatile framework for complex data representation. This work unveils a new problem setting termed \\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel strategy that employs a modular architecture combined with optimization-based meta-learning. Focused on overcoming the limitations of existing methods for continual learning of neural fields, such as catastrophic forgetting and slow convergence, our strategy achieves high-quality reconstruction with significantly improved learning speed. We further introduce Fisher Information Maximization loss for neural radiance fields (FIM-NeRF), which maximizes information gains at the sample level to enhance learning generalization, with proved convergence guarantee and generalization bound. We perform extensive evaluations across image, audio, video reconstruction, and view synthesis tasks on six diverse datasets, demonstrating our method's superiority in reconstruction quality and speed over existing MCL and CL-NF approaches. Notably, our approach attains rapid adaptation of neural fields for city-scale NeRF rendering with reduced parameter requirement. Code is available at https://github.com/seungyoon-woo/mcl-nf.",
    "published": "2025-04-08T08:38:37Z",
    "updated": "2026-02-23T07:58:40Z",
    "link": "http://arxiv.org/pdf/2504.05806v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Seungyoon Woo",
      "Junhyeog Yun",
      "Gunhee Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19574v1",
    "title": "CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment",
    "summary": "Large-language-model (LLM)-based text-to-speech (TTS) systems can generate natural speech, but most are not designed for low-latency dual-streaming synthesis. High-quality dual-streaming TTS depends on accurate text--speech alignment and well-designed training sequences that balance synthesis quality and latency. Prior work often relies on GMM-HMM based forced-alignment toolkits (e.g., MFA), which are pipeline-heavy and less flexible than neural aligners; fixed-ratio interleaving of text and speech tokens struggles to capture text--speech alignment regularities. We propose CTC-TTS, which replaces MFA with a CTC based aligner and introduces a bi-word based interleaving strategy. Two variants are designed: CTC-TTS-L (token concatenation along the sequence length) for higher quality and CTC-TTS-F (embedding stacking along the feature dimension) for lower latency. Experiments show that CTC-TTS outperforms fixed-ratio interleaving and MFA-based baselines on streaming synthesis and zero-shot tasks. Speech samples are available at https://ctctts.github.io/.",
    "published": "2026-02-23T07:44:14Z",
    "updated": "2026-02-23T07:44:14Z",
    "link": "http://arxiv.org/pdf/2602.19574v1.pdf",
    "category": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "authors": [
      "Hanwen Liu",
      "Saierdaer Yusuyin",
      "Hao Huang",
      "Zhijian Ou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19569v1",
    "title": "Temporal-Aware Heterogeneous Graph Reasoning with Multi-View Fusion for Temporal Question Answering",
    "summary": "Question Answering over Temporal Knowledge Graphs (TKGQA) has attracted growing interest for handling time-sensitive queries. However, existing methods still struggle with: 1) weak incorporation of temporal constraints in question representation, causing biased reasoning; 2) limited ability to perform explicit multi-hop reasoning; and 3) suboptimal fusion of language and graph representations. We propose a novel framework with temporal-aware question encoding, multi-hop graph reasoning, and multi-view heterogeneous information fusion. Specifically, our approach introduces: 1) a constraint-aware question representation that combines semantic cues from language models with temporal entity dynamics; 2) a temporal-aware graph neural network for explicit multi-hop reasoning via time-aware message passing; and 3) a multi-view attention mechanism for more effective fusion of question context and temporal graph knowledge. Experiments on multiple TKGQA benchmarks demonstrate consistent improvements over multiple baselines.",
    "published": "2026-02-23T07:36:36Z",
    "updated": "2026-02-23T07:36:36Z",
    "link": "http://arxiv.org/pdf/2602.19569v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Wuzhenghong Wen",
      "Bowen Zhou",
      "Jinwen Huang",
      "Xianjie Wu",
      "Yuwei Sun",
      "Su Pan",
      "Liang Li",
      "Jianting Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19565v1",
    "title": "DICArt: Advancing Category-level Articulated Object Pose Estimation in Discrete State-Spaces",
    "summary": "Articulated object pose estimation is a core task in embodied AI. Existing methods typically regress poses in a continuous space, but often struggle with 1) navigating a large, complex search space and 2) failing to incorporate intrinsic kinematic constraints. In this work, we introduce DICArt (DIsCrete Diffusion for Articulation Pose Estimation), a novel framework that formulates pose estimation as a conditional discrete diffusion process. Instead of operating in a continuous domain, DICArt progressively denoises a noisy pose representation through a learned reverse diffusion procedure to recover the GT pose. To improve modeling fidelity, we propose a flexible flow decider that dynamically determines whether each token should be denoised or reset, effectively balancing the real and noise distributions during diffusion. Additionally, we incorporate a hierarchical kinematic coupling strategy, estimating the pose of each rigid part hierarchically to respect the object's kinematic structure. We validate DICArt on both synthetic and real-world datasets. Experimental results demonstrate its superior performance and robustness. By integrating discrete generative modeling with structural priors, DICArt offers a new paradigm for reliable category-level 6D pose estimation in complex environments.",
    "published": "2026-02-23T07:30:47Z",
    "updated": "2026-02-23T07:30:47Z",
    "link": "http://arxiv.org/pdf/2602.19565v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Li Zhang",
      "Mingyu Mei",
      "Ailing Wang",
      "Xianhui Meng",
      "Yan Zhong",
      "Xinyuan Song",
      "Liu Liu",
      "Rujing Wang",
      "Zaixing He",
      "Cewu Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.03098v2",
    "title": "TextME: Bridging Unseen Modalities Through Text Descriptions",
    "summary": "Expanding multimodal representations to novel modalities is constrained by reliance on large-scale paired datasets (e.g., text-image, text-audio, text-3D, text-molecule), which are costly and often infeasible in domains requiring expert annotation such as medical imaging and molecular analysis. We introduce TextME, the first text-only modality expansion framework, to the best of our knowledge, projecting diverse modalities into LLM embedding space as a unified anchor. Our approach exploits the geometric structure of pretrained contrastive encoders to enable zero-shot cross-modal transfer using only text descriptions, without paired supervision. We empirically validate that such consistent modality gaps exist across image, video, audio, 3D, X-ray, and molecular domains, demonstrating that text-only training can preserve substantial performance of pretrained encoders. We further show that our framework enables emergent cross-modal retrieval between modality pairs not explicitly aligned during training (e.g., audio-to-image, 3D-to-image). These results establish text-only training as a practical alternative to paired supervision for modality expansion.",
    "published": "2026-02-03T04:43:13Z",
    "updated": "2026-02-23T07:25:58Z",
    "link": "http://arxiv.org/pdf/2602.03098v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Soyeon Hong",
      "Jinchan Kim",
      "Jaegook You",
      "Seungtaek Choi",
      "Suha Kwak",
      "Hyunsouk Cho"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19562v1",
    "title": "A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data",
    "summary": "Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\\% of the time (versus 20\\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .",
    "published": "2026-02-23T07:20:11Z",
    "updated": "2026-02-23T07:20:11Z",
    "link": "http://arxiv.org/pdf/2602.19562v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Joseph Bingham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.22512v2",
    "title": "Transitive RL: Value Learning via Divide and Conquer",
    "summary": "In this work, we present Transitive Reinforcement Learning (TRL), a new value learning algorithm based on a divide-and-conquer paradigm. TRL is designed for offline goal-conditioned reinforcement learning (GCRL) problems, where the aim is to find a policy that can reach any state from any other state in the smallest number of steps. TRL converts a triangle inequality structure present in GCRL into a practical divide-and-conquer value update rule. This has several advantages compared to alternative value learning paradigms. Compared to temporal difference (TD) methods, TRL suffers less from bias accumulation, as in principle it only requires $O(\\log T)$ recursions (as opposed to $O(T)$ in TD learning) to handle a length-$T$ trajectory. Unlike Monte Carlo methods, TRL suffers less from high variance as it performs dynamic programming. Experimentally, we show that TRL achieves the best performance in highly challenging, long-horizon benchmark tasks compared to previous offline GCRL algorithms.",
    "published": "2025-10-26T03:32:31Z",
    "updated": "2026-02-23T07:03:22Z",
    "link": "http://arxiv.org/pdf/2510.22512v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Seohong Park",
      "Aditya Oberai",
      "Pranav Atreya",
      "Sergey Levine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19555v1",
    "title": "Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains",
    "summary": "Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.",
    "published": "2026-02-23T06:57:57Z",
    "updated": "2026-02-23T06:57:57Z",
    "link": "http://arxiv.org/pdf/2602.19555v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Xiaochong Jiang",
      "Shiqi Yang",
      "Wenting Yang",
      "Yichen Liu",
      "Cheng Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.10987v3",
    "title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation",
    "summary": "The promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at https://github.com/hsannn/ditto.git.",
    "published": "2025-10-13T03:53:40Z",
    "updated": "2026-02-23T06:55:02Z",
    "link": "http://arxiv.org/pdf/2510.10987v3.pdf",
    "category": [
      "cs.CR",
      "cs.AI"
    ],
    "authors": [
      "Hyeseon An",
      "Shinwoo Park",
      "Suyeon Woo",
      "Yo-Sub Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2211.12817v3",
    "title": "Learning to See the Elephant in the Room: Self-Supervised Context Reasoning in Humans and AI",
    "summary": "Humans rarely perceive objects in isolation but interpret scenes through relationships among co-occurring elements. How such contextual knowledge is acquired without explicit supervision remains unclear. Here we combine human psychophysics experiments with computational modelling to study the emergence of contextual reasoning. Participants were exposed to novel objects embedded in naturalistic scenes that followed predefined contextual rules capturing global context, local context and crowding. After viewing short training videos, participants completed a \"lift-the-flap\" task in which a hidden object had to be inferred from the surrounding context under variations in size, resolution and spatial arrangement. Humans rapidly learned these contextual associations without labels or feedback and generalised robustly across contextual changes. We then introduce SeCo (Self-supervised learning for Context Reasoning), a biologically inspired model that learns contextual relationships from complex scenes. SeCo encodes targets and context with separate vision encoders and stores latent contextual priors in a learnable external memory module. Given contextual cues, the model retrieves likely object representations to infer hidden targets. SeCo outperforms state-of-the-art self-supervised learning approaches and predicts object placements most consistent with human behaviour, highlighting the central role of contextual associations in scene understanding.",
    "published": "2022-11-23T10:02:05Z",
    "updated": "2026-02-23T06:38:15Z",
    "link": "http://arxiv.org/pdf/2211.12817v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xiao Liu",
      "Soumick Sarker",
      "Ankur Sikarwar",
      "Bryan Atista Kiely",
      "Gabriel Kreiman",
      "Zenglin Shi",
      "Mengmi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.16179v4",
    "title": "EnterpriseBench Corecraft: Training Generalizable Agents on High-Fidelity RL Environments",
    "summary": "We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce CoreCraft, the first environment in EnterpriseBench, Surge AI's suite of agentic RL environments. CoreCraft is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM 4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37% to 36.76% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5% on BFCL Parallel, +7.4% on Tau2-Bench Retail, and +6.8% on Tool Decathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.",
    "published": "2026-02-18T04:35:46Z",
    "updated": "2026-02-23T06:33:42Z",
    "link": "http://arxiv.org/pdf/2602.16179v4.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sushant Mehta",
      "Logan Ritchie",
      "Suhaas Garre",
      "Ian Niebres",
      "Nick Heiner",
      "Edwin Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.15620v3",
    "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
    "summary": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design an S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13% ($ρ_{\\mathrm{T}}$=1.0, top-p=1.0) and 3.69% ($ρ_{\\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy, and JustRL.",
    "published": "2026-02-17T14:46:48Z",
    "updated": "2026-02-23T06:22:49Z",
    "link": "http://arxiv.org/pdf/2602.15620v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Shiqi Liu",
      "Zeyu He",
      "Guojian Zhan",
      "Letian Tao",
      "Zhilong Zheng",
      "Jiang Wu",
      "Yinuo Wang",
      "Yang Guan",
      "Kehua Sheng",
      "Bo Zhang",
      "Keqiang Li",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19540v1",
    "title": "A Green Learning Approach to LDCT Image Restoration",
    "summary": "This work proposes a green learning (GL) approach to restore medical images. Without loss of generality, we use low-dose computed tomography (LDCT) images as examples. LDCT images are susceptible to noise and artifacts, where the imaging process introduces distortion. LDCT image restoration is an important preprocessing step for further medical analysis. Deep learning (DL) methods have been developed to solve this problem. We examine an alternative solution using the Green Learning (GL) methodology. The new restoration method is characterized by mathematical transparency, computational and memory efficiency, and high performance. Experiments show that our GL method offers state-of-the-art restoration performance at a smaller model size and with lower inference complexity.",
    "published": "2026-02-23T06:21:56Z",
    "updated": "2026-02-23T06:21:56Z",
    "link": "http://arxiv.org/pdf/2602.19540v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wei Wang",
      "Yixing Wu",
      "C. -C. Jay Kuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19538v1",
    "title": "Cost-Aware Diffusion Active Search",
    "summary": "Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thompson sampling based myopic, greedy approaches for agents to actively decide query or search locations when the number of targets is unknown. Decision making algorithms in such partially observable environments have also shown that agents capable of lookahead over a finite horizon outperform myopic policies for active search. Unfortunately, lookahead algorithms typically rely on building a computationally expensive search tree that is simulated and updated based on the agent's observations and a model of the environment dynamics. Instead, in this work, we leverage the sequence modeling abilities of diffusion models to sample lookahead action sequences that balance the exploration-exploitation trade-off for active search without building an exhaustive search tree. We identify the optimism bias in prior diffusion based reinforcement learning approaches when applied to the active search setting and propose mitigating solutions for efficient cost-aware decision making with both single and multi-agent teams. Our proposed algorithm outperforms standard baselines in offline reinforcement learning in terms of full recovery rate and is computationally more efficient than tree search in cost-aware active decision making.",
    "published": "2026-02-23T06:11:51Z",
    "updated": "2026-02-23T06:11:51Z",
    "link": "http://arxiv.org/pdf/2602.19538v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Arundhati Banerjee",
      "Jeff Schneider"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19536v1",
    "title": "Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection",
    "summary": "Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.",
    "published": "2026-02-23T06:03:07Z",
    "updated": "2026-02-23T06:03:07Z",
    "link": "http://arxiv.org/pdf/2602.19536v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhiwei Ning",
      "Xuanang Gao",
      "Jiaxi Cao",
      "Runze Yang",
      "Huiying Xu",
      "Xinzhong Zhu",
      "Jie Yang",
      "Wei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19534v1",
    "title": "Large Language Model-Assisted UAV Operations and Communications: A Multifaceted Survey and Tutorial",
    "summary": "Uncrewed Aerial Vehicles (UAVs) are widely deployed across diverse applications due to their mobility and agility. Recent advances in Large Language Models (LLMs) offer a transformative opportunity to enhance UAV intelligence beyond conventional optimization-based and learning-based approaches. By integrating LLMs into UAV systems, advanced environmental understanding, swarm coordination, mobility optimization, and high-level task reasoning can be achieved, thereby allowing more adaptive and context-aware aerial operations. This survey systematically explores the intersection of LLMs and UAV technologies and proposes a unified framework that consolidates existing architectures, methodologies, and applications for UAVs. We first present a structured taxonomy of LLM adaptation techniques for UAVs, including pretraining, fine-tuning, Retrieval-Augmented Generation (RAG), and prompt engineering, along with key reasoning capabilities such as Chain-of-Thought (CoT) and In-Context Learning (ICL). We then examine LLM-assisted UAV communications and operations, covering navigation, mission planning, swarm control, safety, autonomy, and network management. After that, the survey further discusses Multimodal LLMs (MLLMs) for human-swarm interaction, perception-driven navigation, and collaborative control. Finally, we address ethical considerations, including bias, transparency, accountability, and Human-in-the-Loop (HITL) strategies, and outline future research directions. Overall, this work positions LLM-assisted UAVs as a foundation for intelligent and adaptive aerial systems.",
    "published": "2026-02-23T05:56:43Z",
    "updated": "2026-02-23T05:56:43Z",
    "link": "http://arxiv.org/pdf/2602.19534v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yousef Emami",
      "Hao Zhou",
      "Radha Reddy",
      "Atefeh Hajijamali Arani",
      "Biliang Wang",
      "Kai Li",
      "Luis Almeida",
      "Zhu Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19533v1",
    "title": "Grokking Finite-Dimensional Algebra",
    "summary": "This paper investigates the grokking phenomenon, which refers to the sudden transition from a long memorization to generalization observed during neural networks training, in the context of learning multiplication in finite-dimensional algebras (FDA). While prior work on grokking has focused mainly on group operations, we extend the analysis to more general algebraic structures, including non-associative, non-commutative, and non-unital algebras. We show that learning group operations is a special case of learning FDA, and that learning multiplication in FDA amounts to learning a bilinear product specified by the algebra's structure tensor. For algebras over the reals, we connect the learning problem to matrix factorization with an implicit low-rank bias, and for algebras over finite fields, we show that grokking emerges naturally as models must learn discrete representations of algebraic elements. This leads us to experimentally investigate the following core questions: (i) how do algebraic properties such as commutativity, associativity, and unitality influence both the emergence and timing of grokking, (ii) how structural properties of the structure tensor of the FDA, such as sparsity and rank, influence generalization, and (iii) to what extent generalization correlates with the model learning latent embeddings aligned with the algebra's representation. Our work provides a unified framework for grokking across algebraic structures and new insights into how mathematical structure governs neural network generalization dynamics.",
    "published": "2026-02-23T05:55:52Z",
    "updated": "2026-02-23T05:55:52Z",
    "link": "http://arxiv.org/pdf/2602.19533v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.RA"
    ],
    "authors": [
      "Pascal Jr Tikeng Notsawo",
      "Guillaume Dumas",
      "Guillaume Rabusseau"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19531v1",
    "title": "A Statistical Approach for Modeling Irregular Multivariate Time Series with Missing Observations",
    "summary": "Irregular multivariate time series with missing values present significant challenges for predictive modeling in domains such as healthcare. While deep learning approaches often focus on temporal interpolation or complex architectures to handle irregularities, we propose a simpler yet effective alternative: extracting time-agnostic summary statistics to eliminate the temporal axis. Our method computes four key features per variable-mean and standard deviation of observed values, as well as the mean and variability of changes between consecutive observations to create a fixed-dimensional representation. These features are then utilized with standard classifiers, such as logistic regression and XGBoost. Evaluated on four biomedical datasets (PhysioNet Challenge 2012, 2019, PAMAP2, and MIMIC-III), our approach achieves state-of-the-art performance, surpassing recent transformer and graph-based models by 0.5-1.7% in AUROC/AUPRC and 1.1-1.7% in accuracy/F1-score, while reducing computational complexity. Ablation studies demonstrate that feature extraction-not classifier choice-drives performance gains, and our summary statistics outperform raw/imputed input in most benchmarks. In particular, we identify scenarios where missing patterns themselves encode predictive signals, as in sepsis prediction (PhysioNet, 2019), where missing indicators alone can achieve 94.2% AUROC with XGBoost, only 1.6% lower than using original raw data as input. Our results challenge the necessity of complex temporal modeling when task objectives permit time-agnostic representations, providing an efficient and interpretable solution for irregular time series classification.",
    "published": "2026-02-23T05:48:17Z",
    "updated": "2026-02-23T05:48:17Z",
    "link": "http://arxiv.org/pdf/2602.19531v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Dingyi Nie",
      "Yixing Wu",
      "C. -C. Jay Kuo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07231v3",
    "title": "EconCausal: A Context-Aware Causal Reasoning Benchmark for Large Language Models in Social Science",
    "summary": "Socio-economic causal effects depend heavily on their specific institutional and environmental context. A single intervention can produce opposite results depending on regulatory or market factors, contexts that are often complex and only partially observed. This poses a significant challenge for large language models (LLMs) in decision-support roles: can they distinguish structural causal mechanisms from surface-level correlations when the context changes?\n  To address this, we introduce EconCausal, a large-scale benchmark comprising 10,490 context-annotated causal triplets extracted from 2,595 high-quality empirical studies published in top-tier economics and finance journals. Through a rigorous four-stage pipeline combining multi-run consensus, context refinement, and multi-critic filtering, we ensure each claim is grounded in peer-reviewed research with explicit identification strategies.\n  Our evaluation reveals critical limitations in current LLMs' context-dependent reasoning. While top models achieve approximately 88 percent accuracy in fixed, explicit contexts, performance drops sharply under context shifts, with a 32.6 percentage point decline, and falls to 37 percent when misinformation is introduced. Furthermore, models exhibit severe over-commitment in ambiguous cases and struggle to recognize null effects, achieving only 9.5 percent accuracy, exposing a fundamental gap between pattern matching and genuine causal reasoning. These findings underscore substantial risks for high-stakes economic decision-making, where the cost of misinterpreting causality is high.\n  The dataset and benchmark are publicly available at https://github.com/econaikaist/econcausal-benchmark.",
    "published": "2025-10-08T17:00:49Z",
    "updated": "2026-02-23T05:21:42Z",
    "link": "http://arxiv.org/pdf/2510.07231v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Donggyu Lee",
      "Hyeok Yun",
      "Meeyoung Cha",
      "Sungwon Park",
      "Sangyoon Park",
      "Jihee Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19519v1",
    "title": "Ada-RS: Adaptive Rejection Sampling for Selective Thinking",
    "summary": "Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.",
    "published": "2026-02-23T05:20:15Z",
    "updated": "2026-02-23T05:20:15Z",
    "link": "http://arxiv.org/pdf/2602.19519v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yirou Ge",
      "Yixi Li",
      "Alec Chiu",
      "Shivani Shekhar",
      "Zijie Pan",
      "Avinash Thangali",
      "Yun-Shiuan Chuang",
      "Chaitanya Kulkarni",
      "Uma Kona",
      "Linsey Pang",
      "Prakhar Mehrotra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19517v1",
    "title": "Classroom Final Exam: An Instructor-Tested Reasoning Benchmark",
    "summary": "We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.",
    "published": "2026-02-23T05:17:41Z",
    "updated": "2026-02-23T05:17:41Z",
    "link": "http://arxiv.org/pdf/2602.19517v1.pdf",
    "category": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Chongyang Gao",
      "Diji Yang",
      "Shuyan Zhou",
      "Xichen Yan",
      "Luchuan Song",
      "Shuo Li",
      "Kezhen Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.08354v2",
    "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
    "summary": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
    "published": "2026-02-09T07:38:22Z",
    "updated": "2026-02-23T05:13:24Z",
    "link": "http://arxiv.org/pdf/2602.08354v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Zixuan Huang",
      "Xin Xia",
      "Yuxi Ren",
      "Jianbin Zheng",
      "Xuanda Wang",
      "Zhixia Zhang",
      "Hongyan Xie",
      "Songshi Liang",
      "Zehao Chen",
      "Xuefeng Xiao",
      "Fuzhen Zhuang",
      "Jianxin Li",
      "Yikun Ban",
      "Deqing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.20629v4",
    "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
    "summary": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
    "published": "2025-11-25T18:49:21Z",
    "updated": "2026-02-23T05:04:31Z",
    "link": "http://arxiv.org/pdf/2511.20629v4.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chieh-Yun Chen",
      "Zhonghao Wang",
      "Qi Chen",
      "Zhifan Ye",
      "Min Shi",
      "Yue Zhao",
      "Yinan Zhao",
      "Hui Qu",
      "Wei-An Lin",
      "Yiru Shen",
      "Ajinkya Kale",
      "Irfan Essa",
      "Humphrey Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.17842v2",
    "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning",
    "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in generating 'believable human-like' behavior in web environments. Prior work has explored augmenting training data with LLM-synthesized rationales and applying supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can improve downstream action prediction. However, the performance of such approaches remains inherently bounded by the reasoning capabilities of the model used to generate the rationales. In this paper, we introduce Shop-R1, a novel reinforcement learning (RL) framework aimed at enhancing the reasoning ability of LLMs for simulation of real human behavior in online shopping environments. Specifically, Shop-R1 decomposes the human behavior simulation task into two stages: rationale generation and action prediction, each guided by distinct reward signals. For rationale generation, we leverage internal model signals (e.g., logit distributions) to guide the reasoning process in a self-supervised manner. For action prediction, we propose a hierarchical reward structure with difficulty-aware scaling to prevent reward hacking and enable fine-grained reward assignment. This design evaluates both high-level action types and the correctness of fine-grained sub-action details (attributes and values), rewarding outputs proportionally to their difficulty. Experimental results show that our method achieves a relative improvement of over 65% compared to the baseline. The project page is available at https://damon-demon.github.io/shop-r1.html.",
    "published": "2025-07-23T18:10:43Z",
    "updated": "2026-02-23T18:12:05Z",
    "link": "http://arxiv.org/pdf/2507.17842v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yimeng Zhang",
      "Tian Wang",
      "Jiri Gesi",
      "Ziyi Wang",
      "Yuxuan Lu",
      "Jiacheng Lin",
      "Sinong Zhan",
      "Vianne Gao",
      "Ruochen Jiao",
      "Junze Liu",
      "Kun Qian",
      "Yuxin Tang",
      "Ran Xue",
      "Houyu Zhang",
      "Qingjun Cui",
      "Yufan Guo",
      "Dakuo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20092v1",
    "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
    "summary": "BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.\n  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.",
    "published": "2026-02-23T18:02:23Z",
    "updated": "2026-02-23T18:02:23Z",
    "link": "http://arxiv.org/pdf/2602.20092v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Leshem Choshen",
      "Ryan Cotterell",
      "Mustafa Omer Gul",
      "Jaap Jumelet",
      "Tal Linzen",
      "Aaron Mueller",
      "Suchir Salhan",
      "Raj Sanjay Shah",
      "Alex Warstadt",
      "Ethan Gotlieb Wilcox"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20091v1",
    "title": "How Retrieved Context Shapes Internal Representations in RAG",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
    "published": "2026-02-23T18:02:04Z",
    "updated": "2026-02-23T18:02:04Z",
    "link": "http://arxiv.org/pdf/2602.20091v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Samuel Yeh",
      "Sharon Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22842v3",
    "title": "Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation",
    "summary": "Transformer-based language models rely on positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods lack theoretical clarity and rely on limited evaluation metrics to substantiate their extrapolation claims. We propose the Bayesian Attention Mechanism (BAM), a theoretical framework that formulates positional encoding as a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE and ALiBi) and motivates a new Generalized Gaussian positional prior that substantially improves long-context generalization. Empirically, BAM enables accurate information retrieval at $500\\times$ the training context length, outperforming previous state-of-the-art context length generalization in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.",
    "published": "2025-05-28T20:22:23Z",
    "updated": "2026-02-23T17:45:58Z",
    "link": "http://arxiv.org/pdf/2505.22842v3.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Arthur S. Bianchessi",
      "Yasmin C. Aguirre",
      "Rodrigo C. Barros",
      "Lucas S. Kupssinskü"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.13139v2",
    "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report",
    "summary": "Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.",
    "published": "2026-02-13T17:47:08Z",
    "updated": "2026-02-23T17:38:41Z",
    "link": "http://arxiv.org/pdf/2602.13139v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Mariia Fedorova",
      "Nikolay Arefyev",
      "Maja Buljan",
      "Jindřich Helcl",
      "Stephan Oepen",
      "Egil Rønningstad",
      "Yves Scherrer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20052v1",
    "title": "Entropy in Large Language Models",
    "summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
    "published": "2026-02-23T17:02:45Z",
    "updated": "2026-02-23T17:02:45Z",
    "link": "http://arxiv.org/pdf/2602.20052v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Marco Scharringhausen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.09109v4",
    "title": "Personalized Help for Optimizing Low-Skilled Users' Strategy",
    "summary": "AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment CICERO, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it.",
    "published": "2024-11-14T00:52:45Z",
    "updated": "2026-02-23T16:56:22Z",
    "link": "http://arxiv.org/pdf/2411.09109v4.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Feng Gu",
      "Wichayaporn Wongkamjan",
      "Jonathan K. Kummerfeld",
      "Denis Peskoff",
      "Jonathan May",
      "Jordan Boyd-Graber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20042v1",
    "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
    "summary": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flattening, \\textbf{normative} representation loss, and \\textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.",
    "published": "2026-02-23T16:51:43Z",
    "updated": "2026-02-23T16:51:43Z",
    "link": "http://arxiv.org/pdf/2602.20042v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Han Bao",
      "Yue Huang",
      "Xiaoda Wang",
      "Zheyuan Zhang",
      "Yujun Zhou",
      "Carl Yang",
      "Xiangliang Zhang",
      "Yanfang Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20020v1",
    "title": "gencat: Generative computerized adaptive testing",
    "summary": "Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\\textbf{GEN}erative \\textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\\% in the key early testing stages.",
    "published": "2026-02-23T16:28:46Z",
    "updated": "2026-02-23T16:28:46Z",
    "link": "http://arxiv.org/pdf/2602.20020v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Wanyong Feng",
      "Andrew Lan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20017v1",
    "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
    "summary": "Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.",
    "published": "2026-02-23T16:23:49Z",
    "updated": "2026-02-23T16:23:49Z",
    "link": "http://arxiv.org/pdf/2602.20017v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Gaurav Najpande",
      "Tampu Ravi Kumar",
      "Manan Roy Choudhury",
      "Neha Valeti",
      "Yanjie Fu",
      "Vivek Gupta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19991v1",
    "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
    "summary": "Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.",
    "published": "2026-02-23T15:57:16Z",
    "updated": "2026-02-23T15:57:16Z",
    "link": "http://arxiv.org/pdf/2602.19991v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yaya Sy",
      "Dioula Doucouré",
      "Christophe Cerisara",
      "Irina Illina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.07543v6",
    "title": "MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts",
    "summary": "With the rapid progress of Multimodal LLMs, evaluating their mathematical reasoning capabilities has become an increasingly important research direction. In particular, visual-textual mathematical reasoning serves as a key indicator of an MLLM's ability to comprehend and solve complex, multi-step quantitative problems. While existing benchmarks such as MathVista and MathVerse have advanced the evaluation of multimodal math proficiency, they primarily rely on digitally rendered content and fall short in capturing the complexity of real-world scenarios. To bridge this gap, we introduce MathScape, a novel benchmark focused on assessing MLLMs' reasoning ability in realistic mathematical contexts. MathScape comprises 1,369 high-quality math problems paired with human-captured real-world images, closely reflecting the challenges encountered in practical educational settings. We conduct a thorough multi-dimensional evaluation across nine leading closed-source MLLMs, three open-source MLLMs with over 20 billion parameters, and seven smaller-scale MLLMs. Our results show that even state-of-the-art models struggle with real-world math tasks, lagging behind human performance, highlighting critical limitations in current model capabilities. Moreover, we find that strong performance on synthetic or digitally rendered images does not guarantee similar effectiveness on real-world tasks. This underscores the necessity of MathScape in the next stage of multimodal mathematical reasoning.",
    "published": "2024-08-14T13:23:43Z",
    "updated": "2026-02-23T15:56:57Z",
    "link": "http://arxiv.org/pdf/2408.07543v6.pdf",
    "category": [
      "cs.CV",
      "cs.CL"
    ],
    "authors": [
      "Hao Liang",
      "Linzhuang Sun",
      "Minxuan Zhou",
      "Zirong Chen",
      "Meiyi Qiang",
      "Mingan Lin",
      "Tianpeng Li",
      "Fan Yang",
      "Zenan Zhou",
      "Wentao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19961v1",
    "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
    "summary": "With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.",
    "published": "2026-02-23T15:27:41Z",
    "updated": "2026-02-23T15:27:41Z",
    "link": "http://arxiv.org/pdf/2602.19961v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Yibo Yan",
      "Jiahao Huo",
      "Guanbo Feng",
      "Mingdong Ou",
      "Yi Cao",
      "Xin Zou",
      "Shuliang Liu",
      "Yuanhuiyi Lyu",
      "Yu Huang",
      "Jungang Li",
      "Kening Zheng",
      "Xu Zheng",
      "Philip S. Yu",
      "James Kwok",
      "Xuming Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19919v1",
    "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
    "summary": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.",
    "published": "2026-02-23T14:58:51Z",
    "updated": "2026-02-23T14:58:51Z",
    "link": "http://arxiv.org/pdf/2602.19919v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Xiang Li",
      "Zikai Wei",
      "Yiyan Qi",
      "Wanyun Zhou",
      "Xiang Liu",
      "Penglei Sun",
      "Yongqi Zhang",
      "Xiaowen Chu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19895v1",
    "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
    "summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
    "published": "2026-02-23T14:37:01Z",
    "updated": "2026-02-23T14:37:01Z",
    "link": "http://arxiv.org/pdf/2602.19895v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Zhongwei Wan",
      "Yun Shen",
      "Zhihao Dou",
      "Donghao Zhou",
      "Yu Zhang",
      "Xin Wang",
      "Hui Shen",
      "Jing Xiong",
      "Chaofan Tao",
      "Zixuan Zhong",
      "Peizhou Huang",
      "Mi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.02840v2",
    "title": "promptolution: A Unified, Modular Framework for Prompt Optimization",
    "summary": "Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers demonstrate its effectiveness, practical adoption is hindered because existing implementations are often tied to unmaintained, isolated research codebases or require invasive integration into application frameworks. To address this, we introduce promptolution, a unified, modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers, supports systematic and reproducible benchmarking, and returns framework-agnostic prompt strings, enabling seamless integration into existing LLM pipelines while remaining agnostic to the underlying model implementation.",
    "published": "2025-12-02T14:53:23Z",
    "updated": "2026-02-23T14:32:06Z",
    "link": "http://arxiv.org/pdf/2512.02840v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Tom Zehle",
      "Timo Heiß",
      "Moritz Schlager",
      "Matthias Aßenmacher",
      "Matthias Feurer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19883v1",
    "title": "Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection",
    "summary": "ODRL's six set-based operators -- isA, isPartOf, hasPart, isAnyOf, isAllOf, isNoneOf -- depend on external domain knowledge that the W3C specification leaves unspecified. Without it, every cross-dataspace policy comparison defaults to Unknown. We present a denotational semantics that maps each ODRL constraint to the set of knowledge-base concepts satisfying it. Conflict detection reduces to denotation intersection under a three-valued verdict -- Conflict, Compatible, or Unknown -- that is sound under incomplete knowledge. The framework covers all three ODRL composition modes (and, or, xone) and all three semantic domains arising in practice: taxonomic (class subsumption), mereological (part-whole containment), and nominal (identity). For cross-dataspace interoperability, we define order-preserving alignments between knowledge bases and prove two guarantees: conflicts are preserved across different KB standards, and unmapped concepts degrade gracefully to Unknown -- never to false conflicts. A runtime soundness theorem ensures that design-time verdicts hold for all execution contexts. The encoding stays within the decidable EPR fragment of first-order logic. We validate it with 154 benchmarks across six knowledge base families (GeoNames, ISO 3166, W3C DPV, a GDPR-derived taxonomy, BCP 47, and ISO 639-3) and four structural KBs targeting adversarial edge cases. Both the Vampire theorem prover and the Z3 SMT solver agree on all 154 verdicts. A key finding is that exclusive composition (xone) requires strictly stronger KB axioms than conjunction or disjunction: open-world semantics blocks exclusivity even when positive evidence appears to satisfy exactly one branch.",
    "published": "2026-02-23T14:28:13Z",
    "updated": "2026-02-23T14:28:13Z",
    "link": "http://arxiv.org/pdf/2602.19883v1.pdf",
    "category": [
      "cs.CL",
      "cs.LO"
    ],
    "authors": [
      "Daham Mustafa",
      "Diego Collarana",
      "Yixin Peng",
      "Rafiqul Haque",
      "Christoph Lange-Bever",
      "Christoph Quix",
      "Stephan Decker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19878v1",
    "title": "Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics",
    "summary": "Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's approximately 34 left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text explicitly references multiple axes. For these operands, a single scalar constraint admits one interpretation per axis, making policy evaluation non-deterministic.\n  We classify ODRL's left operands by value-domain structure (scalar, dimensional, concept-valued), grounded in the ODRL 2.2 specification text, and show that dimensional ambiguity is intrinsic to the constraint syntax.\n  We present an axis-decomposition framework that refines each dimensional operand into axis-specific scalar operands and prove four properties: deterministic interpretation, AABB completeness, sound over-approximation under projection, and conservative extension.\n  Conflict detection operates in two layers: per-axis verdicts are always decidable; box-level verdicts compose through Strong Kleene conjunction into a three-valued logic (Conflict, Compatible, Unknown). For ODRL's disjunctive (odrl:or) and exclusive-or (odrl:xone) logical constraints, where per-axis decomposition does not apply, the framework encodes coupled multi-axis conjectures directly.\n  We instantiate the framework as the ODRL Spatial Axis Profile--15 axis-specific left operands for the five affected base terms--and evaluate it on 117 benchmark problems spanning nine categories across both TPTP FOF (Vampire) and SMT-LIB (Z3) encodings, achieving full concordance between provers. Benchmark scenarios are inspired by constraints arising in cultural heritage dataspaces such as Datenraum Kultur. All meta-theorems are mechanically verified in Isabelle/HOL.",
    "published": "2026-02-23T14:24:46Z",
    "updated": "2026-02-23T14:24:46Z",
    "link": "http://arxiv.org/pdf/2602.19878v1.pdf",
    "category": [
      "cs.CL",
      "cs.LO"
    ],
    "authors": [
      "Daham Mustafa",
      "Diego Collarana",
      "Yixin Peng",
      "Rafiqul Haque",
      "Christoph Lange-Bever",
      "Christoph Quix",
      "Stephan Decker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11826v2",
    "title": "Collaborative Document Editing with Multiple Users and AI Agents",
    "summary": "Current AI writing support tools are largely designed for individuals, complicating collaboration when co-writers must leave the shared workspace to use AI and then communicate and reintegrate results. We propose integrating AI agents directly into collaborative writing environments. Our prototype makes AI use visible to all users through two new shared objects: user-defined agent profiles and tasks. Agent responses appear in the familiar comment feature. In a user study (N=30), 14 teams worked on writing projects during one week. Interaction logs and interviews show that teams incorporated agents into existing norms of authorship, control, and coordination, rather than treating them as team members. Agent profiles were viewed as personal territory, while created agents and outputs became shared resources. We discuss implications for team-based AI interaction, highlighting opportunities and boundaries for treating AI as a shared resource in collaborative work.",
    "published": "2025-09-15T12:11:59Z",
    "updated": "2026-02-23T14:07:06Z",
    "link": "http://arxiv.org/pdf/2509.11826v2.pdf",
    "category": [
      "cs.HC",
      "cs.CL"
    ],
    "authors": [
      "Florian Lehmann",
      "Krystsina Shauchenka",
      "Daniel Buschek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19855v1",
    "title": "SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals",
    "summary": "We present SHIELD, a novel methodology for automated and integrated safety signal detection in clinical trials. SHIELD combines disproportionality analysis with semantic clustering of adverse event (AE) terms applied to MedDRA term embeddings. For each AE, the pipeline computes an information-theoretic disproportionality measure (Information Component) with effect size derived via empirical Bayesian shrinkage. A utility matrix is constructed by weighting semantic term-term similarities by signal magnitude, followed by spectral embedding and clustering to identify groups of related AEs. Resulting clusters are annotated with syndrome-level summary labels using large language models, yielding a coherent, data-driven representation of treatment-associated safety profiles in the form of a network graph and hierarchical tree. We implement the SHIELD framework in the context of a single-arm incidence summary, to compare two treatment arms or for the detection of any treatment effect in a multi-arm trial. We illustrate its ability to recover known safety signals and generate interpretable, cluster-based summaries in a real clinical trial example. This work bridges statistical signal detection with modern natural language processing to enhance safety assessment and causal interpretation in clinical trials.",
    "published": "2026-02-23T13:55:36Z",
    "updated": "2026-02-23T13:55:36Z",
    "link": "http://arxiv.org/pdf/2602.19855v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Francois Vandenhende",
      "Anna Georgiou",
      "Theodoros Psaras",
      "Ellie Karekla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.11851v2",
    "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without",
    "summary": "As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.",
    "published": "2025-09-15T12:31:00Z",
    "updated": "2026-02-23T13:51:33Z",
    "link": "http://arxiv.org/pdf/2509.11851v2.pdf",
    "category": [
      "cs.HC",
      "cs.CL"
    ],
    "authors": [
      "Tim Zindulka",
      "Sven Goller",
      "Daniela Fernandes",
      "Robin Welsch",
      "Daniel Buschek"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.18324v2",
    "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus",
    "summary": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.",
    "published": "2026-02-20T16:24:23Z",
    "updated": "2026-02-23T13:49:57Z",
    "link": "http://arxiv.org/pdf/2602.18324v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Alexandra Ciobotaru",
      "Ana-Maria Bucur",
      "Liviu P. Dinu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19840v1",
    "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
    "summary": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.",
    "published": "2026-02-23T13:40:44Z",
    "updated": "2026-02-23T13:40:44Z",
    "link": "http://arxiv.org/pdf/2602.19840v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jingzhuo Wu",
      "Jiajun Zhang",
      "Keyan Jin",
      "Dehua Ma",
      "Junbo Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19815v1",
    "title": "Keyboards for the Endangered Idu Mishmi Language",
    "summary": "We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training programs, and (2) a Windows desktop keyboard currently undergoing community testing. Both tools support the complete Idu Mishmi character inventory, including schwa, retracted schwa, nasalized vowels, and accented forms. Both operate fully offline with zero network permissions, addressing connectivity constraints and data sovereignty concerns. We describe the design, implementation, and deployment as a replicable model for other endangered language communities.",
    "published": "2026-02-23T13:13:40Z",
    "updated": "2026-02-23T13:13:40Z",
    "link": "http://arxiv.org/pdf/2602.19815v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Akhilesh Kakolu Ramarao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11230v3",
    "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages",
    "summary": "Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code and complete figures are available at https://github.com/LyzanderAndrylie/language-specific-features",
    "published": "2025-07-15T12:00:30Z",
    "updated": "2026-02-23T12:16:53Z",
    "link": "http://arxiv.org/pdf/2507.11230v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lyzander Marciano Andrylie",
      "Inaya Rahmanisa",
      "Mahardika Krisna Ihsani",
      "Alfan Farizki Wicaksono",
      "Haryo Akbarianto Wibowo",
      "Alham Fikri Aji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19743v1",
    "title": "NILE: Formalizing Natural-Language Descriptions of Formal Languages",
    "summary": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate.\n  To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically.\n  In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.",
    "published": "2026-02-23T11:42:56Z",
    "updated": "2026-02-23T11:42:56Z",
    "link": "http://arxiv.org/pdf/2602.19743v1.pdf",
    "category": [
      "cs.FL",
      "cs.CL",
      "cs.LO"
    ],
    "authors": [
      "Tristan Kneisel",
      "Marko Schmellenkamp",
      "Fabian Vehlken",
      "Thomas Zeume"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.13614v3",
    "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
    "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities, but struggle with temporal understanding, especially when questions involve multiple entities, compound operators, and evolving event sequences. Temporal Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a structured format, offer a reliable source for temporal reasoning. However, existing TKG-based LLM reasoning methods still struggle with four major challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving multi-entity temporal synchronization, adapting retrieval to diverse temporal operators, and reusing prior reasoning experience for stability and efficiency. To address these issues, we propose MemoTime, a memory-augmented temporal knowledge graph framework that enhances LLM reasoning through structured grounding, recursive reasoning, and continual experience learning. MemoTime decomposes complex temporal questions into a hierarchical Tree of Time, enabling operator-aware reasoning that enforces monotonic timestamps and co-constrains multiple entities under unified temporal bounds. A dynamic evidence retrieval layer adaptively selects operator-specific retrieval strategies, while a self-evolving experience memory stores verified reasoning traces, toolkit decisions, and sub-question embeddings for cross-type reuse. Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime achieves overall state-of-the-art results, outperforming the strong baseline by up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
    "published": "2025-10-15T14:43:31Z",
    "updated": "2026-02-23T11:42:27Z",
    "link": "http://arxiv.org/pdf/2510.13614v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Xingyu Tan",
      "Xiaoyang Wang",
      "Qing Liu",
      "Xiwei Xu",
      "Xin Yuan",
      "Liming Zhu",
      "Wenjie Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.03867v3",
    "title": "EuroGEST: Investigating gender stereotypes in multilingual language models",
    "summary": "Large language models increasingly support multiple languages, yet most benchmarks for gender bias remain English-centric. We introduce EuroGEST, a dataset designed to measure gender-stereotypical reasoning in LLMs across English and 29 European languages. EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. We use EuroGEST to evaluate 24 multilingual language models from six model families, demonstrating that the strongest stereotypes in all models across all languages are that women are 'beautiful', 'empathetic' and 'neat' and men are 'leaders', 'strong, tough' and 'professional'. We also show that larger models encode gendered stereotypes more strongly and that instruction finetuning does not consistently reduce gendered stereotypes. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.",
    "published": "2025-06-04T11:58:18Z",
    "updated": "2026-02-23T11:19:00Z",
    "link": "http://arxiv.org/pdf/2506.03867v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jacqueline Rowe",
      "Mateusz Klimaszewski",
      "Liane Guillou",
      "Shannon Vallor",
      "Alexandra Birch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.22774v2",
    "title": "Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages",
    "summary": "This paper presents a novel treebank-driven approach to comparing syntactic structures in speech and writing using dependency-parsed corpora. Adopting a fully inductive, bottom-up method, we define syntactic structures as delexicalized dependency (sub)trees and extract them from spoken and written Universal Dependencies (UD) treebanks in two syntactically distinct languages, English and Slovenian. For each corpus, we analyze the size, diversity, and distribution of syntactic inventories, their overlap across modalities, and the structures most characteristic of speech. Results show that, across both languages, spoken corpora contain fewer and less diverse syntactic structures than their written counterparts, with consistent cross-linguistic preferences for certain structural types across modalities. Strikingly, the overlap between spoken and written syntactic inventories is very limited: most structures attested in speech do not occur in writing, pointing to modality-specific preferences in syntactic organization that reflect the distinct demands of real-time interaction and elaborated writing. This contrast is further supported by a keyness analysis of the most frequent speech-specific structures, which highlights patterns associated with interactivity, context-grounding, and economy of expression. We argue that this scalable, language-independent framework offers a useful general method for systematically studying syntactic variation across corpora, laying the groundwork for more comprehensive data-driven theories of grammar in use.",
    "published": "2025-05-28T18:43:26Z",
    "updated": "2026-02-23T10:36:46Z",
    "link": "http://arxiv.org/pdf/2505.22774v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Kaja Dobrovoljc"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.16183v2",
    "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media",
    "summary": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.",
    "published": "2025-12-18T05:08:26Z",
    "updated": "2026-02-23T10:29:46Z",
    "link": "http://arxiv.org/pdf/2512.16183v2.pdf",
    "category": [
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Mengfan Shen",
      "Kangqi Song",
      "Xindi Wang",
      "Wei Jia",
      "Tao Wang",
      "Ziqiang Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.10652v5",
    "title": "ViTextVQA: A Large-Scale Visual Question Answering Dataset and a Novel Multimodal Feature Fusion Method for Vietnamese Text Comprehension in Images",
    "summary": "Visual Question Answering (VQA) is a challenging task that requires the joint understanding of natural language and visual content. While early research primarily focused on recognizing objects and scene context, it often overlooked scene text-an essential source of explicit semantic information. This paper introduces \\textbf{ViTextVQA} (\\textbf{Vi}etnamese \\textbf{Text}-based \\textbf{V}isual \\textbf{Q}uestion \\textbf{A}nswering), the first large-scale Vietnamese dataset specializing in text-based VQA. The dataset contains \\textbf{over 16,000} images and \\textbf{over 50,000} question-answer pairs. To tackle this task efficiently, \\textbf{ViTextBLIP-2} (Vietnamese Text-based Bootstrapped Language-Image Model via Fine-tuning) is proposed, a novel multimodal feature fusion method designed to optimize Vietnamese text-based VQA. Experiments with state-of-the-art models highlight the importance of token ordering in OCR text for answer generation, leading to significant performance improvements. The ViTextVQA dataset is publicly available for research purposes.",
    "published": "2024-04-16T15:28:30Z",
    "updated": "2026-02-23T10:22:38Z",
    "link": "http://arxiv.org/pdf/2404.10652v5.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Quan Van Nguyen",
      "Dan Quang Tran",
      "Huy Quang Pham",
      "Thang Kien-Bao Nguyen",
      "Nghia Hieu Nguyen",
      "Kiet Van Nguyen",
      "Ngan Luu-Thuy Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19643v1",
    "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge",
    "summary": "Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.",
    "published": "2026-02-23T09:41:46Z",
    "updated": "2026-02-23T09:41:46Z",
    "link": "http://arxiv.org/pdf/2602.19643v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Alex Robertson",
      "Huizhi Liang",
      "Mahbub Gani",
      "Rohit Kumar",
      "Srijith Rajamohan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19626v1",
    "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
    "summary": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs.\n  On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
    "published": "2026-02-23T09:14:05Z",
    "updated": "2026-02-23T09:14:05Z",
    "link": "http://arxiv.org/pdf/2602.19626v1.pdf",
    "category": [
      "cs.IT",
      "cs.CL"
    ],
    "authors": [
      "Roberto Tacconelli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19612v1",
    "title": "Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning",
    "summary": "Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.",
    "published": "2026-02-23T08:58:48Z",
    "updated": "2026-02-23T08:58:48Z",
    "link": "http://arxiv.org/pdf/2602.19612v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Borisiuk Anna",
      "Andrey Savchenko",
      "Alexander Panchecko",
      "Elena Tutubalina"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19598v1",
    "title": "Eye-Tracking-while-Reading: A Living Survey of Datasets with Open Library Support",
    "summary": "Eye-tracking-while-reading corpora are a valuable resource for many different disciplines and use cases. Use cases range from studying the cognitive processes underlying reading to machine-learning-based applications, such as gaze-based assessments of reading comprehension. The past decades have seen an increase in the number and size of eye-tracking-while-reading datasets as well as increasing diversity with regard to the stimulus languages covered, the linguistic background of the participants, or accompanying psychometric or demographic data. The spread of data across different disciplines and the lack of data sharing standards across the communities lead to many existing datasets that cannot be easily reused due to a lack of interoperability. In this work, we aim at creating more transparency and clarity with regards to existing datasets and their features across different disciplines by i) presenting an extensive overview of existing datasets, ii) simplifying the sharing of newly created datasets by publishing a living overview online, https://dili-lab.github.io/datasets.html, presenting over 45 features for each dataset, and iii) integrating all publicly available datasets into the Python package pymovements which offers an eye-tracking datasets library. By doing so, we aim to strengthen the FAIR principles in eye-tracking-while-reading research and promote good scientific practices, such as reproducing and replicating studies.",
    "published": "2026-02-23T08:40:50Z",
    "updated": "2026-02-23T08:40:50Z",
    "link": "http://arxiv.org/pdf/2602.19598v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Deborah N. Jakobi",
      "David R. Reich",
      "Paul Prasse",
      "Jana M. Hofmann",
      "Lena S. Bolliger",
      "Lena A. Jäger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19583v1",
    "title": "DEEP: Docker-based Execution and Evaluation Platform",
    "summary": "Comparative evaluation of several systems is a recurrent task in researching. It is a key step before deciding which system to use for our work, or, once our research has been conducted, to demonstrate the potential of the resulting model. Furthermore, it is the main task of competitive, public challenges evaluation. Our proposed software (DEEP) automates both the execution and scoring of machine translation and optical character recognition models. Furthermore, it is easily extensible to other tasks. DEEP is prepared to receive dockerized systems, run them (extracting information at that same time), and assess hypothesis against some references. With this approach, evaluators can achieve a better understanding of the performance of each model. Moreover, the software uses a clustering algorithm based on a statistical analysis of the significance of the results yielded by each model, according to the evaluation metrics. As a result, evaluators are able to identify clusters of performance among the swarm of proposals and have a better understanding of the significance of their differences. Additionally, we offer a visualization web-app to ensure that the results can be adequately understood and interpreted. Finally, we present an exemplary case of use of DEEP.",
    "published": "2026-02-23T08:08:57Z",
    "updated": "2026-02-23T08:08:57Z",
    "link": "http://arxiv.org/pdf/2602.19583v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sergio Gómez González",
      "Miguel Domingo",
      "Francisco Casacuberta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2402.13904v2",
    "title": "Calibrating Large Language Models with Sample Consistency",
    "summary": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
    "published": "2024-02-21T16:15:20Z",
    "updated": "2026-02-23T07:37:56Z",
    "link": "http://arxiv.org/pdf/2402.13904v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Qing Lyu",
      "Kumar Shridhar",
      "Chaitanya Malaviya",
      "Li Zhang",
      "Yanai Elazar",
      "Niket Tandon",
      "Marianna Apidianaki",
      "Mrinmaya Sachan",
      "Chris Callison-Burch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2410.02099v3",
    "title": "A Watermark for Black-Box Language Models",
    "summary": "Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.",
    "published": "2024-10-02T23:39:19Z",
    "updated": "2026-02-23T07:20:10Z",
    "link": "http://arxiv.org/pdf/2410.02099v3.pdf",
    "category": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Dara Bahri",
      "John Wieting"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19549v1",
    "title": "Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework",
    "summary": "Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.",
    "published": "2026-02-23T06:45:19Z",
    "updated": "2026-02-23T06:45:19Z",
    "link": "http://arxiv.org/pdf/2602.19549v1.pdf",
    "category": [
      "cs.CL",
      "cs.CV",
      "cs.IR"
    ],
    "authors": [
      "Yibo Yan",
      "Mingdong Ou",
      "Yi Cao",
      "Xin Zou",
      "Jiahao Huo",
      "Shuliang Liu",
      "James Kwok",
      "Xuming Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19548v1",
    "title": "Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining",
    "summary": "One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.",
    "published": "2026-02-23T06:41:57Z",
    "updated": "2026-02-23T06:41:57Z",
    "link": "http://arxiv.org/pdf/2602.19548v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Jeffrey Li",
      "Josh Gardner",
      "Doug Kang",
      "Fangping Shi",
      "Karanjeet Singh",
      "Chun-Liang Li",
      "Herumb Shandilya",
      "David Hall",
      "Oncel Tuzel",
      "Percy Liang",
      "Ludwig Schmidt",
      "Hadi Pour Ansari",
      "Fartash Faghri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19543v1",
    "title": "Hyper-KGGen: A Skill-Driven Knowledge Extractor for High-Quality Knowledge Hypergraph Generation",
    "summary": "Knowledge hypergraphs surpass traditional binary knowledge graphs by encapsulating complex $n$-ary atomic facts, providing a more comprehensive paradigm for semantic representation. However, constructing high-quality hypergraphs remains challenging due to the \\textit{scenario gap}: generic extractors struggle to generalize across diverse domains with specific jargon, while existing methods often fail to balance structural skeletons with fine-grained details. To bridge this gap, we propose \\textbf{Hyper-KGGen}, a skill-driven framework that reformulates extraction as a dynamic skill-evolving process. First, Hyper-KGGen employs a \\textit{coarse-to-fine} mechanism to systematically decompose documents, ensuring full-dimensional coverage from binary links to complex hyperedges. Crucially, it incorporates an \\textit{adaptive skill acquisition} module that actively distills domain expertise into a Global Skill Library. This is achieved via a stability-based feedback loop, where extraction stability serves as a relative reward signal to induce high-quality skills from unstable traces and missed predictions. Additionally, we present \\textbf{HyperDocRED}, a rigorously annotated benchmark for document-level knowledge hypergraph extraction. Experiments demonstrate that Hyper-KGGen significantly outperforms strong baselines, validating that evolved skills provide substantially richer guidance than static few-shot examples in multi-scenario settings.",
    "published": "2026-02-23T06:32:00Z",
    "updated": "2026-02-23T06:32:00Z",
    "link": "http://arxiv.org/pdf/2602.19543v1.pdf",
    "category": [
      "cs.CL",
      "cs.IR"
    ],
    "authors": [
      "Rizhuo Huang",
      "Yifan Feng",
      "Rundong Xue",
      "Shihui Ying",
      "Jun-Hai Yong",
      "Chuan Shi",
      "Shaoyi Du",
      "Yue Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03688v2",
    "title": "AITutor-EvalKit: Exploring the Capabilities of AI Tutors",
    "summary": "We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotation.",
    "published": "2025-12-03T11:27:50Z",
    "updated": "2026-02-23T06:00:29Z",
    "link": "http://arxiv.org/pdf/2512.03688v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Numaan Naeem",
      "Kaushal Kumar Maurya",
      "Kseniia Petukhova",
      "Ekaterina Kochmar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19526v1",
    "title": "How to Train Your Deep Research Agent? Prompt, Reward, and Policy Optimization in Search-R1",
    "summary": "Deep Research agents tackle knowledge-intensive tasks through multi-round retrieval and decision-oriented generation. While reinforcement learning (RL) has been shown to improve performance in this paradigm, its contributions remain underexplored. To fully understand the role of RL, we conduct a systematic study along three decoupled dimensions: prompt template, reward function, and policy optimization. Our study reveals that: 1) the Fast Thinking template yields greater stability and better performance than the Slow Thinking template used in prior work; 2) the F1-based reward underperforms the EM due to training collapse driven by answer avoidance; this can be mitigated by incorporating action-level penalties, ultimately surpassing EM; 3) REINFORCE outperforms PPO while requiring fewer search actions, whereas GRPO shows the poorest stability among policy optimization methods. Building on these insights, we then introduce Search-R1++, a strong baseline that improves the performance of Search-R1 from 0.403 to 0.442 (Qwen2.5-7B) and 0.289 to 0.331 (Qwen2.5-3B). We hope that our findings can pave the way for more principled and reliable RL training strategies in Deep Research systems.",
    "published": "2026-02-23T05:33:17Z",
    "updated": "2026-02-23T05:33:17Z",
    "link": "http://arxiv.org/pdf/2602.19526v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Yinuo Xu",
      "Shuo Lu",
      "Jianjie Cheng",
      "Meng Wang",
      "Qianlong Xie",
      "Xingxing Wang",
      "Ran He",
      "Jian Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20161v1",
    "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
    "published": "2026-02-23T18:59:58Z",
    "updated": "2026-02-23T18:59:58Z",
    "link": "http://arxiv.org/pdf/2602.20161v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Abdelrahman Shaker",
      "Ahmed Heakl",
      "Jaseel Muhammad",
      "Ritesh Thawkar",
      "Omkar Thawakar",
      "Senmao Li",
      "Hisham Cholakkal",
      "Ian Reid",
      "Eric P. Xing",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.17665v2",
    "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
    "summary": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.",
    "published": "2026-02-19T18:59:54Z",
    "updated": "2026-02-23T18:59:54Z",
    "link": "http://arxiv.org/pdf/2602.17665v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Akashah Shabbir",
      "Muhammad Umer Sheikh",
      "Muhammad Akhtar Munir",
      "Hiyam Debary",
      "Mustansar Fiaz",
      "Muhammad Zaigham Zaheer",
      "Paolo Fraccaro",
      "Fahad Shahbaz Khan",
      "Muhammad Haris Khan",
      "Xiao Xiang Zhu",
      "Salman Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20160v1",
    "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
    "published": "2026-02-23T18:59:45Z",
    "updated": "2026-02-23T18:59:45Z",
    "link": "http://arxiv.org/pdf/2602.20160v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chen Wang",
      "Hao Tan",
      "Wang Yifan",
      "Zhiqin Chen",
      "Yuheng Liu",
      "Kalyan Sunkavalli",
      "Sai Bi",
      "Lingjie Liu",
      "Yiwei Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20157v1",
    "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
    "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.",
    "published": "2026-02-23T18:59:30Z",
    "updated": "2026-02-23T18:59:30Z",
    "link": "http://arxiv.org/pdf/2602.20157v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhongxiao Cong",
      "Qitao Zhao",
      "Minsik Jeon",
      "Shubham Tulsiani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20150v1",
    "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
    "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.",
    "published": "2026-02-23T18:58:24Z",
    "updated": "2026-02-23T18:58:24Z",
    "link": "http://arxiv.org/pdf/2602.20150v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Wei-Cheng Huang",
      "Jiaheng Han",
      "Xiaohan Ye",
      "Zherong Pan",
      "Kris Hauser"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20137v1",
    "title": "Do Large Language Models Understand Data Visualization Rules?",
    "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.",
    "published": "2026-02-23T18:47:51Z",
    "updated": "2026-02-23T18:47:51Z",
    "link": "http://arxiv.org/pdf/2602.20137v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Martin Sinnona",
      "Valentin Bonas",
      "Emmanuel Iarussi",
      "Viviana Siless"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.18406v2",
    "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges",
    "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training$\\unicode{x2013}$for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to learn equivariant operators in a latent space, from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.",
    "published": "2026-02-20T18:14:05Z",
    "updated": "2026-02-23T18:44:49Z",
    "link": "http://arxiv.org/pdf/2602.18406v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Minh Dinh",
      "Stéphane Deny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17258v2",
    "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding",
    "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.",
    "published": "2026-01-24T02:17:07Z",
    "updated": "2026-02-23T18:12:49Z",
    "link": "http://arxiv.org/pdf/2601.17258v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "João Pereira",
      "Vasco Lopes",
      "João Neves",
      "David Semedo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.17807v2",
    "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model",
    "summary": "Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x-10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/",
    "published": "2026-02-19T20:14:14Z",
    "updated": "2026-02-23T18:10:18Z",
    "link": "http://arxiv.org/pdf/2602.17807v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Narges Norouzi",
      "Idil Esen Zulfikar",
      "Niccolò Cavagnero",
      "Tommie Kerssies",
      "Bastian Leibe",
      "Gijs Dubbelman",
      "Daan de Geus"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.20648v2",
    "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
    "summary": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 38.90 AP_3D, surpassing the previous best by +13.98 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
    "published": "2025-11-25T18:59:45Z",
    "updated": "2026-02-23T17:59:38Z",
    "link": "http://arxiv.org/pdf/2511.20648v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yunze Man",
      "Shihao Wang",
      "Guowen Zhang",
      "Johan Bjorck",
      "Zhiqi Li",
      "Liang-Yan Gui",
      "Jim Fan",
      "Jan Kautz",
      "Yu-Xiong Wang",
      "Zhiding Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20084v1",
    "title": "Do Large Language Models Understand Data Visualization Principles?",
    "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.",
    "published": "2026-02-23T17:51:06Z",
    "updated": "2026-02-23T17:51:06Z",
    "link": "http://arxiv.org/pdf/2602.20084v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Martin Sinnona",
      "Valentin Bonas",
      "Viviana Siless",
      "Emmanuel Iarussi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20079v1",
    "title": "SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis",
    "summary": "We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.",
    "published": "2026-02-23T17:45:21Z",
    "updated": "2026-02-23T17:45:21Z",
    "link": "http://arxiv.org/pdf/2602.20079v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xinya Chen",
      "Christopher Wewer",
      "Jiahao Xie",
      "Xinting Hu",
      "Jan Eric Lenssen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.07853v2",
    "title": "Hier-COS: Making Deep Features Hierarchy-aware via Composition of Orthogonal Subspaces",
    "summary": "Traditional classifiers treat all labels as mutually independent, thereby considering all negative classes to be equally incorrect. This approach fails severely in many real-world scenarios, where a known semantic hierarchy defines a partial order of preferences over negative classes. While hierarchy-aware feature representations have shown promise in mitigating this problem, their performance is typically assessed using metrics like MS and AHD. In this paper, we highlight important shortcomings in existing hierarchical evaluation metrics, demonstrating that they are often incapable of measuring true hierarchical performance. Our analysis reveals that existing methods learn sub-optimal hierarchical representations, despite competitive MS and AHD scores. To counter these issues, we introduce Hier-COS, a novel framework for unified hierarchy-aware fine-grained and hierarchical multi-level classification. We show that Hier-COS is theoretically guaranteed to be consistent with the given hierarchy tree. Furthermore, our framework implicitly adapts the learning capacity for different classes based on their position within the hierarchy tree-a vital property absent in existing methods. Finally, to address the limitations of evaluation metrics, we propose HOPS, a ranking-based metric that demonstrably overcomes the deficiencies of current evaluation standards. We benchmark Hier-COS on four challenging datasets, including the deep and imbalanced tieredImageNet-H and iNaturalist-19. Through extensive experiments, we demonstrate that Hier-COS achieves SOTA across all hierarchical metrics for every dataset, while simultaneously beating the top-1 accuracy in all but one case. Lastly, we show that Hier-COS can effectively learn to transform the frozen features extracted from a pretrained backbone (ViT) to be hierarchy-aware, yielding substantial benefits for hierarchical classification performance.",
    "published": "2025-03-10T20:59:41Z",
    "updated": "2026-02-23T17:38:56Z",
    "link": "http://arxiv.org/pdf/2503.07853v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Depanshu Sani",
      "Saket Anand"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20068v1",
    "title": "The Invisible Gorilla Effect in Out-of-distribution Detection",
    "summary": "Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.",
    "published": "2026-02-23T17:24:18Z",
    "updated": "2026-02-23T17:24:18Z",
    "link": "http://arxiv.org/pdf/2602.20068v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Harry Anthony",
      "Ziyun Liang",
      "Hermione Warr",
      "Konstantinos Kamnitsas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20060v1",
    "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
    "summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.",
    "published": "2026-02-23T17:17:26Z",
    "updated": "2026-02-23T17:17:26Z",
    "link": "http://arxiv.org/pdf/2602.20060v1.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Junli Wang",
      "Xueyi Liu",
      "Yinan Zheng",
      "Zebing Xing",
      "Pengfei Li",
      "Guang Li",
      "Kun Ma",
      "Guang Chen",
      "Hangjun Ye",
      "Zhongpu Xia",
      "Long Chen",
      "Qichao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12713v2",
    "title": "Real-Time Sign Language Gestures to Speech Transcription using Deep Learning",
    "summary": "Communication barriers pose significant challenges for individuals with hearing and speech impairments, often limiting their ability to effectively interact in everyday environments. This project introduces a real-time assistive technology solution that leverages advanced deep learning techniques to translate sign language gestures into textual and audible speech. By employing convolution neural networks (CNN) trained on the Sign Language MNIST dataset, the system accurately classifies hand gestures captured live via webcam. Detected gestures are instantaneously translated into their corresponding meanings and transcribed into spoken language using text-to-speech synthesis, thus facilitating seamless communication. Comprehensive experiments demonstrate high model accuracy and robust real-time performance with some latency, highlighting the system's practical applicability as an accessible, reliable, and user-friendly tool for enhancing the autonomy and integration of sign language users in diverse social settings.",
    "published": "2025-08-18T08:25:18Z",
    "updated": "2026-02-23T17:15:46Z",
    "link": "http://arxiv.org/pdf/2508.12713v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Brandone Fonya",
      "Clarence Worrell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.23479v3",
    "title": "MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal Understanding",
    "summary": "Vision-language alignment in multi-modal large language models (MLLMs) relies on supervised fine-tuning (SFT) or reinforcement learning (RL). To align multi-modal large language models (MLLMs) in the post-training stage, supervised fine-tuning (SFT) is a stable choice but requires human annotations and lacks task generalizations, while Reinforcement Learning (RL) searches for better answers from reward signals but suffers from computational overhead and instability. To achieve balance among scalability, efficiency, and alignment generalizations, we propose MergeMix, a unified paradigm that bridges SFT and RL with an efficient Token Merge based Mixup augmentation. As for the Mixup policy, we generate contextual aligned mixed images with the corresponding labels according to the merged attention maps with cluster regions. Then, we enhance the preference-driven paradigm for MLLMs by building preference pairs with raw images and MergeMix-generated ones and optimizing the soft preference margin with the mixed SimPO loss. Extensive experiments demonstrate that MergeMix not only achieves dominant classification accuracy as an augmentation method but also improves generalization abilities and alignment of MLLMs, providing a new learning paradigm for preference alignment with training efficiency and stability.",
    "published": "2025-10-27T16:12:40Z",
    "updated": "2026-02-23T17:06:33Z",
    "link": "http://arxiv.org/pdf/2510.23479v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xin Jin",
      "Siyuan Li",
      "Siyong Jian",
      "Kai Yu",
      "Huan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20053v1",
    "title": "Decoupling Defense Strategies for Robust Image Watermarking",
    "summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\\%, 33\\% and 46\\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.",
    "published": "2026-02-23T17:02:55Z",
    "updated": "2026-02-23T17:02:55Z",
    "link": "http://arxiv.org/pdf/2602.20053v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiahui Chen",
      "Zehang Deng",
      "Zeyu Zhang",
      "Chaoyang Li",
      "Lianchen Jia",
      "Lifeng Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20046v1",
    "title": "Closing the gap in multimodal medical representation alignment",
    "summary": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.",
    "published": "2026-02-23T16:57:39Z",
    "updated": "2026-02-23T16:57:39Z",
    "link": "http://arxiv.org/pdf/2602.20046v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Eleonora Grassucci",
      "Giordano Cicchetti",
      "Danilo Comminiello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20041v1",
    "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
    "summary": "Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) signals were recorded with a 16-channel OpenBCI cap and aligned with motor actions at Delta = 0 ms and future prediction horizons (Delta > 0 ms). After preprocessing, several deep learning models were benchmarked, including convolutional neural networks, recurrent neural networks, and Transformer architectures. ShallowConvNet achieved the highest performance for both action prediction and intent prediction. By combining real-world robotic control with multi-horizon EEG intention decoding, this study introduces a reproducible benchmark and reveals key design insights for predictive deep learning-based BCI systems.",
    "published": "2026-02-23T16:50:21Z",
    "updated": "2026-02-23T16:50:21Z",
    "link": "http://arxiv.org/pdf/2602.20041v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Ghadah Alosaimi",
      "Maha Alsayyari",
      "Yixin Sun",
      "Stamos Katsigiannis",
      "Amir Atapour-Abarghouei",
      "Toby P. Breckon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12846v2",
    "title": "Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration",
    "summary": "Visual-inertial fusion is crucial for a large amount of intelligent and autonomous applications, such as robot navigation and augmented reality. To bootstrap and achieve optimal state estimation, the spatial-temporal displacements between IMU and cameras must be calibrated in advance. Most existing calibration methods adopt continuous-time state representation, more specifically the B-spline. Despite these methods achieve precise spatial-temporal calibration, they suffer from high computational cost caused by continuous-time state representation. To this end, we propose a novel and extremely efficient calibration method that unleashes the power of discrete-time state representation. Moreover, the weakness of discrete-time state representation in temporal calibration is tackled in this paper. With the increasing production of drones, cellphones and other visual-inertial platforms, if one million devices need calibration around the world, saving one minute for the calibration of each device means saving 2083 work days in total. To benefit both the research and industry communities, the open-source implementation is released at https://github.com/JunlinSong/DT-VI-Calib.",
    "published": "2025-09-16T09:05:46Z",
    "updated": "2026-02-23T16:34:59Z",
    "link": "http://arxiv.org/pdf/2509.12846v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "Junlin Song",
      "Antoine Richard",
      "Miguel Olivares-Mendez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.15656v3",
    "title": "A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models",
    "summary": "The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.",
    "published": "2026-02-17T15:30:31Z",
    "updated": "2026-02-23T16:21:27Z",
    "link": "http://arxiv.org/pdf/2602.15656v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mustafa Yurdakul",
      "Zeynep Sena Bastug",
      "Ali Emre Gok",
      "Sakir Taşdemir"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20008v1",
    "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
    "summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets.\n  While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input.\n  The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution.\n  This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames.\n  To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures.\n  Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$).\n  This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.",
    "published": "2026-02-23T16:15:38Z",
    "updated": "2026-02-23T16:15:38Z",
    "link": "http://arxiv.org/pdf/2602.20008v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Louis Fabrice Tshimanga",
      "Andrea Zanola",
      "Federico Del Pup",
      "Manfredo Atzori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25723v3",
    "title": "SAGE: Spatial-visual Adaptive Graph Exploration for Efficient Visual Place Recognition",
    "summary": "Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. The code and model are available at https://github.com/chenshunpeng/SAGE.",
    "published": "2025-09-30T03:34:40Z",
    "updated": "2026-02-23T16:10:03Z",
    "link": "http://arxiv.org/pdf/2509.25723v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shunpeng Chen",
      "Changwei Wang",
      "Rongtao Xu",
      "Xingtian Pei",
      "Yukun Song",
      "Jinzhou Lin",
      "Wenhao Xu",
      "Jingyi Zhang",
      "Li Guo",
      "Shibiao Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.10135v2",
    "title": "$R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via Geometry and Appearance Refinement",
    "summary": "Mesh reconstruction from Neural Radiance Fields (NeRF) is widely used in 3D reconstruction and has been applied across numerous domains. However, existing methods typically rely solely on the given training set images, which restricts supervision to limited observations and makes it difficult to fully constrain geometry and appearance. Moreover, the contribution of each viewpoint for training is not uniform and changes dynamically during the optimization process, which can result in suboptimal guidance for both geometric refinement and rendering quality. To address these limitations, we propose $R^2$-Mesh, a reinforcement learning framework that combines NeRF-rendered pseudo-supervision with online viewpoint selection. Our key insight is to exploit NeRF's rendering ability to synthesize additional high-quality images, enriching training with diverse viewpoint information. To ensure that supervision focuses on the most beneficial perspectives, we introduce a UCB-based strategy with a geometry-aware reward, which dynamically balances exploration and exploitation to identify informative viewpoints throughout training. Within this framework, we jointly optimize SDF geometry and view-dependent appearance under differentiable rendering, while periodically refining meshes to capture fine geometric details. Experiments demonstrate that our method achieves competitive results in both geometric accuracy and rendering quality.",
    "published": "2024-08-19T16:33:17Z",
    "updated": "2026-02-23T16:02:05Z",
    "link": "http://arxiv.org/pdf/2408.10135v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haoyang Wang",
      "Liming Liu",
      "Xinggong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19994v1",
    "title": "RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather",
    "summary": "Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.",
    "published": "2026-02-23T16:01:31Z",
    "updated": "2026-02-23T16:01:31Z",
    "link": "http://arxiv.org/pdf/2602.19994v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Christof Leitgeb",
      "Thomas Puchleitner",
      "Max Peter Ronecker",
      "Daniel Watzenig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19974v1",
    "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection",
    "summary": "Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.",
    "published": "2026-02-23T15:39:53Z",
    "updated": "2026-02-23T15:39:53Z",
    "link": "http://arxiv.org/pdf/2602.19974v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianyu Wang",
      "Zhiyuan Ma",
      "Qian Wang",
      "Xinyi Zhang",
      "Xinwei Long",
      "Bowen Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.12047v3",
    "title": "PSGait: Gait Recognition using Parsing Skeleton",
    "summary": "Gait recognition has emerged as a robust biometric modality due to its non-intrusive nature. Conventional gait recognition methods mainly rely on silhouettes or skeletons. While effective in controlled laboratory settings, their limited information entropy restricts generalization to real-world scenarios. To overcome this, we propose a novel representation called \\textbf{Parsing Skeleton}, which uses a skeleton-guided human parsing method to capture fine-grained body dynamics with much higher information entropy. To effectively explore the capability of the Parsing Skeleton, we also introduce \\textbf{PSGait}, a framework that fuses Parsing Skeleton with silhouettes to enhance individual differentiation. Comprehensive benchmarks demonstrate that PSGait outperforms state-of-the-art multimodal methods while significantly reducing computational resources. As a plug-and-play method, it achieves an improvement of up to 15.7\\% in the accuracy of Rank-1 in various models. These results validate the Parsing Skeleton as a \\textbf{lightweight}, \\textbf{effective}, and highly \\textbf{generalizable} representation for gait recognition in the wild. Code is available at https://github.com/realHarryX/PSGait.",
    "published": "2025-03-15T08:38:47Z",
    "updated": "2026-02-23T15:22:32Z",
    "link": "http://arxiv.org/pdf/2503.12047v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hangrui Xu",
      "Zhengxian Wu",
      "Chuanrui Zhang",
      "Zhuohong Chen",
      "Zhifang Liu",
      "Peng Jiao",
      "Haoqian Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19944v1",
    "title": "Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation",
    "summary": "Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \\textbf{D}iscover-\\textbf{S}egment-\\textbf{S}elect (\\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.",
    "published": "2026-02-23T15:15:37Z",
    "updated": "2026-02-23T15:15:37Z",
    "link": "http://arxiv.org/pdf/2602.19944v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yilong Yang",
      "Jianxin Tian",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.09609v2",
    "title": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing",
    "summary": "Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.",
    "published": "2026-02-10T10:01:16Z",
    "updated": "2026-02-23T15:14:47Z",
    "link": "http://arxiv.org/pdf/2602.09609v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jialun Liu",
      "Tian Li",
      "Xiao Cao",
      "Yukuo Ma",
      "Gonghu Shang",
      "Haibin Huang",
      "Chi Zhang",
      "Xiangzhen Chang",
      "Zhiyong Huang",
      "Jiakui Hu",
      "Zuoxin Li",
      "Yuanzhi Liang",
      "Cong Liu",
      "Junqi Liu",
      "Robby T. Tan",
      "Haitong Tang",
      "Qizhen Weng",
      "Yifan Xu",
      "Liying Yang",
      "Xiaoyan Yang",
      "Peng Yu",
      "Shiwen Zhang",
      "Xuelong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19937v1",
    "title": "Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation",
    "summary": "Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space-including unobserved regions in camera space-significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling (PIPS) strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The PIPS strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. Our method outperforms the state-of-the-art on three pose estimation datasets. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.",
    "published": "2026-02-23T15:10:00Z",
    "updated": "2026-02-23T15:10:00Z",
    "link": "http://arxiv.org/pdf/2602.19937v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yifei Shi",
      "Boyan Wan",
      "Xin Xu",
      "Kai Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19931v1",
    "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
    "summary": "Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.",
    "published": "2026-02-23T15:06:52Z",
    "updated": "2026-02-23T15:06:52Z",
    "link": "http://arxiv.org/pdf/2602.19931v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Pin-Han Huang",
      "Shang-Tse Chen",
      "Hsuan-Tien Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19916v1",
    "title": "Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting",
    "summary": "Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Project page at: https://xiaoxinyyx.github.io/augs.",
    "published": "2026-02-23T14:55:31Z",
    "updated": "2026-02-23T14:55:31Z",
    "link": "http://arxiv.org/pdf/2602.19916v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Yixin Yang",
      "Bojian Wu",
      "Yang Zhou",
      "Hui Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19910v1",
    "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
    "summary": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.",
    "published": "2026-02-23T14:51:09Z",
    "updated": "2026-02-23T14:51:09Z",
    "link": "http://arxiv.org/pdf/2602.19910v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wei He",
      "Xianghan Meng",
      "Zhiyuan Huang",
      "Xianbiao Qi",
      "Rong Xiao",
      "Chun-Guang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19907v1",
    "title": "Gradient based Severity Labeling for Biomarker Classification in OCT",
    "summary": "In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.",
    "published": "2026-02-23T14:46:08Z",
    "updated": "2026-02-23T14:46:08Z",
    "link": "http://arxiv.org/pdf/2602.19907v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib",
      "Stephanie Trejo Corona",
      "Charles Wykoff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.12746v2",
    "title": "Modelling and analysis of the 8 filters from the \"master key filters hypothesis\" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory",
    "summary": "This paper presents the results of analysing and modelling a set of 8 ``master key filters'', which have been extracted by applying a clustering approach to the receptive fields learned in depthwise-separable deep networks based on the ConvNeXt architecture.\n  For this purpose, we first compute spatial spread measures in terms of weighted mean values and weighted variances of the absolute values of the learned filters, which support the working hypotheses that: (i) the learned filters can be modelled by separable filtering operations over the spatial domain, and that (ii) the spatial offsets of the those learned filters that are non-centered are rather close to half a grid unit. Then, we model the clustered ``master key filters'' in terms of difference operators applied to a spatial smoothing operation in terms of the discrete analogue of the Gaussian kernel, and demonstrate that the resulting idealized models of the receptive fields show good qualitative similarity to the learned filters.\n  This modelling is performed in two different ways: (i) using possibly different values of the scale parameters in the coordinate directions for each filter, and (ii) using the same value of the scale parameter in both coordinate directions. Then, we perform the actual model fitting by either (i) requiring spatial spread measures in terms of spatial variances of the absolute values of the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or $l_2$-norms between the idealized receptive field models and the learned filters.\n  Complementary experimental results then demonstrate the idealized models of receptive fields have good predictive properties for replacing the learned filters by idealized filters in depthwise-separable deep networks, thus showing that the learned filters in depthwise-separable deep networks can be well approximated by discrete scale-space filters.",
    "published": "2025-09-16T07:04:45Z",
    "updated": "2026-02-23T14:44:35Z",
    "link": "http://arxiv.org/pdf/2509.12746v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tony Lindeberg",
      "Zahra Babaiee",
      "Peyman M. Kiasari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19900v1",
    "title": "ExpPortrait: Expressive Portrait Generation via Personalized Representation",
    "summary": "While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.",
    "published": "2026-02-23T14:41:35Z",
    "updated": "2026-02-23T14:41:35Z",
    "link": "http://arxiv.org/pdf/2602.19900v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Junyi Wang",
      "Yudong Guo",
      "Boyang Guo",
      "Shengming Yang",
      "Juyong Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19896v1",
    "title": "Monocular Mesh Recovery and Body Measurement of Female Saanen Goats",
    "summary": "The lactation performance of Saanen dairy goats, renowned for their high milk yield, is intrinsically linked to their body size, making accurate 3D body measurement essential for assessing milk production potential, yet existing reconstruction methods lack goat-specific authentic 3D data. To address this limitation, we establish the FemaleSaanenGoat dataset containing synchronized eight-view RGBD videos of 55 female Saanen goats (6-18 months). Using multi-view DynamicFusion, we fuse noisy, non-rigid point cloud sequences into high-fidelity 3D scans, overcoming challenges from irregular surfaces and rapid movement. Based on these scans, we develop SaanenGoat, a parametric 3D shape model specifically designed for female Saanen goats. This model features a refined template with 41 skeletal joints and enhanced udder representation, registered with our scan data. A comprehensive shape space constructed from 48 goats enables precise representation of diverse individual variations. With the help of SaanenGoat model, we get high-precision 3D reconstruction from single-view RGBD input, and achieve automated measurement of six critical body dimensions: body length, height, chest width, chest girth, hip width, and hip height. Experimental results demonstrate the superior accuracy of our method in both 3D reconstruction and body measurement, presenting a novel paradigm for large-scale 3D vision applications in precision livestock farming.",
    "published": "2026-02-23T14:37:07Z",
    "updated": "2026-02-23T14:37:07Z",
    "link": "http://arxiv.org/pdf/2602.19896v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bo Jin",
      "Shichao Zhao",
      "Jin Lyu",
      "Bin Zhang",
      "Tao Yu",
      "Liang An",
      "Yebin Liu",
      "Meili Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19891v1",
    "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
    "summary": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.",
    "published": "2026-02-23T14:33:24Z",
    "updated": "2026-02-23T14:33:24Z",
    "link": "http://arxiv.org/pdf/2602.19891v1.pdf",
    "category": [
      "eess.IV",
      "cs.CV"
    ],
    "authors": [
      "Wen-Liang Lin",
      "Yun-Chien Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19874v1",
    "title": "BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations",
    "summary": "The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\\textbf{Big Ma}$ca$\\textbf{Q}$ue 3D Motion and Animation Dataset ($\\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .",
    "published": "2026-02-23T14:21:15Z",
    "updated": "2026-02-23T14:21:15Z",
    "link": "http://arxiv.org/pdf/2602.19874v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Lucas Martini",
      "Alexander Lappe",
      "Anna Bognár",
      "Rufin Vogels",
      "Martin A. Giese"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.06450v2",
    "title": "Countering Multi-modal Representation Collapse through Rank-targeted Fusion",
    "summary": "Multi-modal fusion methods often suffer from two types of representation collapse: feature collapse where individual dimensions lose their discriminative power (as measured by eigenspectra), and modality collapse where one dominant modality overwhelms the other. Applications like human action anticipation that require fusing multifarious sensor data are hindered by both feature and modality collapse. However, existing methods attempt to counter feature collapse and modality collapse separately. This is because there is no unifying framework that efficiently addresses feature and modality collapse in conjunction. In this paper, we posit the utility of effective rank as an informative measure that can be utilized to quantify and counter both the representation collapses. We propose \\textit{Rank-enhancing Token Fuser}, a theoretically grounded fusion framework that selectively blends less informative features from one modality with complementary features from another modality. We show that our method increases the effective rank of the fused representation. To address modality collapse, we evaluate modality combinations that mutually increase each others' effective rank. We show that depth maintains representational balance when fused with RGB, avoiding modality collapse. We validate our method on action anticipation, where we present \\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on NTURGBD, UTKinect, and DARai demonstrate that our approach significantly outperforms prior state-of-the-art methods by up to 3.74\\%. Our code is available at: \\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.",
    "published": "2025-11-09T16:34:19Z",
    "updated": "2026-02-23T14:20:57Z",
    "link": "http://arxiv.org/pdf/2511.06450v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Seulgi Kim",
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19870v1",
    "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
    "summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.",
    "published": "2026-02-23T14:15:37Z",
    "updated": "2026-02-23T14:15:37Z",
    "link": "http://arxiv.org/pdf/2602.19870v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qiankun Ma",
      "Ziyao Zhang",
      "Haofei Wang",
      "Jie Chen",
      "Zhen Song",
      "Hairong Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19863v1",
    "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation",
    "summary": "Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Code: Coming soon.",
    "published": "2026-02-23T14:09:01Z",
    "updated": "2026-02-23T14:09:01Z",
    "link": "http://arxiv.org/pdf/2602.19863v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Filip Wolf",
      "Blaž Rolih",
      "Luka Čehovin Zajc"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06685v4",
    "title": "MOGS: Monocular Object-guided Gaussian Splatting in Large Scenes",
    "summary": "Recent advances in 3D Gaussian Splatting (3DGS) deliver striking photorealism, and extending it to large scenes opens new opportunities for semantic reasoning and prediction in applications such as autonomous driving. Today's state-of-the-art systems for large scenes primarily originate from LiDAR-based pipelines that utilize long-range depth sensing. However, they require costly high-channel sensors whose dense point clouds strain memory and computation, limiting scalability, fleet deployment, and optimization speed. We present MOGS, a monocular 3DGS framework that replaces active LiDAR depth with object-anchored, metrized dense depth derived from sparse visual-inertial (VI) structure-from-motion (SfM) cues. Our key idea is to exploit image semantics to hypothesize per-object shape priors, anchor them with sparse but metrically reliable SfM points, and propagate the resulting metric constraints across each object to produce dense depth. To address two key challenges, i.e., insufficient SfM coverage within objects and cross-object geometric inconsistency, MOGS introduces (1) a multi-scale shape consensus module that adaptively merges small segments into coarse objects best supported by SfM and fits them with parametric shape models, and (2) a cross-object depth refinement module that optimizes per-pixel depth under a combinatorial objective combining geometric consistency, prior anchoring, and edge-aware smoothness. Experiments on public datasets show that, with a low-cost VI sensor suite, MOGS reduces training time by up to 30.4% and memory consumption by 19.8%, while achieving high-quality rendering competitive with costly LiDAR-based approaches in large scenes.",
    "published": "2025-09-08T13:41:10Z",
    "updated": "2026-02-23T14:07:37Z",
    "link": "http://arxiv.org/pdf/2509.06685v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shengkai Zhang",
      "Yuhe Liu",
      "Jianhua He",
      "Xuedou Xiao",
      "Mozi Chen",
      "Kezhong Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19857v1",
    "title": "Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions",
    "summary": "Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.",
    "published": "2026-02-23T13:56:49Z",
    "updated": "2026-02-23T13:56:49Z",
    "link": "http://arxiv.org/pdf/2602.19857v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rodrigo Mota",
      "Kelvin Cunha",
      "Emanoel dos Santos",
      "Fábio Papais",
      "Francisco Filho",
      "Thales Bezerra",
      "Erico Medeiros",
      "Paulo Borba",
      "Tsang Ing Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19848v1",
    "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation",
    "summary": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.",
    "published": "2026-02-23T13:52:28Z",
    "updated": "2026-02-23T13:52:28Z",
    "link": "http://arxiv.org/pdf/2602.19848v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Francisco Filho",
      "Kelvin Cunha",
      "Fábio Papais",
      "Emanoel dos Santos",
      "Rodrigo Mota",
      "Thales Bezerra",
      "Erico Medeiros",
      "Paulo Borba",
      "Tsang Ing Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.14706v2",
    "title": "LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval",
    "summary": "In this paper, we present LookBench (We use the term \"look\" to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings. LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases. Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs. Grounded in our fine-grained attribute taxonomy, LookBench covers single-item and outfit-level retrieval across. Our experiments reveal that LookBench poses a significant challenge on strong baselines, with many models achieving below $60\\%$ Recall@1. Our proprietary model achieves the best performance on LookBench, and we release an open-source counterpart that ranks second, with both models attaining state-of-the-art results on legacy Fashion200K evaluations. LookBench is designed to be updated semi-annually with new test samples and progressively harder task variants, providing a durable measure of progress. We publicly release our leaderboard, dataset, evaluation code, and trained models.",
    "published": "2026-01-21T06:50:23Z",
    "updated": "2026-02-23T13:42:55Z",
    "link": "http://arxiv.org/pdf/2601.14706v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Gensmo. ai",
      "Chao Gao",
      "Siqiao Xue",
      "Yimin Peng",
      "Jiwen Fu",
      "Tingyi Gu",
      "Shanshan Li",
      "Fan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19832v1",
    "title": "M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting",
    "summary": "The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling between visual and meteorological modalities. To bridge this gap, this paper proposes M3S-Net, a novel multimodal feature fusion network based on multi-scale data for ultra-short-term PV power forecasting. First, a multi-scale partial channel selection network leverages partial convolutions to explicitly isolate the boundary features of optically thin clouds, effectively transcending the precision limitations of coarse-grained binary masking. Second, a multi-scale sequence to image analysis network employs Fast Fourier Transform (FFT)-based time-frequency representation to disentangle the complex periodicity of meteorological data across varying time horizons. Crucially, the model incorporates a cross-modal Mamba interaction module featuring a novel dynamic C-matrix swapping mechanism. By exchanging state-space parameters between visual and temporal streams, this design conditions the state evolution of one modality on the context of the other, enabling deep structural coupling with linear computational complexity, thus overcoming the limitations of shallow concatenation. Experimental validation on the newly constructed fine-grained PV power dataset demonstrates that M3S-Net achieves a mean absolute error reduction of 6.2% in 10-minute forecasts compared to state-of-the-art baselines. The dataset and source code will be available at https://github.com/she1110/FGPD.",
    "published": "2026-02-23T13:30:59Z",
    "updated": "2026-02-23T13:30:59Z",
    "link": "http://arxiv.org/pdf/2602.19832v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Penghui Niu",
      "Taotao Cai",
      "Suqi Zhang",
      "Junhua Gu",
      "Ping Zhang",
      "Qiqi Liu",
      "Jianxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19828v1",
    "title": "TextShield-R1: Reinforced Reasoning for Tampered Text Detection",
    "summary": "The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.",
    "published": "2026-02-23T13:26:18Z",
    "updated": "2026-02-23T13:26:18Z",
    "link": "http://arxiv.org/pdf/2602.19828v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chenfan Qu",
      "Yiwu Zhong",
      "Jian Liu",
      "Xuekang Zhu",
      "Bohan Yu",
      "Lianwen Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19823v1",
    "title": "Open-vocabulary 3D scene perception in industrial environments",
    "summary": "Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM \"IndustrialCLIP\" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.",
    "published": "2026-02-23T13:22:51Z",
    "updated": "2026-02-23T13:22:51Z",
    "link": "http://arxiv.org/pdf/2602.19823v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Keno Moenck",
      "Adrian Philip Florea",
      "Julian Koch",
      "Thorsten Schüppstuhl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17779v4",
    "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding",
    "summary": "Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.",
    "published": "2025-05-23T11:48:48Z",
    "updated": "2026-02-23T12:51:16Z",
    "link": "http://arxiv.org/pdf/2505.17779v4.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Anjie Le",
      "Henan Liu",
      "Yue Wang",
      "Zhenyu Liu",
      "Rongkun Zhu",
      "Taohan Weng",
      "Jinze Yu",
      "Boyang Wang",
      "Yalun Wu",
      "Kaiwen Yan",
      "Quanlin Sun",
      "Meirui Jiang",
      "Jialun Pei",
      "Siya Liu",
      "Haoyun Zheng",
      "Zhoujun Li",
      "Alison Noble",
      "Jacques Souquet",
      "Xiaoqing Guo",
      "Manxi Lin",
      "Hongcheng Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.15886v4",
    "title": "RangeSAM: On the Potential of Visual Foundation Models for Range-View represented LiDAR segmentation",
    "summary": "Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.",
    "published": "2025-09-19T11:33:10Z",
    "updated": "2026-02-23T12:38:59Z",
    "link": "http://arxiv.org/pdf/2509.15886v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Paul Julius Kühn",
      "Duc Anh Nguyen",
      "Arjan Kuijper",
      "Saptarshi Neil Sinha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.01835v4",
    "title": "Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views",
    "summary": "Hyperspectral reconstruction (HSR) from RGB images is a highly promising direction for accurate color reproduction and material color measurement. While most existing approaches rely on a single RGB image - thereby limiting reconstruction accuracy - the majority of modern smartphones are equipped with two or more cameras. In this work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR) framework that leverages a triple-camera smartphone system, where two lenses are equipped with carefully selected spectral filters. Our easy-to-implement configuration, based on theoretical and empirical analysis, allows to obtain more complete and diverse spectral data than traditional single-chamber setups. To support this new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising aligned images from three smartphone cameras and a hyperspectral reference camera across diverse scenes. We further introduce a lightweight alignment module for MI-HSR that effectively fuses multi-view inputs while mitigating parallax- and occlusion-induced artifacts. Proposed module demonstrate consistent quality improvements for modern HSR methods. In a nutshell, our setup allows 30% more accurate estimations of spectra compared to an ordinary RGB camera, while the proposed alignment module boosts the reconstruction quality of SotA methods by an additional 5%. Our findings suggest that spectral filtering of multiple views with commodity hardware unlocks more accurate and practical hyperspectral imaging.",
    "published": "2025-07-02T15:49:12Z",
    "updated": "2026-02-23T12:24:46Z",
    "link": "http://arxiv.org/pdf/2507.01835v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Daniil Reutsky",
      "Daniil Vladimirov",
      "Yasin Mamedov",
      "Georgy Perevozchikov",
      "Nancy Mehta",
      "Egor Ershov",
      "Radu Timofte"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19768v1",
    "title": "TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding",
    "summary": "Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.",
    "published": "2026-02-23T12:18:26Z",
    "updated": "2026-02-23T12:18:26Z",
    "link": "http://arxiv.org/pdf/2602.19768v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fan Yang",
      "Shurong Zheng",
      "Hongyin Zhao",
      "Yufei Zhan",
      "Xin Li",
      "Yousong Zhu",
      "Chaoyang Zhao Ming Tang",
      "Jinqiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19766v1",
    "title": "One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image",
    "summary": "Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \\textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.",
    "published": "2026-02-23T12:15:54Z",
    "updated": "2026-02-23T12:15:54Z",
    "link": "http://arxiv.org/pdf/2602.19766v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pengfei Wang",
      "Liyi Chen",
      "Zhiyuan Ma",
      "Yanjun Guo",
      "Guowen Zhang",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19763v1",
    "title": "Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications",
    "summary": "Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.",
    "published": "2026-02-23T12:12:43Z",
    "updated": "2026-02-23T12:12:43Z",
    "link": "http://arxiv.org/pdf/2602.19763v1.pdf",
    "category": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19756v1",
    "title": "Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis",
    "summary": "Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.",
    "published": "2026-02-23T12:08:28Z",
    "updated": "2026-02-23T12:08:28Z",
    "link": "http://arxiv.org/pdf/2602.19756v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junhyeok Choi",
      "Sangwoo Mo",
      "Minwoo Chae"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05571v2",
    "title": "MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging",
    "summary": "Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT shows promising capability in identifying anatomical correspondence without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance. Code is available online.",
    "published": "2025-12-05T09:53:07Z",
    "updated": "2026-02-23T12:05:15Z",
    "link": "http://arxiv.org/pdf/2512.05571v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xingyu Zhang",
      "Anna Reithmeir",
      "Fryderyk Kögl",
      "Rickmer Braren",
      "Julia A. Schnabel",
      "Daniel M. Lang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19753v1",
    "title": "RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.",
    "published": "2026-02-23T12:02:03Z",
    "updated": "2026-02-23T12:02:03Z",
    "link": "http://arxiv.org/pdf/2602.19753v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Kaifa Yang",
      "Qi Yang",
      "Yiling Xu",
      "Zhu Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05016v2",
    "title": "Generative Neural Video Compression via Video Diffusion Prior",
    "summary": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.",
    "published": "2025-12-04T17:27:32Z",
    "updated": "2026-02-23T11:59:47Z",
    "link": "http://arxiv.org/pdf/2512.05016v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qi Mao",
      "Hao Cheng",
      "Tinghan Yang",
      "Libiao Jin",
      "Siwei Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19736v1",
    "title": "InfScene-SR: Spatially Continuous Inference for Arbitrary-Size Image Super-Resolution",
    "summary": "Image Super-Resolution (SR) aims to recover high-resolution (HR) details from low-resolution (LR) inputs, a task where Denoising Diffusion Probabilistic Models (DDPMs) have recently shown superior performance compared to Generative Adversarial Networks (GANs) based approaches. However, standard diffusion-based SR models, such as SR3, are typically trained on fixed-size patches and struggle to scale to arbitrary-sized images due to memory constraints. Applying these models via independent patch processing leads to visible seams and inconsistent textures across boundaries. In this paper, we propose InfScene-SR, a framework enabling spatially continuous super-resolution for large, arbitrary scenes. We adapt the iterative refinement process of diffusion models with a novel guided and variance-corrected fusion mechanism, allowing for the seamless generation of large-scale high-resolution imagery without retraining. We validate our approach on remote sensing datasets, demonstrating that InfScene-SR not only reconstructs fine details with high perceptual quality but also eliminates boundary artifacts, benefiting downstream tasks such as semantic segmentation.",
    "published": "2026-02-23T11:34:59Z",
    "updated": "2026-02-23T11:34:59Z",
    "link": "http://arxiv.org/pdf/2602.19736v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shoukun Sun",
      "Zhe Wang",
      "Xiang Que",
      "Jiyin Zhang",
      "Xiaogang Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19735v1",
    "title": "VGGT-MPR: VGGT-Enhanced Multimodal Place Recognition in Autonomous Driving Environments",
    "summary": "In autonomous driving, robust place recognition is critical for global localization and loop closure detection. While inter-modality fusion of camera and LiDAR data in multimodal place recognition (MPR) has shown promise in overcoming the limitations of unimodal counterparts, existing MPR methods basically attend to hand-crafted fusion strategies and heavily parameterized backbones that require costly retraining. To address this, we propose VGGT-MPR, a multimodal place recognition framework that adopts the Visual Geometry Grounded Transformer (VGGT) as a unified geometric engine for both global retrieval and re-ranking. In the global retrieval stage, VGGT extracts geometrically-rich visual embeddings through prior depth-aware and point map supervision, and densifies sparse LiDAR point clouds with predicted depth maps to improve structural representation. This enhances the discriminative ability of fused multimodal features and produces global descriptors for fast retrieval. Beyond global retrieval, we design a training-free re-ranking mechanism that exploits VGGT's cross-view keypoint-tracking capability. By combining mask-guided keypoint extraction with confidence-aware correspondence scoring, our proposed re-ranking mechanism effectively refines retrieval results without additional parameter optimization. Extensive experiments on large-scale autonomous driving benchmarks and our self-collected data demonstrate that VGGT-MPR achieves state-of-the-art performance, exhibiting strong robustness to severe environmental changes, viewpoint shifts, and occlusions. Our code and data will be made publicly available.",
    "published": "2026-02-23T11:33:56Z",
    "updated": "2026-02-23T11:33:56Z",
    "link": "http://arxiv.org/pdf/2602.19735v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingyi Xu",
      "Zhangshuo Qi",
      "Zhongmiao Yan",
      "Xuyu Gao",
      "Qianyun Jiao",
      "Songpengcheng Xia",
      "Xieyuanli Chen",
      "Ling Pei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19723v1",
    "title": "Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets",
    "summary": "Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.",
    "published": "2026-02-23T11:20:27Z",
    "updated": "2026-02-23T11:20:27Z",
    "link": "http://arxiv.org/pdf/2602.19723v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yue Zhang",
      "Zhizheng Zhuo",
      "Siyao Xu",
      "Shan Lv",
      "Zhaoxi Liu",
      "Jun Qiu",
      "Qiuli Wang",
      "Yaou Liu",
      "S. Kevin Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19719v1",
    "title": "Generative 6D Pose Estimation via Conditional Flow Matching",
    "summary": "Existing methods for instance-level 6D pose estimation typically rely on neural networks that either directly regress the pose in $\\mathrm{SE}(3)$ or estimate it indirectly via local feature matching. The former struggle with object symmetries, while the latter fail in the absence of distinctive local features. To overcome these limitations, we propose a novel formulation of 6D pose estimation as a conditional flow matching problem in $\\mathbb{R}^3$. We introduce Flose, a generative method that infers object poses via a denoising process conditioned on local features. While prior approaches based on conditional flow matching perform denoising solely based on geometric guidance, Flose integrates appearance-based semantic features to mitigate ambiguities caused by object symmetries. We further incorporate RANSAC-based registration to handle outliers. We validate Flose on five datasets from the established BOP benchmark. Flose outperforms prior methods with an average improvement of +4.5 Average Recall. Project Website : https://tev-fbk.github.io/Flose/",
    "published": "2026-02-23T11:15:12Z",
    "updated": "2026-02-23T11:15:12Z",
    "link": "http://arxiv.org/pdf/2602.19719v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Amir Hamza",
      "Davide Boscaini",
      "Weihang Li",
      "Benjamin Busam",
      "Fabio Poiesi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19715v1",
    "title": "Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision",
    "summary": "Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\\%, outperforming \\texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \\href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.",
    "published": "2026-02-23T11:08:46Z",
    "updated": "2026-02-23T11:08:46Z",
    "link": "http://arxiv.org/pdf/2602.19715v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kartik Kuckreja",
      "Parul Gupta",
      "Muhammad Haris Khan",
      "Abhinav Dhall"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19710v1",
    "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
    "summary": "Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns.\n  To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision.\n  Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
    "published": "2026-02-23T11:00:08Z",
    "updated": "2026-02-23T11:00:08Z",
    "link": "http://arxiv.org/pdf/2602.19710v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Haitao Lin",
      "Hanyang Yu",
      "Jingshun Huang",
      "He Zhang",
      "Yonggen Ling",
      "Ping Tan",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19708v1",
    "title": "ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets",
    "summary": "Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.",
    "published": "2026-02-23T10:59:41Z",
    "updated": "2026-02-23T10:59:41Z",
    "link": "http://arxiv.org/pdf/2602.19708v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hoyoung Kim",
      "Minwoo Jang",
      "Jabin Koo",
      "Sangdoo Yun",
      "Jungseul Ok"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19706v1",
    "title": "HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion",
    "summary": "Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting",
    "published": "2026-02-23T10:57:22Z",
    "updated": "2026-02-23T10:57:22Z",
    "link": "http://arxiv.org/pdf/2602.19706v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yo-Tin Lin",
      "Su-Kai Chen",
      "Hou-Ning Hu",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.14512v2",
    "title": "MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction",
    "summary": "Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.",
    "published": "2026-02-16T06:48:48Z",
    "updated": "2026-02-23T10:51:34Z",
    "link": "http://arxiv.org/pdf/2602.14512v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhicheng He",
      "Yunpeng Zhao",
      "Junde Wu",
      "Ziwei Niu",
      "Zijun Li",
      "Bohan Li",
      "Lanfen Lin",
      "Yueming Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.06742v3",
    "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection",
    "summary": "Landmark detection is central to many medical applications, such as identifying critical structures for treatment planning or defining control points for biometric measurements. However, manual annotation is labor-intensive and requires expert anatomical knowledge. While deep learning shows promise in automating this task, fair evaluation and interpretation of methods in a broader context are hindered by limited public benchmarking, inconsistent baseline implementations, and non-standardized experimentation. To overcome these pitfalls, we present nnLandmark, a self-configuring framework for 3D landmark detection that combines tailored heatmap generation, loss design, inference logic, and a robust set of hyperparameters for heatmap regression, while reusing components from nnU-Net's underlying self-configuration and training engine. nnLandmark achieves state-of-the-art performance across five public and one private dataset, benchmarked against three recently published methods. Its out-of-the-box usability enables training strong landmark detection models on new datasets without expert knowledge or dataset-specific hyperparameter tuning. Beyond accuracy, nnLandmark provides both a strong, common baseline and a flexible, standardized environment for developing and evaluating new methodological contributions. It further streamlines evaluation across multiple datasets by offering data conversion utilities for current public benchmarks. Together, these properties position nnLandmark as a central tool for advancing 3D medical landmark detection through systematic, transparent benchmarking, enabling to genuinely measure methodological progress. The code is available on GitHub: https://github.com/MIC-DKFZ/nnLandmark",
    "published": "2025-04-09T09:53:39Z",
    "updated": "2026-02-23T10:49:43Z",
    "link": "http://arxiv.org/pdf/2504.06742v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alexandra Ertl",
      "Stefan Denner",
      "Robin Peretzke",
      "Shuhan Xiao",
      "David Zimmerer",
      "Maximilian Fischer",
      "Markus Bujotzek",
      "Xin Yang",
      "Peter Neher",
      "Fabian Isensee",
      "Klaus H. Maier-Hein"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.18193v2",
    "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards",
    "summary": "Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.",
    "published": "2026-02-20T12:59:27Z",
    "updated": "2026-02-23T10:48:48Z",
    "link": "http://arxiv.org/pdf/2602.18193v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yiran Yang",
      "Zhaowei Liu",
      "Yuan Yuan",
      "Yukun Song",
      "Xiong Ma",
      "Yinghao Song",
      "Xiangji Zeng",
      "Lu Sun",
      "Yulu Wang",
      "Hai Zhou",
      "Shuai Cui",
      "Zhaohan Gong",
      "Jiefei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19697v1",
    "title": "BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU",
    "summary": "Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF",
    "published": "2026-02-23T10:44:15Z",
    "updated": "2026-02-23T10:44:15Z",
    "link": "http://arxiv.org/pdf/2602.19697v1.pdf",
    "category": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "authors": [
      "Soumya Mazumdar",
      "Vineet Kumar Rakesh",
      "Tapas Samanta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19668v1",
    "title": "Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation",
    "summary": "Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation.\n  We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML.\n  Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.",
    "published": "2026-02-23T10:14:36Z",
    "updated": "2026-02-23T10:14:36Z",
    "link": "http://arxiv.org/pdf/2602.19668v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "He Zhu",
      "Ren Togo",
      "Takahiro Ogawa",
      "Kenji Hirata",
      "Minghui Tang",
      "Takaaki Yoshimura",
      "Hiroyuki Sugimori",
      "Noriko Nishioka",
      "Yukie Shimizu",
      "Kohsuke Kudo",
      "Miki Haseyama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19624v1",
    "title": "Accurate Planar Tracking With Robust Re-Detection",
    "summary": "We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM",
    "published": "2026-02-23T09:13:55Z",
    "updated": "2026-02-23T09:13:55Z",
    "link": "http://arxiv.org/pdf/2602.19624v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jonas Serych",
      "Jiri Matas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19615v1",
    "title": "Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness",
    "summary": "Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.",
    "published": "2026-02-23T09:02:40Z",
    "updated": "2026-02-23T09:02:40Z",
    "link": "http://arxiv.org/pdf/2602.19615v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xin Hu",
      "Haomiao Ni",
      "Yunbei Zhang",
      "Jihun Hamm",
      "Zechen Li",
      "Zhengming Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19611v1",
    "title": "RAID: Retrieval-Augmented Anomaly Detection",
    "summary": "Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \\textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \\href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.",
    "published": "2026-02-23T08:54:27Z",
    "updated": "2026-02-23T08:54:27Z",
    "link": "http://arxiv.org/pdf/2602.19611v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mingxiu Cai",
      "Zhe Zhang",
      "Gaochang Wu",
      "Tianyou Chai",
      "Xiatian Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19596v1",
    "title": "Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception",
    "summary": "Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\\% against state-of-the-art defenses while achieving 47\\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git",
    "published": "2026-02-23T08:38:27Z",
    "updated": "2026-02-23T08:38:27Z",
    "link": "http://arxiv.org/pdf/2602.19596v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yihang Tao",
      "Senkang Hu",
      "Haonan An",
      "Zhengru Fang",
      "Hangcheng Cao",
      "Yuguang Fang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.06751v3",
    "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot",
    "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.",
    "published": "2025-10-08T08:19:15Z",
    "updated": "2026-02-23T08:33:05Z",
    "link": "http://arxiv.org/pdf/2510.06751v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junhan Zhu",
      "Hesong Wang",
      "Mingluo Su",
      "Zefang Wang",
      "Huan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.21990v2",
    "title": "WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM",
    "summary": "While embeddings from multimodal large language models (LLMs) excel as general-purpose representations, their application to dynamic modalities like audio and video remains underexplored. We introduce WAVE (\\textbf{u}nified \\& \\textbf{v}ersatile \\textbf{a}udio-\\textbf{v}isual \\textbf{e}mbeddings), the first LLM-based embedding that creates a unified representation space for text, audio, and video modalities. WAVE employs a novel hierarchical feature fusion strategy and a joint multi-modal, multi-task training approach to enable two key capabilities: any-to-any cross-modal retrieval and the generation of prompt-aware embeddings tailored to user instructions. Experimentally, WAVE sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves superior results in audio and video-to-audio retrieval. Its prompt-aware nature also yields remarkable performance in multimodal question answering, significantly outperforming existing embedding models. Ablation studies validate our joint training strategy, demonstrating improved performance across all modalities. With a newly introduced benchmark for versatile audio-visual learning, WAVE opens up broad possibilities for cross-modal, any-to-any applications. Our code and checkpoints are released at \\href{https://github.com/TCL606/WAVE}{https://github.com/TCL606/WAVE}.",
    "published": "2025-09-26T07:13:37Z",
    "updated": "2026-02-23T08:15:44Z",
    "link": "http://arxiv.org/pdf/2509.21990v2.pdf",
    "category": [
      "cs.CV",
      "cs.SD"
    ],
    "authors": [
      "Changli Tang",
      "Qinfan Xiao",
      "Ke Mei",
      "Tianyi Wang",
      "Fengyun Rao",
      "Chao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.17354v3",
    "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling",
    "summary": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.",
    "published": "2026-01-24T07:58:53Z",
    "updated": "2026-02-23T08:13:48Z",
    "link": "http://arxiv.org/pdf/2601.17354v3.pdf",
    "category": [
      "cs.CV",
      "cs.GR"
    ],
    "authors": [
      "Wenzhi Guo",
      "Guangchi Fang",
      "Shu Yang",
      "Bing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.13430v2",
    "title": "Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning",
    "summary": "Chest X-Ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.",
    "published": "2026-02-13T20:07:34Z",
    "updated": "2026-02-23T08:05:53Z",
    "link": "http://arxiv.org/pdf/2602.13430v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ha-Hieu Pham",
      "Hai-Dang Nguyen",
      "Thanh-Huy Nguyen",
      "Min Xu",
      "Ulas Bagci",
      "Trung-Nghia Le",
      "Huy-Hieu Pham"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.03349v2",
    "title": "Meta-DAN: towards an efficient prediction strategy for page-level handwritten text recognition",
    "summary": "Recent advances in text recognition led to a paradigm shift for page-level recognition, from multi-step segmentation-based approaches to end-to-end attention-based ones. However, the naïve character-level autoregressive decoding process results in long prediction times: it requires several seconds to process a single page image on a modern GPU. We propose the Meta Document Attention Network (Meta-DAN) as a novel decoding strategy to reduce the prediction time while enabling a better context modeling. It relies on two main components: windowed queries, to process several transformer queries altogether, enlarging the context modeling with near future; and multi-token predictions, whose goal is to predict several tokens per query instead of only the next one. We evaluate the proposed approach on 10 full-page handwritten datasets and demonstrate state-of-the-art results on average in terms of character error rate. Source code and weights of trained models are available at https://github.com/FactoDeepLearning/meta_dan.",
    "published": "2025-04-04T11:06:09Z",
    "updated": "2026-02-23T07:50:32Z",
    "link": "http://arxiv.org/pdf/2504.03349v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Denis Coquenet"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19575v1",
    "title": "ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization",
    "summary": "Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.",
    "published": "2026-02-23T07:46:19Z",
    "updated": "2026-02-23T07:46:19Z",
    "link": "http://arxiv.org/pdf/2602.19575v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Minseo Kim",
      "Minchan Kwon",
      "Dongyeun Lee",
      "Yunho Jeon",
      "Junmo Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19571v1",
    "title": "HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies",
    "summary": "Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 \"Thinking\" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.",
    "published": "2026-02-23T07:40:32Z",
    "updated": "2026-02-23T07:40:32Z",
    "link": "http://arxiv.org/pdf/2602.19571v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chang Liu",
      "Yunfan Ye",
      "Qingyang Zhou",
      "Xichen Tan",
      "Mengxuan Luo",
      "Zhenyu Qiu",
      "Wei Peng",
      "Zhiping Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19570v1",
    "title": "VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense",
    "summary": "Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.",
    "published": "2026-02-23T07:39:43Z",
    "updated": "2026-02-23T07:39:43Z",
    "link": "http://arxiv.org/pdf/2602.19570v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Nadav Kadvil",
      "Ayellet Tal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01289v2",
    "title": "Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models",
    "summary": "Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.",
    "published": "2026-02-01T15:45:07Z",
    "updated": "2026-02-23T07:27:27Z",
    "link": "http://arxiv.org/pdf/2602.01289v2.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Dung Anh Hoang",
      "Cuong Pham anh Trung Le",
      "Jianfei Cai",
      "Thanh-Toan Do"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.06391v2",
    "title": "Object-WIPER : Training-Free Object and Associated Effect Removal in Videos",
    "summary": "In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.",
    "published": "2026-01-10T02:28:31Z",
    "updated": "2026-02-23T07:21:22Z",
    "link": "http://arxiv.org/pdf/2601.06391v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Saksham Singh Kushwaha",
      "Sayan Nag",
      "Yapeng Tian",
      "Kuldeep Kulkarni"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19542v1",
    "title": "Vinedresser3D: Agentic Text-guided 3D Editing",
    "summary": "Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for high-quality text-guided 3D editing that operates directly in the latent space of a native 3D generative model. Given a 3D asset and an editing prompt, Vinedresser3D uses a multimodal large language model to infer rich descriptions of the original asset, identify the edit region and edit type (addition, modification, deletion), and generate decomposed structural and appearance-level text guidance. The agent then selects an informative view and applies an image editing model to obtain visual guidance. Finally, an inversion-based rectified-flow inpainting pipeline with an interleaved sampling module performs editing in the 3D latent space, enforcing prompt alignment while maintaining 3D coherence and unedited regions. Experiments on diverse 3D edits demonstrate that Vinedresser3D outperforms prior baselines in both automatic metrics and human preference studies, while enabling precise, coherent, and mask-free 3D editing.",
    "published": "2026-02-23T06:30:36Z",
    "updated": "2026-02-23T06:30:36Z",
    "link": "http://arxiv.org/pdf/2602.19542v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yankuan Chi",
      "Xiang Li",
      "Zixuan Huang",
      "James M. Rehg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01783v2",
    "title": "Harnessing Chain-of-Thought Reasoning in Multimodal Large Language Models for Face Anti-Spoofing",
    "summary": "Face Anti-Spoofing (FAS) typically depends on a single visual modality when defending against presentation attacks such as print attacks, screen replays, and 3D masks, resulting in limited generalization across devices, environments, and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have recently achieved breakthroughs in image-text understanding and semantic reasoning, suggesting that integrating visual and linguistic co-inference into FAS can substantially improve both robustness and interpretability. However, the lack of a high-quality vision-language multimodal dataset has been a critical bottleneck. To address this, we introduce FaceCoT (Face Chain-of-Thought), the first large-scale Visual Question Answering (VQA) dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches model learning with high-quality CoT VQA annotations. Meanwhile, we develop a caption model refined via reinforcement learning to expand the dataset and enhance annotation quality. Furthermore, we introduce a CoT-Enhanced Progressive Learning (CEPL) strategy to better leverage the CoT data and boost model performance on FAS tasks. Extensive experiments demonstrate that models trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple benchmark datasets.",
    "published": "2025-06-02T15:29:41Z",
    "updated": "2026-02-23T06:26:39Z",
    "link": "http://arxiv.org/pdf/2506.01783v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Honglu Zhang",
      "Zhiqin Fang",
      "Ningning Zhao",
      "Saihui Hou",
      "Long Ma",
      "Renwang Pei",
      "Zhaofeng He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2210.11974v2",
    "title": "Face Pyramid Vision Transformer",
    "summary": "A novel Face Pyramid Vision Transformer (FPVT) is proposed to learn a discriminative multi-scale facial representations for face recognition and verification. In FPVT, Face Spatial Reduction Attention (FSRA) and Dimensionality Reduction (FDR) layers are employed to make the feature maps compact, thus reducing the computations. An Improved Patch Embedding (IPE) algorithm is proposed to exploit the benefits of CNNs in ViTs (e.g., shared weights, local context, and receptive fields) to model lower-level edges to higher-level semantic primitives. Within FPVT framework, a Convolutional Feed-Forward Network (CFFN) is proposed that extracts locality information to learn low level facial information. The proposed FPVT is evaluated on seven benchmark datasets and compared with ten existing state-of-the-art methods, including CNNs, pure ViTs, and Convolutional ViTs. Despite fewer parameters, FPVT has demonstrated excellent performance over the compared methods. Project page is available at https://khawar-islam.github.io/fpvt/",
    "published": "2022-10-21T14:03:40Z",
    "updated": "2026-02-23T06:14:00Z",
    "link": "http://arxiv.org/pdf/2210.11974v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Khawar Islam",
      "Muhammad Zaigham Zaheer",
      "Arif Mahmood"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19539v1",
    "title": "Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems",
    "summary": "Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.",
    "published": "2026-02-23T06:13:52Z",
    "updated": "2026-02-23T06:13:52Z",
    "link": "http://arxiv.org/pdf/2602.19539v1.pdf",
    "category": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Xingyu Shen",
      "Tommy Duong",
      "Xiaodong An",
      "Zengqi Zhao",
      "Zebang Hu",
      "Haoyu Hu",
      "Ziyou Wang",
      "Finn Guo",
      "Simiao Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19530v1",
    "title": "ORION: ORthonormal Text Encoding for Universal VLM AdaptatION",
    "summary": "Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.",
    "published": "2026-02-23T05:47:28Z",
    "updated": "2026-02-23T05:47:28Z",
    "link": "http://arxiv.org/pdf/2602.19530v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Omprakash Chakraborty",
      "Jose Dolz",
      "Ismail Ben Ayed"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19523v1",
    "title": "OSInsert: Towards High-authenticity and High-fidelity Image Composition",
    "summary": "Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. Some high-authenticity methods can adjust foreground pose/view to be compatible with background, while some high-fidelity methods can preserve the foreground details accurately. However, existing methods can hardly achieve both goals at the same time. In this work, we propose a two-stage strategy to achieve both goals. In the first stage, we use high-authenticity method to generate reasonable foreground shape, serving as the condition of high-fidelity method in the second stage. The experiments on MureCOM dataset verify the effectiveness of our two-stage strategy. The code and model have been released at https://github.com/bcmi/OSInsert-Image-Composition.",
    "published": "2026-02-23T05:25:05Z",
    "updated": "2026-02-23T05:25:05Z",
    "link": "http://arxiv.org/pdf/2602.19523v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingyuan Wang",
      "Li Niu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20156v1",
    "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
    "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
    "published": "2026-02-23T18:59:27Z",
    "updated": "2026-02-23T18:59:27Z",
    "link": "http://arxiv.org/pdf/2602.20156v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "David Schmotz",
      "Luca Beurer-Kellner",
      "Sahar Abdelnabi",
      "Maksym Andriushchenko"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20153v1",
    "title": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks",
    "summary": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.",
    "published": "2026-02-23T18:59:10Z",
    "updated": "2026-02-23T18:59:10Z",
    "link": "http://arxiv.org/pdf/2602.20153v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Jakob Heiss",
      "Sören Lambrecht",
      "Jakob Weissteiner",
      "Hanna Wutte",
      "Žan Žurič",
      "Josef Teichmann",
      "Bin Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20151v1",
    "title": "Conformal Risk Control for Non-Monotonic Losses",
    "summary": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.",
    "published": "2026-02-23T18:58:54Z",
    "updated": "2026-02-23T18:58:54Z",
    "link": "http://arxiv.org/pdf/2602.20151v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "authors": [
      "Anastasios N. Angelopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03817v3",
    "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models",
    "summary": "Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model's most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model's inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.",
    "published": "2025-10-04T14:14:20Z",
    "updated": "2026-02-23T18:54:13Z",
    "link": "http://arxiv.org/pdf/2510.03817v3.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Philipp Becker",
      "Niklas Freymuth",
      "Serge Thilges",
      "Fabian Otto",
      "Gerhard Neumann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20132v1",
    "title": "LAD: Learning Advantage Distribution for Reasoning",
    "summary": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
    "published": "2026-02-23T18:44:10Z",
    "updated": "2026-02-23T18:44:10Z",
    "link": "http://arxiv.org/pdf/2602.20132v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Wendi Li",
      "Sharon Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20126v1",
    "title": "Adaptation to Intrinsic Dependence in Diffusion Language Models",
    "summary": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.",
    "published": "2026-02-23T18:41:34Z",
    "updated": "2026-02-23T18:41:34Z",
    "link": "http://arxiv.org/pdf/2602.20126v1.pdf",
    "category": [
      "cs.LG",
      "cs.IT",
      "math.ST",
      "stat.ML"
    ],
    "authors": [
      "Yunxiao Zhao",
      "Changxiao Cai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20111v1",
    "title": "Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds",
    "summary": "We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\\ from an unknown distribution $\\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \\log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\\tilde{O}(\\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental.\n  We resolve this question by proving a matching $Ω(\\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \\emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \\emph{inference dimension}, yielding combined error $\\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \\emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.",
    "published": "2026-02-23T18:30:48Z",
    "updated": "2026-02-23T18:30:48Z",
    "link": "http://arxiv.org/pdf/2602.20111v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ezra Edelman",
      "Surbhi Goel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.10471v2",
    "title": "VillageNet: Graph-based, Easily-interpretable, Unsupervised Clustering for Broad Biomedical Applications",
    "summary": "Clustering large high-dimensional datasets with diverse variable is essential for extracting high-level latent information from these datasets. Here, we developed an unsupervised clustering algorithm, we call \"Village-Net\". Village-Net is specifically designed to effectively cluster high-dimension data without priori knowledge on the number of existing clusters. The algorithm operates in two phases: first, utilizing K-Means clustering, it divides the dataset into distinct subsets we refer to as \"villages\". Next, a weighted network is created, with each node representing a village, capturing their proximity relationships. To achieve optimal clustering, we process this network using a community detection algorithm called Walk-likelihood Community Finder (WLCF), a community detection algorithm developed by one of our team members. A salient feature of Village-Net Clustering is its ability to autonomously determine an optimal number of clusters for further analysis based on inherent characteristics of the data. We present extensive benchmarking on extant real-world datasets with known ground-truth labels to showcase its competitive performance, particularly in terms of the normalized mutual information (NMI) score, when compared to other state-of-the-art methods. The algorithm is computationally efficient, boasting a time complexity of O(N*k*d), where N signifies the number of instances, k represents the number of villages and d represents the dimension of the dataset, which makes it well suited for effectively handling large-scale datasets.",
    "published": "2025-01-16T06:56:43Z",
    "updated": "2026-02-23T18:26:51Z",
    "link": "http://arxiv.org/pdf/2501.10471v2.pdf",
    "category": [
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "authors": [
      "Aditya Ballal",
      "Gregory A. DePaul",
      "Esha Datta",
      "Asuka Hatano",
      "Erik Carlsson",
      "Ye Chen-Izu",
      "Javier E. López",
      "Leighton T. Izu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.12389v2",
    "title": "Predictive control of blast furnace temperature in steelmaking with hybrid depth-infused quantum neural networks",
    "summary": "Accurate prediction and stabilization of blast furnace temperatures are crucial for optimizing the efficiency and productivity of steel production. Traditional methods often struggle with the complex and non-linear nature of the temperature fluctuations within blast furnaces. This paper proposes a novel approach that combines hybrid quantum machine learning with pulverized coal injection control to address these challenges. By integrating classical machine learning techniques with quantum computing algorithms, we aim to enhance predictive accuracy and achieve more stable temperature control. For this we utilized a unique prediction-based optimization method. Our method leverages quantum-enhanced feature space exploration and the robustness of classical regression models to forecast temperature variations and optimize pulverized coal injection values. Our results demonstrate a significant improvement in prediction accuracy over 25 percent and our solution improved temperature stability to +-7.6 degrees of target range from the earlier variance of +-50 degrees, highlighting the potential of hybrid quantum machine learning models in industrial steel production applications.",
    "published": "2025-04-16T18:00:46Z",
    "updated": "2026-02-23T18:15:25Z",
    "link": "http://arxiv.org/pdf/2504.12389v2.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Nayoung Lee",
      "Minsoo Shin",
      "Asel Sagingalieva",
      "Arsenii Senokosov",
      "Matvei Anoshin",
      "Ayush Joshi Tripathi",
      "Karan Pinto",
      "Alexey Melnikov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.02853v2",
    "title": "Recurrent Equivariant Constraint Modulation: Learning Per-Layer Symmetry Relaxation from Data",
    "summary": "Equivariant neural networks exploit underlying task symmetries to improve generalization, but strict equivariance constraints can induce more complex optimization dynamics that can hinder learning. Prior work addresses these limitations by relaxing strict equivariance during training, but typically relies on prespecified, explicit, or implicit target levels of relaxation for each network layer, which are task-dependent and costly to tune. We propose Recurrent Equivariant Constraint Modulation (RECM), a layer-wise constraint modulation mechanism that learns appropriate relaxation levels solely from the training signal and the symmetry properties of each layer's input-target distribution, without requiring any prior knowledge about the task-dependent target relaxation level. We demonstrate that under the proposed RECM update, the relaxation level of each layer provably converges to a value upper-bounded by its symmetry gap, namely the degree to which its input-target distribution deviates from exact symmetry. Consequently, layers processing symmetric distributions recover full equivariance, while those with approximate symmetries retain sufficient flexibility to learn non-symmetric solutions when warranted by the data. Empirically, RECM outperforms prior methods across diverse exact and approximate equivariant tasks, including the challenging molecular conformer generation on the GEOM-Drugs dataset.",
    "published": "2026-02-02T21:59:35Z",
    "updated": "2026-02-23T17:55:22Z",
    "link": "http://arxiv.org/pdf/2602.02853v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Stefanos Pertigkiozoglou",
      "Mircea Petrache",
      "Shubhendu Trivedi",
      "Kostas Daniilidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.03313v2",
    "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining",
    "summary": "Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.",
    "published": "2025-09-30T22:45:06Z",
    "updated": "2026-02-23T17:54:32Z",
    "link": "http://arxiv.org/pdf/2510.03313v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Anirudh Subramanyam",
      "Yuxin Chen",
      "Robert L. Grossman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.03596v2",
    "title": "SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network",
    "summary": "Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive attackers. In this work, we study the problem of detecting 5G attacks \\textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network services. We then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.",
    "published": "2026-02-03T14:50:19Z",
    "updated": "2026-02-23T17:35:26Z",
    "link": "http://arxiv.org/pdf/2602.03596v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Cristian Manca",
      "Christian Scano",
      "Giorgio Piras",
      "Fabio Brau",
      "Maura Pintor",
      "Battista Biggio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20070v1",
    "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
    "summary": "We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaφ(x)^\\topη_t$, where $η_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.",
    "published": "2026-02-23T17:26:09Z",
    "updated": "2026-02-23T17:26:09Z",
    "link": "http://arxiv.org/pdf/2602.20070v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Florentin Coeurdoux",
      "Etienne Lempereur",
      "Nathanaël Cuvelle-Magar",
      "Thomas Eboli",
      "Stéphane Mallat",
      "Anastasia Borovykh",
      "Eric Vanden-Eijnden"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20062v1",
    "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
    "summary": "Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.",
    "published": "2026-02-23T17:19:33Z",
    "updated": "2026-02-23T17:19:33Z",
    "link": "http://arxiv.org/pdf/2602.20062v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Nicolas Anguita",
      "Francesco Locatello",
      "Andrew M. Saxe",
      "Marco Mondelli",
      "Flavia Mancini",
      "Samuel Lippl",
      "Clementine Domine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19193v3",
    "title": "SuperMAN: Interpretable and Expressive Networks over Temporally Sparse Heterogeneous Data",
    "summary": "Real-world temporal data often consists of multiple signal types recorded at irregular, asynchronous intervals. For instance, in the medical domain, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling occur in other domains, such as the monitoring of large systems using event log files. Effectively learning from such data requires handling sets of temporal sparse and heterogeneous signals. In this work, we propose Super Mixing Additive Networks (SuperMAN), a novel and interpretable-by-design framework for learning directly from such heterogeneous signals, by modeling them as sets of implicit graphs. SuperMAN provides diverse interpretability capabilities, including node-level, graph-level, and subset-level importance, and enables practitioners to trade finer-grained interpretability for greater expressivity when domain priors are available. SuperMAN achieves state-of-the-art performance in real-world high-stakes tasks, including predicting Crohn's disease onset and hospital length of stay from routine blood test measurements and detecting fake news. Furthermore, we demonstrate how SuperMAN's interpretability properties assist in revealing disease development phase transitions and provide crucial insights in the healthcare domain.",
    "published": "2025-05-25T15:41:01Z",
    "updated": "2026-02-23T16:49:34Z",
    "link": "http://arxiv.org/pdf/2505.19193v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Andrea Zerio",
      "Maya Bechler-Speicher",
      "Maor Huri",
      "Marie Vibeke Vestergaard",
      "Ran Gilad-Bachrach",
      "Tine Jess",
      "Samir Bhatt",
      "Aleksejs Sazonovs"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.17762v2",
    "title": "Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation",
    "summary": "Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.",
    "published": "2025-12-19T16:34:27Z",
    "updated": "2026-02-23T16:40:25Z",
    "link": "http://arxiv.org/pdf/2512.17762v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Luca Miglior",
      "Matteo Tolloso",
      "Alessio Gravina",
      "Davide Bacciu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16824v2",
    "title": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs",
    "summary": "Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated.",
    "published": "2025-06-20T08:26:12Z",
    "updated": "2026-02-23T16:38:34Z",
    "link": "http://arxiv.org/pdf/2506.16824v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Thomas Marwitz",
      "Alexander Colsmann",
      "Ben Breitung",
      "Christoph Brabec",
      "Christoph Kirchlechner",
      "Eva Blasco",
      "Gabriel Cadilha Marques",
      "Horst Hahn",
      "Michael Hirtz",
      "Pavel A. Levkin",
      "Yolita M. Eggeler",
      "Tobias Schlöder",
      "Pascal Friederich"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.12674v2",
    "title": "Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation",
    "summary": "Dynamic relational data arise in many machine learning applications, yet their evolving structure poses challenges for learning representations that remain consistent and interpretable over time. A common approach is to learn time varying node embeddings, whose usefulness depends on well defined stability properties across nodes and across time. We introduce Unfolded Laplacian Spectral Embedding (ULSE), a principled extension of unfolded adjacency spectral embedding to normalized Laplacian operators, a setting where stability guarantees have remained out of reach. We prove that ULSE satisfies both cross-sectional and longitudinal stability under a dynamic stochastic block model. Moreover, the Laplacian formulation yields a dynamic Cheeger-type inequality linking the spectrum of the unfolded normalized Laplacian to worst case conductance over time, providing structural insight into the embeddings. Empirical results on synthetic and real world dynamic networks validate the theory.",
    "published": "2025-08-18T07:13:53Z",
    "updated": "2026-02-23T16:09:58Z",
    "link": "http://arxiv.org/pdf/2508.12674v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "cs.SI"
    ],
    "authors": [
      "Haruka Ezoe",
      "Hiroki Matsumoto",
      "Ryohei Hisano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20001v1",
    "title": "FairFS: Addressing Deep Feature Selection Biases for Recommender System",
    "summary": "Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.",
    "published": "2026-02-23T16:08:32Z",
    "updated": "2026-02-23T16:08:32Z",
    "link": "http://arxiv.org/pdf/2602.20001v1.pdf",
    "category": [
      "cs.IR",
      "cs.LG"
    ],
    "authors": [
      "Xianquan Wang",
      "Zhaocheng Du",
      "Jieming Zhu",
      "Qinglin Jia",
      "Zhenhua Dong",
      "Kai Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.01428v2",
    "title": "Improving the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models",
    "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.",
    "published": "2026-02-01T20:30:59Z",
    "updated": "2026-02-23T15:55:07Z",
    "link": "http://arxiv.org/pdf/2602.01428v2.pdf",
    "category": [
      "cs.LG",
      "cs.CR"
    ],
    "authors": [
      "Weiqing He",
      "Xiang Li",
      "Li Shen",
      "Weijie Su",
      "Qi Long"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19987v1",
    "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
    "summary": "This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.",
    "published": "2026-02-23T15:53:25Z",
    "updated": "2026-02-23T15:53:25Z",
    "link": "http://arxiv.org/pdf/2602.19987v1.pdf",
    "category": [
      "cs.LG",
      "cs.IR"
    ],
    "authors": [
      "Ha-Anh Hoang Nguyen",
      "Tri-Duc Phan Le",
      "Duc-Hoang Pham",
      "Huy-Son Nguyen",
      "Cam-Van Thi Nguyen",
      "Duc-Trong Le",
      "Hoang-Quynh Le"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19984v1",
    "title": "Multivariate time-series forecasting of ASTRI-Horn monitoring data: A Normal Behavior Model",
    "summary": "This study presents a Normal Behavior Model (NBM) developed to forecast monitoring time-series data from the ASTRI-Horn Cherenkov telescope under normal operating conditions. The analysis focused on 15 physical variables acquired by the Telescope Control Unit between September 2022 and July 2024, representing sensor measurements from the Azimuth and Elevation motors. After data cleaning, resampling, feature selection, and correlation analysis, the dataset was segmented into fixed-length intervals, in which the first I samples represented the input sequence provided to the model, while the forecast length, T, indicated the number of future time steps to be predicted. A sliding-window technique was then applied to increase the number of intervals. A Multi-Layer Perceptron (MLP) was trained to perform multivariate forecasting across all features simultaneously. Model performance was evaluated using the Mean Squared Error (MSE) and the Normalized Median Absolute Deviation (NMAD), and it was also benchmarked against a Long Short-Term Memory (LSTM) network. The MLP model demonstrated consistent results across different features and I-T configurations, and matched the performance of the LSTM while converging faster. It achieved an MSE of 0.019+/-0.003 and an NMAD of 0.032+/-0.009 on the test set under its best configuration (4 hidden layers, 720 units per layer, and I-T lengths of 300 samples each, corresponding to 5 hours at 1-minute resolution). Extending the forecast horizon up to 6.5 hours-the maximum allowed by this configuration-did not degrade performance, confirming the model's effectiveness in providing reliable hour-scale predictions. The proposed NBM provides a powerful tool for enabling early anomaly detection in online ASTRI-Horn monitoring time series, offering a basis for the future development of a prognostics and health management system that supports predictive maintenance.",
    "published": "2026-02-23T15:51:50Z",
    "updated": "2026-02-23T15:51:50Z",
    "link": "http://arxiv.org/pdf/2602.19984v1.pdf",
    "category": [
      "astro-ph.IM",
      "astro-ph.HE",
      "cs.LG"
    ],
    "authors": [
      "Federico Incardona",
      "Alessandro Costa",
      "Farida Farsian",
      "Francesco Franchina",
      "Giuseppe Leto",
      "Emilio Mastriani",
      "Kevin Munari",
      "Giovanni Pareschi",
      "Salvatore Scuderi",
      "Sebastiano Spinello",
      "Gino Tosti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19982v1",
    "title": "A Computationally Efficient Multidimensional Vision Transformer",
    "summary": "Vision Transformers have achieved state-of-the-art performance in a wide range\n  of computer vision tasks, but their practical deployment is limited by high\n  computational and memory costs. In this paper, we introduce a novel tensor-based\n  framework for Vision Transformers built upon the Tensor Cosine Product\n  (Cproduct). By exploiting multilinear structures inherent in image data and the\n  orthogonality of cosine transforms, the proposed approach enables efficient\n  attention mechanisms and structured feature representations. We develop the\n  theoretical foundations of the tensor cosine product, analyze its algebraic\n  properties, and integrate it into a new Cproduct-based Vision Transformer\n  architecture (TCP-ViT). Numerical experiments on standard classification and\n  segmentation benchmarks demonstrate that the proposed method achieves a uniform\n  1/C parameter reduction (where C is the number of channels) while\n  maintaining competitive accuracy.",
    "published": "2026-02-23T15:49:46Z",
    "updated": "2026-02-23T15:49:46Z",
    "link": "http://arxiv.org/pdf/2602.19982v1.pdf",
    "category": [
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Alaa El Ichi",
      "Khalide Jbilou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19980v1",
    "title": "Discrete Diffusion Models Exploit Asymmetry to Solve Lookahead Planning Tasks",
    "summary": "While Autoregressive (AR) Transformer-based Generative Language Models are frequently employed for lookahead tasks, recent research suggests a potential discrepancy in their ability to perform planning tasks that require multi-step lookahead. In this work, we investigate the distinct emergent mechanisms that arise when training AR versus Non-Autoregressive (NAR) models, such as Discrete Diffusion Models (dLLMs), on lookahead tasks. By requiring the models to plan ahead to reach the correct conclusion, we analyze how these two paradigms fundamentally differ in their approach to the problem. We identify a critical asymmetry in planning problems: while forward generation requires complex lookahead at branching junctions, reverse generation is often deterministic. This asymmetry creates an opportunity for NAR models. Through mechanistic analysis of training and inference dynamics, we demonstrate that NAR models learn to solve planning tasks by utilizing future tokens to decode backwards, avoiding the need to learn complex traversal mechanisms entirely. Consequently, we report that both AR and NAR models are able to achieve perfect accuracy on the lookahead task. However, NAR models require exponentially fewer training examples and shallower architectures compared to AR models, which often fail to converge without specific curriculum adjustments.",
    "published": "2026-02-23T15:47:27Z",
    "updated": "2026-02-23T15:47:27Z",
    "link": "http://arxiv.org/pdf/2602.19980v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Itamar Trainin",
      "Shauli Ravfogel",
      "Omri Abend",
      "Amir Feder"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.15077v4",
    "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
    "summary": "While Large Language Models (LLMs) have advanced the state-of-the-art in Text-to-SQL, robust reasoning in complex, multi-table environments remains a bottleneck for parameter-efficient models. This paper presents a systematic empirical study on injecting reasoning capabilities into Text-to-SQL through the lens of Reinforcement Learning with Verifiable Rewards (RLVR). We uncover a critical interplay between reward density, advantage scaling, and model capacity. Our analysis yields four primary insights. First, we propose a novel execution-guided dense reward function that significantly outperforms binary signals and existing state-of-the-art rewards by providing granular feedback at the instance level. Second, we analyze the mechanics of advantage calculation, demonstrating that while large models thrive on sparse signals with aggressive advantage scaling, smaller models require dense rewards and conservative scaling to improve Text-to-SQL performance. Third, we evaluate the impact of cold start, showing that distillation does not always improve RLVR performance and that supervised, fine-tuned models are prone to distributional mimicry. Fourth, we map the Pareto frontier of training efficiency, providing insights for optimizing Text-to-SQL reasoning under computational constraints. Our findings culminate in the Think2SQL family: our 4B-parameter model demonstrates reasoning capabilities competitive with state-of-the-art models such as o3. We release our models, datasets, and code to create a blueprint for RLVR optimization in Text-to-SQL at https://anonymous.4open.science/r/Think2SQL-3B7F.",
    "published": "2025-04-21T13:05:26Z",
    "updated": "2026-02-23T15:30:05Z",
    "link": "http://arxiv.org/pdf/2504.15077v4.pdf",
    "category": [
      "cs.LG",
      "cs.DB"
    ],
    "authors": [
      "Simone Papicchio",
      "Simone Rossi",
      "Luca Cagliero",
      "Paolo Papotti"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19967v1",
    "title": "Unlearning Noise in PINNs: A Selective Pruning Framework for PDE Inverse Problems",
    "summary": "Physics-informed neural networks (PINNs) provide a promising framework for solving inverse problems governed by partial differential equations (PDEs) by integrating observational data and physical constraints in a unified optimization objective. However, the ill-posed nature of PDE inverse problems makes them highly sensitive to noise. Even a small fraction of corrupted observations can distort internal neural representations, severely impairing accuracy and destabilizing training. Motivated by recent advances in machine unlearning and structured network pruning, we propose P-PINN, a selective pruning framework designed to unlearn the influence of corrupted data in a pretrained PINN. Specifically, starting from a PINN trained on the full dataset, P-PINN evaluates a joint residual--data fidelity indicator, a weighted combination of data misfit and PDE residuals, to partition the training set into reliable and corrupted subsets. Next, we introduce a bias-based neuron importance measure that quantifies directional activation discrepancies between the two subsets, identifying neurons whose representations are predominantly driven by corrupted samples. Building on this, an iterative pruning strategy then removes noise-sensitive neurons layer by layer. The resulting pruned network is fine-tuned on the reliable data subject to the original PDE constraints, acting as a lightweight post-processing stage rather than a complete retraining. Numerical experiments on extensive PDE inverse-problem benchmarks demonstrate that P-PINN substantially improves robustness, accuracy, and training stability under noisy conditions, achieving up to a 96.6\\% reduction in relative error compared with baseline PINNs. These results indicate that activation-level post hoc pruning is a promising mechanism for enhancing the reliability of physics-informed learning in noise-contaminated settings.",
    "published": "2026-02-23T15:29:50Z",
    "updated": "2026-02-23T15:29:50Z",
    "link": "http://arxiv.org/pdf/2602.19967v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yongsheng Chen",
      "Yong Chen",
      "Wei Guo",
      "Xinghui Zhong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19956v1",
    "title": "Sparse Masked Attention Policies for Reliable Generalization",
    "summary": "In reinforcement learning, abstraction methods that remove unnecessary information from the observation are commonly used to learn policies which generalize better to unseen tasks. However, these methods often overlook a crucial weakness: the function which extracts the reduced-information representation has unknown generalization ability in unseen observations. In this paper, we address this problem by presenting an information removal method which more reliably generalizes to new states. We accomplish this by using a learned masking function which operates on, and is integrated with, the attention weights within an attention-based policy network. We demonstrate that our method significantly improves policy generalization to unseen tasks in the Procgen benchmark compared to standard PPO and masking approaches.",
    "published": "2026-02-23T15:23:17Z",
    "updated": "2026-02-23T15:23:17Z",
    "link": "http://arxiv.org/pdf/2602.19956v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Caroline Horsch",
      "Laurens Engwegen",
      "Max Weltevrede",
      "Matthijs T. J. Spaan",
      "Wendelin Böhmer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21807v5",
    "title": "MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation",
    "summary": "Statistical learning methods for automated variable selection, such as LASSO, elastic nets, or gradient boosting, have become increasingly popular tools for building powerful prediction models. Yet, in practice, analyses are often complicated by missing data. The most widely used approach to address missingness is multiple imputation, which involves creating several completed datasets. However, there is an ongoing debate on how to perform model selection in the presence of multiple imputed datasets. Simple strategies, such as pooling models across datasets, have been shown to have suboptimal properties. Although more sophisticated methods exist, they are often difficult to implement and therefore not widely applied. In contrast, two recent approaches modify the regularization methods LASSO and elastic nets by defining a single loss function, resulting in a unified set of coefficients across imputations. Our key contribution is to extend this principle to the framework of component-wise gradient boosting by proposing MIBoost, a novel algorithm that employs a uniform variable-selection mechanism across imputed datasets. Simulation studies suggest that our approach yields prediction performance comparable to that of these recently proposed methods.",
    "published": "2025-07-29T13:42:38Z",
    "updated": "2026-02-23T15:11:27Z",
    "link": "http://arxiv.org/pdf/2507.21807v5.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Robert Kuchen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19938v1",
    "title": "A Replicate-and-Quantize Strategy for Plug-and-Play Load Balancing of Sparse Mixture-of-Experts LLMs",
    "summary": "Sparse Mixture-of-Experts (SMoE) architectures are increasingly used to scale large language models efficiently, delivering strong accuracy under fixed compute budgets. However, SMoE models often suffer from severe load imbalance across experts, where a small subset of experts receives most tokens while others are underutilized. Prior work has focused mainly on training-time solutions such as routing regularization or auxiliary losses, leaving inference-time behavior, which is critical for deployment, less explored.\n  We present a systematic analysis of expert routing during inference and identify three findings: (i) load imbalance persists and worsens with larger batch sizes, (ii) selection frequency does not reliably reflect expert importance, and (iii) overall expert workload and importance can be estimated using a small calibration set. These insights motivate inference-time mechanisms that rebalance workloads without retraining or router modification.\n  We propose Replicate-and-Quantize (R&Q), a training-free and near-lossless framework for dynamic workload rebalancing. In each layer, heavy-hitter experts are replicated to increase parallel capacity, while less critical experts and replicas are quantized to remain within the original memory budget. We also introduce a Load-Imbalance Score (LIS) to measure routing skew by comparing heavy-hitter load to an equal allocation baseline. Experiments across representative SMoE models and benchmarks show up to 1.4x reduction in imbalance with accuracy maintained within +/-0.6%, enabling more predictable and efficient inference.",
    "published": "2026-02-23T15:11:16Z",
    "updated": "2026-02-23T15:11:16Z",
    "link": "http://arxiv.org/pdf/2602.19938v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zijie Liu",
      "Jie Peng",
      "Jinhao Duan",
      "Zirui Liu",
      "Kaixiong Zhou",
      "Mingfu Liang",
      "Luke Simon",
      "Xi Liu",
      "Zhaozhuo Xu",
      "Tianlong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.01809v3",
    "title": "Much Ado About Noising: Dispelling the Myths of Generative Robotic Control",
    "summary": "Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/",
    "published": "2025-12-01T15:44:53Z",
    "updated": "2026-02-23T15:07:01Z",
    "link": "http://arxiv.org/pdf/2512.01809v3.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Chaoyi Pan",
      "Giri Anantharaman",
      "Nai-Chieh Huang",
      "Claire Jin",
      "Daniel Pfrommer",
      "Chenyang Yuan",
      "Frank Permenter",
      "Guannan Qu",
      "Nicholas Boffi",
      "Guanya Shi",
      "Max Simchowitz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19918v1",
    "title": "RobPI: Robust Private Inference against Malicious Client",
    "summary": "The increased deployment of machine learning inference in various applications has sparked privacy concerns. In response, private inference (PI) protocols have been created to allow parties to perform inference without revealing their sensitive data. Despite recent advances in the efficiency of PI, most current methods assume a semi-honest threat model where the data owner is honest and adheres to the protocol. However, in reality, data owners can have different motivations and act in unpredictable ways, making this assumption unrealistic. To demonstrate how a malicious client can compromise the semi-honest model, we first designed an inference manipulation attack against a range of state-of-the-art private inference protocols. This attack allows a malicious client to modify the model output with 3x to 8x fewer queries than current black-box attacks. Motivated by the attacks, we proposed and implemented RobPI, a robust and resilient private inference protocol that withstands malicious clients. RobPI integrates a distinctive cryptographic protocol that bolsters security by weaving encryption-compatible noise into the logits and features of private inference, thereby efficiently warding off malicious-client attacks. Our extensive experiments on various neural networks and datasets show that RobPI achieves ~91.9% attack success rate reduction and increases more than 10x the number of queries required by malicious-client attacks.",
    "published": "2026-02-23T14:58:08Z",
    "updated": "2026-02-23T14:58:08Z",
    "link": "http://arxiv.org/pdf/2602.19918v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Jiaqi Xue",
      "Mengxin Zheng",
      "Qian Lou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19917v1",
    "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning",
    "summary": "Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \\TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.",
    "published": "2026-02-23T14:57:52Z",
    "updated": "2026-02-23T14:57:52Z",
    "link": "http://arxiv.org/pdf/2602.19917v1.pdf",
    "category": [
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Thanh Nguyen",
      "Tung Luu",
      "Tri Ton",
      "Sungwoong Kim",
      "Chang D. Yoo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19915v1",
    "title": "Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction",
    "summary": "Understanding and predicting microstructure evolution is fundamental to materials science, as it governs the resulting properties and performance of materials. Traditional simulation methods, such as phase-field models, offer high-fidelity results but are computationally expensive due to the need to solve complex partial differential equations at fine spatiotemporal resolutions. To address this challenge, we propose a deep learning-based framework that accelerates microstructure evolution predictions while maintaining high accuracy. Our approach utilizes a fully convolutional spatiotemporal model trained in a self-supervised manner using sequential images generated from simulations of microstructural processes, including grain growth and spinodal decomposition. The trained neural network effectively learns the underlying physical dynamics and can accurately capture both short-term local behaviors and long-term statistical properties of evolving microstructures, while also demonstrating generalization to unseen spatiotemporal domains and variations in configuration and material parameters. Compared to recurrent neural architectures, our model achieves state-of-the-art predictive performance with significantly reduced computational cost in both training and inference. This work establishes a robust baseline for spatiotemporal learning in materials science and offers a scalable, data-driven alternative for fast and reliable microstructure simulations.",
    "published": "2026-02-23T14:55:28Z",
    "updated": "2026-02-23T14:55:28Z",
    "link": "http://arxiv.org/pdf/2602.19915v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Michael Trimboli",
      "Mohammed Alsubaie",
      "Sirani M. Perera",
      "Ke-Gang Wang",
      "Xianqi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19912v1",
    "title": "De novo molecular structure elucidation from mass spectra via flow matching",
    "summary": "Mass spectrometry is a powerful and widely used tool for identifying molecular structures due to its sensitivity and ability to profile complex samples. However, translating spectra into full molecular structures is a difficult, under-defined inverse problem. Overcoming this problem is crucial for enabling biological insight, discovering new metabolites, and advancing chemical research across multiple fields. To this end, we develop MSFlow, a two-stage encoder-decoder flow-matching generative model that achieves state-of-the-art performance on the structure elucidation task for small molecules. In the first stage, we adopt a formula-restricted transformer model for encoding mass spectra into a continuous and chemically informative embedding space, while in the second stage, we train a decoder flow matching model to reconstruct molecules from latent embeddings of mass spectra. We present ablation studies demonstrating the importance of using information-preserving molecular descriptors for encoding mass spectra and motivate the use of our discrete flow-based decoder. Our rigorous evaluation demonstrates that MSFlow can accurately translate up to 45 percent of molecular mass spectra into their corresponding molecular representations - an improvement of up to fourteen-fold over the current state-of-the-art. A trained version of MSFlow is made publicly available on GitHub for non-commercial users.",
    "published": "2026-02-23T14:52:53Z",
    "updated": "2026-02-23T14:52:53Z",
    "link": "http://arxiv.org/pdf/2602.19912v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ghaith Mqawass",
      "Tuan Le",
      "Fabian Theis",
      "Djork-Arné Clevert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.23348v2",
    "title": "Persistent Homology via Finite Topological Spaces",
    "summary": "We propose a functorial framework for persistent homology based on finite topological spaces and their associated posets. Starting from a finite metric space, we associate a filtration of finite topologies whose structure maps are continuous identity maps. By passing functorially to posets and to order complexes, we obtain persistence modules without requiring inclusion relations between the resulting complexes. We show that standard poset-level simplifications preserve persistent invariants and establish stability of the resulting persistence diagrams under perturbations of the input metric in a basic density-based instantiation, illustrating how stability arguments arise naturally in our framework. We further introduce a concrete density-guided construction, designed to be faithful to anchor neighborhood structure at each scale, and demonstrate its practical viability through an implementation tested on real datasets.",
    "published": "2025-12-29T10:14:56Z",
    "updated": "2026-02-23T14:47:12Z",
    "link": "http://arxiv.org/pdf/2512.23348v2.pdf",
    "category": [
      "math.AT",
      "cs.CG",
      "cs.LG"
    ],
    "authors": [
      "Selçuk Kayacan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19903v1",
    "title": "Rethinking Chronological Causal Discovery with Signal Processing",
    "summary": "Causal discovery problems use a set of observations to deduce causality between variables in the real world, typically to answer questions about biological or physical systems. These observations are often recorded at regular time intervals, determined by a user or a machine, depending on the experiment design. There is generally no guarantee that the timing of these recordings matches the timing of the underlying biological or physical events. In this paper, we examine the sensitivity of causal discovery methods to this potential mismatch. We consider empirical and theoretical evidence to understand how causal discovery performance is impacted by changes of sampling rate and window length. We demonstrate that both classical and recent causal discovery methods exhibit sensitivity to these hyperparameters, and we discuss how ideas from signal processing may help us understand these phenomena.",
    "published": "2026-02-23T14:43:15Z",
    "updated": "2026-02-23T14:43:15Z",
    "link": "http://arxiv.org/pdf/2602.19903v1.pdf",
    "category": [
      "eess.SP",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Kurt Butler",
      "Damian Machlanski",
      "Panagiotis Dimitrakopoulos",
      "Sotirios A. Tsaftaris"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19893v1",
    "title": "Generalized Random Direction Newton Algorithms for Stochastic Optimization",
    "summary": "We present a family of generalized Hessian estimators of the objective using random direction stochastic approximation (RDSA) by utilizing only noisy function measurements. The form of each estimator and the order of the bias depend on the number of function measurements. In particular, we demonstrate that estimators with more function measurements exhibit lower-order estimation bias. We show the asymptotic unbiasedness of the estimators. We also perform asymptotic and non-asymptotic convergence analyses for stochastic Newton methods that incorporate our generalized Hessian estimators. Finally, we perform numerical experiments to validate our theoretical findings.",
    "published": "2026-02-23T14:33:39Z",
    "updated": "2026-02-23T14:33:39Z",
    "link": "http://arxiv.org/pdf/2602.19893v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Soumen Pachal",
      "Prashanth L. A.",
      "Shalabh Bhatnagar",
      "Avinash Achar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.08535v4",
    "title": "Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds",
    "summary": "Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and the pathology of anticipatory control. In this work, we introduce the Causal Schrodinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. By leveraging diffusion processes (SDEs), CSB enables probability mass to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes exactly into local, robust transitions. This theorem provides a principled resolution to the Information Bottleneck that plagues monolithic architectures in high dimensions. We empirically validate CSB on a full-rank causal system (d=10^5, intrinsic rank 10^5), where standard structure-blind MLPs fail to converge (MSE ~0.31). By physically implementing the structural decomposition, CSB achieves high-fidelity transport (MSE ~0.06) in just 73.73 seconds on a single GPU. This stands in stark contrast to structure-agnostic O(d^3) baselines, estimated to require over 6 years. Our results demonstrate that CSB breaks the Curse of Dimensionality through structural intelligence, offering a scalable foundation for high-stakes causal discovery in 10^5-node systems. Code is available at: https://github.com/cochran1/causal-schrodinger-bridge",
    "published": "2026-02-09T11:33:12Z",
    "updated": "2026-02-23T14:27:48Z",
    "link": "http://arxiv.org/pdf/2602.08535v4.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Rui Wu",
      "Li YongJun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19859v1",
    "title": "Dirichlet Scale Mixture Priors for Bayesian Neural Networks",
    "summary": "Neural networks are the cornerstone of modern machine learning, yet can be difficult to interpret, give overconfident predictions and are vulnerable to adversarial attacks. Bayesian neural networks (BNNs) provide some alleviation of these limitations, but have problems of their own. The key step of specifying prior distributions in BNNs is no trivial task, yet is often skipped out of convenience. In this work, we propose a new class of prior distributions for BNNs, the Dirichlet scale mixture (DSM) prior, that addresses current limitations in Bayesian neural networks through structured, sparsity-inducing shrinkage. Theoretically, we derive general dependence structures and shrinkage results for DSM priors and show how they manifest under the geometry induced by neural networks. In experiments on simulated and real world data we find that the DSM priors encourages sparse networks through implicit feature selection, show robustness under adversarial attacks and deliver competitive predictive performance with substantially fewer effective parameters. In particular, their advantages appear most pronounced in correlated, moderately small data regimes, and are more amenable to weight pruning. Moreover, by adopting heavy-tailed shrinkage mechanisms, our approach aligns with recent findings that such priors can mitigate the cold posterior effect, offering a principled alternative to the commonly used Gaussian priors.",
    "published": "2026-02-23T13:58:16Z",
    "updated": "2026-02-23T13:58:16Z",
    "link": "http://arxiv.org/pdf/2602.19859v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "August Arnstad",
      "Leiv Rønneberg",
      "Geir Storvik"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19851v1",
    "title": "Orthogonal Uplift Learning with Permutation-Invariant Representations for Combinatorial Treatments",
    "summary": "We study uplift estimation for combinatorial treatments. Uplift measures the pure incremental causal effect of an intervention (e.g., sending a coupon or a marketing message) on user behavior, modeled as a conditional individual treatment effect. Many real-world interventions are combinatorial: a treatment is a policy that specifies context-dependent action distributions rather than a single atomic label. Although recent work considers structured treatments, most methods rely on categorical or opaque encodings, limiting robustness and generalization to rare or newly deployed policies. We propose an uplift estimation framework that aligns treatment representation with causal semantics. Each policy is represented by the mixture it induces over contextaction components and embedded via a permutation-invariant aggregation. This representation is integrated into an orthogonalized low-rank uplift model, extending Robinson-style decompositions to learned, vector-valued treatments. We show that the resulting estimator is expressive for policy-induced causal effects, orthogonally robust to nuisance estimation errors, and stable under small policy perturbations. Experiments on large-scale randomized platform data demonstrate improved uplift accuracy and stability in long-tailed policy regimes",
    "published": "2026-02-23T13:54:12Z",
    "updated": "2026-02-23T13:54:12Z",
    "link": "http://arxiv.org/pdf/2602.19851v1.pdf",
    "category": [
      "stat.ME",
      "cs.LG"
    ],
    "authors": [
      "Xinyan Su",
      "Jiacan Gao",
      "Mingyuan Ma",
      "Xiao Xu",
      "Xinrui Wan",
      "Tianqi Gu",
      "Enyun Yu",
      "Jiecheng Guo",
      "Zhiheng Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19845v1",
    "title": "I Dropped a Neural Net",
    "summary": "A recent Dwarkesh Patel podcast with John Collison and Elon Musk featured an interesting puzzle from Jane Street: they trained a neural net, shuffled all 96 layers, and asked to put them back in order.\n  Given unlabelled layers of a Residual Network and its training dataset, we recover the exact ordering of the layers. The problem decomposes into pairing each block's input and output projections ($48!$ possibilities) and ordering the reassembled blocks ($48!$ possibilities), for a combined search space of $(48!)^2 \\approx 10^{122}$, which is more than the atoms in the observable universe. We show that stability conditions during training like dynamic isometry leave the product $W_{\\text{out}} W_{\\text{in}}$ for correctly paired layers with a negative diagonal structure, allowing us to use diagonal dominance ratio as a signal for pairing. For ordering, we seed-initialize with a rough proxy such as delta-norm or $\\|W_{\\text{out}}\\|_F$ then hill-climb to zero mean squared error.",
    "published": "2026-02-23T13:49:05Z",
    "updated": "2026-02-23T13:49:05Z",
    "link": "http://arxiv.org/pdf/2602.19845v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hyunwoo Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.19375v2",
    "title": "$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation",
    "summary": "Two-time-scale stochastic approximation (SA) is an algorithm with coupled iterations which has found broad applications in reinforcement learning, optimization and game control. In this work, we derive mean squared error bounds for non-linear two-time-scale iterations with contractive mappings. In the setting where both stepsizes are order $Θ(1/k)$, commonly referred to as single time-scale SA with multiple coupled sequences, we obtain the first $O(1/k)$ rate without imposing additional smoothness assumptions. In the setting with true time-scale separation, the previous best bound was $O(1/k^{2/3})$. We improve this to $O(1/k^a)$ for any $a<1$ approaching the optimal $O(1/k)$ rate. The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence whose variance decays sufficiently fast. Additionally, we use an induction-based approach to show that the iterates are bounded in expectation. Our results apply to Polyak averaging, as well as to algorithms from reinforcement learning, and optimization, including gradient descent-ascent and two-time-scale Lagrangian optimization.",
    "published": "2025-04-27T22:45:00Z",
    "updated": "2026-02-23T13:35:08Z",
    "link": "http://arxiv.org/pdf/2504.19375v2.pdf",
    "category": [
      "cs.LG",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Siddharth Chandak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.01839v2",
    "title": "Optimal Local Convergence Rates of Stochastic First-Order Methods under Local $α$-PL",
    "summary": "We study the local convergence rate of stochastic first-order methods under a local $α$-Polyak-Lojasiewicz ($α$-PL) condition in a neighborhood of a target connected component $\\mathcal{M}$ of the local minimizer set. The parameter $α\\in [1,2]$ is the exponent of the gradient norm in the $α$-PL inequality: $α=2$ recovers the classical PL case, $α=1$ corresponds to Holder-type error bounds, and intermediate values interpolate between these regimes. Our performance criterion is the number of oracle queries required to output $\\hat{x}$ with $F(\\hat{x})-l \\le \\varepsilon$, where $l := F(y)$ for any $y \\in \\mathcal{M}$. We work in a local regime where the algorithm is initialized near $\\mathcal{M}$ and, with high probability, its iterates remain in that neighborhood. We establish a lower bound $Ω(\\varepsilon^{-2/α})$ for all stochastic first-order methods in this regime, and we obtain a matching upper bound $\\mathcal{O}(\\varepsilon^{-2/α})$ for $1 \\le α< 2$ via a SARAH-type variance-reduced method with time-varying batch sizes and step sizes. In the convex setting, assuming a local $α$-PL condition on the $\\varepsilon$-sublevel set, we further show a complexity lower bound $\\widetildeΩ(\\varepsilon^{-2/α})$ for reaching an $\\varepsilon$-global optimum, matching the $\\varepsilon$-dependence of known accelerated stochastic subgradient methods.",
    "published": "2024-08-03T18:34:23Z",
    "updated": "2026-02-23T13:29:24Z",
    "link": "http://arxiv.org/pdf/2408.01839v2.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Saeed Masiha",
      "Saber Salehkaleybar",
      "Niao He",
      "Negar Kiyavash",
      "Patrick Thiran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.10572v2",
    "title": "Probability Bounding: Post-Hoc Calibration via Box-Constrained Softmax",
    "summary": "Many studies have observed that modern neural networks achieve high accuracy while producing poorly calibrated probabilities, making calibration a critical practical issue. In this work, we propose probability bounding (PB), a novel post-hoc calibration method that mitigates both underconfidence and overconfidence by learning lower and upper bounds on the output probabilities. To implement PB, we introduce the box-constrained softmax (BCSoftmax) function, a generalization of Softmax that explicitly enforces lower and upper bounds on the output probabilities. While BCSoftmax is formulated as the solution to a box-constrained optimization problem, we develop an exact and efficient algorithm for computing BCSoftmax. We further provide theoretical guarantees for PB and introduce two variants of PB. We demonstrate the effectiveness of our methods experimentally on four real-world datasets, consistently reducing calibration errors. Our Python implementation is available at https://github.com/neonnnnn/torchbcsoftmax.",
    "published": "2025-06-12T11:01:43Z",
    "updated": "2026-02-23T13:23:01Z",
    "link": "http://arxiv.org/pdf/2506.10572v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Kyohei Atarashi",
      "Satoshi Oyama",
      "Hiromi Arai",
      "Hisashi Kashima"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.05295v2",
    "title": "Performance Estimation in Binary Classification Using Calibrated Confidence",
    "summary": "Model monitoring is a critical component of the machine learning lifecycle, safeguarding against undetected drops in the model's performance after deployment. Traditionally, performance monitoring has required access to ground truth labels, which are not always readily available. This can result in unacceptable latency or render performance monitoring altogether impossible. Recently, methods designed to estimate the accuracy of classifier models without access to labels have shown promising results. However, there are various other metrics that might be more suitable for assessing model performance in many cases. Until now, none of these important metrics has received similar interest from the scientific community. In this work, we address this gap by presenting CBPE, a novel method that can estimate any binary classification metric defined using the confusion matrix. In particular, we choose four metrics from this large family: accuracy, precision, recall, and F$_1$, to demonstrate our method. CBPE treats the elements of the confusion matrix as random variables and leverages calibrated confidence scores of the model to estimate their distributions. The desired metric is then also treated as a random variable, whose full probability distribution can be derived from the estimated confusion matrix. CBPE is shown to produce estimates that come with strong theoretical guarantees and valid confidence intervals.",
    "published": "2025-05-08T14:34:44Z",
    "updated": "2026-02-23T13:16:47Z",
    "link": "http://arxiv.org/pdf/2505.05295v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Juhani Kivimäki",
      "Jakub Białek",
      "Wojtek Kuberski",
      "Jukka K. Nurminen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19799v1",
    "title": "Path-conditioned training: a principled way to rescale ReLU neural networks",
    "summary": "Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.",
    "published": "2026-02-23T12:55:48Z",
    "updated": "2026-02-23T12:55:48Z",
    "link": "http://arxiv.org/pdf/2602.19799v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Arthur Lebeurrier",
      "Titouan Vayer",
      "Rémi Gribonval"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.07102v4",
    "title": "Effectively Leveraging Momentum Terms in Stochastic Line Search Frameworks for Fast Optimization of Finite-Sum Problems",
    "summary": "In this work, we address unconstrained finite-sum optimization problems, with particular focus on instances originating in large scale deep learning scenarios. Our main interest lies in the exploration of the relationship between recent line search approaches for stochastic optimization in the overparametrized regime and momentum directions. First, we point out that combining these two elements with computational benefits is not straightforward. To this aim, we propose a solution based on mini-batch persistency. We then introduce an algorithmic framework that exploits a mix of data persistency, conjugate-gradient type rules for the definition of the momentum parameter and stochastic line searches. The resulting algorithm provably possesses convergence properties under suitable assumptions and is empirically shown to outperform other popular methods from the literature, obtaining state-of-the-art results in both convex and nonconvex large scale training problems.",
    "published": "2024-11-11T16:26:33Z",
    "updated": "2026-02-23T12:55:28Z",
    "link": "http://arxiv.org/pdf/2411.07102v4.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Matteo Lapucci",
      "Davide Pucci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19790v1",
    "title": "Drift Localization using Conformal Predictions",
    "summary": "Concept drift -- the change of the distribution over time -- poses significant challenges for learning systems and is of central interest for monitoring. Understanding drift is thus paramount, and drift localization -- determining which samples are affected by the drift -- is essential. While several approaches exist, most rely on local testing schemes, which tend to fail in high-dimensional, low-signal settings. In this work, we consider a fundamentally different approach based on conformal predictions. We discuss and show the shortcomings of common approaches and demonstrate the performance of our approach on state-of-the-art image datasets.",
    "published": "2026-02-23T12:46:50Z",
    "updated": "2026-02-23T12:46:50Z",
    "link": "http://arxiv.org/pdf/2602.19790v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Fabian Hinder",
      "Valerie Vaquet",
      "Johannes Brinkrolf",
      "Barbara Hammer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19789v1",
    "title": "Stop Preaching and Start Practising Data Frugality for Responsible Development of AI",
    "summary": "This position paper argues that the machine learning community must move from preaching to practising data frugality for responsible artificial intelligence (AI) development. For long, progress has been equated with ever-larger datasets, driving remarkable advances but now yielding increasingly diminishing performance gains alongside rising energy use and carbon emissions. While awareness of data frugal approaches has grown, their adoption has remained rhetorical, and data scaling continues to dominate development practice. We argue that this gap between preach and practice must be closed, as continued data scaling entails substantial and under-accounted environmental impacts. To ground our position, we provide indicative estimates of the energy use and carbon emissions associated with the downstream use of ImageNet-1K. We then present empirical evidence that data frugality is both practical and beneficial, demonstrating that coreset-based subset selection can substantially reduce training energy consumption with little loss in accuracy, while also mitigating dataset bias. Finally, we outline actionable recommendations for moving data frugality from rhetorical preach to concrete practice for responsible development of AI.",
    "published": "2026-02-23T12:46:23Z",
    "updated": "2026-02-23T12:46:23Z",
    "link": "http://arxiv.org/pdf/2602.19789v1.pdf",
    "category": [
      "cs.LG",
      "cs.CY"
    ],
    "authors": [
      "Sophia N. Wilson",
      "Guðrún Fjóla Guðmundsdóttir",
      "Andrew Millard",
      "Raghavendra Selvan",
      "Sebastian Mair"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19788v1",
    "title": "Bayesian Meta-Learning with Expert Feedback for Task-Shift Adaptation through Causal Embeddings",
    "summary": "Meta-learning methods perform well on new within-distribution tasks but often fail when adapting to out-of-distribution target tasks, where transfer from source tasks can induce negative transfer. We propose a causally-aware Bayesian meta-learning method, by conditioning task-specific priors on precomputed latent causal task embeddings, enabling transfer based on mechanistic similarity rather than spurious correlations. Our approach explicitly considers realistic deployment settings where access to target-task data is limited, and adaptation relies on noisy (expert-provided) pairwise judgments of causal similarity between source and target tasks. We provide a theoretical analysis showing that conditioning on causal embeddings controls prior mismatch and mitigates negative transfer under task shift. Empirically, we demonstrate reductions in negative transfer and improved out-of-distribution adaptation in both controlled simulations and a large-scale real-world clinical prediction setting for cross-disease transfer, where causal embeddings align with underlying clinical mechanisms.",
    "published": "2026-02-23T12:44:22Z",
    "updated": "2026-02-23T12:44:22Z",
    "link": "http://arxiv.org/pdf/2602.19788v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Lotta Mäkinen",
      "Jorge Loría",
      "Samuel Kaski"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19785v1",
    "title": "Unsupervised Anomaly Detection in NSL-KDD Using $β$-VAE: A Latent Space and Reconstruction Error Approach",
    "summary": "As Operational Technology increasingly integrates with Information Technology, the need for Intrusion Detection Systems becomes more important. This paper explores an unsupervised approach to anomaly detection in network traffic using $β$-Variational Autoencoders on the NSL-KDD dataset. We investigate two methods: leveraging the latent space structure by measuring distances from test samples to the training data projections, and using the reconstruction error as a conventional anomaly detection metric. By comparing these approaches, we provide insights into their respective advantages and limitations in an unsupervised setting. Experimental results highlight the effectiveness of latent space exploitation for classification tasks.",
    "published": "2026-02-23T12:42:00Z",
    "updated": "2026-02-23T12:42:00Z",
    "link": "http://arxiv.org/pdf/2602.19785v1.pdf",
    "category": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "authors": [
      "Dylan Baptiste",
      "Ramla Saddem",
      "Alexandre Philippot",
      "François Foyer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19782v1",
    "title": "Addressing Instrument-Outcome Confounding in Mendelian Randomization through Representation Learning",
    "summary": "Mendelian Randomization (MR) is a prominent observational epidemiological research method designed to address unobserved confounding when estimating causal effects. However, core assumptions -- particularly the independence between instruments and unobserved confounders -- are often violated due to population stratification or assortative mating. Leveraging the increasing availability of multi-environment data, we propose a representation learning framework that exploits cross-environment invariance to recover latent exogenous components of genetic instruments. We provide theoretical guarantees for identifying these latent instruments under various mixing mechanisms and demonstrate the effectiveness of our approach through simulations and semi-synthetic experiments using data from the All of Us Research Hub.",
    "published": "2026-02-23T12:38:26Z",
    "updated": "2026-02-23T12:38:26Z",
    "link": "http://arxiv.org/pdf/2602.19782v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shimeng Huang",
      "Matthew Robinson",
      "Francesco Locatello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19778v1",
    "title": "Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation",
    "summary": "Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.",
    "published": "2026-02-23T12:32:53Z",
    "updated": "2026-02-23T12:32:53Z",
    "link": "http://arxiv.org/pdf/2602.19778v1.pdf",
    "category": [
      "cs.SD",
      "cs.IR",
      "cs.LG",
      "cs.MM"
    ],
    "authors": [
      "Nghia Phan",
      "Rong Jin",
      "Gang Liu",
      "Xiao Dong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19775v1",
    "title": "Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization",
    "summary": "Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.",
    "published": "2026-02-23T12:29:43Z",
    "updated": "2026-02-23T12:29:43Z",
    "link": "http://arxiv.org/pdf/2602.19775v1.pdf",
    "category": [
      "q-bio.QM",
      "cond-mat.stat-mech",
      "cs.LG",
      "physics.comp-ph",
      "q-bio.MN"
    ],
    "authors": [
      "Jose M. G. Vilar",
      "Leonor Saiz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19761v1",
    "title": "Ensemble Machine Learning and Statistical Procedures for Dynamic Predictions of Time-to-Event Outcomes",
    "summary": "Dynamic predictions for longitudinal and time-to-event outcomes have become a versatile tool in precision medicine. Our work is motivated by the application of dynamic predictions in the decision-making process for primary biliary cholangitis patients. For these patients, serial biomarker measurements (e.g., bilirubin and alkaline phosphatase levels) are routinely collected to inform treating physicians of the risk of liver failure and guide clinical decision-making. Two popular statistical approaches to derive dynamic predictions are joint modelling and landmarking. However, recently, machine learning techniques have also been proposed. Each approach has its merits, and no single method exists to outperform all others. Consequently, obtaining the best possible survival estimates is challenging. Therefore, we extend the Super Learner framework to combine dynamic predictions from different models and procedures. Super Learner is an ensemble learning technique that allows users to combine different prediction algorithms to improve predictive accuracy and flexibility. It uses cross-validation and different objective functions of performance (e.g., squared loss) that suit specific applications to build the optimally weighted combination of predictions from a library of candidate algorithms. In our work, we pay special attention to appropriate objective functions for Super Learner to obtain the most optimal weighted combination of dynamic predictions. In our primary biliary cholangitis application, Super Learner presented unique benefits due to its ability to flexibly combine outputs from a diverse set of models with varying assumptions for equal or better predictive performance than any model fit separately.",
    "published": "2026-02-23T12:12:13Z",
    "updated": "2026-02-23T12:12:13Z",
    "link": "http://arxiv.org/pdf/2602.19761v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "authors": [
      "Nina van Gerwen",
      "Sten Willemsen",
      "Bettina E. Hansen",
      "Christophe Corpechot",
      "Marco Carbone",
      "Cynthia Levy",
      "Maria-Carlota Londõno",
      "Atsushi Tanaka",
      "Palak Trivedi",
      "Alejandra Villamil",
      "Gideon Hirschfield",
      "Dimitris Rizopoulos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.05493v2",
    "title": "Exact and Heuristic Algorithms for Constrained Biclustering",
    "summary": "Biclustering, also known as co-clustering or two-way clustering, simultaneously partitions the rows and columns of a data matrix to reveal submatrices with coherent patterns. Incorporating background knowledge into clustering to enhance solution quality and interpretability has attracted growing interest in mathematical optimization and machine learning research. Extending this paradigm to biclustering enables prior information to guide the joint grouping of rows and columns. We study constrained biclustering with pairwise constraints, namely must-link and cannot-link constraints, which specify whether objects should belong to the same or different biclusters. As a model problem, we address the constrained version of the k-densest disjoint biclique problem, which aims to identify k disjoint complete bipartite subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing the total density while satisfying pairwise constraints. We propose both exact and heuristic algorithms. The exact approach is a tailored branch-and-cut algorithm based on a low-dimensional semidefinite programming (SDP) relaxation, strengthened with valid inequalities and solved in a cutting-plane fashion. Exploiting integer programming tools, a rounding scheme converts SDP solutions into feasible biclusterings at each node. For large-scale instances, we introduce an efficient heuristic based on the low-rank factorization of the SDP. The resulting nonlinear optimization problem is tackled with an augmented Lagrangian method, where the subproblem is solved by decomposition through a block-coordinate projected gradient algorithm. Extensive experiments on synthetic and real-world datasets show that the exact method significantly outperforms general-purpose solvers, while the heuristic achieves high-quality solutions efficiently on large instances.",
    "published": "2025-08-07T15:29:22Z",
    "updated": "2026-02-23T11:51:33Z",
    "link": "http://arxiv.org/pdf/2508.05493v2.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Antonio M. Sudoso"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19733v1",
    "title": "Understanding the Curse of Unrolling",
    "summary": "Algorithm unrolling is ubiquitous in machine learning, particularly in hyperparameter optimization and meta-learning, where Jacobians of solution mappings are computed by differentiating through iterative algorithms. Although unrolling is known to yield asymptotically correct Jacobians under suitable conditions, recent work has shown that the derivative iterates may initially diverge from the true Jacobian, a phenomenon known as the curse of unrolling. In this work, we provide a non-asymptotic analysis that explains the origin of this behavior and identifies the algorithmic factors that govern it. We show that truncating early iterations of the derivative computation mitigates the curse while simultaneously reducing memory requirements. Finally, we demonstrate that warm-starting in bilevel optimization naturally induces an implicit form of truncation, providing a practical remedy. Our theoretical findings are supported by numerical experiments on representative examples.",
    "published": "2026-02-23T11:32:39Z",
    "updated": "2026-02-23T11:32:39Z",
    "link": "http://arxiv.org/pdf/2602.19733v1.pdf",
    "category": [
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Sheheryar Mehmood",
      "Florian Knoll",
      "Peter Ochs"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22295v5",
    "title": "Aurora: Towards Universal Generative Multimodal Time Series Forecasting",
    "summary": "Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Cross-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corresponding text or image modalities, thus possessing strong cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on 5 well-recognized benchmarks, including TimeMMD, TSFM-Bench, ProbTS, TFB, and EPF, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.",
    "published": "2025-09-26T12:56:20Z",
    "updated": "2026-02-23T11:29:56Z",
    "link": "http://arxiv.org/pdf/2509.22295v5.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xingjian Wu",
      "Jianxin Jin",
      "Wanghui Qiu",
      "Peng Chen",
      "Yang Shu",
      "Bin Yang",
      "Chenjuan Guo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.13851v2",
    "title": "Inverting Self-Organizing Maps: A Unified Activation-Based Framework",
    "summary": "Self-Organizing Maps (SOMs) provide topology-preserving projections of high-dimensional data, yet their use as generative models remains largely unexplored. We show that the activation pattern of a SOM -- the squared distances to its prototypes -- can be \\emph{inverted} to recover the exact input, following from a classical result in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which inversion is well-posed. Building on this mechanism, we introduce the \\emph{Manifold-Aware Unified SOM Inversion and Control} (MUSIC) update rule, which modifies squared distances to selected prototypes while preserving others, producing controlled, semantically meaningful trajectories aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update and ensures smooth motion in high dimensions. Unlike variational or diffusion-based generative models, MUSIC requires no sampling, latent priors, or learned decoders: it operates entirely on prototype geometry. If no perturbation is applied, inversion recovers the exact input; when a target prototype or cluster is specified, MUSIC produces coherent semantic transitions. We validate the framework on synthetic Gaussian mixtures, MNIST digits, and the Labeled Faces in the Wild dataset. Across all settings, MUSIC trajectories maintain high classifier confidence, produce significantly sharper intermediate images than linear interpolation, and reveal an interpretable geometric structure of the learned map.",
    "published": "2026-01-20T11:02:54Z",
    "updated": "2026-02-23T11:03:39Z",
    "link": "http://arxiv.org/pdf/2601.13851v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Alessandro Londei",
      "Matteo Benati",
      "Denise Lanzieri",
      "Vittorio Loreto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.19073v3",
    "title": "CARMA: Collocation-Aware Resource Manager",
    "summary": "GPUs running deep learning (DL) workloads are frequently underutilized. Collocating multiple DL training tasks on the same GPU can improve utilization but introduces two key risks: (1) out-of-memory (OOM) crashes for newly scheduled tasks, and (2) severe performance interference among co-running tasks, which can negate any throughput gains. These issues reduce system robustness, quality of service, and energy efficiency. We present CARMA, a task-level, collocation-aware resource manager for the server-scale. CARMA addresses collocation challenges via (1) fine-grained monitoring and bookkeeping of GPUs and a collocation risk analysis that filters out the high-risk GPUs; (2) task placement policies that cap GPU utilization to limit OOMs and interference; (3) integration of GPU memory need estimators for DL tasks to minimize OOMs during collocation; and (4) a lightweight recovery method that relaunches jobs crashed due to OOMs. Our evaluation on a DL training workload derived from real-world traces shows that CARMA uses GPUs more efficiently by making more informed collocation decisions: for the best-performing collocation policy, CARMA increases GPU streaming multiprocessor (SM) utilization by 54%, the parallelism achieved per SM by 61%, and memory use by 62%. This results in a ~35% and ~15% reduction in the end-to-end execution time (makespan) and GPU energy consumption, respectively, for this workload.",
    "published": "2025-08-26T14:29:34Z",
    "updated": "2026-02-23T11:03:09Z",
    "link": "http://arxiv.org/pdf/2508.19073v3.pdf",
    "category": [
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "authors": [
      "Ehsan Yousefzadeh-Asl-Miandoab",
      "Florina M. Ciorba",
      "Pınar Tözün"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.14934v2",
    "title": "Activation-Space Uncertainty Quantification for Pretrained Networks",
    "summary": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.",
    "published": "2026-02-16T17:17:08Z",
    "updated": "2026-02-23T10:54:32Z",
    "link": "http://arxiv.org/pdf/2602.14934v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Richard Bergna",
      "Stefan Depeweg",
      "Sergio Calvo-Ordoñez",
      "Jonathan Plenk",
      "Alvaro Cartea",
      "Jose Miguel Hernández-Lobato"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19691v1",
    "title": "Smoothness Adaptivity in Constant-Depth Neural Networks: Optimal Rates via Smooth Activations",
    "summary": "Smooth activation functions are ubiquitous in modern deep learning, yet their theoretical advantages over non-smooth counterparts remain poorly understood. In this work, we characterize both approximation and statistical properties of neural networks with smooth activations over the Sobolev space $W^{s,\\infty}([0,1]^d)$ for arbitrary smoothness $s>0$. We prove that constant-depth networks equipped with smooth activations automatically exploit arbitrarily high orders of target function smoothness, achieving the minimax-optimal approximation and estimation error rates (up to logarithmic factors). In sharp contrast, networks with non-smooth activations, such as ReLU, lack this adaptivity: their attainable approximation order is strictly limited by depth, and capturing higher-order smoothness requires proportional depth growth. These results identify activation smoothness as a fundamental mechanism, alternative to depth, for attaining statistical optimality. Technically, our results are established via a constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. This complexity control ensures statistical learnability under empirical risk minimization (ERM) and removes the impractical sparsity constraints commonly required in prior analyses.",
    "published": "2026-02-23T10:38:12Z",
    "updated": "2026-02-23T10:38:12Z",
    "link": "http://arxiv.org/pdf/2602.19691v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.NA"
    ],
    "authors": [
      "Yuhao Liu",
      "Zilin Wang",
      "Lei Wu",
      "Shaobo Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.16642v2",
    "title": "Optimizer choice matters for the emergence of Neural Collapse",
    "summary": "Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.",
    "published": "2026-02-18T17:32:43Z",
    "updated": "2026-02-23T10:22:26Z",
    "link": "http://arxiv.org/pdf/2602.16642v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jim Zhao",
      "Tin Sum Cheng",
      "Wojciech Masarczyk",
      "Aurelien Lucchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19661v1",
    "title": "PaReGTA: An LLM-based EHR Data Encoding Approach to Capture Temporal Information",
    "summary": "Temporal information in structured electronic health records (EHRs) is often lost in sparse one-hot or count-based representations, while sequence models can be costly and data-hungry. We propose PaReGTA, an LLM-based encoding framework that (i) converts longitudinal EHR events into visit-level templated text with explicit temporal cues, (ii) learns domain-adapted visit embeddings via lightweight contrastive fine-tuning of a sentence-embedding model, and (iii) aggregates visit embeddings into a fixed-dimensional patient representation using hybrid temporal pooling that captures both recency and globally informative visits. Because PaReGTA does not require training from scratch but instead utilizes a pre-trained LLM, it can perform well even in data-limited cohorts. Furthermore, PaReGTA is model-agnostic and can benefit from future EHR-specialized sentence-embedding models. For interpretability, we introduce PaReGTA-RSS (Representation Shift Score), which quantifies clinically defined factor importance by recomputing representations after targeted factor removal and projecting representation shifts through a machine learning model. On 39,088 migraine patients from the All of Us Research Program, PaReGTA outperforms sparse baselines for migraine type classification while deep sequential models were unstable in our cohort.",
    "published": "2026-02-23T10:09:50Z",
    "updated": "2026-02-23T10:09:50Z",
    "link": "http://arxiv.org/pdf/2602.19661v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Kihyuk Yoon",
      "Lingchao Mao",
      "Catherine Chong",
      "Todd J. Schwedt",
      "Chia-Chun Chiang",
      "Jing Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19644v1",
    "title": "Spectral Phase Encoding for Quantum Kernel Methods",
    "summary": "Quantum kernel methods are promising for near-term quantum ma- chine learning, yet their behavior under data corruption remains insuf- ficiently understood. We analyze how quantum feature constructions degrade under controlled additive noise. We introduce Spectral Phase Encoding (SPE), a hybrid construc- tion combining a discrete Fourier transform (DFT) front-end with a diagonal phase-only embedding aligned with the geometry of diagonal quantum maps. Within a unified framework, we compare QK-DFT against alternative quantum variants (QK-PCA, QK-RP) and classi- cal SVM baselines under identical clean-data hyperparameter selection, quantifying robustness via dataset fixed-effects regression with wild cluster bootstrap inference across heterogeneous real-world datasets. Across the quantum family, DFT-based preprocessing yields the smallest degradation rate as noise increases, with statistically sup- ported slope differences relative to PCA and RP. Compared to classical baselines, QK-DFT shows degradation comparable to linear SVM and more stable than RBF SVM under matched tuning. Hardware exper- iments confirm that SPE remains executable and numerically stable for overlap estimation. These results indicate that robustness in quan- tum kernels depends critically on structure-aligned preprocessing and its interaction with diagonal embeddings, supporting a robustness-first perspective for NISQ-era quantum machine learning.",
    "published": "2026-02-23T09:42:42Z",
    "updated": "2026-02-23T09:42:42Z",
    "link": "http://arxiv.org/pdf/2602.19644v1.pdf",
    "category": [
      "cs.LG",
      "quant-ph"
    ],
    "authors": [
      "Pablo Herrero Gómez",
      "Antonio Jimeno Morenilla",
      "David Muñoz-Hernández",
      "Higinio Mora Mora"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19641v1",
    "title": "Evaluating the Impact of Data Anonymization on Image Retrieval",
    "summary": "With the growing importance of privacy regulations such as the General Data Protection Regulation, anonymizing visual data is becoming increasingly relevant across institutions. However, anonymization can negatively affect the performance of Computer Vision systems that rely on visual features, such as Content-Based Image Retrieval (CBIR). Despite this, the impact of anonymization on CBIR has not been systematically studied. This work addresses this gap, motivated by the DOKIQ project, an artificial intelligence-based system for document verification actively used by the State Criminal Police Office Baden-Württemberg. We propose a simple evaluation framework: retrieval results after anonymization should match those obtained before anonymization as closely as possible. To this end, we systematically assess the impact of anonymization using two public datasets and the internal DOKIQ dataset. Our experiments span three anonymization methods, four anonymization degrees, and four training strategies, all based on the state of the art backbone Self-Distillation with No Labels (DINO)v2. Our results reveal a pronounced retrieval bias in favor of models trained on original data, which produce the most similar retrievals after anonymization. The findings of this paper offer practical insights for developing privacy-compliant CBIR systems while preserving performance.",
    "published": "2026-02-23T09:39:06Z",
    "updated": "2026-02-23T09:39:06Z",
    "link": "http://arxiv.org/pdf/2602.19641v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Marvin Chen",
      "Manuel Eberhardinger",
      "Johannes Maucher"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.24183v5",
    "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "summary": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while even exceeding the performance of 671B DeepSeek-R1 on RTLLM. We have released our model, training code, and dataset to facilitate research in EDA and LLM communities.",
    "published": "2025-05-30T03:51:06Z",
    "updated": "2026-02-23T09:24:51Z",
    "link": "http://arxiv.org/pdf/2505.24183v5.pdf",
    "category": [
      "cs.LG",
      "cs.AR",
      "cs.PL"
    ],
    "authors": [
      "Yaoyu Zhu",
      "Di Huang",
      "Hanqi Lyu",
      "Xiaoyun Zhang",
      "Chongxiao Li",
      "Wenxuan Shi",
      "Yutong Wu",
      "Jianan Mu",
      "Jinghua Wang",
      "Yang Zhao",
      "Pengwei Jin",
      "Shuyao Cheng",
      "Shengwen Liang",
      "Xishan Zhang",
      "Rui Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu",
      "Yunji Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19619v1",
    "title": "Is Your Diffusion Sampler Actually Correct? A Sampler-Centric Evaluation of Discrete Diffusion Language Models",
    "summary": "Discrete diffusion language models (dLLMs) provide a fast and flexible alternative to autoregressive models (ARMs) via iterative denoising with parallel updates. However, their evaluation is challenging: existing metrics conflate denoiser approximation error with sampler-induced error from the sampling dynamics, a problem that does not arise for ARMs whose autoregressive sampling exactly reflects the learned probability model. We introduce a sampler-centric oracle framework that replaces learned denoisers with an exact Hidden Markov Model posterior derived from a ground-truth Markov chain, isolating sampler-induced error in a controlled setting. We show that few-step discrete diffusion samplers are not distributionally correct even under an oracle denoiser, with transition-level mismatch that vanishes only as the number of steps approaches the sequence length. Moreover, improvements in negative log-likelihood, generative perplexity, or MAUVE do not imply correct sampling. Code is available at https://luhantang.github.io/dllm_sampler",
    "published": "2026-02-23T09:06:13Z",
    "updated": "2026-02-23T09:06:13Z",
    "link": "http://arxiv.org/pdf/2602.19619v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Luhan Tang",
      "Longxuan Yu",
      "Shaorong Zhang",
      "Greg Ver Steeg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19614v1",
    "title": "Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering",
    "summary": "The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic (\"big-bang\") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.",
    "published": "2026-02-23T09:02:38Z",
    "updated": "2026-02-23T09:02:38Z",
    "link": "http://arxiv.org/pdf/2602.19614v1.pdf",
    "category": [
      "cs.SE",
      "cs.LG"
    ],
    "authors": [
      "Chih-Hong Cheng",
      "Brian Hsuan-Cheng Liao",
      "Adam Molin",
      "Hasan Esen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.00502v2",
    "title": "Diffusion Alignment as Variational Expectation-Maximization",
    "summary": "Diffusion alignment aims to optimize diffusion models for the downstream objective. While existing methods based on reinforcement learning or direct backpropagation achieve considerable success in maximizing rewards, they often suffer from reward over-optimization and mode collapse. We introduce Diffusion Alignment as Variational Expectation-Maximization (DAV), a framework that formulates diffusion alignment as an iterative process alternating between two complementary phases: the E-step and the M-step. In the E-step, we employ test-time search to generate diverse and reward-aligned samples. In the M-step, we refine the diffusion model using samples discovered by the E-step. We demonstrate that DAV can optimize reward while preserving diversity for both continuous and discrete tasks: text-to-image synthesis and DNA sequence design. Our code is available at https://github.com/Jaewoopudding/dav.",
    "published": "2025-10-01T04:34:07Z",
    "updated": "2026-02-23T08:58:39Z",
    "link": "http://arxiv.org/pdf/2510.00502v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Jaewoo Lee",
      "Minsu Kim",
      "Sanghyeok Choi",
      "Inhyuck Song",
      "Sujin Yun",
      "Hyeongyu Kang",
      "Woocheol Shin",
      "Taeyoung Yun",
      "Kiyoung Om",
      "Jinkyoo Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18945v3",
    "title": "MIST: Mutual Information Estimation Via Supervised Training",
    "summary": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
    "published": "2025-11-24T09:55:28Z",
    "updated": "2026-02-23T08:55:36Z",
    "link": "http://arxiv.org/pdf/2511.18945v3.pdf",
    "category": [
      "cs.LG",
      "cs.IT"
    ],
    "authors": [
      "German Gritsai",
      "Megan Richards",
      "Maxime Méloux",
      "Kyunghyun Cho",
      "Maxime Peyrard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19610v1",
    "title": "Variational Inference for Bayesian MIDAS Regression",
    "summary": "We develop a Coordinate Ascent Variational Inference (CAVI) algorithm for Bayesian Mixed Data Sampling (MIDAS) regression with linear weight parameteri zations. The model separates impact coe cients from weighting function parameters through a normalization constraint, creating a bilinear structure that renders generic Hamiltonian Monte Carlo samplers unreliable while preserving conditional conju gacy exploitable by CAVI. Each variational update admits a closed-form solution: Gaussian for regression coe cients and weight parameters, Inverse-Gamma for the error variance. The algorithm propagates uncertainty across blocks through second moments, distinguishing it from naive plug-in approximations. In a Monte Carlo study spanning 21 data-generating con gurations with up to 50 predictors, CAVI produces posterior means nearly identical to a block Gibbs sampler benchmark while achieving speedups of 107x to 1,772x (Table 9). Generic automatic di eren tiation VI (ADVI), by contrast, produces bias 714 times larger while being orders of magnitude slower, con rming the value of model-speci c derivations. Weight function parameters maintain excellent calibration (coverage above 92%) across all con gurations. Impact coe cient credible intervals exhibit the underdispersion characteristic of mean- eld approximations, with coverage declining from 89% to 55% as the number of predictors grows a documented trade-o between speed and interval calibration that structured variational methods can address. An empirical application to realized volatility forecasting on S&P 500 daily returns con rms that CAVI and Gibbs sampling yield virtually identical point forecasts, with CAVI completing each monthly estimation in under 10 milliseconds.",
    "published": "2026-02-23T08:51:26Z",
    "updated": "2026-02-23T08:51:26Z",
    "link": "http://arxiv.org/pdf/2602.19610v1.pdf",
    "category": [
      "cs.LG",
      "stat.CO",
      "stat.ME",
      "stat.ML"
    ],
    "authors": [
      "Luigi Simeone"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19600v1",
    "title": "Manifold-Aligned Generative Transport",
    "summary": "High-dimensional generative modeling is fundamentally a manifold-learning problem: real data concentrate near a low-dimensional structure embedded in the ambient space. Effective generators must therefore balance support fidelity -- placing probability mass near the data manifold -- with sampling efficiency. Diffusion models often capture near-manifold structure but require many iterative denoising steps and can leak off-support; normalizing flows sample in one pass but are limited by invertibility and dimension preservation. We propose MAGT (Manifold-Aligned Generative Transport), a flow-like generator that learns a one-shot, manifold-aligned transport from a low-dimensional base distribution to the data space. Training is performed at a fixed Gaussian smoothing level, where the score is well-defined and numerically stable. We approximate this fixed-level score using a finite set of latent anchor points with self-normalized importance sampling, yielding a tractable objective. MAGT samples in a single forward pass, concentrates probability near the learned support, and induces an intrinsic density with respect to the manifold volume measure, enabling principled likelihood evaluation for generated samples. We establish finite-sample Wasserstein bounds linking smoothing level and score-approximation accuracy to generative fidelity, and empirically improve fidelity and manifold concentration across synthetic and benchmark datasets while sampling substantially faster than diffusion models.",
    "published": "2026-02-23T08:42:40Z",
    "updated": "2026-02-23T08:42:40Z",
    "link": "http://arxiv.org/pdf/2602.19600v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Xinyu Tian",
      "Xiaotong Shen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2301.00201v4",
    "title": "Exploring Singularities in point clouds with the graph Laplacian: An explicit approach",
    "summary": "We develop theory and methods that use the graph Laplacian to analyze the geometry of the underlying manifold of datasets. Our theory provides theoretical guarantees and explicit bounds on the functional forms of the graph Laplacian when it acts on functions defined close to singularities of the underlying manifold. We use these explicit bounds to develop tests for singularities and propose methods that can be used to estimate geometric properties of singularities in the datasets.",
    "published": "2022-12-31T13:48:42Z",
    "updated": "2026-02-23T08:41:22Z",
    "link": "http://arxiv.org/pdf/2301.00201v4.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.DG"
    ],
    "authors": [
      "Martin Andersson",
      "Benny Avelin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19594v1",
    "title": "ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?",
    "summary": "We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model.",
    "published": "2026-02-23T08:37:53Z",
    "updated": "2026-02-23T08:37:53Z",
    "link": "http://arxiv.org/pdf/2602.19594v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ayush Nangia",
      "Shikhar Mishra",
      "Aman Gokrani",
      "Paras Chopra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19582v1",
    "title": "Advantage-based Temporal Attack in Reinforcement Learning",
    "summary": "Extensive research demonstrates that Deep Reinforcement Learning (DRL) models are susceptible to adversarially constructed inputs (i.e., adversarial examples), which can mislead the agent to take suboptimal or unsafe actions. Recent methods improve attack effectiveness by leveraging future rewards to guide adversarial perturbation generation over sequential time steps (i.e., reward-based attacks). However, these methods are unable to capture dependencies between different time steps in the perturbation generation process, resulting in a weak temporal correlation between the current perturbation and previous perturbations.In this paper, we propose a novel method called Advantage-based Adversarial Transformer (AAT), which can generate adversarial examples with stronger temporal correlations (i.e., time-correlated adversarial examples) to improve the attack performance. AAT employs a multi-scale causal self-attention (MSCSA) mechanism to dynamically capture dependencies between historical information from different time periods and the current state, thus enhancing the correlation between the current perturbation and the previous perturbation. Moreover, AAT introduces a weighted advantage mechanism, which quantifies the effectiveness of a perturbation in a given state and guides the generation process toward high-performance adversarial examples by sampling high-advantage regions. Extensive experiments demonstrate that the performance of AAT matches or surpasses mainstream adversarial attack baselines on Atari, DeepMind Control Suite and Google football tasks.",
    "published": "2026-02-23T08:08:23Z",
    "updated": "2026-02-23T08:08:23Z",
    "link": "http://arxiv.org/pdf/2602.19582v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Shenghong He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19580v1",
    "title": "Leap+Verify: Regime-Adaptive Speculative Weight Prediction for Accelerating Neural Network Training",
    "summary": "We introduce Leap+Verify, a framework that applies speculative execution -- predicting future model weights and validating predictions before acceptance -- to accelerate neural network training. Inspired by speculative decoding in language model inference and by the Automatically Scalable Computation (ASC) architecture for program execution, Leap+Verify decomposes training into three dynamically detected regimes (chaotic, transition, stable) using activation-space cosine similarity as a real-time Lyapunov proxy signal. Within each regime, analytic weight predictors (momentum, linear, quadratic extrapolation) attempt to forecast model parameters K training steps ahead; predictions are accepted only when validated against a held-out loss criterion. We evaluate Leap+Verify on GPT-2 124M and Qwen 2.5-1.5B trained on WikiText-103 across five random seeds, sweeping prediction depth K in {5, 10, 25, 50, 75, 100}. Momentum-based prediction (Adam moment extrapolation) fails catastrophically at both scales, with predicted losses exceeding actuals by 100-10,000x -- a universal norm explosion in optimizer-state extrapolation. Finite-difference predictors (linear, quadratic) succeed where momentum fails: at 124M, they achieve 24% strict acceptance at K=5 in stable regimes; at 1.5B, they achieve 37% strict acceptance in transition regimes. The scale-dependent finding is in regime distribution: GPT-2 124M spends 34% of training in stable regime, while Qwen 1.5B spends 64% in chaotic regime and reaches stable in only 0-2 of 40 checkpoints. Larger models are more predictable when predictable, but less often predictable -- the practical bottleneck shifts from predictor accuracy to regime availability. Cross-seed results are highly consistent (less than 1% validation loss variance), and the three-regime framework produces identical phase boundaries (plus or minus 50 steps) across seeds.",
    "published": "2026-02-23T08:01:44Z",
    "updated": "2026-02-23T08:01:44Z",
    "link": "http://arxiv.org/pdf/2602.19580v1.pdf",
    "category": [
      "cs.LG",
      "econ.GN"
    ],
    "authors": [
      "Jeremy McEntire"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19578v1",
    "title": "Goal-Oriented Influence-Maximizing Data Acquisition for Learning and Optimization",
    "summary": "Active data acquisition is central to many learning and optimization tasks in deep neural networks, yet remains challenging because most approaches rely on predictive uncertainty estimates that are difficult to obtain reliably. To this end, we propose Goal-Oriented Influence- Maximizing Data Acquisition (GOIMDA), an active acquisition algorithm that avoids explicit posterior inference while remaining uncertainty-aware through inverse curvature. GOIMDA selects inputs by maximizing their expected influence on a user-specified goal functional, such as test loss, predictive entropy, or the value of an optimizer-recommended design. Leveraging first-order influence functions, we derive a tractable acquisition rule that combines the goal gradient, training-loss curvature, and candidate sensitivity to model parameters. We show theoretically that, for generalized linear models, GOIMDA approximates predictive-entropy minimization up to a correction term accounting for goal alignment and prediction bias, thereby, yielding uncertainty-aware behavior without maintaining a Bayesian posterior. Empirically, across learning tasks (including image and text classification) and optimization tasks (including noisy global optimization benchmarks and neural-network hyperparameter tuning), GOIMDA consistently reaches target performance with substantially fewer labeled samples or function evaluations than uncertainty-based active learning and Gaussian-process Bayesian optimization baselines.",
    "published": "2026-02-23T07:57:11Z",
    "updated": "2026-02-23T07:57:11Z",
    "link": "http://arxiv.org/pdf/2602.19578v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Weichi Yao",
      "Bianca Dumitrascu",
      "Bryan R. Goldsmith",
      "Yixin Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19552v1",
    "title": "The Sample Complexity of Replicable Realizable PAC Learning",
    "summary": "In this paper, we consider the problem of replicable realizable PAC learning. We construct a particularly hard learning problem and show a sample complexity lower bound with a close to $(\\log|H|)^{3/2}$ dependence on the size of the hypothesis class $H$. Our proof uses several novel techniques and works by defining a particular Cayley graph associated with $H$ and analyzing a suitable random walk on this graph by examining the spectral properties of its adjacency matrix.\n  Furthermore, we show an almost matching upper bound for the lower bound instance, meaning if a stronger lower bound exists, one would have to consider a different instance of the problem.",
    "published": "2026-02-23T06:52:11Z",
    "updated": "2026-02-23T06:52:11Z",
    "link": "http://arxiv.org/pdf/2602.19552v1.pdf",
    "category": [
      "cs.LG",
      "cs.CC",
      "cs.DS"
    ],
    "authors": [
      "Kasper Green Larsen",
      "Markus Engelund Mathiasen",
      "Chirag Pabbaraju",
      "Clement Svendsen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.10996v8",
    "title": "Mixed-Reality Digital Twins: Leveraging the Physical and Virtual Worlds for Hybrid Sim2Real Transition of Multi-Agent Reinforcement Learning Policies",
    "summary": "Multi-agent reinforcement learning (MARL) for cyber-physical vehicle systems usually requires a significantly long training time due to their inherent complexity. Furthermore, deploying the trained policies in the real world demands a feature-rich environment along with multiple physical embodied agents, which may not be feasible due to monetary, physical, energy, or safety constraints. This work seeks to address these pain points by presenting a mixed-reality (MR) digital twin (DT) framework capable of: (i) boosting training speeds by selectively scaling parallelized simulation workloads on-demand, and (ii) immersing the MARL policies across hybrid simulation-to-reality (sim2real) experiments. The viability and performance of the proposed framework are highlighted through two representative use cases, which cover cooperative as well as competitive classes of MARL problems. We study the effect of: (i) agent and environment parallelization on training time, and (ii) systematic domain randomization on zero-shot sim2real transfer, across both case studies. Results indicate up to 76.3% reduction in training time with the proposed parallelization scheme and sim2real gap as low as 2.9% using the proposed deployment method.",
    "published": "2024-03-16T18:47:04Z",
    "updated": "2026-02-23T06:49:35Z",
    "link": "http://arxiv.org/pdf/2403.10996v8.pdf",
    "category": [
      "cs.RO",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Chinmay Vilas Samak",
      "Tanmay Vilas Samak",
      "Venkat Narayan Krovi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11891v2",
    "title": "Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?",
    "summary": "We study A/B experiments that are designed to compare the performance of two recommendation algorithms. Prior work has observed that the stable unit treatment value assumption (SUTVA) often does not hold in large-scale recommendation systems, and hence the estimate for the global treatment effect (GTE) is biased. Specifically, units under the treatment and control algorithms contribute to a shared pool of data that subsequently train both algorithms, resulting in interference between the two groups. In this paper, we investigate when such interference may affect our decision making on which algorithm is better. We formalize this insight under a multi-armed bandit framework and theoretically characterize when the sign of the difference-in-means estimator of the GTE under data sharing aligns with or contradicts the sign of the true GTE. Our analysis identifies the level of exploration versus exploitation as a key determinant of how data sharing impacts decision making, and we propose a detection procedure based on ramp-up experiments to signal incorrect algorithm comparison in practice.",
    "published": "2025-07-16T04:14:52Z",
    "updated": "2026-02-23T06:14:15Z",
    "link": "http://arxiv.org/pdf/2507.11891v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Shuangning Li",
      "Chonghuan Wang",
      "Jingyan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19528v1",
    "title": "Beyond Accuracy: A Unified Random Matrix Theory Diagnostic Framework for Crash Classification Models",
    "summary": "Crash classification models in transportation safety are typically evaluated using accuracy, F1, or AUC, metrics that cannot reveal whether a model is silently overfitting. We introduce a spectral diagnostic framework grounded in Random Matrix Theory (RMT) and Heavy-Tailed Self-Regularization (HTSR) that spans the ML taxonomy: weight matrices for BERT/ALBERT/Qwen2.5, out-of-fold increment matrices for XGBoost/Random Forest, empirical Hessians for Logistic Regression, induced affinity matrices for Decision Trees, and Graph Laplacians for KNN. Evaluating nine model families on two Iowa DOT crash classification tasks (173,512 and 371,062 records respectively), we find that the power-law exponent $α$ provides a structural quality signal: well-regularized models consistently yield $α$ within $[2, 4]$ (mean $2.87 \\pm 0.34$), while overfit variants show $α< 2$ or spectral collapse. We observe a strong rank correlation between $α$ and expert agreement (Spearman $ρ= 0.89$, $p < 0.001$), suggesting spectral quality captures model behaviors aligned with expert reasoning. We propose an $α$-based early stopping criterion and a spectral model selection protocol, and validate both against cross-validated F1 baselines. Sparse Lanczos approximations make the framework scalable to large datasets.",
    "published": "2026-02-23T05:42:54Z",
    "updated": "2026-02-23T05:42:54Z",
    "link": "http://arxiv.org/pdf/2602.19528v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Ibne Farabi Shihab",
      "Sanjeda Akter",
      "Anuj Sharma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.14208v2",
    "title": "Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws",
    "summary": "Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.",
    "published": "2026-02-15T16:06:45Z",
    "updated": "2026-02-23T05:35:42Z",
    "link": "http://arxiv.org/pdf/2602.14208v2.pdf",
    "category": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Jinbo Wang",
      "Binghui Li",
      "Zhanpeng Zhou",
      "Mingze Wang",
      "Yuxuan Sun",
      "Jiaqi Zhang",
      "Xunliang Cai",
      "Lei Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.26376v2",
    "title": "Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings",
    "summary": "Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal predictability and major drivers of extreme weather in winter. Accurate and efficient probabilistic forecasting of these events remains a persistent challenge for Numerical Weather Prediction (NWP) systems due to computational bottlenecks and limitations in physical representation. While data-driven forecasting is rapidly evolving, its application to the complex, three-dimensional dynamics of SSWs remains underexplored. Here, we bridge this gap by developing a Flow Matching-based generative AI model (FM-Cast) for efficient and skillful probabilistic forecasting of the spatiotemporal evolution of stratospheric circulation in winter. Evaluated across 18 major SSW events (1998-2024), FM-Cast successfully forecasts the onset, intensity, and 3D morphology of the polar vortex up to 15 days in advance for most cases. Notably, it achieves long-range probabilistic forecast skill comparable to or exceeding leading operational NWP systems (ECMWF and CMA) while generating a 30-day forecast with 50-member ensemble, in just two minutes on a consumer GPU. Furthermore, using idealized \"perfect troposphere\" experiments, we uncover distinct predictability regimes: events driven by continuous wave forcing versus those governed by an initial trigger and subsequent stratospheric dynamical memory. This work establishes a computationally efficient paradigm for probabilistic stratospheric forecasting that simultaneously deepens our physical understanding of atmosphere-climate dynamics.",
    "published": "2025-10-30T11:16:22Z",
    "updated": "2026-02-23T05:26:01Z",
    "link": "http://arxiv.org/pdf/2510.26376v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ningning Tao",
      "Fei Xie",
      "Baoxiang Pan",
      "Hongyu Wang",
      "Han Huang",
      "Zhongpu Qiu",
      "Ke Gui",
      "Jiali Luo",
      "Xiaosong Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07078v3",
    "title": "E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models",
    "summary": "Speech Foundation Models encounter significant performance degradation when deployed in real-world scenarios involving acoustic domain shifts, such as background noise and speaker accents. Test-time adaptation (TTA) has recently emerged as a viable strategy to address such domain shifts at inference time without requiring access to source data or labels. However, existing TTA approaches, particularly those relying on backpropagation, are memory-intensive, limiting their applicability in speech tasks and resource-constrained settings. Although backpropagation-free methods offer improved efficiency, existing ones exhibit poor accuracy. This is because they are predominantly developed for vision tasks, which fundamentally differ from speech task formulations, noise characteristics, and model architecture, posing unique transferability challenges. In this paper, we introduce E-BATS, the first Efficient BAckpropagation-free TTA framework designed explicitly for speech foundation models. E-BATS achieves a balance between adaptation effectiveness and memory efficiency through three key components: (i) lightweight prompt adaptation for a forward-pass-based feature alignment, (ii) a multi-scale loss to capture both global (utterance-level) and local distribution shifts (token-level) and (iii) a test-time exponential moving average mechanism for stable adaptation across utterances. Experiments conducted on four noisy speech datasets spanning sixteen acoustic conditions demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to backpropagation-based methods. By enabling scalable and robust adaptation under acoustic variability, this work paves the way for developing more efficient adaptation approaches for practical speech processing systems in real-world environments.",
    "published": "2025-06-08T10:33:37Z",
    "updated": "2026-02-23T05:24:58Z",
    "link": "http://arxiv.org/pdf/2506.07078v3.pdf",
    "category": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "authors": [
      "Jiaheng Dong",
      "Hong Jia",
      "Soumyajit Chatterjee",
      "Abhirup Ghosh",
      "James Bailey",
      "Ting Dang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.00774v2",
    "title": "A Novel VAE-DML Fusion Framework for Causal Analysis of Greenwashing in the Mining Industry",
    "summary": "Against the backdrop of the global green transition and \"dual carbon\" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.",
    "published": "2026-01-31T15:32:14Z",
    "updated": "2026-02-23T05:01:16Z",
    "link": "http://arxiv.org/pdf/2602.00774v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yuxin Lu",
      "Zhen Peng",
      "Xiqiang Xia",
      "Jie Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17543v3",
    "title": "MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation",
    "summary": "Music-driven 3D dance generation has attracted increasing attention in recent years, with promising applications in choreography, virtual reality, and creative content creation. Previous research has generated promising realistic dance movement from audio signals. However, traditional methods underutilize genre conditioning, often treating it as auxiliary modifiers rather than core semantic drivers. This oversight compromises music-motion synchronization and disrupts dance genre continuity, particularly during complex rhythmic transitions, thereby leading to visually unsatisfactory effects. To address the challenge, we propose MEGADance, a novel architecture for music-driven 3D dance generation. By decoupling choreographic consistency into dance generality and genre specificity, MEGADance demonstrates significant dance quality and strong genre controllability. It consists of two stages: (1) High-Fidelity Dance Quantization Stage (HFDQ), which encodes dance motions into a latent representation by Finite Scalar Quantization (FSQ) and reconstructs them with kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage (GADG), which maps music into the latent representation by synergistic utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate the state-of-the-art performance of MEGADance both qualitatively and quantitatively. Code is available at https://github.com/XulongT/MEGADance.",
    "published": "2025-05-23T06:47:06Z",
    "updated": "2026-02-23T07:09:36Z",
    "link": "http://arxiv.org/pdf/2505.17543v3.pdf",
    "category": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Kaixing Yang",
      "Xulong Tang",
      "Ziqiao Peng",
      "Yuxuan Hu",
      "Jun He",
      "Hongyan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20063v1",
    "title": "Spherical Hermite Maps",
    "summary": "Spherical functions appear throughout computer graphics, from spherical harmonic lighting and precomputed radiance transfer to neural radiance fields and procedural planet rendering. Efficient evaluation is critical for real-time applications, yet existing approaches face a quality-performance trade-off: bilinear LUT sampling is fast but produces faceting, while bicubic filtering requires 16 texture samples. Most implementations use finite differences for normals, requiring extra samples and introducing noise. This paper presents Spherical Hermite Maps, a derivative-augmented LUT representation that resolves this trade-off. By storing function values alongside scaled partial derivatives at each texel of a padded cubemap, bicubic-Hermite reconstruction is enabled from only four texture samples (a 2x2 footprint) while providing continuous gradients from the same samples. The key insight is that Hermite interpolation reconstructs smooth derivatives as a byproduct of value reconstruction, making surface normals effectively free. In controlled experiments, Spherical Hermite Maps improve PSNR by 8-41 dB over bilinear interpolation and match 16-tap bicubic quality at one-quarter the cost. Analytic normals reduce mean angular error by 9-13% on complex surfaces while yielding stable specular highlights. Three applications demonstrate versatility: spherical harmonic glyph visualization, radial depth-map impostors for mesh level-of-detail, and procedural planet/asteroid rendering with spherical heightfields.",
    "published": "2026-02-23T17:22:19Z",
    "updated": "2026-02-23T17:22:19Z",
    "link": "http://arxiv.org/pdf/2602.20063v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Mohamed Abouagour",
      "Eleftherios Garyfallidis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.04493v2",
    "title": "Continuum Robot State Estimation with Actuation Uncertainty",
    "summary": "Continuum robots are flexible, thin manipulators capable of navigating confined or delicate environments making them well suited for surgical applications. Previous approaches to continuum robot state estimation typically rely on simplified, deterministic actuation models. In contrast, our method jointly estimates robot shape, external loads, internal stresses, and actuation inputs. We adopt a discrete Cosserat rod formulation and show that, when paired with a midpoint integration rule, it achieves high numerical accuracy with relatively few state nodes. This discretization naturally induces a factor-graph structure for sparse nonlinear optimization on SE(3). We extend the formulation with actuation factors for tendon-driven robots and combine multiple rod graphs for parallel continuum robots with closed-loop topologies. By explicitly including actuation variables in the state, the linearized system can be reused to extract manipulator Jacobians, which we leverage in performing trajectory tracking. Finally, we validate the approach experimentally on a surgical concentric tube robot. Overall, our approach enables principled, real-time estimation across multiple continuum robot architectures, accounting for actuation uncertainty and providing direct access to manipulator Jacobians.",
    "published": "2026-01-08T01:53:42Z",
    "updated": "2026-02-23T18:57:12Z",
    "link": "http://arxiv.org/pdf/2601.04493v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "James M. Ferguson",
      "Alan Kuntz",
      "Tucker Hermans"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.20054v1",
    "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement",
    "summary": "This work assesses the hydrodynamic efficiency of Underwater Unmanned Vehicles (UUVs) equipped with soft morphing wings compared to conventional rigid wings. Unlike rigid wings, deformable counterparts can alter their aerodynamic properties on demand. Improvements in hydrodynamic efficiency extend a UUV's operational range and may determine mission feasibility. Structural and Computational Fluid Dynamics (CFD) simulations were conducted for both a soft morphing wing and a UUV incorporating it. The results show that a UUV employing soft wings achieves 9.75 percent higher overall efficiency than an equivalent vehicle with traditional rigid wings. These findings confirm the potential of soft robotics to enhance underwater vehicle performance, particularly in applications requiring pressure-agnostic operation.",
    "published": "2026-02-23T17:04:21Z",
    "updated": "2026-02-23T17:04:21Z",
    "link": "http://arxiv.org/pdf/2602.20054v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "A. Giordano",
      "G. De Meurichy",
      "V. Telazzi",
      "C. Mucignat",
      "I. Lunati",
      "D. A. L. M. Louchard",
      "M. Iovieno",
      "S. F. Armanini",
      "M. Kovac"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26518v2",
    "title": "Memory-Efficient 2D/3D Shape Assembly of Robot Swarms",
    "summary": "Mean-shift-based approaches have recently emerged as a representative class of methods for robot swarm shape assembly. They rely on image-based target-shape representations to compute local density gradients and perform mean-shift exploration, which constitute their core mechanism. However, such representations incur substantial memory overhead, especially for high-resolution or 3D shapes. To address this limitation, we propose a memory-efficient tree representation that hierarchically encodes user-specified shapes in both 2D and 3D. Based on this representation, we design a behavior-based distributed controller for assignment-free shape assembly. Comparative 2D and 3D simulations against a state-of-the-art mean-shift algorithm show one to two orders of magnitude lower memory usage and two to four times faster shape entry. Physical experiments with 6 to 7 UAVs further validate real-world practicality.",
    "published": "2025-09-30T16:54:59Z",
    "updated": "2026-02-23T17:03:31Z",
    "link": "http://arxiv.org/pdf/2509.26518v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Shuoyu Yue",
      "Pengpeng Li",
      "Yang Xu",
      "Kunrui Ze",
      "Xingjian Long",
      "Huazi Cao",
      "Guibin Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19943v1",
    "title": "Scaling Law of Neural Koopman Operators",
    "summary": "Data-driven neural Koopman operator theory has emerged as a powerful tool for linearizing and controlling nonlinear robotic systems. However, the performance of these data-driven models fundamentally depends on the trade-off between sample size and model dimensions, a relationship for which the scaling laws have remained unclear. This paper establishes a rigorous framework to address this challenge by deriving and empirically validating scaling laws that connect sample size, latent space dimension, and downstream control quality. We derive a theoretical upper bound on the Koopman approximation error, explicitly decomposing it into sampling error and projection error. We show that these terms decay at specific rates relative to dataset size and latent dimension, providing a rigorous basis for the scaling law. Based on the theoretical results, we introduce two lightweight regularizers for the neural Koopman operator: a covariance loss to help stabilize the learned latent features and an inverse control loss to ensure the model aligns with physical actuation. The results from systematic experiments across six robotic environments confirm that model fitting error follows the derived scaling laws, and the regularizers improve dynamic model fitting fidelity, with enhanced closed-loop control performance. Together, our results provide a simple recipe for allocating effort between data collection and model capacity when learning Koopman dynamics for control.",
    "published": "2026-02-23T15:13:43Z",
    "updated": "2026-02-23T15:13:43Z",
    "link": "http://arxiv.org/pdf/2602.19943v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Abulikemu Abuduweili",
      "Yuyang Pang",
      "Feihan Li",
      "Changliu Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26308v3",
    "title": "Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation",
    "summary": "Out-of-distribution states in robot manipulation often lead to unpredictable robot behavior or task failure, limiting success rates and increasing risk of damage. Anomaly detection (AD) can identify deviations from expected patterns in data, which can be used to trigger failsafe behaviors and recovery strategies. Prior work has applied data-driven AD on time series data for specific robotic tasks, however the transferability of an AD approach between different robot control strategies and task types has not been shown. Leveraging time series data, such as force/torque signals, allows to directly capture robot-environment interactions, crucial for manipulation and online failure detection. As robotic tasks can have widely signal characteristics and requirements, AD methods which can be applied in the same way to a wide range of tasks is needed, ideally with good data efficiency. We examine three industrial robotic tasks, robotic cabling, screwing, and sanding, each with multi-modal time series data and several anomalies. Several autoencoderbased methods are compared, and we evaluate the generalization across different robotic tasks and control methods (diffusion policy-, position-, and impedance-controlled). This allows us to validate the integration of AD in complex tasks involving tighter tolerances and variation from both the robot and its environment. Additionally, we evaluate data efficiency, detection latency, and task characteristics which support robust detection. The results indicate reliable detection with AUROC exceeding 0.96 in failures in the cabling and screwing task, such as incorrect or misaligned parts and obstructed targets. In the polishing task, only severe failures were reliably detected, while more subtle failure types remained undetected.",
    "published": "2025-09-30T14:22:45Z",
    "updated": "2026-02-23T14:52:42Z",
    "link": "http://arxiv.org/pdf/2509.26308v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Niklas Grambow",
      "Lisa-Marie Fenner",
      "Felipe Kempkes",
      "Philip Hotz",
      "Dingyuan Wan",
      "Jörg Krüger",
      "Kevin Haninger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19898v1",
    "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform",
    "summary": "In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manipulators is limited. UGVs can carry required payloads to assess and manipulate the mission environment, but need to be able to deal with difficult and unstructured terrain such as rubble and stairs. The ability of tracked platforms with articulated arms (flippers) to reconfigure their geometry makes them particularly effective for navigating challenging terrain. In this paper, we present Athena, an open-hardware rescue ground robot research platform with four individually reconfigurable flippers and a reliable low-cost remote emergency stop (E-Stop) solution. A novel mounting solution using an industrial PU belt and tooth inserts allows the replacement and testing of different track profiles. The manipulator with a maximum reach of 1.54m can be used to operate doors, valves, and other objects of interest. Full CAD & PCB files, as well as all low-level software, are released as open-source contributions.",
    "published": "2026-02-23T14:38:23Z",
    "updated": "2026-02-23T14:38:23Z",
    "link": "http://arxiv.org/pdf/2602.19898v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Stefan Fabian",
      "Aljoscha Schmidt",
      "Jonas Süß",
      " Dishant",
      "Aum Oza",
      "Oskar von Stryk"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19862v1",
    "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems",
    "summary": "In-Motion physical coupling of multiple mobile ground robots has the potential to enable new applications like in-motion transfer that improves efficiency in handling and transferring goods, which tackles current challenges in logistics. A key challenge lies in achieving reliable autonomous in-motion physical coupling of two mobile ground robots starting at any initial position. Existing approaches neglect the modeling of the docking interface and the strategy for approaching it, resulting in uncontrolled collisions that make in-motion physical coupling either impossible or inefficient. To address this challenge, we propose a central mpc approach that explicitly models the dynamics and states of two omnidirectional wheeled robots, incorporates constraints related to their docking interface, and implements an approaching strategy for rendezvous and docking. This novel approach enables omnidirectional wheeled robots with a docking interface to physically couple in motion regardless of their initial position. In addition, it makes in-motion transfer possible, which is 19.75% more time- and 21.04% energy-efficient compared to a non-coupling approach in a logistic scenario.",
    "published": "2026-02-23T14:01:37Z",
    "updated": "2026-02-23T14:01:37Z",
    "link": "http://arxiv.org/pdf/2602.19862v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO"
    ],
    "authors": [
      "Lars Fischer",
      "Daniel Flögel",
      "Sören Hohmann"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19850v1",
    "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data",
    "summary": "Real-time prediction of deformation in highly compliant soft materials remains a significant challenge in soft robotics. While vision-based soft tactile sensors can track internal marker displacements, learning-based models for 3D contact estimation heavily depend on their training datasets, inherently limiting their ability to generalize to complex scenarios such as multi-point sensing. To address this limitation, we introduce TactiVerse, a U-Net-based framework that formulates contact geometry estimation as a spatial heatmap prediction task. Even when trained exclusively on a limited dataset of single-point indentations, our architecture achieves highly accurate single-point sensing, yielding a superior mean absolute error of 0.0589 mm compared to the 0.0612 mm of a conventional regression-based CNN baseline. Furthermore, we demonstrate that augmenting the training dataset with multi-point contact data substantially enhances the sensor's multi-point sensing capabilities, significantly improving the overall mean MAE for two-point discrimination from 1.214 mm to 0.383 mm. By successfully extrapolating complex contact geometries from fundamental interactions, this methodology unlocks advanced multi-point and large-area shape sensing. Ultimately, it significantly streamlines the development of marker-based soft sensors, offering a highly scalable solution for real-world tactile perception.",
    "published": "2026-02-23T13:53:14Z",
    "updated": "2026-02-23T13:53:14Z",
    "link": "http://arxiv.org/pdf/2602.19850v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Junhui Lee",
      "Hyosung Kim",
      "Saekwang Nam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19764v1",
    "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling",
    "summary": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.",
    "published": "2026-02-23T12:12:51Z",
    "updated": "2026-02-23T12:12:51Z",
    "link": "http://arxiv.org/pdf/2602.19764v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Yirui Sun",
      "Guangyu Zhuge",
      "Keliang Liu",
      "Jie Gu",
      "Zhihao xia",
      "Qionglin Ren",
      "Chunxu tian",
      "Zhongxue Ga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.12924v2",
    "title": "Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance",
    "summary": "In this letter, we introduce Geometric Model Predictive Path Integral (GMPPI), a sampling-based controller capable of tracking agile trajectories while avoiding obstacles. In each iteration, GMPPI generates a large number of candidate rollout trajectories and then averages them to create a nominal control to be followed by the controlled Unmanned Aerial Vehicle (UAV). Classical Model Predictive Path Integral (MPPI) faces a trade-off between tracking precision and obstacle avoidance; high-noise random rollouts are inefficient for tracking but necessary for collision avoidance. To this end, we propose leveraging geometric SE(3) control to generate a portion of GMPPI rollouts. To maximize their benefit, we introduce a UAV-tailored cost function balancing tracking performance with obstacle avoidance. All generated rollouts are projected onto depth images for collision avoidance, representing, to our knowledge, the first method utilizing depth data directly in a UAV MPPI loop. Simulations show GMPPI matches the tracking error of an obstacle-blind geometric controller while exceeding the avoidance capabilities of state-of-the-art planners and learning-based controllers. Real-world experiments demonstrate flight at speeds up to 17 m/s and obstacle avoidance up to 10 m/s.",
    "published": "2025-10-14T18:56:25Z",
    "updated": "2026-02-23T12:06:20Z",
    "link": "http://arxiv.org/pdf/2510.12924v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Pavel Pochobradský",
      "Ondřej Procházka",
      "Robert Pěnička",
      "Vojtěch Vonásek",
      "Martin Saska"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19699v1",
    "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization",
    "summary": "Trajectory Optimization (TO) and Reinforcement Learning (RL) offer complementary strengths for solving optimal control problems. TO efficiently computes locally optimal solutions but can struggle with non-convexity, while RL is more robust to non-convexity at the cost of significantly higher computational demands. CACTO (Continuous Actor-Critic with Trajectory Optimization) was introduced to combine these advantages by learning a warm-start policy that guides the TO solver towards low-cost trajectories. However, scalability remains a key limitation, as increasing system complexity significantly raises the computational cost of TO. This work introduces CACTO-BIC to address these challenges. CACTO-BIC improves data efficiency by biasing initial-state sampling leveraging a property of the value function associated with locally optimal policies; moreover, it reduces computation time by exploiting GPU acceleration. Empirical evaluations show improved sample efficiency and faster computation compared to CACTO. Comparisons with PPO demonstrate that our approach can achieve similar solutions in less time. Finally, experiments on the AlienGO quadruped robot demonstrate that CACTO-BIC can scale to high-dimensional systems and is suitable for real-time applications.",
    "published": "2026-02-23T10:45:36Z",
    "updated": "2026-02-23T10:45:36Z",
    "link": "http://arxiv.org/pdf/2602.19699v1.pdf",
    "category": [
      "cs.RO",
      "math.OC"
    ],
    "authors": [
      "Elisa Alboni",
      "Pietro Noah Crestaz",
      "Elias Fontanari",
      "Andrea Del Prete"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19653v1",
    "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array",
    "summary": "Distributed Manipulator Systems, composed of arrays of robotic actuators necessitate dense actuator arrays to effectively manipulate small objects. This paper presents a system composed of modular 3-DoF robotic tiles interconnected by a compliant surface layer, forming a continuous, controllable manipulation surface. The compliant layer permits increased actuator spacing without compromising object manipulation capabilities, significantly reducing actuator density while maintaining robust control, even for smaller objects. We characterize the coupled workspace of the array and develop a manipulation strategy capable of translating objects to arbitrary positions within an N X N array. The approach is validated experimentally using a minimal 2 X 2 prototype, demonstrating the successful manipulation of objects with varied shapes and sizes.",
    "published": "2026-02-23T09:54:32Z",
    "updated": "2026-02-23T09:54:32Z",
    "link": "http://arxiv.org/pdf/2602.19653v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Bailey Dacre",
      "Rodrigo Moreno",
      "Jørn Lambertsen",
      "Kasper Stoy",
      "Andrés Faíña"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.17448v2",
    "title": "Switching Among Feedback-Linearizing Output Sets (Melds): Dwell-Time and Compatibility Guarantees",
    "summary": "We study switching among multiple square selections of output functions (melds) drawn from a deck of candidate outputs for nonlinear systems that are static feedback linearizable via outputs. Fixing an operating point, each meld induces a distinct feedback-linearizing coordinate chart defined on a common neighborhood. Switching between melds therefore produces state-dependent coordinate mismatches that are not captured by classical switched-system analyses. We quantify this effect through Lipschitz bounds on the cross-chart maps over a compact safe set and introduce a reference-compatibility constant that measures mismatch among reference families across melds. We derive an explicit dwell-time condition depending on controller decay rates and the compatibility constant, that guarantees exponential decay of the active-output tracking errors between switches, seamless tracking of outputs shared by consecutive melds, and uniform boundedness of the state error within the safe set. A planar 3R manipulator illustrates the results.",
    "published": "2025-10-20T11:35:01Z",
    "updated": "2026-02-23T08:30:19Z",
    "link": "http://arxiv.org/pdf/2510.17448v2.pdf",
    "category": [
      "cs.RO",
      "math.DS"
    ],
    "authors": [
      "Mirko Mizzoni",
      "Pieter van Goor",
      "Barbara Bazzana",
      "Antonio Franchi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19577v1",
    "title": "Chasing Ghosts: A Simulation-to-Real Olfactory Navigation Stack with Optional Vision Augmentation",
    "summary": "Autonomous odor source localization remains a challenging problem for aerial robots due to turbulent airflow, sparse and delayed sensory signals, and strict payload and compute constraints. While prior unmanned aerial vehicle (UAV)-based olfaction systems have demonstrated gas distribution mapping or reactive plume tracing, they rely on predefined coverage patterns, external infrastructure, or extensive sensing and coordination. In this work, we present a complete, open-source UAV system for online odor source localization using a minimal sensor suite. The system integrates custom olfaction hardware, onboard sensing, and a learning-based navigation policy trained in simulation and deployed on a real quadrotor. Through our minimal framework, the UAV is able to navigate directly toward an odor source without constructing an explicit gas distribution map or relying on external positioning systems. Vision is incorporated as an optional complementary modality to accelerate navigation under certain conditions. We validate the proposed system through real-world flight experiments in a large indoor environment using an ethanol source, demonstrating consistent source-finding behavior under realistic airflow conditions. The primary contribution of this work is a reproducible system and methodological framework for UAV-based olfactory navigation and source finding under minimal sensing assumptions. We elaborate on our hardware design and open source our UAV firmware, simulation code, olfaction-vision dataset, and circuit board to the community. Code, data, and designs will be made available at https://github.com/KordelFranceTech/ChasingGhosts.",
    "published": "2026-02-23T07:51:27Z",
    "updated": "2026-02-23T07:51:27Z",
    "link": "http://arxiv.org/pdf/2602.19577v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Kordel K. France",
      "Ovidiu Daescu",
      "Latifur Khan",
      "Rohith Peddi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.13850v3",
    "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement",
    "summary": "We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce Humanoid Hanoi, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines. Project page: https://osudrl.github.io/Humanoid_Hanoi/",
    "published": "2026-02-14T19:11:02Z",
    "updated": "2026-02-23T07:14:01Z",
    "link": "http://arxiv.org/pdf/2602.13850v3.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Minku Kim",
      "Kuan-Chia Chen",
      "Aayam Shrestha",
      "Li Fuxin",
      "Stefan Lee",
      "Alan Fern"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19532v1",
    "title": "Bellman Value Decomposition for Task Logic in Safe Optimal Control",
    "summary": "Real-world tasks involve nuanced combinations of goal and safety specifications. In high dimensions, the challenge is exacerbated: formal automata become cumbersome, and the combination of sparse rewards tends to require laborious tuning. In this work, we consider the innate structure of the Bellman Value as a means to naturally organize the problem for improved automatic performance. Namely, we prove the Bellman Value for a complex task defined in temporal logic can be decomposed into a graph of Bellman Values, connected by a set of well-known Bellman equations (BEs): the Reach-Avoid BE, the Avoid BE, and a novel type, the Reach-Avoid-Loop BE. To solve the Value and optimal policy, we propose VDPPO, which embeds the decomposed Value graph into a two-layer neural net, bootstrapping the implicit dependencies. We conduct a variety of simulated and hardware experiments to test our method on complex, high-dimensional tasks involving heterogeneous teams and nonlinear dynamics. Ultimately, we find this approach greatly improves performance over existing baselines, balancing safety and liveness automatically.",
    "published": "2026-02-23T05:48:58Z",
    "updated": "2026-02-23T05:48:58Z",
    "link": "http://arxiv.org/pdf/2602.19532v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "William Sharpless",
      "Oswin So",
      "Dylan Hirsch",
      "Sylvia Herbert",
      "Chuchu Fan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2602.19518v1",
    "title": "Anticipate, Adapt, Act: A Hybrid Framework for Task Planning",
    "summary": "Anticipating and adapting to failures is a key capability robots need to collaborate effectively with humans in complex domains. This continues to be a challenge despite the impressive performance of state of the art AI planning systems and Large Language Models (LLMs) because of the uncertainty associated with the tasks and their outcomes. Toward addressing this challenge, we present a hybrid framework that integrates the generic prediction capabilities of an LLM with the probabilistic sequential decision-making capability of Relational Dynamic Influence Diagram Language. For any given task, the robot reasons about the task and the capabilities of the human attempting to complete it; predicts potential failures due to lack of ability (in the human) or lack of relevant domain objects; and executes actions to prevent such failures or recover from them. Experimental evaluation in the VirtualHome 3D simulation environment demonstrates substantial improvement in performance compared with state of the art baselines.",
    "published": "2026-02-23T05:18:11Z",
    "updated": "2026-02-23T05:18:11Z",
    "link": "http://arxiv.org/pdf/2602.19518v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Nabanita Dash",
      "Ayush Kaura",
      "Shivam Singh",
      "Ramandeep Singh",
      "Snehasis Banerjee",
      "Mohan Sridharan",
      "K. Madhava Krishna"
    ]
  }
]