[
  {
    "id": "http://arxiv.org/abs/2512.10957v1",
    "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
    "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "link": "http://arxiv.org/pdf/2512.10957v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yukai Shi",
      "Weiyu Li",
      "Zihao Wang",
      "Hongyang Li",
      "Xingyu Chen",
      "Ping Tan",
      "Lei Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10952v1",
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "summary": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "link": "http://arxiv.org/pdf/2512.10952v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Xiaona Zhou",
      "Yingyan Zeng",
      "Ran Jin",
      "Ismini Lourentzou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10949v1",
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "published": "2025-12-11T18:59:52Z",
    "updated": "2025-12-11T18:59:52Z",
    "link": "http://arxiv.org/pdf/2512.10949v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Yiwen Tang",
      "Zoey Guo",
      "Kaixin Zhu",
      "Ray Zhang",
      "Qizhi Chen",
      "Dongzhi Jiang",
      "Junli Liu",
      "Bohan Zeng",
      "Haoming Song",
      "Delin Qu",
      "Tianyi Bai",
      "Dan Xu",
      "Wentao Zhang",
      "Bin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10946v1",
    "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
    "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
    "published": "2025-12-11T18:59:46Z",
    "updated": "2025-12-11T18:59:46Z",
    "link": "http://arxiv.org/pdf/2512.10946v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Wendi Chen",
      "Han Xue",
      "Yi Wang",
      "Fangyuan Zhou",
      "Jun Lv",
      "Yang Jin",
      "Shirun Tang",
      "Chuan Wen",
      "Cewu Lu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10943v1",
    "title": "AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation",
    "summary": "Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT",
    "published": "2025-12-11T18:59:34Z",
    "updated": "2025-12-11T18:59:34Z",
    "link": "http://arxiv.org/pdf/2512.10943v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Sharath Girish",
      "Viacheslav Ivanov",
      "Tsai-Shien Chen",
      "Hao Chen",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10941v1",
    "title": "Mull-Tokens: Modality-Agnostic Latent Thinking",
    "summary": "Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.",
    "published": "2025-12-11T18:59:08Z",
    "updated": "2025-12-11T18:59:08Z",
    "link": "http://arxiv.org/pdf/2512.10941v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Arijit Ray",
      "Ahmed Abdelkader",
      "Chengzhi Mao",
      "Bryan A. Plummer",
      "Kate Saenko",
      "Ranjay Krishna",
      "Leonidas Guibas",
      "Wen-Sheng Chu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10940v1",
    "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
    "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
    "published": "2025-12-11T18:59:05Z",
    "updated": "2025-12-11T18:59:05Z",
    "link": "http://arxiv.org/pdf/2512.10940v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Xiang Fan",
      "Sharath Girish",
      "Vivek Ramanujan",
      "Chaoyang Wang",
      "Ashkan Mirzaei",
      "Petr Sushko",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Ranjay Krishna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10938v1",
    "title": "Stronger Normalization-Free Transformers",
    "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(αx + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "published": "2025-12-11T18:58:49Z",
    "updated": "2025-12-11T18:58:49Z",
    "link": "http://arxiv.org/pdf/2512.10938v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Mingzhi Chen",
      "Taiming Lu",
      "Jiachen Zhu",
      "Mingjie Sun",
      "Zhuang Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10937v1",
    "title": "On Decision-Making Agents and Higher-Order Causal Processes",
    "summary": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.",
    "published": "2025-12-11T18:58:33Z",
    "updated": "2025-12-11T18:58:33Z",
    "link": "http://arxiv.org/pdf/2512.10937v1.pdf",
    "category": [
      "cs.AI",
      "quant-ph"
    ],
    "authors": [
      "Matt Wilson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10936v1",
    "title": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks",
    "summary": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).",
    "published": "2025-12-11T18:58:17Z",
    "updated": "2025-12-11T18:58:17Z",
    "link": "http://arxiv.org/pdf/2512.10936v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Kristina Korotkova",
      "Aleksandr Katrutsa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10935v1",
    "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
    "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
    "published": "2025-12-11T18:57:39Z",
    "updated": "2025-12-11T18:57:39Z",
    "link": "http://arxiv.org/pdf/2512.10935v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Jay Karhade",
      "Nikhil Keetha",
      "Yuchen Zhang",
      "Tanisha Gupta",
      "Akash Sharma",
      "Sebastian Scherer",
      "Deva Ramanan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10932v1",
    "title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models",
    "summary": "Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.",
    "published": "2025-12-11T18:57:05Z",
    "updated": "2025-12-11T18:57:05Z",
    "link": "http://arxiv.org/pdf/2512.10932v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Shengao Wang",
      "Wenqi Wang",
      "Zecheng Wang",
      "Max Whitton",
      "Michael Wakeham",
      "Arjun Chandra",
      "Joey Huang",
      "Pengyue Zhu",
      "Helen Chen",
      "David Li",
      "Jeffrey Li",
      "Shawn Li",
      "Andrew Zagula",
      "Amy Zhao",
      "Andrew Zhu",
      "Sayaka Nakamura",
      "Yuki Yamamoto",
      "Jerry Jun Yokono",
      "Aaron Mueller",
      "Bryan A. Plummer",
      "Kate Saenko",
      "Venkatesh Saligrama",
      "Boqing Gong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10926v1",
    "title": "Decoupled Q-Chunking",
    "summary": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
    "published": "2025-12-11T18:52:51Z",
    "updated": "2025-12-11T18:52:51Z",
    "link": "http://arxiv.org/pdf/2512.10926v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "authors": [
      "Qiyang Li",
      "Seohong Park",
      "Sergey Levine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20101v4",
    "title": "PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving",
    "summary": "While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.",
    "published": "2025-04-27T01:08:25Z",
    "updated": "2025-12-11T18:49:32Z",
    "link": "http://arxiv.org/pdf/2504.20101v4.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Fei Fang",
      "Yifan Hua",
      "Shengze Wang",
      "Ruilin Zhou",
      "Yi Liu",
      "Chen Qian",
      "Xiaoxue Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10922v1",
    "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale",
    "summary": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.",
    "published": "2025-12-11T18:47:48Z",
    "updated": "2025-12-11T18:47:48Z",
    "link": "http://arxiv.org/pdf/2512.10922v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Max Zimmer",
      "Christophe Roux",
      "Moritz Wagner",
      "Deborah Hendrych",
      "Sebastian Pokutta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10903v1",
    "title": "Multi-Granular Node Pruning for Circuit Discovery",
    "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.",
    "published": "2025-12-11T18:32:15Z",
    "updated": "2025-12-11T18:32:15Z",
    "link": "http://arxiv.org/pdf/2512.10903v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Muhammad Umair Haider",
      "Hammad Rizwan",
      "Hassan Sajjad",
      "A. B. Siddique"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10895v1",
    "title": "LLMs Can Assist with Proposal Selection at Large User Facilities",
    "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.",
    "published": "2025-12-11T18:23:56Z",
    "updated": "2025-12-11T18:23:56Z",
    "link": "http://arxiv.org/pdf/2512.10895v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Lijie Ding",
      "Janell Thomson",
      "Jon Taylor",
      "Changwoo Do"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.18156v3",
    "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology",
    "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety",
    "published": "2025-06-22T19:58:19Z",
    "updated": "2025-12-11T18:18:42Z",
    "link": "http://arxiv.org/pdf/2506.18156v3.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Akash Kundu",
      "Rishika Goswami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.09884v3",
    "title": "Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation",
    "summary": "We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first nonasymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\\sqrt{n}$, which significantly improves on the rates of convergence in prior works.",
    "published": "2025-02-14T03:20:30Z",
    "updated": "2025-12-11T18:05:48Z",
    "link": "http://arxiv.org/pdf/2502.09884v3.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Seo Taek Kong",
      "Sihan Zeng",
      "Thinh T. Doan",
      "R. Srikant"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10866v1",
    "title": "UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting",
    "summary": "We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.",
    "published": "2025-12-11T17:59:44Z",
    "updated": "2025-12-11T17:59:44Z",
    "link": "http://arxiv.org/pdf/2512.10866v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Ruslan Gokhman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10863v1",
    "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
    "summary": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
    "published": "2025-12-11T17:57:24Z",
    "updated": "2025-12-11T17:57:24Z",
    "link": "http://arxiv.org/pdf/2512.10863v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Jingli Lin",
      "Runsen Xu",
      "Shaohao Zhu",
      "Sihan Yang",
      "Peizhou Cao",
      "Yunlong Ran",
      "Miao Hu",
      "Chenming Zhu",
      "Yiman Xie",
      "Yilin Long",
      "Wenbo Hu",
      "Dahua Lin",
      "Tai Wang",
      "Jiangmiao Pang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10857v1",
    "title": "Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants",
    "summary": "Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.",
    "published": "2025-12-11T17:53:38Z",
    "updated": "2025-12-11T17:53:38Z",
    "link": "http://arxiv.org/pdf/2512.10857v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "authors": [
      "Chirag Modi",
      "Jiequn Han",
      "Eric Vanden-Eijnden",
      "Joan Bruna"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10600v3",
    "title": "Faster Results from a Smarter Schedule: Reframing Collegiate Cross Country through Analysis of the National Running Club Database",
    "summary": "Collegiate cross country teams often build their season schedules on intuition rather than evidence, partly because large-scale performance datasets are not publicly accessible. To address this limitation, we introduce the National Running Club Database (NRCD), the first openly available dataset to aggregate 23,725 race results from 7,594 collegiate club athletes across the 2023-2025 seasons. Unlike existing resources, NRCD includes detailed course metadata, allowing us to develop two standardized performance metrics: Converted Only (distance correction) and Standardized (distance, weather, and elevation adjusted). Using these standardized measures, we find that athletes with slower initial performances exhibit the greatest improvement within a season, and that race frequency is the strongest predictor of improvement. Using six machine learning models, random forest achieves the highest accuracy (r squared equals 0.92), revealing that athletes who race more frequently progress significantly faster than those who do not. At the team level, programs whose athletes race at least four times during the regular season have substantially higher odds of placing in the top 15 at nationals (chi-squared less than 0.01). These results challenge common coaching practices that favor minimal racing before championship meets. Our findings demonstrate that a data-informed scheduling strategy improves both individual development and team competitiveness. The NRCD provides a new foundation for evidence-based decision-making in collegiate cross country and opens opportunities for further research on standardized, longitudinal athlete performance modeling.",
    "published": "2025-09-12T17:50:23Z",
    "updated": "2025-12-11T17:28:17Z",
    "link": "http://arxiv.org/pdf/2509.10600v3.pdf",
    "category": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Jonathan A. Karr",
      "Ryan M. Fryer",
      "Ben Darden",
      "Nicholas Pell",
      "Kayla Ambrose",
      "Evan Hall",
      "Ramzi K. Bualuan",
      "Nitesh V. Chawla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.19086v3",
    "title": "MaskedManipulator: Versatile Whole-Body Manipulation",
    "summary": "We tackle the challenges of synthesizing versatile, physically simulated human motions for full-body object manipulation. Unlike prior methods that are focused on detailed motion tracking, trajectory following, or teleoperation, our framework enables users to specify versatile high-level objectives such as target object poses or body poses. To achieve this, we introduce MaskedManipulator, a generative control policy distilled from a tracking controller trained on large-scale human motion capture data. This two-stage learning process allows the system to perform complex interaction behaviors, while providing intuitive user control over both character and object motions. MaskedManipulator produces goal-directed manipulation behaviors that expand the scope of interactive animation systems beyond task-specific solutions.",
    "published": "2025-05-25T10:46:14Z",
    "updated": "2025-12-11T17:25:33Z",
    "link": "http://arxiv.org/pdf/2505.19086v3.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "authors": [
      "Chen Tessler",
      "Yifeng Jiang",
      "Erwin Coumans",
      "Zhengyi Luo",
      "Gal Chechik",
      "Xue Bin Peng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10822v1",
    "title": "V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions",
    "summary": "Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.",
    "published": "2025-12-11T17:14:37Z",
    "updated": "2025-12-11T17:14:37Z",
    "link": "http://arxiv.org/pdf/2512.10822v1.pdf",
    "category": [
      "cs.AI",
      "cs.RO"
    ],
    "authors": [
      "Mumuksh Tayal",
      "Manan Tayal",
      "Aditya Singh",
      "Shishir Kolathaya",
      "Ravi Prakash"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10821v1",
    "title": "Agile Deliberation: Concept Deliberation for Subjective Visual Classification",
    "summary": "From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through \"concept deliberation\", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called \"Agile Deliberation\" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.",
    "published": "2025-12-11T17:13:09Z",
    "updated": "2025-12-11T17:13:09Z",
    "link": "http://arxiv.org/pdf/2512.10821v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "authors": [
      "Leijie Wang",
      "Otilia Stretcu",
      "Wei Qiao",
      "Thomas Denby",
      "Krishnamurthy Viswanathan",
      "Enming Luo",
      "Chun-Ta Lu",
      "Tushar Dogra",
      "Ranjay Krishna",
      "Ariel Fuxman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10817v1",
    "title": "Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values",
    "summary": "We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.",
    "published": "2025-12-11T17:08:28Z",
    "updated": "2025-12-11T17:08:28Z",
    "link": "http://arxiv.org/pdf/2512.10817v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "authors": [
      "Brian P. Powell",
      "Jordan A. Caraballo-Vega",
      "Mark L. Carroll",
      "Thomas Maxwell",
      "Andrew Ptak",
      "Greg Olmschenk",
      "Jorge Martinez-Palomera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10807v1",
    "title": "HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition",
    "summary": "Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.",
    "published": "2025-12-11T16:52:50Z",
    "updated": "2025-12-11T16:52:50Z",
    "link": "http://arxiv.org/pdf/2512.10807v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Wang Lu",
      "Yao Zhu",
      "Jindong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.20018v2",
    "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models",
    "summary": "This paper investigates real-time decision support systems that leverage low-latency AI models, bringing together recent progress in holistic AI-driven decision tools, integration with Edge-IoT technologies, and approaches for effective human-AI teamwork. It looks into how large language models can assist decision-making, especially when resources are limited. The research also examines the effects of technical developments such as DeLLMa, methods for compressing models, and improvements for analytics on edge devices, while also addressing issues like limited resources and the need for adaptable frameworks. Through a detailed review, the paper offers practical perspectives on development strategies and areas of application, adding to the field by pointing out opportunities for more efficient and flexible AI-supported systems. The conclusions set the stage for future breakthroughs in this fast-changing area, highlighting how AI can reshape real-time decision support.",
    "published": "2025-06-24T21:22:25Z",
    "updated": "2025-12-11T16:41:25Z",
    "link": "http://arxiv.org/pdf/2506.20018v2.pdf",
    "category": [
      "cs.AI",
      "cs.AR"
    ],
    "authors": [
      "Zechun Deng",
      "Ziwei Liu",
      "Ziqian Bi",
      "Junhao Song",
      "Chia Xin Liang",
      "Joe Yeong",
      "Xinyuan Song",
      "Junfeng Hao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10794v1",
    "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "summary": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
    "published": "2025-12-11T16:39:53Z",
    "updated": "2025-12-11T16:39:53Z",
    "link": "http://arxiv.org/pdf/2512.10794v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Jaskirat Singh",
      "Xingjian Leng",
      "Zongze Wu",
      "Liang Zheng",
      "Richard Zhang",
      "Eli Shechtman",
      "Saining Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10793v1",
    "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification",
    "summary": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.",
    "published": "2025-12-11T16:39:07Z",
    "updated": "2025-12-11T16:39:07Z",
    "link": "http://arxiv.org/pdf/2512.10793v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Michael Schlee",
      "Christoph Weisser",
      "Timo Kivimäki",
      "Melchizedek Mashiku",
      "Benjamin Saefken"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08868v2",
    "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
    "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
    "published": "2025-12-09T18:00:26Z",
    "updated": "2025-12-11T16:38:57Z",
    "link": "http://arxiv.org/pdf/2512.08868v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Rui Min",
      "Zile Qiao",
      "Ze Xu",
      "Jiawen Zhai",
      "Wenyu Gao",
      "Xuanzhong Chen",
      "Haozhen Sun",
      "Zhen Zhang",
      "Xinyu Wang",
      "Hong Zhou",
      "Wenbiao Yin",
      "Bo Zhang",
      "Xuan Zhou",
      "Ming Yan",
      "Yong Jiang",
      "Haicheng Liu",
      "Liang Ding",
      "Ling Zou",
      "Yi R. Fung",
      "Yalong Li",
      "Pengjun Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15432v3",
    "title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data",
    "summary": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.",
    "published": "2025-08-21T10:35:41Z",
    "updated": "2025-12-11T16:38:00Z",
    "link": "http://arxiv.org/pdf/2508.15432v3.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Bidyapati Pradhan",
      "Surajit Dasgupta",
      "Amit Kumar Saha",
      "Omkar Anustoop",
      "Sriram Puttagunta",
      "Vipul Mittal",
      "Gopal Sarda"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10791v1",
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
    "published": "2025-12-11T16:35:14Z",
    "updated": "2025-12-11T16:35:14Z",
    "link": "http://arxiv.org/pdf/2512.10791v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Aileen Cheng",
      "Alon Jacovi",
      "Amir Globerson",
      "Ben Golan",
      "Charles Kwong",
      "Chris Alberti",
      "Connie Tao",
      "Eyal Ben-David",
      "Gaurav Singh Tomar",
      "Lukas Haas",
      "Yonatan Bitton",
      "Adam Bloniarz",
      "Aijun Bai",
      "Andrew Wang",
      "Anfal Siddiqui",
      "Arturo Bajuelos Castillo",
      "Aviel Atias",
      "Chang Liu",
      "Corey Fry",
      "Daniel Balle",
      "Deepanway Ghosal",
      "Doron Kukliansky",
      "Dror Marcus",
      "Elena Gribovskaya",
      "Eran Ofek",
      "Honglei Zhuang",
      "Itay Laish",
      "Jan Ackermann",
      "Lily Wang",
      "Meg Risdal",
      "Megan Barnes",
      "Michael Fink",
      "Mohamed Amin",
      "Moran Ambar",
      "Natan Potikha",
      "Nikita Gupta",
      "Nitzan Katz",
      "Noam Velan",
      "Ofir Roval",
      "Ori Ram",
      "Polina Zablotskaia",
      "Prathamesh Bang",
      "Priyanka Agrawal",
      "Rakesh Ghiya",
      "Sanjay Ganapathy",
      "Simon Baumgartner",
      "Sofia Erell",
      "Sushant Prakash",
      "Thibault Sellam",
      "Vikram Rao",
      "Xuanhui Wang",
      "Yaroslav Akulov",
      "Yulong Yang",
      "Zhen Yang",
      "Zhixin Lai",
      "Zhongru Wu",
      "Anca Dragan",
      "Avinatan Hassidim",
      "Fernando Pereira",
      "Slav Petrov",
      "Srinivasan Venkatachary",
      "Tulsee Doshi",
      "Yossi Matias",
      "Sasha Goldshtein",
      "Dipanjan Das"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10789v1",
    "title": "Natural Language Interface for Firewall Configuration",
    "summary": "This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.",
    "published": "2025-12-11T16:33:33Z",
    "updated": "2025-12-11T16:33:33Z",
    "link": "http://arxiv.org/pdf/2512.10789v1.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "F. Taghiyev",
      "A. Aslanbayli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10787v1",
    "title": "Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly",
    "summary": "Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.",
    "published": "2025-12-11T16:31:29Z",
    "updated": "2025-12-11T16:31:29Z",
    "link": "http://arxiv.org/pdf/2512.10787v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Moshe Lahmy",
      "Roi Yozevitch"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10785v1",
    "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
    "summary": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
    "published": "2025-12-11T16:29:38Z",
    "updated": "2025-12-11T16:29:38Z",
    "link": "http://arxiv.org/pdf/2512.10785v1.pdf",
    "category": [
      "physics.ed-ph",
      "cs.AI",
      "cs.HC"
    ],
    "authors": [
      "Holger Maus",
      "Paul Tschisgale",
      "Fabian Kieser",
      "Stefan Petersen",
      "Peter Wulff"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10772v1",
    "title": "Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation",
    "summary": "Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.",
    "published": "2025-12-11T16:09:54Z",
    "updated": "2025-12-11T16:09:54Z",
    "link": "http://arxiv.org/pdf/2512.10772v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Kevin Glocker",
      "Kätriin Kukk",
      "Romina Oji",
      "Marcel Bollmann",
      "Marco Kuhlmann",
      "Jenny Kunz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06112v2",
    "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
    "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.",
    "published": "2025-12-05T19:36:46Z",
    "updated": "2025-12-11T16:06:13Z",
    "link": "http://arxiv.org/pdf/2512.06112v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Yifang Xu",
      "Jiahao Cui",
      "Feipeng Cai",
      "Zhihao Zhu",
      "Hanlin Shang",
      "Shan Luan",
      "Mingwang Xu",
      "Neng Zhang",
      "Yaoyi Li",
      "Jia Cai",
      "Siyu Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14932v2",
    "title": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale",
    "summary": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",
    "published": "2025-05-20T21:38:28Z",
    "updated": "2025-12-11T16:02:17Z",
    "link": "http://arxiv.org/pdf/2505.14932v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Isabelle Lee",
      "Sarah Liaw",
      "Dani Yogatama"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10758v1",
    "title": "Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework",
    "summary": "The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.\n  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.\n  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.\n  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.",
    "published": "2025-12-11T15:53:19Z",
    "updated": "2025-12-11T15:53:19Z",
    "link": "http://arxiv.org/pdf/2512.10758v1.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Kaihua Ding"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.01951v2",
    "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
    "summary": "With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.",
    "published": "2025-04-02T17:56:08Z",
    "updated": "2025-12-11T15:33:30Z",
    "link": "http://arxiv.org/pdf/2504.01951v2.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Massimiliano Luca",
      "Ciro Beneduce",
      "Bruno Lepri",
      "Jacopo Staiano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10739v1",
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
    "published": "2025-12-11T15:26:28Z",
    "updated": "2025-12-11T15:26:28Z",
    "link": "http://arxiv.org/pdf/2512.10739v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Songyang Gao",
      "Yuzhe Gu",
      "Zijian Wu",
      "Lingkai Kong",
      "Wenwei Zhang",
      "Zhongrui Cai",
      "Fan Zheng",
      "Tianyou Ma",
      "Junhao Shen",
      "Haiteng Zhao",
      "Duanyang Zhang",
      "Huilun Zhang",
      "Kuikun Liu",
      "Chengqi Lyu",
      "Yanhui Duan",
      "Chiyu Chen",
      "Ningsheng Ma",
      "Jianfei Gao",
      "Han Lyu",
      "Dahua Lin",
      "Kai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10735v1",
    "title": "LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation",
    "summary": "Graph Neural Networks (GNNs) have emerged as a dominant paradigm for graph classification. Specifically, most existing GNNs mainly rely on the message passing strategy between neighbor nodes, where the expressivity is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Although a number of k-WL-based GNNs have been proposed to overcome this limitation, their computational cost increases rapidly with k, significantly restricting the practical applicability. Moreover, since the k-WL models mainly operate on node tuples, these k-WL-based GNNs cannot retain fine-grained node- or edge-level semantics required by attribution methods (e.g., Integrated Gradients), leading to the less interpretable problem. To overcome the above shortcomings, in this paper, we propose a novel Line Graph Aggregation Network (LGAN), that constructs a line graph from the induced subgraph centered at each node to perform the higher-order aggregation. We theoretically prove that the LGAN not only possesses the greater expressive power than the 2-WL under injective aggregation assumptions, but also has lower time complexity. Empirical evaluations on benchmarks demonstrate that the LGAN outperforms state-of-the-art k-WL-based GNNs, while offering better interpretability.",
    "published": "2025-12-11T15:23:46Z",
    "updated": "2025-12-11T15:23:46Z",
    "link": "http://arxiv.org/pdf/2512.10735v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Lin Du",
      "Lu Bai",
      "Jincheng Li",
      "Lixin Cui",
      "Hangyuan Du",
      "Lichi Zhang",
      "Yuting Chen",
      "Zhao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10734v1",
    "title": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation",
    "summary": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.",
    "published": "2025-12-11T15:18:59Z",
    "updated": "2025-12-11T15:18:59Z",
    "link": "http://arxiv.org/pdf/2512.10734v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Rebekka Görge",
      "Sujan Sai Gannamaneni",
      "Tabea Naeven",
      "Hammam Abdelwahab",
      "Héctor Allende-Cid",
      "Armin B. Cremers",
      "Lennard Helmer",
      "Michael Mock",
      "Anna Schmitz",
      "Songkai Xue",
      "Elif Yildirir",
      "Maximilian Poretschkin",
      "Stefan Wrobel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10713v1",
    "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
    "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
    "published": "2025-12-11T14:49:56Z",
    "updated": "2025-12-11T14:49:56Z",
    "link": "http://arxiv.org/pdf/2512.10713v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Itay Dreyfuss",
      "Antonio Abu Nassar",
      "Samuel Ackerman",
      "Axel Ben David",
      "Rami Katan",
      "Orna Raz",
      "Marcel Zalmanovici"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.01538v2",
    "title": "AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge",
    "summary": "While current AI-driven methods excel at deriving empirical models from individual experiments, a significant challenge remains in uncovering the common fundamental physics that underlie these models -- a task at which human physicists are adept. To bridge this gap, we introduce AI-Newton, a novel framework for concept-driven scientific discovery. Our system autonomously derives general physical laws directly from raw, multi-experiment data, operating without supervision or prior physical knowledge. Its core innovations are twofold: (1) proposing interpretable physical concepts to construct laws, and (2) progressively generalizing these laws to broader domains. Applied to a large, noisy dataset of mechanics experiments, AI-Newton successfully rediscovers foundational and universal laws, such as Newton's second law, the conservation of energy, and the universal gravitation. This work represents a significant advance toward autonomous, human-like scientific discovery.",
    "published": "2025-04-02T09:25:34Z",
    "updated": "2025-12-11T14:46:15Z",
    "link": "http://arxiv.org/pdf/2504.01538v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.SC",
      "hep-ph",
      "physics.class-ph"
    ],
    "authors": [
      "You-Le Fang",
      "Dong-Shan Jian",
      "Xiang Li",
      "Yan-Qing Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10702v1",
    "title": "COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators",
    "summary": "Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.\n  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.\n  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.\n  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.",
    "published": "2025-12-11T14:41:37Z",
    "updated": "2025-12-11T14:41:37Z",
    "link": "http://arxiv.org/pdf/2512.10702v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Wei Fang",
      "Chiyao Wang",
      "Wenshuai Ma",
      "Hui Liu",
      "Jianqiang Hu",
      "Xiaona Niu",
      "Yi Chu",
      "Mingming Zhang",
      "Jingxiao Yang",
      "Dongwei Zhang",
      "Zelin Li",
      "Pengyun Liu",
      "Jiawei Zheng",
      "Pengke Zhang",
      "Chaoshi Qin",
      "Wangang Guo",
      "Bin Wang",
      "Yugang Xue",
      "Wei Zhang",
      "Zikuan Wang",
      "Rui Zhu",
      "Yihui Cao",
      "Quanmao Lu",
      "Rui Meng",
      "Yan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10698v1",
    "title": "How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning",
    "summary": "Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.",
    "published": "2025-12-11T14:40:33Z",
    "updated": "2025-12-11T14:40:33Z",
    "link": "http://arxiv.org/pdf/2512.10698v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI"
    ],
    "authors": [
      "Jianbo Wang",
      "Galina Sidorenko",
      "Johan Thunberg"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10696v1",
    "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
    "summary": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
    "published": "2025-12-11T14:40:01Z",
    "updated": "2025-12-11T14:40:01Z",
    "link": "http://arxiv.org/pdf/2512.10696v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Zouying Cao",
      "Jiaji Deng",
      "Li Yu",
      "Weikang Zhou",
      "Zhaoyang Liu",
      "Bolin Ding",
      "Hai Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10691v1",
    "title": "Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning",
    "summary": "Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning (\"thinking\") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.",
    "published": "2025-12-11T14:36:14Z",
    "updated": "2025-12-11T14:36:14Z",
    "link": "http://arxiv.org/pdf/2512.10691v1.pdf",
    "category": [
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Benjamin Gundersen",
      "Nicolas Deperrois",
      "Samuel Ruiperez-Campillo",
      "Thomas M. Sutter",
      "Julia E. Vogt",
      "Michael Moor",
      "Farhad Nooralahzadeh",
      "Michael Krauthammer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10688v1",
    "title": "Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition",
    "summary": "Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.",
    "published": "2025-12-11T14:35:13Z",
    "updated": "2025-12-11T14:35:13Z",
    "link": "http://arxiv.org/pdf/2512.10688v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Lingfeng Liu",
      "Yixin Song",
      "Dazhong Shen",
      "Bing Yin",
      "Hao Li",
      "Yanyong Zhang",
      "Chao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10687v1",
    "title": "Challenges of Evaluating LLM Safety for User Welfare",
    "summary": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.",
    "published": "2025-12-11T14:34:40Z",
    "updated": "2025-12-11T14:34:40Z",
    "link": "http://arxiv.org/pdf/2512.10687v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Manon Kempermann",
      "Sai Suresh Macharla Vasu",
      "Mahalakshmi Raveenthiran",
      "Theo Farrell",
      "Ingmar Weber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10675v1",
    "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
    "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
    "published": "2025-12-11T14:22:14Z",
    "updated": "2025-12-11T14:22:14Z",
    "link": "http://arxiv.org/pdf/2512.10675v1.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      " Gemini Robotics Team",
      "Coline Devin",
      "Yilun Du",
      "Debidatta Dwibedi",
      "Ruiqi Gao",
      "Abhishek Jindal",
      "Thomas Kipf",
      "Sean Kirmani",
      "Fangchen Liu",
      "Anirudha Majumdar",
      "Andrew Marmon",
      "Carolina Parada",
      "Yulia Rubanova",
      "Dhruv Shah",
      "Vikas Sindhwani",
      "Jie Tan",
      "Fei Xia",
      "Ted Xiao",
      "Sherry Yang",
      "Wenhao Yu",
      "Allan Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10671v1",
    "title": "AEBNAS: Strengthening Exit Branches in Early-Exit Networks through Hardware-Aware Neural Architecture Search",
    "summary": "Early-exit networks are effective solutions for reducing the overall energy consumption and latency of deep learning models by adjusting computation based on the complexity of input data. By incorporating intermediate exit branches into the architecture, they provide less computation for simpler samples, which is particularly beneficial for resource-constrained devices where energy consumption is crucial. However, designing early-exit networks is a challenging and time-consuming process due to the need to balance efficiency and performance. Recent works have utilized Neural Architecture Search (NAS) to design more efficient early-exit networks, aiming to reduce average latency while improving model accuracy by determining the best positions and number of exit branches in the architecture. Another important factor affecting the efficiency and accuracy of early-exit networks is the depth and types of layers in the exit branches. In this paper, we use hardware-aware NAS to strengthen exit branches, considering both accuracy and efficiency during optimization. Our performance evaluation on the CIFAR-10, CIFAR-100, and SVHN datasets demonstrates that our proposed framework, which considers varying depths and layers for exit branches along with adaptive threshold tuning, designs early-exit networks that achieve higher accuracy with the same or lower average number of MACs compared to the state-of-the-art approaches.",
    "published": "2025-12-11T14:17:49Z",
    "updated": "2025-12-11T14:17:49Z",
    "link": "http://arxiv.org/pdf/2512.10671v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Oscar Robben",
      "Saeed Khalilian",
      "Nirvana Meratnia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10665v1",
    "title": "On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity",
    "summary": "As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.",
    "published": "2025-12-11T14:13:53Z",
    "updated": "2025-12-11T14:13:53Z",
    "link": "http://arxiv.org/pdf/2512.10665v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Muhua Huang",
      "Qinlin Zhao",
      "Xiaoyuan Yi",
      "Xing Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10655v1",
    "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
    "summary": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
    "published": "2025-12-11T14:01:47Z",
    "updated": "2025-12-11T14:01:47Z",
    "link": "http://arxiv.org/pdf/2512.10655v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Tong Zhang",
      "Carlos Hinojosa",
      "Bernard Ghanem"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.08139v2",
    "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
    "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.",
    "published": "2025-08-11T16:12:36Z",
    "updated": "2025-12-11T13:49:54Z",
    "link": "http://arxiv.org/pdf/2508.08139v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tianyi Zhou",
      "Johanne Medina",
      "Sanjay Chawla"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.01730v2",
    "title": "PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation",
    "summary": "Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on ImageNet with a 64$\\times$ reduction in the number of inference timesteps compared to existing approaches.",
    "published": "2025-05-03T07:55:29Z",
    "updated": "2025-12-11T13:45:50Z",
    "link": "http://arxiv.org/pdf/2505.01730v2.pdf",
    "category": [
      "cs.NE",
      "cs.AI"
    ],
    "authors": [
      "Pranav Ramesh",
      "Gopalakrishnan Srinivasan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10640v1",
    "title": "Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification",
    "summary": "Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.",
    "published": "2025-12-11T13:45:31Z",
    "updated": "2025-12-11T13:45:31Z",
    "link": "http://arxiv.org/pdf/2512.10640v1.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Liang Peng",
      "Haopeng Liu",
      "Yixuan Ye",
      "Cheng Liu",
      "Wenjun Shen",
      "Si Wu",
      "Hau-San Wong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17012v2",
    "title": "SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence",
    "summary": "Existing evaluations of multimodal large language models (MLLMs) on spatial intelligence are typically fragmented and limited in scope. In this work, we aim to conduct a holistic assessment of the spatial understanding capabilities of modern MLLMs and propose complementary data-driven and agent-based solutions. Specifically, we make the following contributions: (i) we introduce SpatialScore, to our knowledge, the most comprehensive and diverse benchmark for multimodal spatial intelligence to date. It covers multiple visual data types, input modalities, and question-answering formats, and contains approximately 5K manually verified samples spanning 30 distinct tasks; (ii) using SpatialScore, we extensively evaluate 40 representative MLLMs, revealing persistent challenges and a substantial gap between current models and human-level spatial intelligence; (iii) to advance model capabilities, we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks and significantly improves the performance of existing models (e.g., Qwen3-VL); (iv) to complement this data-driven route with a training-free paradigm, we develop SpatialAgent, a multi-agent system equipped with 12 specialized spatial perception tools that supports both Plan-Execute and ReAct reasoning, enabling substantial gains in spatial reasoning without additional model training. Extensive experiments and in-depth analyses demonstrate the effectiveness of our benchmark, corpus, and agent framework. We expect these resources to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence. All data, code, and models will be released to the research community.",
    "published": "2025-05-22T17:59:03Z",
    "updated": "2025-12-11T13:21:59Z",
    "link": "http://arxiv.org/pdf/2505.17012v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haoning Wu",
      "Xiao Huang",
      "Yaohui Chen",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09443v2",
    "title": "Advancing Mathematical Research via Human-AI Interactive Theorem Proving",
    "summary": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",
    "published": "2025-12-10T09:16:27Z",
    "updated": "2025-12-11T13:10:50Z",
    "link": "http://arxiv.org/pdf/2512.09443v2.pdf",
    "category": [
      "cs.HC",
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Chenyi Li",
      "Zhijian Lai",
      "Dong An",
      "Jiang Hu",
      "Zaiwen Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10611v1",
    "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
    "summary": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
    "published": "2025-12-11T13:04:44Z",
    "updated": "2025-12-11T13:04:44Z",
    "link": "http://arxiv.org/pdf/2512.10611v1.pdf",
    "category": [
      "cs.AI",
      "cs.NE"
    ],
    "authors": [
      "Minghao LI",
      "Ruihang Wang",
      "Rui Tan",
      "Yonggang Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.17438v3",
    "title": "Object-centric proto-symbolic behavioural reasoning from pixels",
    "summary": "Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels -- ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, as well as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\land C)$ and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.",
    "published": "2024-11-26T13:54:24Z",
    "updated": "2025-12-11T12:52:59Z",
    "link": "http://arxiv.org/pdf/2411.17438v3.pdf",
    "category": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Ruben van Bergen",
      "Justus Hübotter",
      "Alma Lago",
      "Pablo Lanillos"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10596v1",
    "title": "Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval",
    "summary": "Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \\textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\\%, nearly doubling the 23.86\\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.",
    "published": "2025-12-11T12:43:41Z",
    "updated": "2025-12-11T12:43:41Z",
    "link": "http://arxiv.org/pdf/2512.10596v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "J. Xiao",
      "Y. Guo",
      "X. Zi",
      "K. Thiyagarajan",
      "C. Moreira",
      "M. Prasad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10563v1",
    "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning",
    "summary": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.",
    "published": "2025-12-11T11:50:50Z",
    "updated": "2025-12-11T11:50:50Z",
    "link": "http://arxiv.org/pdf/2512.10563v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Xin Guan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09543v2",
    "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
    "summary": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
    "published": "2025-12-10T11:28:48Z",
    "updated": "2025-12-11T11:33:34Z",
    "link": "http://arxiv.org/pdf/2512.09543v2.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Arihant Tripathy",
      "Ch Pavan Harshit",
      "Karthik Vaidhyanathan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10551v1",
    "title": "LLM-Auction: Generative Auction towards LLM-Native Advertising",
    "summary": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.",
    "published": "2025-12-11T11:31:20Z",
    "updated": "2025-12-11T11:31:20Z",
    "link": "http://arxiv.org/pdf/2512.10551v1.pdf",
    "category": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Chujie Zhao",
      "Qun Hu",
      "Shiping Song",
      "Dagui Chen",
      "Han Zhu",
      "Jian Xu",
      "Bo Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.10537v3",
    "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps",
    "summary": "The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance.",
    "published": "2025-05-15T17:47:30Z",
    "updated": "2025-12-11T11:18:01Z",
    "link": "http://arxiv.org/pdf/2505.10537v3.pdf",
    "category": [
      "cs.NI",
      "cs.AI"
    ],
    "authors": [
      "Filippo Olimpieri",
      "Noemi Giustini",
      "Andrea Lacava",
      "Salvatore D'Oro",
      "Tommaso Melodia",
      "Francesca Cuomo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10534v1",
    "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
    "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
    "published": "2025-12-11T11:05:04Z",
    "updated": "2025-12-11T11:05:04Z",
    "link": "http://arxiv.org/pdf/2512.10534v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Haiteng Zhao",
      "Junhao Shen",
      "Yiming Zhang",
      "Songyang Gao",
      "Kuikun Liu",
      "Tianyou Ma",
      "Fan Zheng",
      "Dahua Lin",
      "Wenwei Zhang",
      "Kai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09830v2",
    "title": "LLMs in Interpreting Legal Documents",
    "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
    "published": "2025-12-10T17:09:13Z",
    "updated": "2025-12-11T11:01:35Z",
    "link": "http://arxiv.org/pdf/2512.09830v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Simone Corbo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.14396v2",
    "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning",
    "summary": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.",
    "published": "2025-11-18T12:01:06Z",
    "updated": "2025-12-11T10:52:27Z",
    "link": "http://arxiv.org/pdf/2511.14396v2.pdf",
    "category": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "authors": [
      "Xiuxiu Qi",
      "Yu Yang",
      "Jiannong Cao",
      "Luyao Bai",
      "Chongshan Fan",
      "Chengtai Cao",
      "Hongpeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10510v1",
    "title": "Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning",
    "summary": "Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.",
    "published": "2025-12-11T10:30:04Z",
    "updated": "2025-12-11T10:30:04Z",
    "link": "http://arxiv.org/pdf/2512.10510v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Chihyeon Song",
      "Jaewoo Lee",
      "Jinkyoo Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.01524v2",
    "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat",
    "summary": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.",
    "published": "2025-06-02T10:38:02Z",
    "updated": "2025-12-11T10:22:55Z",
    "link": "http://arxiv.org/pdf/2506.01524v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Qi Lin",
      "Weikai Xu",
      "Lisi Chen",
      "Bin Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10501v1",
    "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
    "summary": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
    "published": "2025-12-11T10:22:02Z",
    "updated": "2025-12-11T10:22:02Z",
    "link": "http://arxiv.org/pdf/2512.10501v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Lim Chien Her",
      "Ming Yan",
      "Yunshu Bai",
      "Ruihao Li",
      "Hao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10492v1",
    "title": "UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning",
    "summary": "Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.",
    "published": "2025-12-11T10:14:13Z",
    "updated": "2025-12-11T10:14:13Z",
    "link": "http://arxiv.org/pdf/2512.10492v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Jiaxi Wu",
      "Tiantian Zhang",
      "Yuxing Wang",
      "Yongzhe Chang",
      "Xueqian Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.13162v2",
    "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
    "summary": "Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at https://lmb-freiburg.github.io/orbis.github.io/.",
    "published": "2025-07-17T14:29:34Z",
    "updated": "2025-12-11T10:05:50Z",
    "link": "http://arxiv.org/pdf/2507.13162v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Arian Mousakhan",
      "Sudhanshu Mittal",
      "Silvio Galesso",
      "Karim Farid",
      "Thomas Brox"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.08464v3",
    "title": "A Generation Framework with Strict Constraints for Crystal Materials Design",
    "summary": "The design of crystal materials plays a critical role in areas such as new energy development, biomedical engineering, and semiconductors. Recent advances in data-driven methods have enabled the generation of diverse crystal structures. However, most existing approaches still rely on random sampling without strict constraints, requiring multiple post-processing steps to identify stable candidates with the desired physical and chemical properties. In this work, we present a new constrained generation framework that takes multiple constraints as input and enables the generation of crystal structures with specific chemical and properties. In this framework, intermediate constraints, such as symmetry information and composition ratio, are generated by a constraint generator based on large language models (LLMs), which considers the target properties. These constraints are then used by a subsequent crystal structure generator to ensure that the structure generation process is under control. Our method generates crystal structures with a probability of meeting the target properties that is more than twice that of existing approaches. Furthermore, nearly 100% of the generated crystals strictly adhere to predefined chemical composition, eliminating the risks of supply chain during production.",
    "published": "2024-11-13T09:36:50Z",
    "updated": "2025-12-11T09:59:41Z",
    "link": "http://arxiv.org/pdf/2411.08464v3.pdf",
    "category": [
      "cs.AI",
      "cond-mat.mtrl-sci"
    ],
    "authors": [
      "Chao Huang",
      "Jiahui Chen",
      "Chen Chen",
      "Chen Chen",
      "Chunyan Chen",
      "Renjie Su",
      "Shiyu Du"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.17552v3",
    "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning",
    "summary": "The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.",
    "published": "2025-09-22T09:16:34Z",
    "updated": "2025-12-11T09:40:22Z",
    "link": "http://arxiv.org/pdf/2509.17552v3.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Tianle Zhang",
      "Wanlong Fang",
      "Jonathan Woo",
      "Paridhi Latawa",
      "Deepak A. Subramanian",
      "Alvin Chan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18271v3",
    "title": "Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models",
    "summary": "Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.",
    "published": "2025-11-23T03:44:54Z",
    "updated": "2025-12-11T09:39:01Z",
    "link": "http://arxiv.org/pdf/2511.18271v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Tianyang Han",
      "Junhao Su",
      "Junjie Hu",
      "Peizhen Yang",
      "Hengyu Shi",
      "Junfeng Luo",
      "Jialin Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10461v1",
    "title": "T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method",
    "summary": "Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.",
    "published": "2025-12-11T09:35:13Z",
    "updated": "2025-12-11T09:35:13Z",
    "link": "http://arxiv.org/pdf/2512.10461v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "authors": [
      "Haoyu Zhu",
      "Yao Zhang",
      "Jiashen Ren",
      "Qingchun Hou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.14566v2",
    "title": "Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak",
    "summary": "Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.",
    "published": "2025-11-18T15:09:09Z",
    "updated": "2025-12-11T09:34:43Z",
    "link": "http://arxiv.org/pdf/2511.14566v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Lucia Makaiova",
      "Martin Fajcik",
      "Antonin Jarolim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.03772v5",
    "title": "GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control",
    "summary": "Group Relative Policy Optimization (GRPO) is a promising policy-based approach for Large Language Model alignment, yet its performance is often limited by training instability and suboptimal convergence. In this paper, we identify and analyze two main GRPO issues: (i) the token-level penalization, where valuable tokens shared across different responses receive contradictory feedback signals, leading to conflicting gradient updates that can reduce their likelihood; and (ii) the policy collapse, where negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, destabilizing training process. To address these issues we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which prevents conflicting gradients on valuable tokens by skipping negative updates while amplifying positive ones and filters out completions whose entropy exceeds a provable threshold, to prevent policy collapse. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, as validated through multiple experiments on GSM8K, MATH, AIME 2024, AIME 2025 and AMC 2023.",
    "published": "2025-08-05T08:15:01Z",
    "updated": "2025-12-11T09:23:26Z",
    "link": "http://arxiv.org/pdf/2508.03772v5.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Marco Simoni",
      "Aleksandar Fontana",
      "Giulio Rossolini",
      "Andrea Saracino",
      "Paolo Mori"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.25106v2",
    "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
    "summary": "Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing benchmarks primarily evaluate DRAs on generic quality metrics and overlook personalization, a critical dimension for individual users. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench (PDR-Bench), the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures Personalization Alignment, Content Quality, and Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.",
    "published": "2025-09-29T17:39:17Z",
    "updated": "2025-12-11T09:21:18Z",
    "link": "http://arxiv.org/pdf/2509.25106v2.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "authors": [
      "Yuan Liang",
      "Jiaxian Li",
      "Yuqing Wang",
      "Piaohong Wang",
      "Motong Tian",
      "Pai Liu",
      "Shuofei Qiao",
      "Runnan Fang",
      "He Zhu",
      "Ge Zhang",
      "Minghao Liu",
      "Yuchen Eleanor Jiang",
      "Ningyu Zhang",
      "Wangchunshu Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10449v1",
    "title": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection",
    "summary": "The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the \"Lazy Reviewer\" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these \"LLM-as-a-Judge\" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping \"Reject\" decisions to \"Accept,\" for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like \"Maximum Mark Magyk\" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.",
    "published": "2025-12-11T09:13:36Z",
    "updated": "2025-12-11T09:13:36Z",
    "link": "http://arxiv.org/pdf/2512.10449v1.pdf",
    "category": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "authors": [
      "Devanshu Sahoo",
      "Manish Prasad",
      "Vasudev Majhi",
      "Jahnvi Singh",
      "Vinay Chamola",
      "Yash Sinha",
      "Murari Mandal",
      "Dhruv Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10445v1",
    "title": "Maximum Risk Minimization with Random Forests",
    "summary": "We consider a regression setting where observations are collected in different environments modeled by different data distributions. The field of out-of-distribution (OOD) generalization aims to design methods that generalize better to test environments whose distributions differ from those observed during training. One line of such works has proposed to minimize the maximum risk across environments, a principle that we refer to as MaxRM (Maximum Risk Minimization). In this work, we introduce variants of random forests based on the principle of MaxRM. We provide computationally efficient algorithms and prove statistical consistency for our primary method. Our proposed method can be used with each of the following three risks: the mean squared error, the negative reward (which relates to the explained variance), and the regret (which quantifies the excess risk relative to the best predictor). For MaxRM with regret as the risk, we prove a novel out-of-sample guarantee over unseen test distributions. Finally, we evaluate the proposed methods on both simulated and real-world data.",
    "published": "2025-12-11T09:10:52Z",
    "updated": "2025-12-11T09:10:52Z",
    "link": "http://arxiv.org/pdf/2512.10445v1.pdf",
    "category": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "authors": [
      "Francesco Freni",
      "Anya Fries",
      "Linus Kühne",
      "Markus Reichstein",
      "Jonas Peters"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10443v1",
    "title": "Clustered Federated Learning with Hierarchical Knowledge Distillation",
    "summary": "Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\\%.",
    "published": "2025-12-11T09:08:35Z",
    "updated": "2025-12-11T09:08:35Z",
    "link": "http://arxiv.org/pdf/2512.10443v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Sabtain Ahmad",
      "Meerzhan Kanatbekova",
      "Ivona Brandic",
      "Atakan Aral"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10437v1",
    "title": "An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time",
    "summary": "This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.",
    "published": "2025-12-11T08:56:03Z",
    "updated": "2025-12-11T08:56:03Z",
    "link": "http://arxiv.org/pdf/2512.10437v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Stylianos Kandylakis",
      "Christos Orfanopoulos",
      "Georgios Siolas",
      "Panayiotis Tsanakas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2408.14338v2",
    "title": "Machine Learning for Quantifier Selection in cvc5",
    "summary": "In this work we considerably improve the state-of-the-art SMT solving on first-order quantified problems by efficient machine learning guidance of quantifier selection. Quantifiers represent a significant challenge for SMT and are technically a source of undecidability. In our approach, we train an efficient machine learning model that informs the solver which quantifiers should be instantiated and which not. Each quantifier may be instantiated multiple times and the set of the active quantifiers changes as the solving progresses. Therefore, we invoke the ML predictor many times, during the whole run of the solver. To make this efficient, we use fast ML models based on gradient boosting decision trees. We integrate our approach into the state-of-the-art cvc5 SMT solver and show a considerable increase of the system's holdout-set performance after training it on a large set of first-order problems collected from the Mizar Mathematical Library.",
    "published": "2024-08-26T15:07:35Z",
    "updated": "2025-12-11T08:53:43Z",
    "link": "http://arxiv.org/pdf/2408.14338v2.pdf",
    "category": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "authors": [
      "Jan Jakubův",
      "Mikoláš Janota",
      "Jelle Piepenbrock",
      "Josef Urban"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.14381v2",
    "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation",
    "summary": "With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.",
    "published": "2025-05-20T14:03:24Z",
    "updated": "2025-12-11T08:51:09Z",
    "link": "http://arxiv.org/pdf/2505.14381v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yuyang Dong",
      "Nobuhiro Ueda",
      "Krisztián Boros",
      "Daiki Ito",
      "Takuya Sera",
      "Masafumi Oyamada"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10433v1",
    "title": "Targeted Data Protection for Diffusion Model by Matching Training Trajectory",
    "summary": "Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.",
    "published": "2025-12-11T08:47:41Z",
    "updated": "2025-12-11T08:47:41Z",
    "link": "http://arxiv.org/pdf/2512.10433v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hojun Lee",
      "Mijin Koo",
      "Yeji Song",
      "Nojun Kwak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08609v2",
    "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
    "summary": "Automatic Heuristic Design (AHD) is an effective framework for solving complex optimization problems. The development of large language models (LLMs) enables the automated generation of heuristics. Existing LLM-based evolutionary methods rely on population strategies and are prone to local optima. Integrating LLMs with Monte Carlo Tree Search (MCTS) improves the trade-off between exploration and exploitation, but multi-round cognitive integration remains limited and search diversity is constrained. To overcome these limitations, this paper proposes a novel cognitive-guided MCTS framework (CogMCTS). CogMCTS tightly integrates the cognitive guidance mechanism of LLMs with MCTS to achieve efficient automated heuristic optimization. The framework employs multi-round cognitive feedback to incorporate historical experience, node information, and negative outcomes, dynamically improving heuristic generation. Dual-track node expansion combined with elite heuristic management balances the exploration of diverse heuristics and the exploitation of high-quality experience. In addition, strategic mutation modifies the heuristic forms and parameters to further enhance the diversity of the solution and the overall optimization performance. The experimental results indicate that CogMCTS outperforms existing LLM-based AHD methods in stability, efficiency, and solution quality.",
    "published": "2025-12-09T13:54:18Z",
    "updated": "2025-12-11T08:46:55Z",
    "link": "http://arxiv.org/pdf/2512.08609v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Hui Wang",
      "Yang Liu",
      "Xiaoyu Zhang",
      "Chaoxu Mu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.15952v4",
    "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding",
    "summary": "Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking \"what is intelligence?\" (ontological), SCL asks \"under what conditions does cognition emerge?\" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling \"executable epistemology\" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.",
    "published": "2025-10-10T18:03:32Z",
    "updated": "2025-12-11T08:40:26Z",
    "link": "http://arxiv.org/pdf/2510.15952v4.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Myung Ho Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10429v1",
    "title": "Representation of the structure of graphs by sequences of instructions",
    "summary": "The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.",
    "published": "2025-12-11T08:40:06Z",
    "updated": "2025-12-11T08:40:06Z",
    "link": "http://arxiv.org/pdf/2512.10429v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ezequiel Lopez-Rubio"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10422v1",
    "title": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers",
    "summary": "Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\\footnote{https://github.com/meaningful96/CoopRAG}",
    "published": "2025-12-11T08:35:17Z",
    "updated": "2025-12-11T08:35:17Z",
    "link": "http://arxiv.org/pdf/2512.10422v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Youmin Ko",
      "Sungjong Seo",
      "Hyunjoon Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.22258v2",
    "title": "Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks",
    "summary": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.",
    "published": "2025-09-26T12:20:01Z",
    "updated": "2025-12-11T08:31:33Z",
    "link": "http://arxiv.org/pdf/2509.22258v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Miao Jing",
      "Mengting Jia",
      "Junling Lin",
      "Zhongxia Shen",
      "Huan Gao",
      "Mingkun Xu",
      "Shangyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10416v1",
    "title": "Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction",
    "summary": "Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.",
    "published": "2025-12-11T08:29:27Z",
    "updated": "2025-12-11T08:29:27Z",
    "link": "http://arxiv.org/pdf/2512.10416v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Wenfei Guan",
      "Jilin Mei",
      "Tong Shen",
      "Xumin Wu",
      "Shuo Wang",
      "Cheng Min",
      "Yu Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10415v1",
    "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
    "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.",
    "published": "2025-12-11T08:28:33Z",
    "updated": "2025-12-11T08:28:33Z",
    "link": "http://arxiv.org/pdf/2512.10415v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Devanshu Sahoo",
      "Vasudev Majhi",
      "Arjun Neekhra",
      "Yash Sinha",
      "Murari Mandal",
      "Dhruv Kumar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10414v1",
    "title": "Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention",
    "summary": "Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.",
    "published": "2025-12-11T08:27:02Z",
    "updated": "2025-12-11T08:27:02Z",
    "link": "http://arxiv.org/pdf/2512.10414v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yang Yu",
      "Zhuangzhuang Chen",
      "Siqi Wang",
      "Lanqing Li",
      "Xiaomeng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.04629v2",
    "title": "BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation",
    "summary": "Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging these capabilities, we further apply the model to multi-step retrosynthetic planning, achieving state-of-the-art performance on RetroBench and demonstrating its superior efficacy as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.",
    "published": "2025-12-04T10:00:16Z",
    "updated": "2025-12-11T08:24:32Z",
    "link": "http://arxiv.org/pdf/2512.04629v2.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Chenyang Zuo",
      "Siqi Fan",
      "Zaiqing Nie"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10411v1",
    "title": "Sliding Window Attention Adaptation",
    "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
    "published": "2025-12-11T08:21:24Z",
    "updated": "2025-12-11T08:21:24Z",
    "link": "http://arxiv.org/pdf/2512.10411v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Yijiong Yu",
      "Jiale Liu",
      "Qingyun Wu",
      "Huazheng Wang",
      "Ji Pei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03040v2",
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "published": "2025-12-02T18:59:44Z",
    "updated": "2025-12-11T08:18:10Z",
    "link": "http://arxiv.org/pdf/2512.03040v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zeqi Xiao",
      "Yiwei Zhao",
      "Lingxiao Li",
      "Yushi Lan",
      "Ning Yu",
      "Rahul Garg",
      "Roshni Cooper",
      "Mohammad H. Taghavi",
      "Xingang Pan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.00090v3",
    "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
    "summary": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
    "published": "2025-10-30T04:57:26Z",
    "updated": "2025-12-11T08:10:13Z",
    "link": "http://arxiv.org/pdf/2511.00090v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Huanlin Gao",
      "Ping Chen",
      "Fuyuan Shi",
      "Chao Tan",
      "Zhaoxiang Liu",
      "Fang Zhao",
      "Kai Wang",
      "Shiguo Lian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10402v1",
    "title": "The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks",
    "summary": "Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.",
    "published": "2025-12-11T08:09:07Z",
    "updated": "2025-12-11T08:09:07Z",
    "link": "http://arxiv.org/pdf/2512.10402v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Zhou Feng",
      "Jiahao Chen",
      "Chunyi Zhou",
      "Yuwen Pu",
      "Tianyu Du",
      "Jinbao Li",
      "Jianhai Chen",
      "Shouling Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10398v1",
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "published": "2025-12-11T08:05:58Z",
    "updated": "2025-12-11T08:05:58Z",
    "link": "http://arxiv.org/pdf/2512.10398v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Zhaodong Wang",
      "Zhenting Qi",
      "Sherman Wong",
      "Nathan Hu",
      "Samuel Lin",
      "Jun Ge",
      "Erwin Gao",
      "Yining Yang",
      "Ben Maurer",
      "Wenlin Chen",
      "David Recordon",
      "Yilun Du",
      "Minlan Yu",
      "Ying Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.21051v4",
    "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense",
    "summary": "The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided numerous benefits in our daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent advancements in the large language models (LLMs) offer promising solutions for security intelligence. By exploiting the powerful capabilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel defense architecture that proactively mitigates various DoS threats in cloud networks. LLM-PD can efficiently make decisions through comprehensive data analysis and sequential reasoning, as well as dynamically create and deploy actionable defense mechanisms. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. Our case study on three distinct DoS attacks demonstrates its remarkable ability in terms of defense effectiveness and efficiency when compared with other existing methods.",
    "published": "2024-12-30T16:09:28Z",
    "updated": "2025-12-11T08:02:12Z",
    "link": "http://arxiv.org/pdf/2412.21051v4.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "authors": [
      "Yuyang Zhou",
      "Guang Cheng",
      "Kang Du",
      "Zihan Chen",
      "Yuyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18735v2",
    "title": "Thinking Ahead: Foresight Intelligence in MLLMs and World Models",
    "summary": "In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.",
    "published": "2025-11-24T04:04:59Z",
    "updated": "2025-12-11T08:02:01Z",
    "link": "http://arxiv.org/pdf/2511.18735v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Zhantao Gong",
      "Liaoyuan Fan",
      "Qing Guo",
      "Xun Xu",
      "Xulei Yang",
      "Shijie Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10393v1",
    "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
    "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
    "published": "2025-12-11T07:58:10Z",
    "updated": "2025-12-11T07:58:10Z",
    "link": "http://arxiv.org/pdf/2512.10393v1.pdf",
    "category": [
      "cs.SE",
      "cs.AI"
    ],
    "authors": [
      "Guoqiang Chen",
      "Lingyun Ying",
      "Ziyang Song",
      "Daguang Liu",
      "Qiang Wang",
      "Zhiqi Wang",
      "Li Hu",
      "Shaoyin Cheng",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13631v2",
    "title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
    "summary": "Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations.",
    "published": "2025-05-19T18:08:09Z",
    "updated": "2025-12-11T07:51:20Z",
    "link": "http://arxiv.org/pdf/2505.13631v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Andrei Manolache",
      "Luiz F. O. Chamon",
      "Mathias Niepert"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10388v1",
    "title": "The Best of the Two Worlds: Harmonizing Semantic and Hash IDs for Sequential Recommendation",
    "summary": "Conventional Sequential Recommender Systems (SRS) typically assign unique Hash IDs (HID) to construct item embeddings. These HID embeddings effectively learn collaborative information from historical user-item interactions, making them vulnerable to situations where most items are rarely consumed (the long-tail problem). Recent methods that incorporate auxiliary information often suffer from noisy collaborative sharing caused by co-occurrence signals or semantic homogeneity caused by flat dense embeddings. Semantic IDs (SIDs), with their capability of code sharing and multi-granular semantic modeling, provide a promising alternative. However, the collaborative overwhelming phenomenon hinders the further development of SID-based methods. The quantization mechanisms commonly compromise the uniqueness of identifiers required for modeling head items, creating a performance seesaw between head and tail items. To address this dilemma, we propose \\textbf{\\name}, a novel framework that harmonizes the SID and HID. Specifically, we devise a dual-branch modeling architecture that enables the model to capture both the multi-granular semantics within SID while preserving the unique collaborative identity of HID. Furthermore, we introduce a dual-level alignment strategy that bridges the two representations, facilitating knowledge transfer and supporting robust preference modeling. Extensive experiments on three real-world datasets show that \\name~ effectively balances recommendation quality for both head and tail items while surpassing the existing baselines. The implementation code can be found online\\footnote{https://github.com/ziwliu8/H2Rec}.",
    "published": "2025-12-11T07:50:53Z",
    "updated": "2025-12-11T07:50:53Z",
    "link": "http://arxiv.org/pdf/2512.10388v1.pdf",
    "category": [
      "cs.IR",
      "cs.AI"
    ],
    "authors": [
      "Ziwei Liu",
      "Yejing Wang",
      "Qidong Liu",
      "Zijian Zhang",
      "Chong Chen",
      "Wei Huang",
      "Xiangyu Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10384v1",
    "title": "Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies",
    "summary": "Large Vision Language Models (LVLMs) have made remarkable progress, enabling sophisticated vision-language interaction and dialogue applications. However, existing benchmarks primarily focus on reasoning tasks, often neglecting fine-grained recognition, which is crucial for practical application scenarios. To address this gap, we introduce the Fine-grained Recognition Open World (FROW) benchmark, designed for detailed evaluation of LVLMs with GPT-4o. On the basis of that, we propose a novel optimization strategy from two perspectives: \\textit{data construction} and \\textit{training process}, to improve the performance of LVLMs. Our dataset includes mosaic data, which combines multiple short-answer responses, and open-world data, generated from real-world questions and answers using GPT-4o, creating a comprehensive framework for evaluating fine-grained recognition in LVLMs. Experiments show that mosaic data improves category recognition accuracy by 1\\% and open-world data boosts FROW benchmark accuracy by 10\\%-20\\% and content accuracy by 6\\%-12\\%. Meanwhile, incorporating fine-grained data into the pre-training phase can improve the model's category recognition accuracy by up to 10\\%. The benchmark will be available at https://github.com/pc-inno/FROW.",
    "published": "2025-12-11T07:48:34Z",
    "updated": "2025-12-11T07:48:34Z",
    "link": "http://arxiv.org/pdf/2512.10384v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Cong Pang",
      "Hongtao Yu",
      "Zixuan Chen",
      "Lewei Lu",
      "Xin Lou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.04522v2",
    "title": "Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction",
    "summary": "Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.",
    "published": "2025-10-06T06:29:49Z",
    "updated": "2025-12-11T07:48:20Z",
    "link": "http://arxiv.org/pdf/2510.04522v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Yisen Gao",
      "Xingcheng Fu",
      "Qingyun Sun",
      "Jianxin Li",
      "Xianxian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10375v1",
    "title": "Neural personal sound zones with flexible bright zone control",
    "summary": "Personal sound zone (PSZ) reproduction system, which attempts to create distinct virtual acoustic scenes for different listeners at their respective positions within the same spatial area using one loudspeaker array, is a fundamental technology in the application of virtual reality. For practical applications, the reconstruction targets must be measured on the same fixed receiver array used to record the local room impulse responses (RIRs) from the loudspeaker array to the control points in each PSZ, which makes the system inconvenient and costly for real-world use. In this paper, a 3D convolutional neural network (CNN) designed for PSZ reproduction with flexible control microphone grid and alternative reproduction target is presented, utilizing the virtual target scene as inputs and the PSZ pre-filters as output. Experimental results of the proposed method are compared with the traditional method, demonstrating that the proposed method is able to handle varied reproduction targets on flexible control point grid using only one training session. Furthermore, the proposed method also demonstrates the capability to learn global spatial information from sparse sampling points distributed in PSZs.",
    "published": "2025-12-11T07:41:15Z",
    "updated": "2025-12-11T07:41:15Z",
    "link": "http://arxiv.org/pdf/2512.10375v1.pdf",
    "category": [
      "cs.SD",
      "cs.AI"
    ],
    "authors": [
      "Wenye Zhu",
      "Jun Tang",
      "Xiaofei Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10372v1",
    "title": "D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning",
    "summary": "The rising demand for collaborative machine learning and data analytics calls for secure and decentralized data sharing frameworks that balance privacy, trust, and incentives. Existing approaches, including federated learning (FL) and blockchain-based data markets, fall short: FL often depends on trusted aggregators and lacks Byzantine robustness, while blockchain frameworks struggle with computation-intensive training and incentive integration.\n  We present \\prot, a decentralized data marketplace that unifies federated learning, blockchain arbitration, and economic incentives into a single framework for privacy-preserving data sharing. \\prot\\ enables data buyers to submit bid-based requests via blockchain smart contracts, which manage auctions, escrow, and dispute resolution. Computationally intensive training is delegated to \\cone\\ (\\uline{Co}mpute \\uline{N}etwork for \\uline{E}xecution), an off-chain distributed execution layer. To safeguard against adversarial behavior, \\prot\\ integrates a modified YODA protocol with exponentially growing execution sets for resilient consensus, and introduces Corrected OSMD to mitigate malicious or low-quality contributions from sellers. All protocols are incentive-compatible, and our game-theoretic analysis establishes honesty as the dominant strategy.\n  We implement \\prot\\ on Ethereum and evaluate it over benchmark datasets -- MNIST, Fashion-MNIST, and CIFAR-10 -- under varying adversarial settings. \\prot\\ achieves up to 99\\% accuracy on MNIST and 90\\% on Fashion-MNIST, with less than 3\\% degradation up to 30\\% Byzantine nodes, and 56\\% accuracy on CIFAR-10 despite its complexity. Our results show that \\prot\\ ensures privacy, maintains robustness under adversarial conditions, and scales efficiently with the number of participants, making it a practical foundation for real-world decentralized data sharing.",
    "published": "2025-12-11T07:38:05Z",
    "updated": "2025-12-11T07:38:05Z",
    "link": "http://arxiv.org/pdf/2512.10372v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "authors": [
      "Yash Srivastava",
      "Shalin Jain",
      "Sneha Awathare",
      "Nitin Awathare"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10371v1",
    "title": "AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management",
    "summary": "The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.",
    "published": "2025-12-11T07:37:38Z",
    "updated": "2025-12-11T07:37:38Z",
    "link": "http://arxiv.org/pdf/2512.10371v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Shizuo Tian",
      "Hao Wen",
      "Yuxuan Chen",
      "Jiacheng Liu",
      "Shanhui Zhao",
      "Guohong Liu",
      "Ju Ren",
      "Yunxin Liu",
      "Yuanchun Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10370v1",
    "title": "LLM-Empowered Representation Learning for Emerging Item Recommendation",
    "summary": "In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.",
    "published": "2025-12-11T07:36:44Z",
    "updated": "2025-12-11T07:36:44Z",
    "link": "http://arxiv.org/pdf/2512.10370v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ziying Zhang",
      "Quanming Yao",
      "Yaqing Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.20586v3",
    "title": "PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic",
    "summary": "Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics, such as accuracy and precision, fail to appropriately capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the Parallel Trust Assessment System (PaTAS), a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through Trust Nodes and Trust Functions that propagate input, parameter, and activation trust across the network. The framework defines a Parameter Trust Update mechanism to refine parameter reliability during training and an Inference-Path Trust Assessment (IPTA) method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a foundation for evaluating model reliability across the AI lifecycle.",
    "published": "2025-11-25T18:15:36Z",
    "updated": "2025-12-11T07:35:44Z",
    "link": "http://arxiv.org/pdf/2511.20586v3.pdf",
    "category": [
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Koffi Ismael Ouattara",
      "Ioannis Krontiris",
      "Theo Dimitrakos",
      "Dennis Eisermann",
      "Houda Labiod",
      "Frank Kargl"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10365v1",
    "title": "GPG: Generalized Policy Gradient Theorem for Transformer-based Policies",
    "summary": "We present the Generalized Policy Gradient (GPG) Theorem, specifically designed for Transformer-based policies. Notably, we demonstrate that both standard Policy Gradient Theorem and GRPO emerge as special cases within our GPG framework. Furthermore, we explore its practical applications in training Large Language Models (LLMs), offering new insights into efficient policy optimization.",
    "published": "2025-12-11T07:30:33Z",
    "updated": "2025-12-11T07:30:33Z",
    "link": "http://arxiv.org/pdf/2512.10365v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "authors": [
      "Hangyu Mao",
      "Guangting Dong",
      "Zhicheng Dou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10362v1",
    "title": "Visual Funnel: Resolving Contextual Blindness in Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) demonstrate impressive reasoning capabilities, but often fail to perceive fine-grained visual details, limiting their applicability in precision-demanding tasks. While methods that crop salient regions of an image offer a partial solution, we identify a critical limitation they introduce: \"Contextual Blindness\". This failure occurs due to structural disconnect between high-fidelity details (from the crop) and the broader global context (from the original image), even when all necessary visual information is present. We argue that this limitation stems not from a lack of information 'Quantity', but from a lack of 'Structural Diversity' in the model's input. To resolve this, we propose Visual Funnel, a training-free, two-step approach. Visual Funnel first performs Contextual Anchoring to identify the region of interest in a single forward pass. It then constructs an Entropy-Scaled Portfolio that preserves the hierarchical context - ranging from focal detail to broader surroundings - by dynamically determining crop sizes based on attention entropy and refining crop centers. Through extensive experiments, we demonstrate that Visual Funnel significantly outperforms naive single-crop and unstructured multi-crop baselines. Our results further validate that simply adding more unstructured crops provides limited or even detrimental benefits, confirming that the hierarchical structure of our portfolio is key to resolving Contextual Blindness.",
    "published": "2025-12-11T07:22:54Z",
    "updated": "2025-12-11T07:22:54Z",
    "link": "http://arxiv.org/pdf/2512.10362v1.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Woojun Jung",
      "Jaehoon Go",
      "Mingyu Jeon",
      "Sunjae Yoon",
      "Junyeong Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10350v1",
    "title": "Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories",
    "summary": "Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems.\n  We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors.\n  Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion.\n  Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.",
    "published": "2025-12-11T07:06:14Z",
    "updated": "2025-12-11T07:06:14Z",
    "link": "http://arxiv.org/pdf/2512.10350v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Nicolas Tacheny"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10348v1",
    "title": "REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature",
    "summary": "Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.",
    "published": "2025-12-11T07:05:36Z",
    "updated": "2025-12-11T07:05:36Z",
    "link": "http://arxiv.org/pdf/2512.10348v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Wenhan Wu",
      "Zhili He",
      "Huanghuang Liang",
      "Yili Gong",
      "Jiawei Jiang",
      "Chuang Hu",
      "Dazhao Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03454v2",
    "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles",
    "summary": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.",
    "published": "2025-12-03T05:14:16Z",
    "updated": "2025-12-11T07:03:44Z",
    "link": "http://arxiv.org/pdf/2512.03454v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Haicheng Liao",
      "Huanming Shen",
      "Bonan Wang",
      "Yongkang Li",
      "Yihong Tang",
      "Chengyue Wang",
      "Dingyi Zhuang",
      "Kehua Chen",
      "Hai Yang",
      "Chengzhong Xu",
      "Zhenning Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.16203v3",
    "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models",
    "summary": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.",
    "published": "2025-11-20T10:14:32Z",
    "updated": "2025-12-11T06:59:01Z",
    "link": "http://arxiv.org/pdf/2511.16203v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yuping Yan",
      "Yuhan Xie",
      "Yixin Zhang",
      "Lingjuan Lyu",
      "Handing Wang",
      "Yaochu Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.26631v3",
    "title": "Learning Generalizable Shape Completion with SIM(3) Equivariance",
    "summary": "3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.",
    "published": "2025-09-30T17:58:55Z",
    "updated": "2025-12-11T06:52:51Z",
    "link": "http://arxiv.org/pdf/2509.26631v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Yuqing Wang",
      "Zhaiyu Chen",
      "Xiao Xiang Zhu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10341v1",
    "title": "A Privacy-Preserving Cloud Architecture for Distributed Machine Learning at Scale",
    "summary": "Distributed machine learning systems require strong privacy guarantees, verifiable compliance, and scalable deploy- ment across heterogeneous and multi-cloud environments. This work introduces a cloud-native privacy-preserving architecture that integrates federated learning, differential privacy, zero- knowledge compliance proofs, and adaptive governance powered by reinforcement learning. The framework supports secure model training and inference without centralizing sensitive data, while enabling cryptographically verifiable policy enforcement across institutions and cloud platforms. A full prototype deployed across hybrid Kubernetes clusters demonstrates reduced membership- inference risk, consistent enforcement of formal privacy budgets, and stable model performance under differential privacy. Ex- perimental evaluation across multi-institution workloads shows that the architecture maintains utility with minimal overhead while providing continuous, risk-aware governance. The pro- posed framework establishes a practical foundation for deploying trustworthy and compliant distributed machine learning systems at scale.",
    "published": "2025-12-11T06:46:46Z",
    "updated": "2025-12-11T06:46:46Z",
    "link": "http://arxiv.org/pdf/2512.10341v1.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Vinoth Punniyamoorthy",
      "Ashok Gadi Parthi",
      "Mayilsamy Palanigounder",
      "Ravi Kiran Kodali",
      "Bikesh Kumar",
      "Kabilan Kannan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.19195v3",
    "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
    "summary": "Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are $\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive",
    "published": "2025-10-22T03:02:38Z",
    "updated": "2025-12-11T06:46:07Z",
    "link": "http://arxiv.org/pdf/2510.19195v3.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Kai Zeng",
      "Zhanqian Wu",
      "Kaixin Xiong",
      "Xiaobao Wei",
      "Xiangyu Guo",
      "Zhenxin Zhu",
      "Kalok Ho",
      "Lijun Zhou",
      "Bohan Zeng",
      "Ming Lu",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Hangjun Ye",
      "Wentao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10339v1",
    "title": "On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering",
    "summary": "Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.",
    "published": "2025-12-11T06:44:08Z",
    "updated": "2025-12-11T06:44:08Z",
    "link": "http://arxiv.org/pdf/2512.10339v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Ziseok Lee",
      "Minyeong Hwang",
      "Sanghyun Jo",
      "Wooyeol Lee",
      "Jihyung Ko",
      "Young Bin Park",
      "Jae-Mun Choi",
      "Eunho Yang",
      "Kyungsu Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07702v2",
    "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment",
    "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.",
    "published": "2025-12-08T16:49:19Z",
    "updated": "2025-12-11T06:42:25Z",
    "link": "http://arxiv.org/pdf/2512.07702v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Sangha Park",
      "Eunji Kim",
      "Yeongtak Oh",
      "Jooyoung Choi",
      "Sungroh Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10336v1",
    "title": "Multilingual VLM Training: Adapting an English-Trained VLM to French",
    "summary": "Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.",
    "published": "2025-12-11T06:38:51Z",
    "updated": "2025-12-11T06:38:51Z",
    "link": "http://arxiv.org/pdf/2512.10336v1.pdf",
    "category": [
      "cs.CL",
      "cs.AI"
    ],
    "authors": [
      "Jules Lahmi",
      "Alexis Roger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07730v2",
    "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination",
    "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.",
    "published": "2025-12-08T17:20:07Z",
    "updated": "2025-12-11T06:37:50Z",
    "link": "http://arxiv.org/pdf/2512.07730v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Sangha Park",
      "Seungryong Yoo",
      "Jisoo Mok",
      "Sungroh Yoon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10322v1",
    "title": "User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation",
    "summary": "Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.",
    "published": "2025-12-11T06:11:45Z",
    "updated": "2025-12-11T06:11:45Z",
    "link": "http://arxiv.org/pdf/2512.10322v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yongqiang Yu",
      "Xuhui Li",
      "Hazza Mahmood",
      "Jinxing Zhou",
      "Haodong Hong",
      "Longtao Jiang",
      "Zhiqiang Xu",
      "Qi Wu",
      "Xiaojun Chang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10317v1",
    "title": "Translating Informal Proofs into Formal Proofs Using a Chain of States",
    "summary": "We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.",
    "published": "2025-12-11T06:08:34Z",
    "updated": "2025-12-11T06:08:34Z",
    "link": "http://arxiv.org/pdf/2512.10317v1.pdf",
    "category": [
      "cs.LO",
      "cs.AI"
    ],
    "authors": [
      "Ziyu Wang",
      "Bowen Yang",
      "Shihao Zhou",
      "Chenyi Li",
      "Yuan Zhang",
      "Bin Dong",
      "Zaiwen Wen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10313v1",
    "title": "EpiPlanAgent: Agentic Automated Epidemic Response Planning",
    "summary": "Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.",
    "published": "2025-12-11T06:03:17Z",
    "updated": "2025-12-11T06:03:17Z",
    "link": "http://arxiv.org/pdf/2512.10313v1.pdf",
    "category": [
      "cs.AI",
      "cs.CY"
    ],
    "authors": [
      "Kangkun Mao",
      "Fang Xu",
      "Jinru Ding",
      "Yidong Jiang",
      "Yujun Yao",
      "Yirong Chen",
      "Junming Liu",
      "Xiaoqin Wu",
      "Qian Wu",
      "Xiaoyan Huang",
      "Jie Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10312v1",
    "title": "High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments",
    "summary": "This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.",
    "published": "2025-12-11T06:02:13Z",
    "updated": "2025-12-11T06:02:13Z",
    "link": "http://arxiv.org/pdf/2512.10312v1.pdf",
    "category": [
      "cs.DC",
      "cs.AI"
    ],
    "authors": [
      "Julian Rodriguez",
      "Piotr Lopez",
      "Emiliano Lerma",
      "Rafael Medrano",
      "Jacobo Hernandez"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10305v1",
    "title": "InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck",
    "summary": "Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.",
    "published": "2025-12-11T05:51:02Z",
    "updated": "2025-12-11T05:51:02Z",
    "link": "http://arxiv.org/pdf/2512.10305v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Quanmin Wei",
      "Penglin Dai",
      "Wei Li",
      "Bingyi Liu",
      "Xiao Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10304v1",
    "title": "Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance",
    "summary": "As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.",
    "published": "2025-12-11T05:49:26Z",
    "updated": "2025-12-11T05:49:26Z",
    "link": "http://arxiv.org/pdf/2512.10304v1.pdf",
    "category": [
      "cs.AI",
      "cs.ET"
    ],
    "authors": [
      "Byeong Ho Kang",
      "Wenli Yang",
      "Muhammad Bilal Amin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10300v1",
    "title": "Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules",
    "summary": "Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.",
    "published": "2025-12-11T05:42:53Z",
    "updated": "2025-12-11T05:42:53Z",
    "link": "http://arxiv.org/pdf/2512.10300v1.pdf",
    "category": [
      "cs.AI"
    ],
    "authors": [
      "Yanbei Jiang",
      "Xueqi Ma",
      "Shu Liu",
      "Sarah Monazam Erfani",
      "Tongliang Liu",
      "James Bailey",
      "Jey Han Lau",
      "Krista A. Ehinger"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.16069v2",
    "title": "Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots",
    "summary": "As design thinking education grows in secondary and tertiary contexts, educators face the challenge of evaluating creative artefacts that combine visual and textual elements. Traditional rubric-based assessment is laborious, time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in large, multi-section cohorts. This paper presents an exploratory study investigating the reliability and perceived accuracy of AI-assisted assessment compared to TA-assisted assessment in evaluating student posters in design thinking education. Two activities were conducted with 33 Ministry of Education (MOE) Singapore school teachers to (1) compare AI-generated scores with TA grading across three key dimensions: empathy and user understanding, identification of pain points and opportunities, and visual communication, and (2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid scores. Results showed low statistical agreement between instructor and AI scores for empathy and pain points, with slightly higher alignment for visual communication. Teachers preferred TA-assigned scores in six of ten samples. Qualitative feedback highlighted the potential of AI for formative feedback, consistency, and student self-reflection, but raised concerns about its limitations in capturing contextual nuance and creative insight. The study underscores the need for hybrid assessment models that integrate computational efficiency with human insights. This research contributes to the evolving conversation on responsible AI adoption in creative disciplines, emphasizing the balance between automation and human judgment for scalable and pedagogically sound assessment.",
    "published": "2025-10-17T07:09:21Z",
    "updated": "2025-12-11T05:38:34Z",
    "link": "http://arxiv.org/pdf/2510.16069v2.pdf",
    "category": [
      "cs.CY",
      "cs.AI"
    ],
    "authors": [
      "Sumbul Khan",
      "Wei Ting Liow",
      "Lay Kee Ang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10296v1",
    "title": "FLARE: A Wireless Side-Channel Fingerprinting Attack on Federated Learning",
    "summary": "Federated Learning (FL) enables collaborative model training across distributed devices while safeguarding data and user privacy. However, FL remains susceptible to privacy threats that can compromise data via direct means. That said, indirectly compromising the confidentiality of the FL model architecture (e.g., a convolutional neural network (CNN) or a recurrent neural network (RNN)) on a client device by an outsider remains unexplored. If leaked, this information can enable next-level attacks tailored to the architecture. This paper proposes a novel side-channel fingerprinting attack, leveraging flow-level and packet-level statistics of encrypted wireless traffic from an FL client to infer its deep learning model architecture. We name it FLARE, a fingerprinting framework based on FL Architecture REconnaissance. Evaluation across various CNN and RNN variants-including pre-trained and custom models trained over IEEE 802.11 Wi-Fi-shows that FLARE achieves over 98% F1-score in closed-world and up to 91% in open-world scenarios. These results reveal that CNN and RNN models leak distinguishable traffic patterns, enabling architecture fingerprinting even under realistic FL settings with hardware, software, and data heterogeneity. To our knowledge, this is the first work to fingerprint FL model architectures by sniffing encrypted wireless traffic, exposing a critical side-channel vulnerability in current FL systems.",
    "published": "2025-12-11T05:32:34Z",
    "updated": "2025-12-11T05:32:34Z",
    "link": "http://arxiv.org/pdf/2512.10296v1.pdf",
    "category": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Md Nahid Hasan Shuvo",
      "Moinul Hossain",
      "Anik Mallik",
      "Jeffrey Twigg",
      "Fikadu Dagefu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20566v2",
    "title": "Balanced Online Class-Incremental Learning via Dual Classifiers",
    "summary": "Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Inclusive Separation for Online iNcremental learning (BISON), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BISON method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BISON, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods.",
    "published": "2025-04-29T09:13:00Z",
    "updated": "2025-12-11T05:18:44Z",
    "link": "http://arxiv.org/pdf/2504.20566v2.pdf",
    "category": [
      "cs.LG",
      "cs.AI"
    ],
    "authors": [
      "Shunjie Wen",
      "Thomas Heinis",
      "Dong-Wan Choi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.22613v2",
    "title": "Variational analysis of determinantal varieties",
    "summary": "Determinantal varieties -- the sets of bounded-rank matrices or tensors -- have attracted growing interest in low-rank optimization. The tangent cone to low-rank sets is widely studied and underpins a range of geometric methods. The second-order geometry, which encodes curvature information, is more intricate. In this work, we develop a unified framework to derive explicit formulas for both first- and second-order tangent sets to various low-rank sets, including low-rank matrices, tensors, symmetric matrices, and positive semidefinite matrices. The framework also accommodates the intersection of a low-rank set and another set satisfying mild assumptions, thereby yielding a tangent intersection rule. Through the lens of tangent sets, we establish a necessary and sufficient condition under which a nonsmooth problem and its smooth parameterization share equivalent second-order stationary points. Moreover, we exploit tangent sets to characterize optimality conditions for low-rank optimization and prove that verifying second-order optimality is NP-hard. In a separate line of analysis, we investigate variational geometry of the graph of the normal cone to matrix varieties, deriving the explicit Bouligand tangent cone, Fréchet and Mordukhovich normal cones to the graph. These results are further applied to develop optimality conditions for low-rank bilevel programs.",
    "published": "2025-11-27T16:48:07Z",
    "updated": "2025-12-11T05:13:53Z",
    "link": "http://arxiv.org/pdf/2511.22613v2.pdf",
    "category": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "authors": [
      "Yan Yang",
      "Bin Gao",
      "Ya-xiang Yuan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07062v2",
    "title": "$\\mathrm{D}^\\mathrm{3}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction",
    "summary": "Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^\\mathrm{3}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^\\mathrm{3}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^\\mathrm{3}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.",
    "published": "2025-12-08T00:39:32Z",
    "updated": "2025-12-11T05:07:55Z",
    "link": "http://arxiv.org/pdf/2512.07062v2.pdf",
    "category": [
      "cs.CV",
      "cs.AI"
    ],
    "authors": [
      "Changliang Xia",
      "Chengyou Jia",
      "Minnan Luo",
      "Zhuohang Dang",
      "Xin Shen",
      "Bowen Ping"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10931v1",
    "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
    "summary": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
    "published": "2025-12-11T18:57:02Z",
    "updated": "2025-12-11T18:57:02Z",
    "link": "http://arxiv.org/pdf/2512.10931v1.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "George Yakushev",
      "Nataliia Babina",
      "Masoud Vahid Dastgerdi",
      "Vyacheslav Zhdanovskiy",
      "Alina Shutova",
      "Denis Kuznedelev"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10918v1",
    "title": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences",
    "summary": "Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences.",
    "published": "2025-12-11T18:44:44Z",
    "updated": "2025-12-11T18:44:44Z",
    "link": "http://arxiv.org/pdf/2512.10918v1.pdf",
    "category": [
      "cs.HC",
      "cs.CL"
    ],
    "authors": [
      "Yiyang Wang",
      "Chen Chen",
      "Tica Lin",
      "Vishnu Raj",
      "Josh Kimball",
      "Alex Cabral",
      "Josiah Hester"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10882v1",
    "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity",
    "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.",
    "published": "2025-12-11T18:11:46Z",
    "updated": "2025-12-11T18:11:46Z",
    "link": "http://arxiv.org/pdf/2512.10882v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hauke Licht"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10865v1",
    "title": "Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python",
    "summary": "This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.",
    "published": "2025-12-11T17:58:17Z",
    "updated": "2025-12-11T17:58:17Z",
    "link": "http://arxiv.org/pdf/2512.10865v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lilin Qiu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.17090v3",
    "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions",
    "summary": "Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.",
    "published": "2025-06-20T15:53:51Z",
    "updated": "2025-12-11T17:53:55Z",
    "link": "http://arxiv.org/pdf/2506.17090v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Murtaza Nazir",
      "Matthew Finlayson",
      "John X. Morris",
      "Xiang Ren",
      "Swabha Swayamdipta"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09015v2",
    "title": "Luxical: High-Speed Lexical-Dense Text Embeddings",
    "summary": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.",
    "published": "2025-12-09T18:58:44Z",
    "updated": "2025-12-11T17:14:51Z",
    "link": "http://arxiv.org/pdf/2512.09015v2.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      " DatologyAI",
      " :",
      "Luke Merrick",
      "Alex Fang",
      "Aldo Carranza",
      "Alvin Deng",
      "Amro Abbas",
      "Brett Larsen",
      "Cody Blakeney",
      "Darren Teh",
      "David Schwab",
      "Fan Pan",
      "Haakon Mongstad",
      "Haoli Yin",
      "Jack Urbanek",
      "Jason Lee",
      "Jason Telanoff",
      "Josh Wills",
      "Kaleigh Mentzer",
      "Paul Burstein",
      "Parth Doshi",
      "Paul Burnstein",
      "Pratyush Maini",
      "Ricardo Monti",
      "Rishabh Adiga",
      "Scott Loftin",
      "Siddharth Joshi",
      "Spandan Das",
      "Tony Jiang",
      "Vineeth Dorna",
      "Zhengping Wang",
      "Bogdan Gaza",
      "Ari Morcos",
      "Matthew Leavitt"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10780v1",
    "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting",
    "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.",
    "published": "2025-12-11T16:15:42Z",
    "updated": "2025-12-11T16:15:42Z",
    "link": "http://arxiv.org/pdf/2512.10780v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Manurag Khullar",
      "Utkarsh Desai",
      "Poorva Malviya",
      "Aman Dalmia",
      "Zheyuan Ryan Shi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.13886v8",
    "title": "Game-RL: Synthesizing Multimodal Verifiable Game Data to Boost VLMs' General Reasoning",
    "summary": "Vision-language reinforcement learning (RL) has primarily focused on narrow domains (e.g. geometry or chart reasoning). This leaves broader training scenarios and resources underexplored, limiting the exploration and learning of Vision Language Models (VLMs) through RL. We find video games inherently provide rich visual elements and mechanics that are easy to verify. To fully use the multimodal and verifiable reward in video games, we propose Game-RL, constructing diverse game tasks for RL training to boost VLMs general reasoning ability. To obtain training data, we propose Code2Logic, a novel approach that adapts game code to synthesize game reasoning task data, thus obtaining the GameQA dataset of 30 games and 158 tasks with controllable difficulty gradation. Unexpectedly, RL training solely on GameQA enables multiple VLMs to achieve performance improvements across 7 diverse vision-language benchmarks, demonstrating the value of Game-RL for enhancing VLMs' general reasoning. Furthermore, this suggests that video games may serve as valuable scenarios and resources to boost general reasoning abilities. Our code, dataset and models are available at the GitHub repository.",
    "published": "2025-05-20T03:47:44Z",
    "updated": "2025-12-11T15:59:47Z",
    "link": "http://arxiv.org/pdf/2505.13886v8.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jingqi Tong",
      "Jixin Tang",
      "Hangcheng Li",
      "Yurong Mou",
      "Ming Zhang",
      "Jun Zhao",
      "Yanbo Wen",
      "Fan Song",
      "Jiahao Zhan",
      "Yuyang Lu",
      "Chaoran Tao",
      "Zhiyuan Guo",
      "Jizhou Yu",
      "Tianhao Cheng",
      "Zhiheng Xi",
      "Changhao Jiang",
      "Zhangyue Yin",
      "Yining Zheng",
      "Weifeng Ge",
      "Guanhua Chen",
      "Tao Gui",
      "Xipeng Qiu",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10756v1",
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
    "published": "2025-12-11T15:47:38Z",
    "updated": "2025-12-11T15:47:38Z",
    "link": "http://arxiv.org/pdf/2512.10756v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Zijian Wu",
      "Lingkai Kong",
      "Wenwei Zhang",
      "Songyang Gao",
      "Yuzhe Gu",
      "Zhongrui Cai",
      "Tianyou Ma",
      "Yuhong Liu",
      "Zhi Wang",
      "Runyuan Ma",
      "Guangyu Wang",
      "Wei Li",
      "Conghui He",
      "Dahua Lin",
      "Kai Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10741v1",
    "title": "TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage",
    "summary": "Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.\n  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.\n  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.",
    "published": "2025-12-11T15:29:33Z",
    "updated": "2025-12-11T15:29:33Z",
    "link": "http://arxiv.org/pdf/2512.10741v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Elroy Galbraith",
      "Chadwick Sutherland",
      "Donahue Morgan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.18708v2",
    "title": "The Spatial Semantics of Iconic Gesture",
    "summary": "The current multimodal turn in linguistic theory leaves a crucial question unanswered: what is the meaning of iconic gestures, and how does it compose with speech meaning? We argue for a separation of linguistic and visual levels of meaning and introduce a spatial gesture semantics that closes this gap. Iconicity is differentiated into three aspects: Firstly, an interpretation of the form of a gesture in terms of a translation from kinematic gesture annotations into vector sequences (iconic model). Secondly, a truth-functional evaluation of the iconic model within spatially extended domains (embedding). Since a simple embedding is too strong, we identify a number of transformations that can be applied to iconic models, namely rotation, scaling, perspective fixation, and quotation of handshape. Thirdly, the linguistic description or classification of an iconic model (informational evaluation). Since the informational evaluation of an iconic gesture is a heuristic act, it needs a place in a semantic theory of visual communication. Informational evaluation lifts a gesture to a quasi-linguistic level that can interact with verbal content. This interaction is either vacuous, or regimented by usual lexicon-driven inferences discussed in dynamic semantic frameworks.",
    "published": "2024-04-29T13:58:03Z",
    "updated": "2025-12-11T14:29:42Z",
    "link": "http://arxiv.org/pdf/2404.18708v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Andy Lücking",
      "Alexander Henlein",
      "Alexander Mehler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05414v3",
    "title": "LMSpell: Neural Spell Checking for Low-Resource Languages",
    "summary": "Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.",
    "published": "2025-12-05T04:14:09Z",
    "updated": "2025-12-11T14:22:42Z",
    "link": "http://arxiv.org/pdf/2512.05414v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Akesh Gunathilake",
      "Nadil Karunarathna",
      "Tharusha Bandaranayake",
      "Nisansa de Silva",
      "Surangika Ranathunga"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08158v2",
    "title": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs",
    "summary": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.",
    "published": "2025-10-09T12:38:16Z",
    "updated": "2025-12-11T13:48:33Z",
    "link": "http://arxiv.org/pdf/2510.08158v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Shuzhou Yuan",
      "Ercong Nie",
      "Yinuo Sun",
      "Chenxuan Zhao",
      "William LaCroix",
      "Michael Färber"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10630v1",
    "title": "From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages",
    "summary": "Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.",
    "published": "2025-12-11T13:29:25Z",
    "updated": "2025-12-11T13:29:25Z",
    "link": "http://arxiv.org/pdf/2512.10630v1.pdf",
    "category": [
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Smiljana Antonijevic Ubois"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10624v1",
    "title": "AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence",
    "summary": "Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.",
    "published": "2025-12-11T13:24:40Z",
    "updated": "2025-12-11T13:24:40Z",
    "link": "http://arxiv.org/pdf/2512.10624v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Bo Yang",
      "Lanfei Feng",
      "Yunkui Chen",
      "Yu Zhang",
      "Jianyu Zhang",
      "Xiao Xu",
      "Nueraili Aierken",
      "Shijian Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.03270v2",
    "title": "SCALE: Upscaled Continual Learning of Large Language Models",
    "summary": "We revisit continual pre-training for large language models and argue that progress now depends more on scaling the right structure than on scaling parameters alone. We introduce SCALE, a width upscaling architecture that inserts lightweight expansion into linear modules while freezing all pre-trained parameters. This preserves the residual and attention topologies and increases capacity without perturbing the base model's original functionality. SCALE is guided by two principles: Persistent Preservation, which maintains the base model's behavior via preservation-oriented initialization and freezing of the pre-trained weights, and Collaborative Adaptation, which selectively trains a subset of expansion components to acquire new knowledge with minimal interference. We instantiate these ideas as SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and SCALE-Route, an optional routing extension that performs token-level routing between preservation and adaptation heads. On a controlled synthetic biography benchmark, SCALE mitigates the severe forgetting observed with depth expansion while still acquiring new knowledge. In continual pre-training on a Korean corpus, SCALE variants achieve less forgetting on English evaluations and competitive gains on Korean benchmarks, with these variants offering the best overall stability-plasticity trade-off. Accompanying analysis clarifies when preservation provably holds and why the interplay between preservation and adaptation stabilizes optimization compared to standard continual learning setups.",
    "published": "2025-11-05T08:05:50Z",
    "updated": "2025-12-11T12:41:40Z",
    "link": "http://arxiv.org/pdf/2511.03270v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Jin-woo Lee",
      "Junhwa Choi",
      "Bongkyu Hwang",
      "Jinho Choo",
      "Bogun Kim",
      "JeongSeon Yi",
      "Joonseok Lee",
      "DongYoung Jung",
      "Jaeseon Park",
      "Kyoungwon Park",
      "Suk-hoon Jung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10575v1",
    "title": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems",
    "summary": "Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.",
    "published": "2025-12-11T12:04:46Z",
    "updated": "2025-12-11T12:04:46Z",
    "link": "http://arxiv.org/pdf/2512.10575v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Hang Ding",
      "Qiming Feng",
      "Dongqi Liu",
      "Qi Zhao",
      "Tao Yao",
      "Shuo Wang",
      "Dongsheng Chen",
      "Jian Li",
      "Zhenye Gan",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yabiao Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05647v2",
    "title": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight",
    "summary": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.",
    "published": "2025-12-05T11:47:33Z",
    "updated": "2025-12-11T11:58:25Z",
    "link": "http://arxiv.org/pdf/2512.05647v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Giorgos Antoniou",
      "Giorgos Filandrianos",
      "Aggelos Vlachos",
      "Giorgos Stamou",
      "Lampros Kollimenos",
      "Konstantinos Skianis",
      "Michalis Vazirgiannis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10561v1",
    "title": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models",
    "summary": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.",
    "published": "2025-12-11T11:46:48Z",
    "updated": "2025-12-11T11:46:48Z",
    "link": "http://arxiv.org/pdf/2512.10561v1.pdf",
    "category": [
      "cs.CL",
      "cs.LG"
    ],
    "authors": [
      "Amartya Roy",
      "Elamparithy M",
      "Kripabandhu Ghosh",
      "Ponnurangam Kumaraguru",
      "Adrian de Wynter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10545v1",
    "title": "XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs",
    "summary": "Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.",
    "published": "2025-12-11T11:22:53Z",
    "updated": "2025-12-11T11:22:53Z",
    "link": "http://arxiv.org/pdf/2512.10545v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Iñaki Lacunza",
      "José Javier Saiz",
      "Alexander Shvets",
      "Aitor Gonzalez-Agirre",
      "Marta Villegas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2407.03859v3",
    "title": "Anthropocentric bias in language model evaluation",
    "summary": "Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (\"auxiliary oversight\"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (\"mechanistic chauvinism\"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.",
    "published": "2024-07-04T11:44:28Z",
    "updated": "2025-12-11T11:10:15Z",
    "link": "http://arxiv.org/pdf/2407.03859v3.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Raphaël Millière",
      "Charles Rathkopf"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2502.00791v5",
    "title": "Vision-centric Token Compression in Large Language Model",
    "summary": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.",
    "published": "2025-02-02T13:10:06Z",
    "updated": "2025-12-11T09:27:00Z",
    "link": "http://arxiv.org/pdf/2502.00791v5.pdf",
    "category": [
      "cs.CL",
      "cs.CV"
    ],
    "authors": [
      "Ling Xing",
      "Alex Jinpeng Wang",
      "Rui Yan",
      "Xiangbo Shu",
      "Jinhui Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10453v1",
    "title": "Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs",
    "summary": "What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.\n  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.",
    "published": "2025-12-11T09:17:35Z",
    "updated": "2025-12-11T09:17:35Z",
    "link": "http://arxiv.org/pdf/2512.10453v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Lars G. B. Johnsen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10441v1",
    "title": "Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis",
    "summary": "This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.",
    "published": "2025-12-11T09:06:45Z",
    "updated": "2025-12-11T09:06:45Z",
    "link": "http://arxiv.org/pdf/2512.10441v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nour El Houda Ben Chaabene",
      "Hamza Hammami",
      "Laid Kahloul"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10440v1",
    "title": "Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT",
    "summary": "Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.",
    "published": "2025-12-11T09:02:45Z",
    "updated": "2025-12-11T09:02:45Z",
    "link": "http://arxiv.org/pdf/2512.10440v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Nour El Houda Ben Chaabene",
      "Hamza Hammami"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10435v1",
    "title": "Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring \"Tortured Phrases\" in Scientific Literature",
    "summary": "The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate \"tortured phrases\", statistically improbable synonyms (e.g. \"counterfeit consciousness\" for \"artificial intelligence\"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.",
    "published": "2025-12-11T08:53:25Z",
    "updated": "2025-12-11T08:53:25Z",
    "link": "http://arxiv.org/pdf/2512.10435v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Agniva Maiti",
      "Prajwal Panth",
      "Suresh Chandra Satapathy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10430v1",
    "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
    "published": "2025-12-11T08:40:10Z",
    "updated": "2025-12-11T08:40:10Z",
    "link": "http://arxiv.org/pdf/2512.10430v1.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Dmitrii Stoianov",
      "Danil Taranets",
      "Olga Tsymboi",
      "Ramil Latypov",
      "Almaz Dautov",
      "Vladislav Kruglikov",
      "Nikita Surkov",
      "German Abramov",
      "Pavel Gein",
      "Dmitry Abulkhanov",
      "Mikhail Gashkov",
      "Viktor Zelenkovskiy",
      "Artem Batalov",
      "Aleksandr Medvedev",
      "Anatolii Potapov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10403v1",
    "title": "BRACE: A Benchmark for Robust Audio Caption Quality Evaluation",
    "summary": "Automatic audio captioning is essential for audio understanding, enabling applications such as accessibility and content indexing. However, evaluating the quality of audio captions remains a major challenge, especially in reference-free settings where high-quality ground-truth captions are unavailable. While CLAPScore is currently the most widely used reference-free Audio Caption Evaluation Metric(ACEM), its robustness under diverse conditions has not been systematically validated.\n  To address this gap, we introduce BRACE, a new benchmark designed to evaluate audio caption alignment quality in a reference-free setting. BRACE is primarily designed for assessing ACEMs, and can also be extended to measure the modality alignment abilities of Large Audio Language Model(LALM). BRACE consists of two sub-benchmarks: BRACE-Main for fine-grained caption comparison and BRACE-Hallucination for detecting subtle hallucinated content. We construct these datasets through high-quality filtering, LLM-based corruption, and human annotation.\n  Given the widespread adoption of CLAPScore as a reference-free ACEM and the increasing application of LALMs in audio-language tasks, we evaluate both approaches using the BRACE benchmark, testing CLAPScore across various CLAP model variants and assessing multiple LALMs.\n  Notably, even the best-performing CLAP-based ACEM achieves only a 70.01 F1-score on the BRACE-Main benchmark, while the best LALM reaches just 63.19.\n  By revealing the limitations of CLAP models and LALMs, our BRACE benchmark offers valuable insights into the direction of future research.",
    "published": "2025-12-11T08:09:24Z",
    "updated": "2025-12-11T08:09:24Z",
    "link": "http://arxiv.org/pdf/2512.10403v1.pdf",
    "category": [
      "cs.SD",
      "cs.CL"
    ],
    "authors": [
      "Tianyu Guo",
      "Hongyu Chen",
      "Hao Liang",
      "Meiyi Qiang",
      "Bohan Zeng",
      "Linzhuang Sun",
      "Bin Cui",
      "Wentao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.10287v3",
    "title": "OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models",
    "summary": "Since Multimodal Large Language Models (MLLMs) are increasingly being integrated into everyday tools and intelligent agents, growing concerns have arisen regarding their possible output of unsafe contents, ranging from toxic language and biased imagery to privacy violations and harmful misinformation. Current safety benchmarks remain highly limited in both modality coverage and performance evaluations, often neglecting the extensive landscape of content safety. In this work, we introduce OutSafe-Bench, the first most comprehensive content safety evaluation test suite designed for the multimodal era. OutSafe-Bench includes a large-scale dataset that spans four modalities, featuring over 18,000 bilingual (Chinese and English) text prompts, 4,500 images, 450 audio clips and 450 videos, all systematically annotated across nine critical content risk categories. In addition to the dataset, we introduce a Multidimensional Cross Risk Score (MCRS), a novel metric designed to model and assess overlapping and correlated content risks across different categories. To ensure fair and robust evaluation, we propose FairScore, an explainable automated multi-reviewer weighted aggregation framework. FairScore selects top-performing models as adaptive juries, thereby mitigating biases from single-model judgments and enhancing overall evaluation reliability. Our evaluation of nine state-of-the-art MLLMs reveals persistent and substantial safety vulnerabilities, underscoring the pressing need for robust safeguards in MLLMs.",
    "published": "2025-11-13T13:18:27Z",
    "updated": "2025-12-11T06:53:44Z",
    "link": "http://arxiv.org/pdf/2511.10287v3.pdf",
    "category": [
      "cs.LG",
      "cs.CL"
    ],
    "authors": [
      "Yuping Yan",
      "Yuhan Xie",
      "Yuanshuai Li",
      "Yingchao Yu",
      "Lingjuan Lyu",
      "Yaochu Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15257v2",
    "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners",
    "summary": "Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-weight LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively disentangled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training methods such as supervised fine-tuning or reinforcement learning, our training-free language-reasoning disentanglement achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization.",
    "published": "2025-05-21T08:35:05Z",
    "updated": "2025-12-11T05:53:45Z",
    "link": "http://arxiv.org/pdf/2505.15257v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Weixiang Zhao",
      "Jiahe Guo",
      "Yang Deng",
      "Tongtong Wu",
      "Wenxuan Zhang",
      "Yulin Hu",
      "Xingyu Sui",
      "Yanyan Zhao",
      "Wanxiang Che",
      "Bing Qin",
      "Tat-Seng Chua",
      "Ting Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.15456v2",
    "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment",
    "summary": "Personalized alignment is essential for enabling large language models (LLMs) to engage effectively in user-centric dialogue. While recent prompt-based and offline optimization methods offer preliminary solutions, they fall short in cold-start scenarios and long-term personalization due to their inherently static and shallow designs. In this work, we introduce the Reinforcement Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts with a simulated user model to iteratively infer and refine user profiles through dialogue. The training process is guided by a dual-level reward structure: the Profile Reward encourages accurate construction of user representations, while the Response Reward incentivizes generation of responses consistent with the inferred profile. We instantiate RLPA by fine-tuning Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art performance in personalized dialogue. Empirical evaluations demonstrate that Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines, and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o. Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting user preferences, sustaining long-term personalization and delivering more efficient inference compared to recent reasoning-focused LLMs. These results emphasize the potential of dynamic profile inference as a more effective paradigm for building personalized dialogue systems.",
    "published": "2025-05-21T12:38:36Z",
    "updated": "2025-12-11T05:49:50Z",
    "link": "http://arxiv.org/pdf/2505.15456v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Weixiang Zhao",
      "Xingyu Sui",
      "Yulin Hu",
      "Jiahe Guo",
      "Haixiao Liu",
      "Biye Li",
      "Yanyan Zhao",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05832v2",
    "title": "Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments",
    "summary": "This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates' speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate's argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.",
    "published": "2025-12-05T15:56:17Z",
    "updated": "2025-12-11T05:30:02Z",
    "link": "http://arxiv.org/pdf/2512.05832v2.pdf",
    "category": [
      "cs.CL",
      "cs.CY"
    ],
    "authors": [
      "Yifei Tong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.17208v2",
    "title": "A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents",
    "summary": "LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.",
    "published": "2025-11-21T12:41:17Z",
    "updated": "2025-12-11T05:13:54Z",
    "link": "http://arxiv.org/pdf/2511.17208v2.pdf",
    "category": [
      "cs.CL"
    ],
    "authors": [
      "Sizhe Zhou",
      "Jiawei Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10959v1",
    "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
    "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
    "published": "2025-12-11T18:59:59Z",
    "updated": "2025-12-11T18:59:59Z",
    "link": "http://arxiv.org/pdf/2512.10959v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tjark Behrens",
      "Anton Obukhov",
      "Bingxin Ke",
      "Fabio Tosi",
      "Matteo Poggi",
      "Konrad Schindler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10958v1",
    "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
    "summary": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
    "published": "2025-12-11T18:59:58Z",
    "updated": "2025-12-11T18:59:58Z",
    "link": "http://arxiv.org/pdf/2512.10958v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ao Liang",
      "Lingdong Kong",
      "Tianyi Yan",
      "Hongsi Liu",
      "Wesley Yang",
      "Ziqi Huang",
      "Wei Yin",
      "Jialong Zuo",
      "Yixuan Hu",
      "Dekai Zhu",
      "Dongyue Lu",
      "Youquan Liu",
      "Guangfeng Jiang",
      "Linfeng Li",
      "Xiangtai Li",
      "Long Zhuo",
      "Lai Xing Ng",
      "Benoit R. Cottereau",
      "Changxin Gao",
      "Liang Pan",
      "Wei Tsang Ooi",
      "Ziwei Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10956v1",
    "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\n  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "link": "http://arxiv.org/pdf/2512.10956v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wentao Zhou",
      "Xuweiyi Chen",
      "Vignesh Rajagopal",
      "Jeffrey Chen",
      "Rohan Chandra",
      "Zezhou Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10955v1",
    "title": "Omni-Attribute: Open-vocabulary Attribute Encoder for Visual Concept Personalization",
    "summary": "Visual concept personalization aims to transfer only specific image attributes, such as identity, expression, lighting, and style, into unseen contexts. However, existing methods rely on holistic embeddings from general-purpose image encoders, which entangle multiple visual factors and make it difficult to isolate a single attribute. This often leads to information leakage and incoherent synthesis. To address this limitation, we introduce Omni-Attribute, the first open-vocabulary image attribute encoder designed to learn high-fidelity, attribute-specific representations. Our approach jointly designs the data and model: (i) we curate semantically linked image pairs annotated with positive and negative attributes to explicitly teach the encoder what to preserve or suppress; and (ii) we adopt a dual-objective training paradigm that balances generative fidelity with contrastive disentanglement. The resulting embeddings prove effective for open-vocabulary attribute retrieval, personalization, and compositional generation, achieving state-of-the-art performance across multiple benchmarks.",
    "published": "2025-12-11T18:59:56Z",
    "updated": "2025-12-11T18:59:56Z",
    "link": "http://arxiv.org/pdf/2512.10955v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Guocheng Gordon Qian",
      "Kuan-Chieh Jackson Wang",
      "Egor Nemchinov",
      "Moayed Haji-Ali",
      "Riza Alp Guler",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Anil Kag",
      "Jun-Yan Zhu",
      "Sergey Tulyakov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10953v1",
    "title": "Bidirectional Normalizing Flow: From Data to Noise and Back",
    "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "link": "http://arxiv.org/pdf/2512.10953v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Yiyang Lu",
      "Qiao Sun",
      "Xianbang Wang",
      "Zhicheng Jiang",
      "Hanhong Zhao",
      "Kaiming He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10954v1",
    "title": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration",
    "summary": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.",
    "published": "2025-12-11T18:59:55Z",
    "updated": "2025-12-11T18:59:55Z",
    "link": "http://arxiv.org/pdf/2512.10954v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sicheng Mo",
      "Thao Nguyen",
      "Richard Zhang",
      "Nick Kolkin",
      "Siddharth Srinivasan Iyer",
      "Eli Shechtman",
      "Krishna Kumar Singh",
      "Yong Jae Lee",
      "Bolei Zhou",
      "Yuheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10950v1",
    "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
    "summary": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
    "published": "2025-12-11T18:59:53Z",
    "updated": "2025-12-11T18:59:53Z",
    "link": "http://arxiv.org/pdf/2512.10950v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qitao Zhao",
      "Hao Tan",
      "Qianqian Wang",
      "Sai Bi",
      "Kai Zhang",
      "Kalyan Sunkavalli",
      "Shubham Tulsiani",
      "Hanwen Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10948v1",
    "title": "ClusIR: Towards Cluster-Guided All-in-One Image Restoration",
    "summary": "All-in-One Image Restoration (AiOIR) aims to recover high-quality images from diverse degradations within a unified framework. However, existing methods often fail to explicitly model degradation types and struggle to adapt their restoration behavior to complex or mixed degradations. To address these issues, we propose ClusIR, a Cluster-Guided Image Restoration framework that explicitly models degradation semantics through learnable clustering and propagates cluster-aware cues across spatial and frequency domains for adaptive restoration. Specifically, ClusIR comprises two key components: a Probabilistic Cluster-Guided Routing Mechanism (PCGRM) and a Degradation-Aware Frequency Modulation Module (DAFMM). The proposed PCGRM disentangles degradation recognition from expert activation, enabling discriminative degradation perception and stable expert routing. Meanwhile, DAFMM leverages the cluster-guided priors to perform adaptive frequency decomposition and targeted modulation, collaboratively refining structural and textural representations for higher restoration fidelity. The cluster-guided synergy seamlessly bridges semantic cues with frequency-domain modulation, empowering ClusIR to attain remarkable restoration results across a wide range of degradations. Extensive experiments on diverse benchmarks validate that ClusIR reaches competitive performance under several scenarios.",
    "published": "2025-12-11T18:59:47Z",
    "updated": "2025-12-11T18:59:47Z",
    "link": "http://arxiv.org/pdf/2512.10948v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shengkai Hu",
      "Jiaqi Ma",
      "Jun Wan",
      "Wenwen Min",
      "Yongcheng Jing",
      "Lefei Zhang",
      "Dacheng Tao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10947v1",
    "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
    "summary": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
    "published": "2025-12-11T18:59:46Z",
    "updated": "2025-12-11T18:59:46Z",
    "link": "http://arxiv.org/pdf/2512.10947v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiawei Yang",
      "Ziyu Chen",
      "Yurong You",
      "Yan Wang",
      "Yiming Li",
      "Yuxiao Chen",
      "Boyi Li",
      "Boris Ivanovic",
      "Marco Pavone",
      "Yue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10945v1",
    "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
    "summary": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
    "published": "2025-12-11T18:59:44Z",
    "updated": "2025-12-11T18:59:44Z",
    "link": "http://arxiv.org/pdf/2512.10945v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Henghui Ding",
      "Chang Liu",
      "Shuting He",
      "Kaining Ying",
      "Xudong Jiang",
      "Chen Change Loy",
      "Yu-Gang Jiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10942v1",
    "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
    "summary": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
    "published": "2025-12-11T18:59:22Z",
    "updated": "2025-12-11T18:59:22Z",
    "link": "http://arxiv.org/pdf/2512.10942v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Delong Chen",
      "Mustafa Shukor",
      "Theo Moutakanni",
      "Willy Chung",
      "Jade Yu",
      "Tejaswi Kasarla",
      "Allen Bolourchi",
      "Yann LeCun",
      "Pascale Fung"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05104v2",
    "title": "EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation",
    "summary": "All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.",
    "published": "2025-12-04T18:59:10Z",
    "updated": "2025-12-11T18:59:22Z",
    "link": "http://arxiv.org/pdf/2512.05104v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiaqi Ma",
      "Shengkai Hu",
      "Xu Zhang",
      "Jun Wan",
      "Jiaxing Huang",
      "Lefei Zhang",
      "Salman Khan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10939v1",
    "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
    "summary": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
    "published": "2025-12-11T18:59:02Z",
    "updated": "2025-12-11T18:59:02Z",
    "link": "http://arxiv.org/pdf/2512.10939v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Madhav Agarwal",
      "Mingtian Zhang",
      "Laura Sevilla-Lara",
      "Steven McDonagh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10927v1",
    "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
    "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
    "published": "2025-12-11T18:53:15Z",
    "updated": "2025-12-11T18:53:15Z",
    "link": "http://arxiv.org/pdf/2512.10927v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yulu Gan",
      "Ligeng Zhu",
      "Dandan Shan",
      "Baifeng Shi",
      "Hongxu Yin",
      "Boris Ivanovic",
      "Song Han",
      "Trevor Darrell",
      "Jitendra Malik",
      "Marco Pavone",
      "Boyi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10894v1",
    "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
    "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
    "published": "2025-12-11T18:23:03Z",
    "updated": "2025-12-11T18:23:03Z",
    "link": "http://arxiv.org/pdf/2512.10894v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peiying Zhang",
      "Nanxuan Zhao",
      "Matthew Fisher",
      "Yiran Xu",
      "Jing Liao",
      "Difan Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10888v1",
    "title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction",
    "summary": "Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.",
    "published": "2025-12-11T18:19:00Z",
    "updated": "2025-12-11T18:19:00Z",
    "link": "http://arxiv.org/pdf/2512.10888v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Brandon Smock",
      "Valerie Faucon-Morin",
      "Max Sokolov",
      "Libin Liang",
      "Tayyibah Khanam",
      "Maury Courtland"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.00846v2",
    "title": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent",
    "summary": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.",
    "published": "2025-11-30T11:32:54Z",
    "updated": "2025-12-11T18:16:38Z",
    "link": "http://arxiv.org/pdf/2512.00846v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Neeraj Anand",
      "Rishabh Jain",
      "Sohan Patnaik",
      "Balaji Krishnamurthy",
      "Mausoom Sarkar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10881v1",
    "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
    "published": "2025-12-11T18:09:48Z",
    "updated": "2025-12-11T18:09:48Z",
    "link": "http://arxiv.org/pdf/2512.10881v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kehong Gong",
      "Zhengyu Wen",
      "Weixia He",
      "Mingxi Xu",
      "Qi Wang",
      "Ning Zhang",
      "Zhengyu Li",
      "Dongze Lian",
      "Wei Zhao",
      "Xiaoyu He",
      "Mingyuan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10867v1",
    "title": "From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models",
    "summary": "This paper introduces the concept of Microscopic Spatial Intelligence (MiSI), the capability to perceive and reason about the spatial relationships of invisible microscopic entities, which is fundamental to scientific discovery. To assess the potential of Vision-Language Models (VLMs) in this domain, we propose a systematic benchmark framework MiSI-Bench. This framework features over 163,000 question-answer pairs and 587,000 images derived from approximately 4,000 molecular structures, covering nine complementary tasks that evaluate abilities ranging from elementary spatial transformations to complex relational identifications. Experimental results reveal that current state-of-the-art VLMs perform significantly below human level on this benchmark. However, a fine-tuned 7B model demonstrates substantial potential, even surpassing humans in spatial transformation tasks, while its poor performance in scientifically-grounded tasks like hydrogen bond recognition underscores the necessity of integrating explicit domain knowledge for progress toward scientific AGI. The datasets are available at https://huggingface.co/datasets/zongzhao/MiSI-bench.",
    "published": "2025-12-11T18:00:21Z",
    "updated": "2025-12-11T18:00:21Z",
    "link": "http://arxiv.org/pdf/2512.10867v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zongzhao Li",
      "Xiangzhe Kong",
      "Jiahui Su",
      "Zongyang Ma",
      "Mingze Li",
      "Songyou Li",
      "Yuelin Zhang",
      "Yu Rong",
      "Tingyang Xu",
      "Deli Zhao",
      "Wenbing Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10860v1",
    "title": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation",
    "summary": "Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/",
    "published": "2025-12-11T17:54:31Z",
    "updated": "2025-12-11T17:54:31Z",
    "link": "http://arxiv.org/pdf/2512.10860v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Kehong Gong",
      "Zhengyu Wen",
      "Mingxi Xu",
      "Weixia He",
      "Qi Wang",
      "Ning Zhang",
      "Zhengyu Li",
      "Chenbin Li",
      "Dongze Lian",
      "Wei Zhao",
      "Xiaoyu He",
      "Mingyuan Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05094v2",
    "title": "From Generated Human Videos to Physically Plausible Robot Trajectories",
    "summary": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.",
    "published": "2025-12-04T18:56:03Z",
    "updated": "2025-12-11T17:37:53Z",
    "link": "http://arxiv.org/pdf/2512.05094v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV"
    ],
    "authors": [
      "James Ni",
      "Zekai Wang",
      "Wei Lin",
      "Amir Bar",
      "Yann LeCun",
      "Trevor Darrell",
      "Jitendra Malik",
      "Roei Herzig"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10840v1",
    "title": "PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning",
    "summary": "6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .",
    "published": "2025-12-11T17:29:25Z",
    "updated": "2025-12-11T17:29:25Z",
    "link": "http://arxiv.org/pdf/2512.10840v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jianqi Chen",
      "Biao Zhang",
      "Xiangjun Tang",
      "Peter Wonka"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.06285v2",
    "title": "FE-MCFormer: An interpretable fault diagnosis framework for rotating machinery under strong noise based on time-frequency fusion transformer",
    "summary": "Many fault diagnosis methods of rotating machines are based on discriminative features extracted from signals collected from the key components such as bearings. However, under complex operating conditions, periodic impulsive characteristics in the signal related to weak fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn interpretable fault-related features in such scenarios. This paper proposes a novel transformer framework (FE-MCFormer) to extract interpretable time-frequency features, with the aim of improving the fault detection accuracy and intrepretability of rotating machines under strong noise. First, a Fourier adaptive reconstruction embedding layer is introduced as a global information encoder in the model. Subsequently, a time-frequency fusion module is designed, further improve the model robustness and interpretability. The effectiveness of FE-MCFormer in machine fault diagnosis is validated through three case studies.",
    "published": "2025-05-07T07:58:48Z",
    "updated": "2025-12-11T17:26:32Z",
    "link": "http://arxiv.org/pdf/2505.06285v2.pdf",
    "category": [
      "eess.SP",
      "cs.CV"
    ],
    "authors": [
      "Yuhan Yuan",
      "Xiaomo Jiang",
      "Haibin Yang",
      "Haixin Zhao",
      "Shengbo Wang",
      "Xueyu Cheng",
      "Jigang Meng",
      "Shuhua Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.03477v2",
    "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis",
    "summary": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms. Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.",
    "published": "2025-12-03T06:09:14Z",
    "updated": "2025-12-11T17:17:07Z",
    "link": "http://arxiv.org/pdf/2512.03477v2.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Zijian Gu",
      "Yuxi Liu",
      "Zhenhao Zhang",
      "Song Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10818v1",
    "title": "Self-Ensemble Post Learning for Noisy Domain Generalization",
    "summary": "While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.",
    "published": "2025-12-11T17:09:35Z",
    "updated": "2025-12-11T17:09:35Z",
    "link": "http://arxiv.org/pdf/2512.10818v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wang Lu",
      "Jindong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10808v1",
    "title": "Graph Laplacian Transformer with Progressive Sampling for Prostate Cancer Grading",
    "summary": "Prostate cancer grading from whole-slide images (WSIs) remains a challenging task due to the large-scale nature of WSIs, the presence of heterogeneous tissue structures, and difficulty of selecting diagnostically relevant regions. Existing approaches often rely on random or static patch selection, leading to the inclusion of redundant or non-informative regions that degrade performance. To address this, we propose a Graph Laplacian Attention-Based Transformer (GLAT) integrated with an Iterative Refinement Module (IRM) to enhance both feature learning and spatial consistency. The IRM iteratively refines patch selection by leveraging a pretrained ResNet50 for local feature extraction and a foundation model in no-gradient mode for importance scoring, ensuring only the most relevant tissue regions are preserved. The GLAT models tissue-level connectivity by constructing a graph where patches serve as nodes, ensuring spatial consistency through graph Laplacian constraints and refining feature representations via a learnable filtering mechanism that enhances discriminative histological structures. Additionally, a convex aggregation mechanism dynamically adjusts patch importance to generate a robust WSI-level representation. Extensive experiments on five public and one private dataset demonstrate that our model outperforms state-of-the-art methods, achieving higher performance and spatial consistency while maintaining computational efficiency.",
    "published": "2025-12-11T16:55:57Z",
    "updated": "2025-12-11T16:55:57Z",
    "link": "http://arxiv.org/pdf/2512.10808v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Masum Shah Junayed",
      "John Derek Van Vessem",
      "Qian Wan",
      "Gahie Nam",
      "Sheida Nabavi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10805v1",
    "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
    "summary": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
    "published": "2025-12-11T16:48:07Z",
    "updated": "2025-12-11T16:48:07Z",
    "link": "http://arxiv.org/pdf/2512.10805v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Akshay Kulkarni",
      "Tsui-Wei Weng",
      "Vivek Narayanaswamy",
      "Shusen Liu",
      "Wesam A. Sakla",
      "Kowshik Thopalli"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.05771v5",
    "title": "Equivariant Test-Time Training with Operator Sketching for Imaging Inverse Problems",
    "summary": "Equivariant Imaging (EI) regularization has become the de-facto technique for unsupervised training of deep imaging networks, without any need of ground-truth data. Observing that the EI-based unsupervised training paradigm currently has significant computational redundancy leading to inefficiency in high-dimensional applications, we propose a sketched EI regularization which leverages the randomized sketching techniques for acceleration. We apply our sketched EI regularization to develop an accelerated deep internal learning framework, which can be efficiently applied for test-time network adaptation. Additionally, for network adaptation tasks, we propose a parameter-efficient approach to accelerate both EI and Sketched-EI via optimizing only the normalization layers. Our numerical study on X-ray CT and multicoil magnetic resonance image reconstruction tasks demonstrate that our approach can achieve significant computational acceleration over the standard EI counterpart, especially in test-time training tasks.",
    "published": "2024-11-08T18:33:03Z",
    "updated": "2025-12-11T16:43:35Z",
    "link": "http://arxiv.org/pdf/2411.05771v5.pdf",
    "category": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Guixian Xu",
      "Jinglai Li",
      "Junqi Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.22438v2",
    "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps",
    "summary": "Accurate pest population monitoring and tracking their dynamic changes are crucial for precision agriculture decision-making. A common limitation in existing vision-based automatic pest counting research is that models are typically evaluated on datasets with ground truth but deployed in real-world scenarios without assessing the reliability of counting results due to the lack of ground truth. To this end, this paper proposed a method for comprehensively evaluating pest counting confidence in the image, based on information related to counting results and external environmental conditions. First, a pest detection network is used for pest detection and counting, extracting counting result-related information. Then, the pest images undergo image quality assessment, image complexity assessment, and pest distribution uniformity assessment. And the changes in image clarity caused by stirring during image acquisition are quantified by calculating the average gradient magnitude. Notably, we designed a hypothesis-driven multi-factor sensitivity analysis method to select the optimal image quality assessment and image complexity assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for pest distribution uniformity assessment. Finally, the obtained information related to counting results and external environmental conditions is input into a regression model for prediction, resulting in the final pest counting confidence. To the best of our knowledge, this is the first study dedicated to comprehensively evaluating counting confidence in counting tasks, and quantifying the relationship between influencing factors and counting confidence through a model. Experimental results show our method reduces MSE by 31.7% and improves R2 by 15.2% on the pest counting confidence test set, compared to the baseline built primarily on information related to counting results.",
    "published": "2025-05-19T18:01:58Z",
    "updated": "2025-12-11T16:30:58Z",
    "link": "http://arxiv.org/pdf/2506.22438v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xumin Gao",
      "Mark Stevens",
      "Grzegorz Cielniak"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.05391v2",
    "title": "LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models",
    "summary": "Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.",
    "published": "2025-12-05T03:16:46Z",
    "updated": "2025-12-11T16:04:05Z",
    "link": "http://arxiv.org/pdf/2512.05391v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qingqiao Hu",
      "Weimin Lyu",
      "Meilong Xu",
      "Kehan Qi",
      "Xiaoling Hu",
      "Saumya Gupta",
      "Jiawei Zhou",
      "Chao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10765v1",
    "title": "Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography",
    "summary": "Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.",
    "published": "2025-12-11T16:03:35Z",
    "updated": "2025-12-11T16:03:35Z",
    "link": "http://arxiv.org/pdf/2512.10765v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Rene Lisasi",
      "Michele Esposito",
      "Chen Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09363v2",
    "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
    "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
    "published": "2025-12-10T06:50:16Z",
    "updated": "2025-12-11T15:59:50Z",
    "link": "http://arxiv.org/pdf/2512.09363v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ke Xing",
      "Xiaojie Jin",
      "Longfei Li",
      "Yuyang Yin",
      "Hanwen Liang",
      "Guixun Luo",
      "Chen Fang",
      "Jue Wang",
      "Konstantinos N. Plataniotis",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10750v1",
    "title": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation",
    "summary": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.",
    "published": "2025-12-11T15:43:33Z",
    "updated": "2025-12-11T15:43:33Z",
    "link": "http://arxiv.org/pdf/2512.10750v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tianyu Zhou",
      "Junyi Tang",
      "Zehui Li",
      "Dahong Qian",
      "Suncheng Xiang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09583v2",
    "title": "UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision",
    "summary": "Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/",
    "published": "2025-12-10T12:22:37Z",
    "updated": "2025-12-11T15:21:30Z",
    "link": "http://arxiv.org/pdf/2512.09583v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alberto Rota",
      "Mert Kiray",
      "Mert Asim Karaoglu",
      "Patrick Ruhkamp",
      "Elena De Momi",
      "Nassir Navab",
      "Benjamin Busam"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10730v1",
    "title": "IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation",
    "summary": "Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.",
    "published": "2025-12-11T15:16:06Z",
    "updated": "2025-12-11T15:16:06Z",
    "link": "http://arxiv.org/pdf/2512.10730v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuan-Ming Li",
      "Qize Yang",
      "Nan Lei",
      "Shenghao Fu",
      "Ling-An Zeng",
      "Jian-Fang Hu",
      "Xihan Wei",
      "Wei-Shi Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.07661v2",
    "title": "Optimization-Guided Diffusion for Interactive Scene Generation",
    "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.",
    "published": "2025-12-08T15:56:18Z",
    "updated": "2025-12-11T15:08:39Z",
    "link": "http://arxiv.org/pdf/2512.07661v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shihao Li",
      "Naisheng Ye",
      "Tianyu Li",
      "Kashyap Chitta",
      "Tuo An",
      "Peng Su",
      "Boyang Wang",
      "Haiou Liu",
      "Chen Lv",
      "Hongyang Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10725v1",
    "title": "Video Depth Propagation",
    "summary": "Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth",
    "published": "2025-12-11T15:08:37Z",
    "updated": "2025-12-11T15:08:37Z",
    "link": "http://arxiv.org/pdf/2512.10725v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Luigi Piccinelli",
      "Thiemo Wandel",
      "Christos Sakaridis",
      "Wim Abbeloos",
      "Luc Van Gool"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10719v1",
    "title": "SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving",
    "summary": "End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.",
    "published": "2025-12-11T14:59:07Z",
    "updated": "2025-12-11T14:59:07Z",
    "link": "http://arxiv.org/pdf/2512.10719v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Peizheng Li",
      "Zhenghao Zhang",
      "David Holtz",
      "Hang Yu",
      "Yutong Yang",
      "Yuzhi Lai",
      "Rui Song",
      "Andreas Geiger",
      "Andreas Zell"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.03539v3",
    "title": "Panoramic Out-of-Distribution Segmentation",
    "summary": "Panoramic imaging enables capturing 360° images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception, which is critical to applications, such as autonomous driving and augmented reality, etc. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to pixel distortions and background clutter. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), with the aim of achieving comprehensive and safe scene understanding. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities and advances the development of panoramic understanding. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.",
    "published": "2025-05-06T13:51:26Z",
    "updated": "2025-12-11T14:56:58Z",
    "link": "http://arxiv.org/pdf/2505.03539v3.pdf",
    "category": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "authors": [
      "Mengfei Duan",
      "Yuheng Zhang",
      "Yihong Cao",
      "Fei Teng",
      "Kai Luo",
      "Jiaming Zhang",
      "Kailun Yang",
      "Zhiyong Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10715v1",
    "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
    "summary": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
    "published": "2025-12-11T14:50:23Z",
    "updated": "2025-12-11T14:50:23Z",
    "link": "http://arxiv.org/pdf/2512.10715v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Matias Cosarinsky",
      "Nicolas Gaggion",
      "Rodrigo Echeveste",
      "Enzo Ferrante"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06277v2",
    "title": "ExAct: A Video-Language Benchmark for Expert Action Analysis",
    "summary": "We present ExAct, a new video-language benchmark for expert-level understanding of skilled physical human activities. Our new benchmark contains 3521 expert-curated video question-answer pairs spanning 11 physical activities in 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct requires the correct answer to be selected from five carefully designed candidate options, thus necessitating a nuanced, fine-grained, expert-level understanding of physical human skills. Evaluating the recent state-of-the-art VLMs on ExAct reveals a substantial performance gap relative to human expert performance. Specifically, the best-performing GPT-4o model achieves only 44.70% accuracy, well below the 82.02% attained by trained human specialists/experts. We believe that ExAct will be beneficial for developing and evaluating VLMs capable of precise understanding of human skills in various physical and procedural domains. Dataset and code are available at https://texaser.github.io/exact_project_page/",
    "published": "2025-06-06T17:58:51Z",
    "updated": "2025-12-11T14:46:48Z",
    "link": "http://arxiv.org/pdf/2506.06277v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Han Yi",
      "Yulu Pan",
      "Feihong He",
      "Xinyu Liu",
      "Benjamin Zhang",
      "Oluwatumininu Oguntola",
      "Gedas Bertasius"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2112.04662v4",
    "title": "Dual Cluster Contrastive learning for Object Re-Identification",
    "summary": "Recently, cluster contrastive learning has been proven effective for object ReID by computing the contrastive loss between the individual features and the cluster memory. However, existing methods that use the individual features to momentum update the cluster memory will fluctuate over the training examples, especially for the outlier samples. Unlike the individual-based updating mechanism, the centroid-based updating mechanism that applies the mean feature of each cluster to update the cluster memory can reduce the impact of individual samples. Therefore, we formulate the individual-based updating and centroid-based updating mechanisms in a unified cluster contrastive framework, named Dual Cluster Contrastive framework (DCC), which maintains two types of memory banks: individual and centroid cluster memory banks. Significantly, the individual cluster memory considers just one individual at a time to take a single step for updating. The centroid cluster memory applies the mean feature of each cluster to update the corresponding cluster memory. During optimization, besides the vallina contrastive loss of each memory, a cross-view consistency constraint is applied to exchange the benefits of two memories for generating a discriminative description for the object ReID. Note that DCC can be easily applied for unsupervised or supervised object ReID by using ground-truth labels or the generated pseudo-labels. Extensive experiments on three benchmarks, \\emph{e.g.,} Market-1501, MSMT17, and VeRi-776, under \\textbf{supervised Object ReID} and \\textbf{unsupervised Object ReID} demonstrate the superiority of the proposed DCC.",
    "published": "2021-12-09T02:43:25Z",
    "updated": "2025-12-11T14:40:26Z",
    "link": "http://arxiv.org/pdf/2112.04662v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hantao Yao",
      "Changsheng Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10685v1",
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "summary": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
    "published": "2025-12-11T14:34:11Z",
    "updated": "2025-12-11T14:34:11Z",
    "link": "http://arxiv.org/pdf/2512.10685v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Lars Mescheder",
      "Wei Dong",
      "Shiwei Li",
      "Xuyang Bai",
      "Marcel Santos",
      "Peiyun Hu",
      "Bruno Lecouat",
      "Mingmin Zhen",
      "Amaël Delaunoy",
      "Tian Fang",
      "Yanghai Tsin",
      "Stephan R. Richter",
      "Vladlen Koltun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.09881v3",
    "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation",
    "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in semantic segmentation (DGSS) highlight a subtle complementarity that motivates Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS aims to generate pixel-level masks for unseen categories while maintaining robustness across unseen domains, a critical capability for real-world scenarios such as autonomous driving in adverse conditions. We introduce Vireo, a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS and DGSS for the first time. Vireo builds upon the frozen Visual Foundation Models (VFMs) and incorporates scene geometry via Depth VFMs to extract domain-invariant structural features. To bridge the gap between visual and textual modalities under domain shift, we propose three key components: (1) GeoText Prompts, which align geometric features with language cues and progressively refine VFM encoder representations; (2) Coarse Mask Prior Embedding (CMPE) for enhancing gradient flow for faster convergence and stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH), which fuses refined structural and semantic features for robust prediction. Comprehensive evaluation on these components demonstrates the effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art performance and surpasses existing methods by a large margin in both domain generalization and open-vocabulary recognition, offering a unified and scalable solution for robust visual understanding in diverse and dynamic environments. Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.",
    "published": "2025-06-11T15:54:47Z",
    "updated": "2025-12-11T14:33:57Z",
    "link": "http://arxiv.org/pdf/2506.09881v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Siyu Chen",
      "Ting Han",
      "Chengzheng Fu",
      "Changshe Zhang",
      "Chaolei Wang",
      "Jinhe Su",
      "Guorong Cai",
      "Meiliu Wu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10683v1",
    "title": "Optimal transport unlocks end-to-end learning for single-molecule localization",
    "summary": "Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.",
    "published": "2025-12-11T14:30:16Z",
    "updated": "2025-12-11T14:30:16Z",
    "link": "http://arxiv.org/pdf/2512.10683v1.pdf",
    "category": [
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Romain Seailles",
      "Jean-Baptiste Masson",
      "Jean Ponce",
      "Julien Mairal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10674v1",
    "title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching",
    "summary": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.",
    "published": "2025-12-11T14:20:17Z",
    "updated": "2025-12-11T14:20:17Z",
    "link": "http://arxiv.org/pdf/2512.10674v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Javier Villena Toro",
      "Mehdi Tarkian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10668v1",
    "title": "XDen-1K: A Density Field Dataset of Real-World Objects",
    "summary": "A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.",
    "published": "2025-12-11T14:15:42Z",
    "updated": "2025-12-11T14:15:42Z",
    "link": "http://arxiv.org/pdf/2512.10668v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingxuan Zhang",
      "Tianqi Yu",
      "Yatu Zhang",
      "Jinze Wu",
      "Kaixin Yao",
      "Jingyang Liu",
      "Yuyao Zhang",
      "Jiayuan Gu",
      "Jingyi Yu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.21888v4",
    "title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding",
    "summary": "We address Embodied Reference Understanding, the task of predicting the object a person in the scene refers to through pointing gesture and language. This requires multimodal reasoning over text, visual pointing cues, and scene context, yet existing methods often fail to fully exploit visual disambiguation signals. We also observe that while the referent often aligns with the head-to-fingertip direction, in many cases it aligns more closely with the wrist-to-fingertip direction, making a single-line assumption overly limiting. To address this, we propose a dual-model framework, where one model learns from the head-to-fingertip direction and the other from the wrist-to-fingertip direction. We introduce a Gaussian ray heatmap representation of these lines and use them as input to provide a strong supervisory signal that encourages the model to better attend to pointing cues. To fuse their complementary strengths, we present the CLIP-Aware Pointing Ensemble module, which performs a hybrid ensemble guided by CLIP features. We further incorporate an auxiliary object center prediction head to enhance referent localization. We validate our approach on YouRefIt, achieving 75.0 mAP at 0.25 IoU, alongside state-of-the-art CLIP and C_D scores, and demonstrate its generality on unseen CAESAR and ISL Pointing, showing robust performance across benchmarks.",
    "published": "2025-07-29T15:00:21Z",
    "updated": "2025-12-11T14:06:14Z",
    "link": "http://arxiv.org/pdf/2507.21888v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Fevziye Irem Eyiokur",
      "Dogucan Yaman",
      "Hazım Kemal Ekenel",
      "Alexander Waibel"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10660v1",
    "title": "NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation",
    "summary": "The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.",
    "published": "2025-12-11T14:05:18Z",
    "updated": "2025-12-11T14:05:18Z",
    "link": "http://arxiv.org/pdf/2512.10660v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hanfeng Wu",
      "Marlon Steiner",
      "Michael Schmidt",
      "Alvaro Marcos-Ramiro",
      "Christoph Stiller"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10652v1",
    "title": "TriDF: Evaluating Perception, Detection, and Hallucination for Interpretable DeepFake Detection",
    "summary": "Advances in generative modeling have made it increasingly easy to fabricate realistic portrayals of individuals, creating serious risks for security, communication, and public trust. Detecting such person-driven manipulations requires systems that not only distinguish altered content from authentic media but also provide clear and reliable reasoning. In this paper, we introduce TriDF, a comprehensive benchmark for interpretable DeepFake detection. TriDF contains high-quality forgeries from advanced synthesis models, covering 16 DeepFake types across image, video, and audio modalities. The benchmark evaluates three key aspects: Perception, which measures the ability of a model to identify fine-grained manipulation artifacts using human-annotated evidence; Detection, which assesses classification performance across diverse forgery families and generators; and Hallucination, which quantifies the reliability of model-generated explanations. Experiments on state-of-the-art multimodal large language models show that accurate perception is essential for reliable detection, but hallucination can severely disrupt decision-making, revealing the interdependence of these three aspects. TriDF provides a unified framework for understanding the interaction between detection accuracy, evidence identification, and explanation reliability, offering a foundation for building trustworthy systems that address real-world synthetic media threats.",
    "published": "2025-12-11T14:01:01Z",
    "updated": "2025-12-11T14:01:01Z",
    "link": "http://arxiv.org/pdf/2512.10652v1.pdf",
    "category": [
      "cs.CV",
      "cs.CR"
    ],
    "authors": [
      "Jian-Yu Jiang-Lin",
      "Kang-Yang Huang",
      "Ling Zou",
      "Ling Lo",
      "Sheng-Ping Yang",
      "Yu-Wen Tseng",
      "Kun-Hsiang Lin",
      "Chia-Ling Chen",
      "Yu-Ting Ta",
      "Yan-Tsung Wang",
      "Po-Ching Chen",
      "Hongxia Xie",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.22663v2",
    "title": "Architecture Decoupling Is Not All You Need For Unified Multimodal Model",
    "summary": "Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.",
    "published": "2025-11-27T17:55:25Z",
    "updated": "2025-12-11T13:59:22Z",
    "link": "http://arxiv.org/pdf/2511.22663v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Dian Zheng",
      "Manyuan Zhang",
      "Hongyu Li",
      "Kai Zou",
      "Hongbo Liu",
      "Ziyu Guo",
      "Kaituo Feng",
      "Yexin Liu",
      "Ying Luo",
      "Yan Feng",
      "Peng Pei",
      "Xunliang Cai",
      "Hongsheng Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.02541v2",
    "title": "Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data",
    "summary": "Shearography is a non-destructive testing method for detecting subsurface defects, offering high sensitivity and full-field inspection capabilities. However, its industrial adoption remains limited due to the need for expert interpretation. To reduce reliance on labeled data and manual evaluation, this study explores unsupervised learning methods for automated anomaly detection in shearographic images. Three architectures are evaluated: a fully connected autoencoder, a convolutional autoencoder, and a student-teacher feature matching model. All models are trained solely on defect-free data. A controlled dataset was developed using a custom specimen with reproducible defect patterns, enabling systematic acquisition of shearographic measurements under both ideal and realistic deformation conditions. Two training subsets were defined: one containing only undistorted, defect-free samples, and one additionally including globally deformed, yet defect-free, data. The latter simulates practical inspection conditions by incorporating deformation-induced fringe patterns that may obscure localized anomalies. The models are evaluated in terms of binary classification and, for the student-teacher model, spatial defect localization. Results show that the student-teacher approach achieves superior classification robustness and enables precise localization. Compared to the autoencoder-based models, it demonstrates improved separability of feature representations, as visualized through t-SNE embeddings. Additionally, a YOLOv8 model trained on labeled defect data serves as a reference to benchmark localization quality. This study underscores the potential of unsupervised deep learning for scalable, label-efficient shearographic inspection in industrial environments.",
    "published": "2025-11-04T12:48:02Z",
    "updated": "2025-12-11T13:49:46Z",
    "link": "http://arxiv.org/pdf/2511.02541v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jessica Plassmann",
      "Nicolas Schuler",
      "Georg von Freymann",
      "Michael Schuth"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10628v1",
    "title": "K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices",
    "summary": "Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.",
    "published": "2025-12-11T13:26:58Z",
    "updated": "2025-12-11T13:26:58Z",
    "link": "http://arxiv.org/pdf/2512.10628v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bishoy Galoaa",
      "Pau Closas",
      "Sarah Ostadabbas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08511v2",
    "title": "Thinking with Images via Self-Calling Agent",
    "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\\%$ with $\\sim 75\\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
    "published": "2025-12-09T11:53:21Z",
    "updated": "2025-12-11T13:21:56Z",
    "link": "http://arxiv.org/pdf/2512.08511v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenxi Yang",
      "Yuzhong Zhao",
      "Fang Wan",
      "Qixiang Ye"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10619v1",
    "title": "DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM",
    "summary": "Document parsing aims to transform unstructured PDF images into semi-structured data, facilitating the digitization and utilization of information in diverse domains. While vision language models (VLMs) have significantly advanced this task, achieving reliable, high-quality parsing in real-world scenarios remains challenging. Common practice often selects the top-performing model on standard benchmarks. However, these benchmarks may carry dataset-specific biases, leading to inconsistent model rankings and limited correlation with real-world performance. Moreover, benchmark metrics typically provide only overall scores, which can obscure distinct error patterns in output. This raises a key challenge: how can we reliably and comprehensively assess document parsing quality in the wild? We address this problem with DOCR-Inspector, which formalizes document parsing assessment as fine-grained error detection and analysis. Leveraging VLM-as-a-Judge, DOCR-Inspector analyzes a document image and its parsed output, identifies all errors, assigns them to one of 28 predefined types, and produces a comprehensive quality assessment. To enable this capability, we construct DOCRcase-200K for training and propose the Chain-of-Checklist reasoning paradigm to enable the hierarchical structure of parsing quality assessment. For empirical validation, we introduce DOCRcaseBench, a set of 882 real-world document parsing cases with manual annotations. On this benchmark, DOCR-Inspector-7B outperforms commercial models like Gemini 2.5 Pro, as well as leading open-source models. Further experiments demonstrate that its quality assessments provide valuable guidance for parsing results refinement, making DOCR-Inspector both a practical evaluator and a driver for advancing document parsing systems at scale. Model and code are released at: https://github.com/ZZZZZQT/DOCR-Inspector.",
    "published": "2025-12-11T13:16:33Z",
    "updated": "2025-12-11T13:16:33Z",
    "link": "http://arxiv.org/pdf/2512.10619v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qintong Zhang",
      "Junyuan Zhang",
      "Zhifei Ren",
      "Linke Ouyang",
      "Zichen Wen",
      "Junbo Niu",
      "Yuan Qu",
      "Bin Wang",
      "Ka-Ho Chow",
      "Conghui He",
      "Wentao Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10617v1",
    "title": "Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces",
    "summary": "We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.",
    "published": "2025-12-11T13:14:18Z",
    "updated": "2025-12-11T13:14:18Z",
    "link": "http://arxiv.org/pdf/2512.10617v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bishoy Galoaa",
      "Xiangyu Bai",
      "Sarah Ostadabbas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10608v1",
    "title": "Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation",
    "summary": "In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the \"black-box\" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings.",
    "published": "2025-12-11T13:03:03Z",
    "updated": "2025-12-11T13:03:03Z",
    "link": "http://arxiv.org/pdf/2512.10608v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mohammad Sadegh Gholizadeh",
      "Amir Arsalan Rezapour"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10607v1",
    "title": "Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos",
    "summary": "We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.",
    "published": "2025-12-11T13:03:03Z",
    "updated": "2025-12-11T13:03:03Z",
    "link": "http://arxiv.org/pdf/2512.10607v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Bishoy Galoaa",
      "Sarah Ostadabbas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09383v2",
    "title": "Perception-Inspired Color Space Design for Photo White Balance Editing",
    "summary": "White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.\n  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.\n  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.",
    "published": "2025-12-10T07:27:16Z",
    "updated": "2025-12-11T12:40:16Z",
    "link": "http://arxiv.org/pdf/2512.09383v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yang Cheng",
      "Ziteng Cui",
      "Shenghan Su",
      "Lin Gu",
      "Zenghui Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.05191v2",
    "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
    "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA.",
    "published": "2025-06-05T16:04:08Z",
    "updated": "2025-12-11T12:37:26Z",
    "link": "http://arxiv.org/pdf/2506.05191v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yake Wei",
      "Yu Miao",
      "Dongzhan Zhou",
      "Di Hu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10592v1",
    "title": "Salient Object Detection in Complex Weather Conditions via Noise Indicators",
    "summary": "Salient object detection (SOD), a foundational task in computer vision, has advanced from single-modal to multi-modal paradigms to enhance generalization. However, most existing SOD methods assume low-noise visual conditions, overlooking the degradation of segmentation accuracy caused by weather-induced noise in real-world scenarios. In this paper, we propose a SOD framework tailored for diverse weather conditions, encompassing a specific encoder and a replaceable decoder. To enable handling of varying weather noises, we introduce a one-hot vector as a noise indicator to represent different weather types and design a Noise Indicator Fusion Module (NIFM). The NIFM takes both semantic features and the noise indicator as dual inputs and is inserted between consecutive stages of the encoder to embed weather-aware priors via adaptive feature modulation. Critically, the proposed specific encoder retains compatibility with mainstream SOD decoders. Extensive experiments are conducted on the WXSOD dataset under varying training data scales (100%, 50%, 30% of the full training set), three encoder and seven decoder configurations. Results show that the proposed SOD framework (particularly the NIFM-enhanced specific encoder) improves segmentation accuracy under complex weather conditions compared to a vanilla encoder.",
    "published": "2025-12-11T12:33:51Z",
    "updated": "2025-12-11T12:33:51Z",
    "link": "http://arxiv.org/pdf/2512.10592v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Quan Chen",
      "Xiaokai Yang",
      "Tingyu Wang",
      "Rongfeng Lu",
      "Xichun Sheng",
      "Yaoqi Sun",
      "Chenggang Yan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10581v1",
    "title": "Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration",
    "summary": "All-in-one image restoration aims to handle diverse degradations (e.g., noise, blur, adverse weather) within a unified framework, yet existing methods increasingly rely on complex architectures (e.g., Mixture-of-Experts, diffusion models) and elaborate degradation prompt strategies. In this work, we reveal a critical insight: well-crafted feature extraction inherently encodes degradation-carrying information, and a symmetric U-Net architecture is sufficient to unleash these cues effectively. By aligning feature scales across encoder-decoder and enabling streamlined cross-scale propagation, our symmetric design preserves intrinsic degradation signals robustly, rendering simple additive fusion in skip connections sufficient for state-of-the-art performance. Our primary baseline, SymUNet, is built on this symmetric U-Net and achieves better results across benchmark datasets than existing approaches while reducing computational cost. We further propose a semantic enhanced variant, SE-SymUNet, which integrates direct semantic injection from frozen CLIP features via simple cross-attention to explicitly amplify degradation priors. Extensive experiments on several benchmarks validate the superiority of our methods. Both baselines SymUNet and SE-SymUNet establish simpler and stronger foundations for future advancements in all-in-one image restoration. The source code is available at https://github.com/WenlongJiao/SymUNet.",
    "published": "2025-12-11T12:20:31Z",
    "updated": "2025-12-11T12:20:31Z",
    "link": "http://arxiv.org/pdf/2512.10581v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Wenlong Jiao",
      "Heyang Lee",
      "Ping Wang",
      "Pengfei Zhu",
      "Qinghua Hu",
      "Dongwei Ren"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.00756v2",
    "title": "Towards Open-World Human Action Segmentation Using Graph Convolutional Networks",
    "summary": "Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples.\n  We evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.",
    "published": "2025-07-01T14:00:39Z",
    "updated": "2025-12-11T12:09:54Z",
    "link": "http://arxiv.org/pdf/2507.00756v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Hao Xing",
      "Kai Zhe Boey",
      "Gordon Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.00752v2",
    "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation",
    "summary": "Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.",
    "published": "2025-07-01T13:55:57Z",
    "updated": "2025-12-11T12:06:40Z",
    "link": "http://arxiv.org/pdf/2507.00752v2.pdf",
    "category": [
      "cs.CV",
      "cs.RO"
    ],
    "authors": [
      "Hao Xing",
      "Kai Zhe Boey",
      "Yuankai Wu",
      "Darius Burschka",
      "Gordon Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10571v1",
    "title": "Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner",
    "summary": "Recent advancements in video generation highlight that realistic audio-visual synchronization is crucial for engaging content creation. However, existing video editing methods largely overlook audio-visual synchronization and lack the fine-grained spatial and temporal controllability required for precise instance-level edits. In this paper, we propose AVI-Edit, a framework for audio-sync video instance editing. We propose a granularity-aware mask refiner that iteratively refines coarse user-provided masks into precise instance-level regions. We further design a self-feedback audio agent to curate high-quality audio guidance, providing fine-grained temporal control. To facilitate this task, we additionally construct a large-scale dataset with instance-centric correspondence and comprehensive annotations. Extensive experiments demonstrate that AVI-Edit outperforms state-of-the-art methods in visual quality, condition following, and audio-visual synchronization. Project page: https://hjzheng.net/projects/AVI-Edit/.",
    "published": "2025-12-11T11:58:53Z",
    "updated": "2025-12-11T11:58:53Z",
    "link": "http://arxiv.org/pdf/2512.10571v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haojie Zheng",
      "Shuchen Weng",
      "Jingqi Liu",
      "Siqi Yang",
      "Boxin Shi",
      "Xinlong Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.02899v2",
    "title": "Glance: Accelerating Diffusion Models with 1 Sample",
    "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
    "published": "2025-12-02T16:05:21Z",
    "updated": "2025-12-11T11:53:22Z",
    "link": "http://arxiv.org/pdf/2512.02899v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhuobai Dong",
      "Rui Zhao",
      "Songjie Wu",
      "Junchao Yi",
      "Linjie Li",
      "Zhengyuan Yang",
      "Lijuan Wang",
      "Alex Jinpeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10562v1",
    "title": "Data-Efficient American Sign Language Recognition via Few-Shot Prototypical Networks",
    "summary": "Isolated Sign Language Recognition (ISLR) is critical for bridging the communication gap between the Deaf and Hard-of-Hearing (DHH) community and the hearing world. However, robust ISLR is fundamentally constrained by data scarcity and the long-tail distribution of sign vocabulary, where gathering sufficient examples for thousands of unique signs is prohibitively expensive. Standard classification approaches struggle under these conditions, often overfitting to frequent classes while failing to generalize to rare ones. To address this bottleneck, we propose a Few-Shot Prototypical Network framework adapted for a skeleton based encoder. Unlike traditional classifiers that learn fixed decision boundaries, our approach utilizes episodic training to learn a semantic metric space where signs are classified based on their proximity to dynamic class prototypes. We integrate a Spatiotemporal Graph Convolutional Network (ST-GCN) with a novel Multi-Scale Temporal Aggregation (MSTA) module to capture both rapid and fluid motion dynamics. Experimental results on the WLASL dataset demonstrate the superiority of this metric learning paradigm: our model achieves 43.75% Top-1 and 77.10% Top-5 accuracy on the test set. Crucially, this outperforms a standard classification baseline sharing the identical backbone architecture by over 13%, proving that the prototypical training strategy effectively outperforms in a data scarce situation where standard classification fails. Furthermore, the model exhibits strong zero-shot generalization, achieving nearly 30% accuracy on the unseen SignASL dataset without fine-tuning, offering a scalable pathway for recognizing extensive sign vocabularies with limited data.",
    "published": "2025-12-11T11:50:03Z",
    "updated": "2025-12-11T11:50:03Z",
    "link": "http://arxiv.org/pdf/2512.10562v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Meher Md Saad"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.08400v2",
    "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
    "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git",
    "published": "2025-12-09T09:33:53Z",
    "updated": "2025-12-11T11:39:09Z",
    "link": "http://arxiv.org/pdf/2512.08400v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Samitha Nuwan Thilakarathna",
      "Ercan Avsar",
      "Martin Mathias Nielsen",
      "Malte Pedersen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10554v1",
    "title": "Grounding Everything in Tokens for Multimodal Large Language Models",
    "summary": "Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.",
    "published": "2025-12-11T11:38:50Z",
    "updated": "2025-12-11T11:38:50Z",
    "link": "http://arxiv.org/pdf/2512.10554v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiangxuan Ren",
      "Zhongdao Wang",
      "Liping Hou",
      "Pin Tang",
      "Guoqing Wang",
      "Chao Ma"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10548v1",
    "title": "Blink: Dynamic Visual Token Resolution for Enhanced Multimodal Understanding",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress on various vision-language tasks, yet their visual perception remains limited. Humans, in comparison, perceive complex scenes efficiently by dynamically scanning and focusing on salient regions in a sequential \"blink-like\" process. Motivated by this strategy, we first investigate whether MLLMs exhibit similar behavior. Our pilot analysis reveals that MLLMs naturally attend to different visual regions across layers and that selectively allocating more computation to salient tokens can enhance visual perception. Building on this insight, we propose Blink, a dynamic visual token resolution framework that emulates the human-inspired process within a single forward pass. Specifically, Blink includes two modules: saliency-guided scanning and dynamic token resolution. It first estimates the saliency of visual tokens in each layer based on the attention map, and extends important tokens through a plug-and-play token super-resolution (TokenSR) module. In the next layer, it drops the extended tokens when they lose focus. This dynamic mechanism balances broad exploration and fine-grained focus, thereby enhancing visual perception adaptively and efficiently. Extensive experiments validate Blink, demonstrating its effectiveness in enhancing visual perception and multimodal understanding.",
    "published": "2025-12-11T11:27:25Z",
    "updated": "2025-12-11T11:27:25Z",
    "link": "http://arxiv.org/pdf/2512.10548v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuchen Feng",
      "Zhenyu Zhang",
      "Naibin Gu",
      "Yilong Chen",
      "Peng Fu",
      "Zheng Lin",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu",
      "Weiping Wang",
      "Haifeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.00087v2",
    "title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data",
    "summary": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.",
    "published": "2025-11-26T11:57:22Z",
    "updated": "2025-12-11T11:15:19Z",
    "link": "http://arxiv.org/pdf/2512.00087v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ivo Bueno",
      "Ruikun Hou",
      "Babette Bühler",
      "Tim Fütterer",
      "James Drimalla",
      "Jonathan Kyle Foster",
      "Peter Youngs",
      "Peter Gerjets",
      "Ulrich Trautwein",
      "Enkelejda Kasneci"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10524v1",
    "title": "Mode-Seeking for Inverse Problems with Diffusion Models",
    "summary": "A pre-trained unconditional diffusion model, combined with posterior sampling or maximum a posteriori (MAP) estimation techniques, can solve arbitrary inverse problems without task-specific training or fine-tuning. However, existing posterior sampling and MAP estimation methods often rely on modeling approximations and can be computationally demanding. In this work, we propose the variational mode-seeking loss (VML), which, when minimized during each reverse diffusion step, guides the generated sample towards the MAP estimate. VML arises from a novel perspective of minimizing the Kullback-Leibler (KL) divergence between the diffusion posterior $p(\\mathbf{x}_0|\\mathbf{x}_t)$ and the measurement posterior $p(\\mathbf{x}_0|\\mathbf{y})$, where $\\mathbf{y}$ denotes the measurement. Importantly, for linear inverse problems, VML can be analytically derived and need not be approximated. Based on further theoretical insights, we propose VML-MAP, an empirically effective algorithm for solving inverse problems, and validate its efficacy over existing methods in both performance and computational time, through extensive experiments on diverse image-restoration tasks across multiple datasets.",
    "published": "2025-12-11T10:51:34Z",
    "updated": "2025-12-11T10:51:34Z",
    "link": "http://arxiv.org/pdf/2512.10524v1.pdf",
    "category": [
      "cs.LG",
      "cs.CV"
    ],
    "authors": [
      "Sai Bharath Chandra Gutha",
      "Ricardo Vinuesa",
      "Hossein Azizpour"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10521v1",
    "title": "Take a Peek: Efficient Encoder Adaptation for Few-Shot Semantic Segmentation via LoRA",
    "summary": "Few-shot semantic segmentation (FSS) aims to segment novel classes in query images using only a small annotated support set. While prior research has mainly focused on improving decoders, the encoder's limited ability to extract meaningful features for unseen classes remains a key bottleneck. In this work, we introduce \\textit{Take a Peek} (TaP), a simple yet effective method that enhances encoder adaptability for both FSS and cross-domain FSS (CD-FSS). TaP leverages Low-Rank Adaptation (LoRA) to fine-tune the encoder on the support set with minimal computational overhead, enabling fast adaptation to novel classes while mitigating catastrophic forgetting. Our method is model-agnostic and can be seamlessly integrated into existing FSS pipelines. Extensive experiments across multiple benchmarks--including COCO $20^i$, Pascal $5^i$, and cross-domain datasets such as DeepGlobe, ISIC, and Chest X-ray--demonstrate that TaP consistently improves segmentation performance across diverse models and shot settings. Notably, TaP delivers significant gains in complex multi-class scenarios, highlighting its practical effectiveness in realistic settings. A rank sensitivity analysis also shows that strong performance can be achieved even with low-rank adaptations, ensuring computational efficiency. By addressing a critical limitation in FSS--the encoder's generalization to novel classes--TaP paves the way toward more robust, efficient, and generalizable segmentation systems. The code is available at https://github.com/pasqualedem/TakeAPeek.",
    "published": "2025-12-11T10:47:01Z",
    "updated": "2025-12-11T10:47:01Z",
    "link": "http://arxiv.org/pdf/2512.10521v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Pasquale De Marinis",
      "Gennaro Vessio",
      "Giovanna Castellano"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10517v1",
    "title": "3D Blood Pulsation Maps",
    "summary": "We present Pulse3DFace, the first dataset of its kind for estimating 3D blood pulsation maps. These maps can be used to develop models of dynamic facial blood pulsation, enabling the creation of synthetic video data to improve and validate remote pulse estimation methods via photoplethysmography imaging. Additionally, the dataset facilitates research into novel multi-view-based approaches for mitigating illumination effects in blood pulsation analysis. Pulse3DFace consists of raw videos from 15 subjects recorded at 30 Hz with an RGB camera from 23 viewpoints, blood pulse reference measurements, and facial 3D scans generated using monocular structure-from-motion techniques. It also includes processed 3D pulsation maps compatible with the texture space of the 3D head model FLAME. These maps provide signal-to-noise ratio, local pulse amplitude, phase information, and supplementary data. We offer a comprehensive evaluation of the dataset's illumination conditions, map consistency, and its ability to capture physiologically meaningful features in the facial and neck skin regions.",
    "published": "2025-12-11T10:40:31Z",
    "updated": "2025-12-11T10:40:31Z",
    "link": "http://arxiv.org/pdf/2512.10517v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Maurice Rohr",
      "Tobias Reinhardt",
      "Tizian Dege",
      "Justus Thies",
      "Christoph Hoog Antink"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10498v1",
    "title": "Robust Shape from Focus via Multiscale Directional Dilated Laplacian and Recurrent Network",
    "summary": "Shape-from-Focus (SFF) is a passive depth estimation technique that infers scene depth by analyzing focus variations in a focal stack. Most recent deep learning-based SFF methods typically operate in two stages: first, they extract focus volumes (a per pixel representation of focus likelihood across the focal stack) using heavy feature encoders; then, they estimate depth via a simple one-step aggregation technique that often introduces artifacts and amplifies noise in the depth map. To address these issues, we propose a hybrid framework. Our method computes multi-scale focus volumes traditionally using handcrafted Directional Dilated Laplacian (DDL) kernels, which capture long-range and directional focus variations to form robust focus volumes. These focus volumes are then fed into a lightweight, multi-scale GRU-based depth extraction module that iteratively refines an initial depth estimate at a lower resolution for computational efficiency. Finally, a learned convex upsampling module within our recurrent network reconstructs high-resolution depth maps while preserving fine scene details and sharp boundaries. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach outperforms state-of-the-art deep learning and traditional methods, achieving superior accuracy and generalization across diverse focal conditions.",
    "published": "2025-12-11T10:19:52Z",
    "updated": "2025-12-11T10:19:52Z",
    "link": "http://arxiv.org/pdf/2512.10498v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Khurram Ashfaq",
      "Muhammad Tariq Mahmood"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.04519v2",
    "title": "l0-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion",
    "summary": "Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an $\\ell_0$-regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the $\\ell_0$-regularized CSC problem, we design a learnable $\\ell_0$-regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an $\\ell_0$-regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet's training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection \\textcolor[rgb]{ 0, 0, 0}{and semantic segmentation} in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/gargi884/FNet-MMIF.",
    "published": "2024-11-07T08:20:29Z",
    "updated": "2025-12-11T10:16:19Z",
    "link": "http://arxiv.org/pdf/2411.04519v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Gargi Panda",
      "Soumitra Kundu",
      "Saumik Bhattacharya",
      "Aurobinda Routray"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06613v2",
    "title": "Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach",
    "summary": "Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification, predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.\n  We introduce DiatomCascadeNet (H-COFGS), a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.\n  H-COFGS matches flat baselines at the species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5% of misclassified species are correctly predicted at the genus level, versus 67.2% for flat baselines. H-COFGS reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).\n  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.",
    "published": "2025-12-07T01:06:13Z",
    "updated": "2025-12-11T09:51:53Z",
    "link": "http://arxiv.org/pdf/2512.06613v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yueying Ke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.08591v3",
    "title": "Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints",
    "summary": "3D human generation is increasingly significant in various applications. However, the direct use of 2D generative methods in 3D generation often results in losing local details, while methods that reconstruct geometry from generated images struggle with global view consistency. In this work, we introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details. To achieve this, we employ the Fourier occupancy field (FOF) representation, enabling the direct generation of 3D shapes as preliminary results with 2D generative models. With the proposed high-frequency enhancer and the multi-view recarving strategy, our method can seamlessly integrate the details from different views into a uniform global shape. To better utilize the 3D human prior and enhance control over the generated geometry, we introduce a compact spherical embedding of 3D joints. This allows for an effective guidance of pose during the generation process. Additionally, our method can generate 3D humans guided by textual inputs. Our experimental results demonstrate the capability of our method to ensure global structure, local details, high resolution, and low computational cost simultaneously. More results and the code can be found on our project page at http://cic.tju.edu.cn/faculty/likun/projects/Joint2Human.",
    "published": "2023-12-14T01:24:22Z",
    "updated": "2025-12-11T09:43:34Z",
    "link": "http://arxiv.org/pdf/2312.08591v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Muxin Zhang",
      "Qiao Feng",
      "Zhuo Su",
      "Chao Wen",
      "Zhou Xue",
      "Kun Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.02671v2",
    "title": "Test-Time Distillation for Continual Model Adaptation",
    "summary": "Deep neural networks often suffer performance degradation upon deployment due to distribution shifts. Continual Test-Time Adaptation (CTTA) aims to address this issue in an unsupervised manner, yet existing methods, which rely on self-supervision, are prone to an inherent self-referential feedback loop that amplifies initial prediction errors, leading to model drift. We revisit this limitation and propose Test-Time Distillation (TTD), which reframes adaptation as a distillation process guided by a frozen Vision-Language Model (VLM) as an external signal. While promising, we find that direct distillation is fraught with two pitfalls: the Generalist Trap, where the VLM's broad but non-specialized knowledge leads to suboptimal performance on specific tasks and shifts, and the Entropy Bias, where naive model fusion techniques based on entropy fail due to the disparate calibration of heterogeneous models. These pitfalls motivate our insight: the key is to build a robust supervisory signal and leverage it to guide the target model toward stable adaptation. Hence, we present CoDiRe, a Continual Distillation and Rectification framework for TTD. CoDiRe first constructs a robust blended teacher by dynamically fusing the predictions of the VLM and the target model. Critically, it circumvents the Entropy Bias by leveraging Maximum Softmax Probability (MSP) as a more reliable confidence metric for weighting each model's expertise. Then applies an Optimal Transport based rectification to further align predictions with the blended teacher, enabling continuous and stable adaptation. Extensive experiments show that CoDiRe outperforms state-of-the-art baselines, exceeding CoTTA by 10.55% while using only 48% of its time cost on ImageNet-C.",
    "published": "2025-06-03T09:16:51Z",
    "updated": "2025-12-11T09:34:01Z",
    "link": "http://arxiv.org/pdf/2506.02671v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiao Chen",
      "Jiazhen Huang",
      "Zhiming Liu",
      "Qinting Jiang",
      "Fanding Huang",
      "Jingyan Jiang",
      "Zhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.12299v2",
    "title": "Sublinear Variational Optimization of Gaussian Mixture Models with Millions to Billions of Parameters",
    "summary": "Gaussian Mixture Models (GMMs) range among the most frequently used models in machine learning. However, training large, general GMMs becomes computationally prohibitive for datasets that have many data points $N$ of high-dimensionality $D$. For GMMs with arbitrary covariances, we here derive a highly efficient variational approximation, which is then integrated with mixtures of factor analyzers (MFAs). For GMMs with $C$ components, our proposed algorithm substantially reduces runtime complexity from $\\mathcal{O}(NCD^2)$ per iteration to a complexity scaling linearly with $D$ and sublinearly with $NC$. In numerical experiments, we first validate that the complexity reduction results in a sublinear scaling for the entire GMM optimization process. Second, we show on large-scale benchmarks that the sublinear algorithm results in speed-ups of an order-of-magnitude compared to the state-of-the-art. Third, as a proof of concept, we finally train GMMs with over 10 billion parameters on about 100 million images, observing training times of less than nine hours on a single state-of-the-art CPU. Finally, and forth, we demonstrate the effectiveness of large-scale GMMs on the task of zero-shot image denoising, where sublinear training results in state-of-the-art denoising times while competitive denoising performance is maintained.",
    "published": "2025-01-21T17:11:25Z",
    "updated": "2025-12-11T09:27:19Z",
    "link": "http://arxiv.org/pdf/2501.12299v2.pdf",
    "category": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "authors": [
      "Sebastian Salwig",
      "Till Kahlke",
      "Florian Hirschberger",
      "Dennis Forster",
      "Jörg Lücke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10450v1",
    "title": "Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment",
    "summary": "Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.",
    "published": "2025-12-11T09:14:51Z",
    "updated": "2025-12-11T09:14:51Z",
    "link": "http://arxiv.org/pdf/2512.10450v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Han Li",
      "Shaohui Li",
      "Wenrui Dai",
      "Chenglin Li",
      "Xinlong Pan",
      "Haipeng Wang",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.21336v3",
    "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation",
    "summary": "The integration of AI-assisted biomedical image analysis into clinical practice demands AI-generated findings that are not only accurate but also interpretable to clinicians. However, existing biomedical AI models generally lack the ability to simultaneously generate diagnostic findings and localize corresponding biomedical objects. This limitation makes it challenging for clinicians to correlate AI-generated findings with visual evidence (e.g., tiny lesions) in images and interpret the results of AI models. To address this challenge, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation, which is capable of generating accurate diagnostic findings and simultaneously segmenting the corresponding biomedical targets. UniBiomed is based on a novel integration of Multi-modal Large Language Model and Segment Anything Model, which can effectively unify diverse biomedical tasks in universal training for advancing grounded interpretation. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, region annotations, and text descriptions across ten biomedical imaging modalities. Extensive validation on 70 internal and 14 external datasets demonstrated the state-of-the-art performance of UniBiomed in diverse biomedical tasks, including image segmentation, disease recognition, region-aware diagnosis, vision question answering, and report generation. In summary, UniBiomed is a powerful and versatile biomedical foundation model, unlocking the untapped grounded interpretation capability for optimizing AI-assisted biomedical image analysis.",
    "published": "2025-04-30T05:51:48Z",
    "updated": "2025-12-11T09:08:31Z",
    "link": "http://arxiv.org/pdf/2504.21336v3.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Linshan Wu",
      "Yuxiang Nie",
      "Sunan He",
      "Jiaxin Zhuang",
      "Luyang Luo",
      "Tao Li",
      "Zhuoyao Xie",
      "Dexuan Chen",
      "Yinghua Zhao",
      "Neeraj Mahboobani",
      "Varut Vardhanabhuti",
      "Ronald Cheong Kin Chan",
      "Yifan Peng",
      "Pranav Rajpurkar",
      "Hao Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2503.11352v2",
    "title": "Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference Frame Bias via Frame-Invariant Similarity Measures",
    "summary": "The ability of robots to recognize human gestures facilitates a natural and accessible human-robot collaboration. However, most work in gesture recognition remains rooted in reference frame-dependent representations. This poses a challenge when reference frames vary due to different work cell layouts, imprecise frame calibrations, or other environmental changes. This paper investigated the use of invariant trajectory descriptors for robust hand palm motion gesture recognition under reference frame changes. First, a novel dataset of recorded Hand Palm Motion (HPM) gestures is introduced. The motion gestures in this dataset were specifically designed to be distinguishable without dependence on specific reference frames or directional cues. Afterwards, multiple invariant trajectory descriptor approaches were benchmarked to assess how their performances generalize to this novel HPM dataset. After this offline benchmarking, the best scoring approach is validated for online recognition by developing a real-time Proof of Concept (PoC). In this PoC, hand palm motion gestures were used to control the real-time movement of a manipulator arm. The PoC demonstrated a high recognition reliability in real-time operation, achieving an $F_1$-score of 92.3%. This work demonstrates the effectiveness of the invariant descriptor approach as a standalone solution. Moreover, we believe that the invariant descriptor approach can also be utilized within other state-of-the-art pattern recognition and learning systems to improve their robustness against reference frame variations.",
    "published": "2025-03-14T12:40:43Z",
    "updated": "2025-12-11T08:55:40Z",
    "link": "http://arxiv.org/pdf/2503.11352v2.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "cs.HC"
    ],
    "authors": [
      "Arno Verduyn",
      "Maxim Vochten",
      "Joris De Schutter"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18925v2",
    "title": "AttenDence: Maximizing Attention Confidence for Test Time Adaptation",
    "summary": "Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective. This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.",
    "published": "2025-11-24T09:32:01Z",
    "updated": "2025-12-11T08:40:51Z",
    "link": "http://arxiv.org/pdf/2511.18925v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yash Mali"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10421v1",
    "title": "Neural Collapse in Test-Time Adaptation",
    "summary": "Test-Time Adaptation (TTA) enhances model robustness to out-of-distribution (OOD) data by updating the model online during inference, yet existing methods lack theoretical insights into the fundamental causes of performance degradation under domain shifts. Recently, Neural Collapse (NC) has been proposed as an emergent geometric property of deep neural networks (DNNs), providing valuable insights for TTA. In this work, we extend NC to the sample-wise level and discover a novel phenomenon termed Sample-wise Alignment Collapse (NC3+), demonstrating that a sample's feature embedding, obtained by a trained model, aligns closely with the corresponding classifier weight. Building on NC3+, we identify that the performance degradation stems from sample-wise misalignment in adaptation which exacerbates under larger distribution shifts. This indicates the necessity of realigning the feature embeddings with their corresponding classifier weights. However, the misalignment makes pseudo-labels unreliable under domain shifts. To address this challenge, we propose NCTTA, a novel feature-classifier alignment method with hybrid targets to mitigate the impact of unreliable pseudo-labels, which blends geometric proximity with predictive confidence. Extensive experiments demonstrate the effectiveness of NCTTA in enhancing robustness to domain shifts. For example, NCTTA outperforms Tent by 14.52% on ImageNet-C.",
    "published": "2025-12-11T08:34:58Z",
    "updated": "2025-12-11T08:34:58Z",
    "link": "http://arxiv.org/pdf/2512.10421v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiao Chen",
      "Zhongjing Du",
      "Jiazhen Huang",
      "Xu Jiang",
      "Li Lu",
      "Jingyan Jiang",
      "Zhi Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10419v1",
    "title": "TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning",
    "summary": "Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.",
    "published": "2025-12-11T08:34:26Z",
    "updated": "2025-12-11T08:34:26Z",
    "link": "http://arxiv.org/pdf/2512.10419v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Phu Pham",
      "Damon Conover",
      "Aniket Bera"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.13901v5",
    "title": "Dressing the Imagination: A Dataset for AI-Powered Translation of Text into Fashion Outfits and A Novel NeRA Adapter for Enhanced Feature Adaptation",
    "summary": "Specialized datasets that capture the fashion industry's rich language and styling elements can boost progress in AI-driven fashion design. We present FLORA, (Fashion Language Outfit Representation for Apparel Generation), the first comprehensive dataset containing 4,330 curated pairs of fashion outfits and corresponding textual descriptions. Each description utilizes industry-specific terminology and jargon commonly used by professional fashion designers, providing precise and detailed insights into the outfits. Hence, the dataset captures the delicate features and subtle stylistic elements necessary to create high-fidelity fashion designs.\n  We demonstrate that fine-tuning generative models on the FLORA dataset significantly enhances their capability to generate accurate and stylistically rich images from textual descriptions of fashion sketches. FLORA will catalyze the creation of advanced AI models capable of comprehending and producing subtle, stylistically rich fashion designs. It will also help fashion designers and end-users to bring their ideas to life.\n  As a second orthogonal contribution, we introduce NeRA (Nonlinear low-rank Expressive Representation Adapter), a novel adapter architecture based on Kolmogorov-Arnold Networks (KAN). Unlike traditional PEFT techniques such as LoRA, LoKR, DoRA, and LoHA that use MLP adapters, NeRA uses learnable spline-based nonlinear transformations, enabling superior modeling of complex semantic relationships, achieving strong fidelity, faster convergence and semantic alignment. Extensive experiments on our proposed FLORA and LAION-5B datasets validate the superiority of NeRA over existing adapters.\n  We will open-source both the FLORA dataset and our implementation code.",
    "published": "2024-11-21T07:27:45Z",
    "updated": "2025-12-11T08:28:15Z",
    "link": "http://arxiv.org/pdf/2411.13901v5.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Gayatri Deshmukh",
      "Somsubhra De",
      "Chirag Sehgal",
      "Jishu Sen Gupta",
      "Sparsh Mittal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2206.13356v2",
    "title": "Effective Online Exam Proctoring by Combining Lightweight Face Detection and Deep Recognition",
    "summary": "Online exams, conducted via video conferencing platforms such as Zoom, have become popular in educational institutions since COVID-19. While convenient, ensuring the integrity and security of online exams remains challenging, as traditional invigilation struggles to effectively monitor multiple student video feeds in real time. In this paper, we present iExam, an effective online exam proctoring and analysis system that combines lightweight face detection and deep recognition. iExam employs real-time face detection to assist invigilators in continuously monitoring student presence, and leverages deep face recognition for post-exam video analysis to identify abnormal behaviors--including face disappearance, face rotation, and identity substitution. To realize this system, we address three core challenges: (i) designing a lightweight approach to efficiently capture and analyze exam video streams in real time; (ii) developing an enhanced OCR method to automatically extract student identities from dynamically positioned Zoom name tags, enabling reliable ground truth labeling without manual intervention; and (iii) optimizing the training and inference pipeline to significantlyreduce resource and time requirements on ordinary teacher devices. Extensive experiments demonstrate that iExam achieves 90.4% accuracy for real-time face detection and 98.4% accuracy for post-exam face recognition, while maintaining low overhead. These results show that iExam can substantially enhance the automation and reliability of online exam proctoring in practice.",
    "published": "2022-06-27T15:03:25Z",
    "updated": "2025-12-11T08:21:20Z",
    "link": "http://arxiv.org/pdf/2206.13356v2.pdf",
    "category": [
      "cs.CV",
      "eess.IV"
    ],
    "authors": [
      "Xu Yang",
      "Juantao Zhong",
      "Daoyuan Wu",
      "Xiao Yi",
      "Jimmy H. M. Lee",
      "Tan Lee",
      "Peng Han"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10408v1",
    "title": "MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos",
    "summary": "The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.",
    "published": "2025-12-11T08:18:22Z",
    "updated": "2025-12-11T08:18:22Z",
    "link": "http://arxiv.org/pdf/2512.10408v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Qiyue Sun",
      "Tailin Chen",
      "Yinghui Zhang",
      "Yuchen Zhang",
      "Jiangbei Yue",
      "Jianbo Jiao",
      "Zeyu Fu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.07752v2",
    "title": "DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream",
    "summary": "Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.",
    "published": "2025-10-09T03:43:27Z",
    "updated": "2025-12-11T08:13:51Z",
    "link": "http://arxiv.org/pdf/2510.07752v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Junhao He",
      "Jiaxu Wang",
      "Jia Li",
      "Mingyuan Sun",
      "Qiang Zhang",
      "Jiahang Cao",
      "Ziyi Zhang",
      "Yi Gu",
      "Jingkai Sun",
      "Renjing Xu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10386v1",
    "title": "Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method",
    "summary": "High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.",
    "published": "2025-12-11T07:49:28Z",
    "updated": "2025-12-11T07:49:28Z",
    "link": "http://arxiv.org/pdf/2512.10386v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ge Zhang",
      "Chunyang Wang",
      "Bo Xiao",
      "Xuelian Liu",
      "Bin Liu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10379v1",
    "title": "Self-Supervised Contrastive Embedding Adaptation for Endoscopic Image Matching",
    "summary": "Accurate spatial understanding is essential for image-guided surgery, augmented reality integration and context awareness. In minimally invasive procedures, where visual input is the sole intraoperative modality, establishing precise pixel-level correspondences between endoscopic frames is critical for 3D reconstruction, camera tracking, and scene interpretation. However, the surgical domain presents distinct challenges: weak perspective cues, non-Lambertian tissue reflections, and complex, deformable anatomy degrade the performance of conventional computer vision techniques. While Deep Learning models have shown strong performance in natural scenes, their features are not inherently suited for fine-grained matching in surgical images and require targeted adaptation to meet the demands of this domain. This research presents a novel Deep Learning pipeline for establishing feature correspondences in endoscopic image pairs, alongside a self-supervised optimization framework for model training. The proposed methodology leverages a novel-view synthesis pipeline to generate ground-truth inlier correspondences, subsequently utilized for mining triplets within a contrastive learning paradigm. Through this self-supervised approach, we augment the DINOv2 backbone with an additional Transformer layer, specifically optimized to produce embeddings that facilitate direct matching through cosine similarity thresholding. Experimental evaluation demonstrates that our pipeline surpasses state-of-the-art methodologies on the SCARED datasets improved matching precision and lower epipolar error compared to the related work. The proposed framework constitutes a valuable contribution toward enabling more accurate high-level computer vision applications in surgical endoscopy.",
    "published": "2025-12-11T07:44:00Z",
    "updated": "2025-12-11T07:44:00Z",
    "link": "http://arxiv.org/pdf/2512.10379v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Alberto Rota",
      "Elena De Momi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10376v1",
    "title": "RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds",
    "summary": "Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.",
    "published": "2025-12-11T07:41:33Z",
    "updated": "2025-12-11T07:41:33Z",
    "link": "http://arxiv.org/pdf/2512.10376v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jingyun Fu",
      "Zhiyu Xiang",
      "Na Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10369v1",
    "title": "Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.",
    "published": "2025-12-11T07:36:35Z",
    "updated": "2025-12-11T07:36:35Z",
    "link": "http://arxiv.org/pdf/2512.10369v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Zhankuo Xu",
      "Chaoran Feng",
      "Yingtao Li",
      "Jianbin Zhao",
      "Jiashu Yang",
      "Wangbo Yu",
      "Li Yuan",
      "Yonghong Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.07811v2",
    "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning",
    "summary": "Video Question Answering (VideoQA) aims to answer natural language questions based on the given video, with prior work primarily focusing on identifying the duration of relevant segments, referred to as explicit visual evidence. However, explicit visual evidence is not always directly available, particularly when questions target symbolic meanings or deeper intentions, leading to significant performance degradation. To fill this gap, we introduce a novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo $\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering questions in scenarios where explicit visual evidence is inaccessible. Given an implicit question and its corresponding video, I-VQA requires answering based on the contextual visual cues present within the video. To tackle I-VQA, we propose a novel reasoning framework, IRM (Implicit Reasoning Model), incorporating dual-stream modeling of contextual actions and intent clues as implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the Visual Enhancement Module (VEM). AIM deduces and preserves question-related dual clues by generating clue candidates and performing relation deduction. VEM enhances contextual visual representation by leveraging key contextual clues. Extensive experiments validate the effectiveness of our IRM in I-VQA tasks, outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$, $1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on similar implicit advertisement understanding and future prediction in traffic-VQA. Datasets and codes are available for double-blind review in anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.",
    "published": "2025-06-09T14:38:14Z",
    "updated": "2025-12-11T07:35:16Z",
    "link": "http://arxiv.org/pdf/2506.07811v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tieyuan Chen",
      "Huabin Liu",
      "Yi Wang",
      "Chaofan Gan",
      "Mingxi Lyu",
      "Ziran Qin",
      "Shijie Li",
      "Liquan Shen",
      "Junhui Hou",
      "Zheng Wang",
      "Weiyao Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.18333v2",
    "title": "ConsistCompose: Unified Multimodal Layout Control for Image Composition",
    "summary": "Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.",
    "published": "2025-11-23T08:14:53Z",
    "updated": "2025-12-11T07:30:18Z",
    "link": "http://arxiv.org/pdf/2511.18333v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xuanke Shi",
      "Boxuan Li",
      "Xiaoyang Han",
      "Zhongang Cai",
      "Lei Yang",
      "Dahua Lin",
      "Quan Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10363v1",
    "title": "Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos",
    "summary": "Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \\textbf{P}oint-\\textbf{to}-\\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\\% on R5@0.1 on MAD).",
    "published": "2025-12-11T07:25:48Z",
    "updated": "2025-12-11T07:25:48Z",
    "link": "http://arxiv.org/pdf/2512.10363v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Mingyu Jeon",
      "Jisoo Yang",
      "Sungjin Han",
      "Jinkwon Hwang",
      "Sunjae Yoon",
      "Jonghee Kim",
      "Junyeoung Kim"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10359v1",
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "summary": "Video Question Answering (VideoQA) task serves as a critical playground for evaluating whether foundation models can effectively perceive, understand, and reason about dynamic real-world scenarios. However, existing Multimodal Large Language Models (MLLMs) struggle with simultaneously modeling spatial relationships within video frames and understanding the causal dynamics of temporal evolution on complex and reasoning-intensive VideoQA task. In this work, we equip MLLM with a comprehensive and extensible Video Toolkit, to enhance MLLM's spatiotemporal reasoning capabilities and ensure the harmony between the quantity and diversity of tools. To better control the tool invocation sequence and avoid toolchain shortcut issues, we propose a Spatiotemporal Reasoning Framework (STAR) that strategically schedules temporal and spatial tools, thereby progressively localizing the key area in the video. Our STAR framework enhances GPT-4o using lightweight tools, achieving an 8.2% gain on VideoMME and 4.6% on LongVideoBench. We believe that our proposed Video Toolkit and STAR framework make an important step towards building autonomous and intelligent video analysis assistants. The code is publicly available at https://github.com/fansunqi/VideoTool.",
    "published": "2025-12-11T07:17:57Z",
    "updated": "2025-12-11T07:17:57Z",
    "link": "http://arxiv.org/pdf/2512.10359v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Sunqi Fan",
      "Jiashuo Cui",
      "Meng-Hao Guo",
      "Shuojin Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10357v1",
    "title": "mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar",
    "summary": "mmWave radars struggle to detect or count individuals in dense, static (non-moving) groups due to limitations in spatial resolution and reliance on movement for detection. We present mmCounter, which accurately counts static people in dense indoor spaces (up to three people per square meter). mmCounter achieves this by extracting ultra-low frequency (< 1 Hz) signals, primarily from breathing and micro-scale body movements such as slight torso shifts, and applying novel signal processing techniques to differentiate these subtle signals from background noise and nearby static objects. Our problem differs significantly from existing studies on breathing rate estimation, which assume the number of people is known a priori. In contrast, mmCounter utilizes a novel multi-stage signal processing pipeline to extract relevant low-frequency sources along with their spatial information and map these sources to individual people, enabling accurate counting. Extensive evaluations in various environments demonstrate that mmCounter delivers an 87% average F1 score and 0.6 mean absolute error in familiar environments, and a 60% average F1 score and 1.1 mean absolute error in previously untested environments. It can count up to seven individuals in a three square meter space, such that there is no side-by-side spacing and only a one-meter front-to-back distance.",
    "published": "2025-12-11T07:16:47Z",
    "updated": "2025-12-11T07:16:47Z",
    "link": "http://arxiv.org/pdf/2512.10357v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Tarik Reza Toha",
      " Shao-Jung",
      " Lu",
      "Shahriar Nirjon"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10353v1",
    "title": "Hybrid Transformer-Mamba Architecture for Weakly Supervised Volumetric Medical Segmentation",
    "summary": "Weakly supervised semantic segmentation offers a label-efficient solution to train segmentation models for volumetric medical imaging. However, existing approaches often rely on 2D encoders that neglect the inherent volumetric nature of the data. We propose TranSamba, a hybrid Transformer-Mamba architecture designed to capture 3D context for weakly supervised volumetric medical segmentation. TranSamba augments a standard Vision Transformer backbone with Cross-Plane Mamba blocks, which leverage the linear complexity of state space models for efficient information exchange across neighboring slices. The information exchange enhances the pairwise self-attention within slices computed by the Transformer blocks, directly contributing to the attention maps for object localization. TranSamba achieves effective volumetric modeling with time complexity that scales linearly with the input volume depth and maintains constant memory usage for batch processing. Extensive experiments on three datasets demonstrate that TranSamba establishes new state-of-the-art performance, consistently outperforming existing methods across diverse modalities and pathologies. Our source code and trained models are openly accessible at: https://github.com/YihengLyu/TranSamba.",
    "published": "2025-12-11T07:09:32Z",
    "updated": "2025-12-11T07:09:32Z",
    "link": "http://arxiv.org/pdf/2512.10353v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yiheng Lyu",
      "Lian Xu",
      "Mohammed Bennamoun",
      "Farid Boussaid",
      "Coen Arrow",
      "Girish Dwivedi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10352v1",
    "title": "Topology-Agnostic Animal Motion Generation from Text Prompt",
    "summary": "Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.",
    "published": "2025-12-11T07:08:29Z",
    "updated": "2025-12-11T07:08:29Z",
    "link": "http://arxiv.org/pdf/2512.10352v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Keyi Chen",
      "Mingze Sun",
      "Zhenyu Liu",
      "Zhangquan Chen",
      "Ruqi Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10342v1",
    "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
    "summary": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.",
    "published": "2025-12-11T06:46:51Z",
    "updated": "2025-12-11T06:46:51Z",
    "link": "http://arxiv.org/pdf/2512.10342v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Shresth Grover",
      "Priyank Pathak",
      "Akash Kumar",
      "Vibhav Vineet",
      "Yogesh S Rawat"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10340v1",
    "title": "Zero-shot Adaptation of Stable Diffusion via Plug-in Hierarchical Degradation Representation for Real-World Super-Resolution",
    "summary": "Real-World Image Super-Resolution (Real-ISR) aims to recover high-quality images from low-quality inputs degraded by unknown and complex real-world factors. Real-world scenarios involve diverse and coupled degradations, making it necessary to provide diffusion models with richer and more informative guidance. However, existing methods often assume known degradation severity and rely on CLIP text encoders that cannot capture numerical severity, limiting their generalization ability. To address this, we propose \\textbf{HD-CLIP} (\\textbf{H}ierarchical \\textbf{D}egradation CLIP), which decomposes a low-quality image into a semantic embedding and an ordinal degradation embedding that captures ordered relationships and allows interpolation across unseen levels. Furthermore, we integrated it into diffusion models via classifier-free guidance (CFG) and proposed classifier-free projection guidance (CFPG). HD-CLIP leverages semantic cues to guide generative restoration while using degradation cues to suppress undesired hallucinations and artifacts. As a \\textbf{plug-and-play module}, HD-CLIP can be seamlessly integrated into various super-resolution frameworks without training, significantly improving detail fidelity and perceptual realism across diverse real-world datasets.",
    "published": "2025-12-11T06:45:28Z",
    "updated": "2025-12-11T06:45:28Z",
    "link": "http://arxiv.org/pdf/2512.10340v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yi-Cheng Liao",
      "Shyang-En Weng",
      "Yu-Syuan Xu",
      "Chi-Wei Hsiao",
      "Wei-Chen Chiu",
      "Ching-Chun Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10334v1",
    "title": "A Conditional Generative Framework for Synthetic Data Augmentation in Segmenting Thin and Elongated Structures in Biological Images",
    "summary": "Thin and elongated filamentous structures, such as microtubules and actin filaments, often play important roles in biological systems. Segmenting these filaments in biological images is a fundamental step for quantitative analysis. Recent advances in deep learning have significantly improved the performance of filament segmentation. However, there is a big challenge in acquiring high quality pixel-level annotated dataset for filamentous structures, as the dense distribution and geometric properties of filaments making manual annotation extremely laborious and time-consuming. To address the data shortage problem, we propose a conditional generative framework based on the Pix2Pix architecture to generate realistic filaments in microscopy images from binary masks. We also propose a filament-aware structural loss to improve the structure similarity when generating synthetic images. Our experiments have demonstrated the effectiveness of our approach and outperformed existing model trained without synthetic data.",
    "published": "2025-12-11T06:36:44Z",
    "updated": "2025-12-11T06:36:44Z",
    "link": "http://arxiv.org/pdf/2512.10334v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yi Liu",
      "Yichi Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10327v1",
    "title": "Simple Yet Effective Selective Imputation for Incomplete Multi-view Clustering",
    "summary": "Incomplete multi-view data, where different views suffer from missing and unbalanced observations, pose significant challenges for clustering. Existing imputation-based methods attempt to estimate missing views to restore data associations, but indiscriminate imputation often introduces noise and bias, especially when the available information is insufficient. Imputation-free methods avoid this risk by relying solely on observed data, but struggle under severe incompleteness due to the lack of cross-view complementarity. To address this issue, we propose Informativeness-based Selective imputation Multi-View Clustering (ISMVC). Our method evaluates the imputation-relevant informativeness of each missing position based on intra-view similarity and cross-view consistency, and selectively imputes only when sufficient support is available. Furthermore, we integrate this selection with a variational autoencoder equipped with a mixture-of-Gaussians prior to learn clustering-friendly latent representations. By performing distribution-level imputation, ISMVC not only stabilizes the aggregation of posterior distributions but also explicitly models imputation uncertainty, enabling robust fusion and preventing overconfident reconstructions. Compared with existing cautious imputation strategies that depend on training dynamics or model feedback, our method is lightweight, data-driven, and model-agnostic. It can be readily integrated into existing IMC models as a plug-in module. Extensive experiments on multiple benchmark datasets under a more realistic and challenging unbalanced missing scenario demonstrate that our method outperforms both imputation-based and imputation-free approaches.",
    "published": "2025-12-11T06:22:23Z",
    "updated": "2025-12-11T06:22:23Z",
    "link": "http://arxiv.org/pdf/2512.10327v1.pdf",
    "category": [
      "cs.CV",
      "cs.MM"
    ],
    "authors": [
      "Cai Xu",
      "Jinlong Liu",
      "Yilin Zhang",
      "Ziyu Guan",
      "Wei Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10326v1",
    "title": "StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology",
    "summary": "Foundation models trained with self-supervised learning (SSL) on large-scale histological images have significantly accelerated the development of computational pathology. These models can serve as backbones for region-of-interest (ROI) image analysis or patch-level feature extractors in whole-slide images (WSIs) based on multiple instance learning (MIL). Existing pathology foundation models (PFMs) are typically pre-trained on Hematoxylin-Eosin (H&E) stained pathology images. However, images with special stains, such as immunohistochemistry, are also frequently used in clinical practice. PFMs pre-trained mainly on H\\&E-stained images may be limited in clinical applications involving special stains. To address this issue, we propose StainNet, a specialized foundation model for special stains based on the vision transformer (ViT) architecture. StainNet adopts a self-distillation SSL approach and is trained on over 1.4 million patch images cropping from 20,231 publicly available special staining WSIs in the HISTAI database. To evaluate StainNet, we conduct experiments on an in-house slide-level liver malignancy classification task and two public ROI-level datasets to demonstrate its strong ability. We also perform few-ratio learning and retrieval evaluations, and compare StainNet with recently larger PFMs to further highlight its strengths. We have released the StainNet model weights at: https://huggingface.co/JWonderLand/StainNet.",
    "published": "2025-12-11T06:21:17Z",
    "updated": "2025-12-11T06:21:17Z",
    "link": "http://arxiv.org/pdf/2512.10326v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Jiawen Li",
      "Jiali Hu",
      "Xitong Ling",
      "Yongqiang Lv",
      "Yuxuan Chen",
      "Yizhi Wang",
      "Tian Guan",
      "Yifei Liu",
      "Yonghong He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10324v1",
    "title": "EchoingPixels: Cross-Modal Adaptive Token Reduction for Efficient Audio-Visual LLMs",
    "summary": "Audio-Visual Large Language Models (AV-LLMs) face prohibitive computational overhead from massive audio and video tokens. Token reduction, while extensively explored for video-only LLMs, is insufficient for the audio-visual domain, as these unimodal methods cannot leverage audio-visual cross-modal synergies. Furthermore, the distinct and dynamic information densities of audio and video render static budgets per modality suboptimal. How to perform token reduction on a joint audio-visual stream thus remains an unaddressed bottleneck. To fill this gap, we introduce EchoingPixels, a framework inspired by the coexistence and interaction of visuals and sound in real-world scenes. The core of our framework is the Cross-Modal Semantic Sieve (CS2), a module enabling early audio-visual interaction. Instead of compressing modalities independently, CS2 co-attends to the joint multimodal stream and reduces tokens from an entire combined pool of audio-visual tokens rather than using fixed budgets per modality. This single-pool approach allows it to adaptively allocate the token budget across both modalities and dynamically identify salient tokens in concert. To ensure this aggressive reduction preserves the vital temporal modeling capability, we co-design a Synchronization-Augmented RoPE (Sync-RoPE) to maintain critical temporal relationships for the sparsely selected tokens. Extensive experiments demonstrate that EchoingPixels achieves performance comparable to strong baselines using only 5-20% of the original tokens, with a 2-3x speedup and memory reduction.",
    "published": "2025-12-11T06:18:58Z",
    "updated": "2025-12-11T06:18:58Z",
    "link": "http://arxiv.org/pdf/2512.10324v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Chao Gong",
      "Depeng Wang",
      "Zhipeng Wei",
      "Ya Guo",
      "Huijia Zhu",
      "Jingjing Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10321v1",
    "title": "Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset",
    "summary": "We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.",
    "published": "2025-12-11T06:11:24Z",
    "updated": "2025-12-11T06:11:24Z",
    "link": "http://arxiv.org/pdf/2512.10321v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Hyunsoo Lee",
      "Daeum Jeon",
      "Hyeokjae Oh"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10319v1",
    "title": "Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot",
    "summary": "Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.",
    "published": "2025-12-11T06:11:05Z",
    "updated": "2025-12-11T06:11:05Z",
    "link": "http://arxiv.org/pdf/2512.10319v1.pdf",
    "category": [
      "cs.RO",
      "cs.CV",
      "eess.SY"
    ],
    "authors": [
      "Muhammad Usama",
      "Muhammad Ibrahim Khan",
      "Ahmad Hasan",
      "Muhammad Shaaf Nadeem",
      "Khawaja Fahad Iqbal",
      "Jawad Aslam",
      "Mian Ashfaq Ali",
      "Asad Nisar Awan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17951v4",
    "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes",
    "summary": "We present SplatCo, a structure-view collaborative Gaussian splatting framework for high-fidelity rendering of complex outdoor scenes. SplatCo builds upon three novel components: 1) a cross-structure collaboration module that combines global tri-plane representations, which capture coarse scene layouts, with local context grid features representing fine details. This fusion is achieved through a hierarchical compensation mechanism, ensuring both global spatial awareness and local detail preservation; 2) a cross-view pruning mechanism that removes overfitted or inaccurate Gaussians based on structural consistency, thereby improving storage efficiency and preventing rendering artifacts; 3) a structure view co-learning module that aggregates structural gradients with view gradients,thereby steering the optimization of Gaussian geometric and appearance attributes more robustly. By combining these key components, SplatCo effectively achieves high-fidelity rendering for large-scale scenes. Code and project page are available at https://splatco-tech.github.io.",
    "published": "2025-05-23T14:27:12Z",
    "updated": "2025-12-11T06:09:04Z",
    "link": "http://arxiv.org/pdf/2505.17951v4.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Haihong Xiao",
      "Jianan Zou",
      "Yuxin Zhou",
      "Ying He",
      "Wenxiong Kang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10316v1",
    "title": "ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation",
    "summary": "Weakly supervised semantic segmentation (WSSS) in histopathology relies heavily on classification backbones, yet these models often localize only the most discriminative regions and struggle to capture the full spatial extent of tissue structures. Vision-language models such as CONCH offer rich semantic alignment and morphology-aware representations, while modern segmentation backbones like SegFormer preserve fine-grained spatial cues. However, combining these complementary strengths remains challenging, especially under weak supervision and without dense annotations. We propose a prototype learning framework for WSSS in histopathological images that integrates morphology-aware representations from CONCH, multi-scale structural cues from SegFormer, and text-guided semantic alignment to produce prototypes that are simultaneously semantically discriminative and spatially coherent. To effectively leverage these heterogeneous sources, we introduce text-guided prototype initialization that incorporates pathology descriptions to generate more complete and semantically accurate pseudo-masks. A structural distillation mechanism transfers spatial knowledge from SegFormer to preserve fine-grained morphological patterns and local tissue boundaries during prototype learning. Our approach produces high-quality pseudo masks without pixel-level annotations, improves localization completeness, and enhances semantic consistency across tissue types. Experiments on BCSS-WSSS datasets demonstrate that our prototype learning framework outperforms existing WSSS methods while remaining computationally efficient through frozen foundation model backbones and lightweight trainable adapters.",
    "published": "2025-12-11T06:08:29Z",
    "updated": "2025-12-11T06:08:29Z",
    "link": "http://arxiv.org/pdf/2512.10316v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Khang Le",
      "Ha Thach",
      "Anh M. Vu",
      "Trang T. K. Vo",
      "Han H. Huynh",
      "David Yang",
      "Minh H. N. Le",
      "Thanh-Huy Nguyen",
      "Akash Awasthi",
      "Chandra Mohan",
      "Zhu Han",
      "Hien Van Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10314v1",
    "title": "DualProtoSeg: Simple and Efficient Design with Text- and Image-Guided Prototype Learning for Weakly Supervised Histopathology Image Segmentation",
    "summary": "Weakly supervised semantic segmentation (WSSS) in histopathology seeks to reduce annotation cost by learning from image-level labels, yet it remains limited by inter-class homogeneity, intra-class heterogeneity, and the region-shrinkage effect of CAM-based supervision. We propose a simple and effective prototype-driven framework that leverages vision-language alignment to improve region discovery under weak supervision. Our method integrates CoOp-style learnable prompt tuning to generate text-based prototypes and combines them with learnable image prototypes, forming a dual-modal prototype bank that captures both semantic and appearance cues. To address oversmoothing in ViT representations, we incorporate a multi-scale pyramid module that enhances spatial precision and improves localization quality. Experiments on the BCSS-WSSS benchmark show that our approach surpasses existing state-of-the-art methods, and detailed analyses demonstrate the benefits of text description diversity, context length, and the complementary behavior of text and image prototypes. These results highlight the effectiveness of jointly leveraging textual semantics and visual prototype learning for WSSS in digital pathology.",
    "published": "2025-12-11T06:03:28Z",
    "updated": "2025-12-11T06:03:28Z",
    "link": "http://arxiv.org/pdf/2512.10314v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Anh M. Vu",
      "Khang P. Le",
      "Trang T. K. Vo",
      "Ha Thach",
      "Huy Hung Nguyen",
      "David Yang",
      "Han H. Huynh",
      "Quynh Nguyen",
      "Tuan M. Pham",
      "Tuan-Anh Le",
      "Minh H. N. Le",
      "Thanh-Huy Nguyen",
      "Akash Awasthi",
      "Chandra Mohan",
      "Zhu Han",
      "Hien Van Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10310v1",
    "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
    "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
    "published": "2025-12-11T05:57:48Z",
    "updated": "2025-12-11T05:57:48Z",
    "link": "http://arxiv.org/pdf/2512.10310v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Duo Zheng",
      "Shijia Huang",
      "Yanyang Li",
      "Liwei Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2404.17230v6",
    "title": "ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion",
    "summary": "We introduce ObjectAdd, a training-free diffusion modification method to add user-expected objects into user-specified area. The motive of ObjectAdd stems from: first, describing everything in one prompt can be difficult, and second, users often need to add objects into the generated image. To accommodate with real world, our ObjectAdd maintains accurate image consistency after adding objects with technical innovations in: (1) embedding-level concatenation to ensure correct text embedding coalesce; (2) object-driven layout control with latent and attention injection to ensure objects accessing user-specified area; (3) prompted image inpainting in an attention refocusing & object expansion fashion to ensure rest of the image stays the same. With a text-prompted image, our ObjectAdd allows users to specify a box and an object, and achieves: (1) adding object inside the box area; (2) exact content outside the box area; (3) flawless fusion between the two areas",
    "published": "2024-04-26T08:02:07Z",
    "updated": "2025-12-11T05:52:03Z",
    "link": "http://arxiv.org/pdf/2404.17230v6.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Ziyue Zhang",
      "Mingbao Lin",
      "Quanjian Song",
      "Yuxin Zhang",
      "Rongrong Ji"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.09441v2",
    "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling",
    "summary": "High-resolution image synthesis with diffusion models often suffers from energy instabilities and guidance artifacts that degrade visual quality. We analyze the latent energy landscape during sampling and propose adaptive classifier-free guidance (CFG) schedules that maintain stable energy trajectories. Our approach introduces energy-aware scheduling strategies that modulate guidance strength over time, achieving superior stability scores (0.9998) and consistency metrics (0.9873) compared to fixed-guidance approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling yields optimal performance, providing sharper, more faithful images while reducing artifacts. Our energy profiling framework serves as a powerful diagnostic tool for understanding and improving diffusion model behavior.",
    "published": "2025-07-13T01:21:10Z",
    "updated": "2025-12-11T05:45:12Z",
    "link": "http://arxiv.org/pdf/2507.09441v2.pdf",
    "category": [
      "cs.GR",
      "cs.CV"
    ],
    "authors": [
      "Ankit Sanjyal"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.17476v2",
    "title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts",
    "summary": "The detection and grounding of multimedia manipulation has emerged as a critical challenge in combating AI-generated disinformation. While existing methods have made progress in recent years, we identify two fundamental limitations in current approaches: (1) Underestimation of MLLM-driven deception risk: prevailing techniques primarily address rule-based text manipulations, yet fail to account for sophisticated misinformation synthesized by multimodal large language models (MLLMs) that can dynamically generate semantically coherent, contextually plausible yet deceptive narratives conditioned on manipulated images; (2) Unrealistic misalignment artifacts: currently focused scenarios rely on artificially misaligned content that lacks semantic coherence, rendering them easily detectable. To address these gaps holistically, we propose a new adversarial pipeline that leverages MLLMs to generate high-risk disinformation. Our approach begins with constructing the MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered using state-of-the-art editing techniques and then paired with MLLM-generated deceptive texts that maintain semantic consistency with the visual manipulations. Building upon this foundation, we present the Artifact-aware Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations: Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning, to tame MLLMs for the MDSM problem. Comprehensive experiments validate our framework's superior generalization capabilities as a unified architecture for detecting MLLM-powered multimodal deceptions. In cross-domain testing on the MDSM dataset, AMD achieves the best average performance, with 88.18 ACC, 60.25 mAP, and 61.02 mIoU scores.",
    "published": "2025-05-23T04:58:27Z",
    "updated": "2025-12-11T05:36:31Z",
    "link": "http://arxiv.org/pdf/2505.17476v2.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Yuchen Zhang",
      "Yaxiong Wang",
      "Yujiao Wu",
      "Lianwei Wu",
      "Li Zhu",
      "Zhedong Zheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10293v1",
    "title": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings",
    "summary": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.",
    "published": "2025-12-11T05:20:24Z",
    "updated": "2025-12-11T05:20:24Z",
    "link": "http://arxiv.org/pdf/2512.10293v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Karthikeya KV",
      "Narendra Bandaru"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10286v1",
    "title": "ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions",
    "summary": "Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.",
    "published": "2025-12-11T05:05:07Z",
    "updated": "2025-12-11T05:05:07Z",
    "link": "http://arxiv.org/pdf/2512.10286v1.pdf",
    "category": [
      "cs.CV"
    ],
    "authors": [
      "Xiaoxue Wu",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Yu Qiao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10934v1",
    "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
    "summary": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.\n  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
    "published": "2025-12-11T18:57:29Z",
    "updated": "2025-12-11T18:57:29Z",
    "link": "http://arxiv.org/pdf/2512.10934v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Zamirddine Mari",
      "Jérôme Pasquet",
      "Julien Seinturier"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10929v1",
    "title": "Noisy Quantum Learning Theory",
    "summary": "We develop a framework for learning from noisy quantum experiments, focusing on fault-tolerant devices accessing uncharacterized systems through noisy couplings. Our starting point is the complexity class $\\textsf{NBQP}$ (\"noisy BQP\"), modeling noisy fault-tolerant quantum computers that cannot, in general, error-correct the oracle systems they query. Using this class, we show that for natural oracle problems, noise can eliminate exponential quantum learning advantages of ideal noiseless learners while preserving a superpolynomial gap between NISQ and fault-tolerant devices. Beyond oracle separations, we study concrete noisy learning tasks. For purity testing, the exponential two-copy advantage collapses under a single application of local depolarizing noise. Nevertheless, we identify a setting motivated by AdS/CFT in which noise-resilient structure restores a quantum learning advantage in a noisy regime. We then analyze noisy Pauli shadow tomography, deriving lower bounds that characterize how instance size, quantum memory, and noise control sample complexity, and design algorithms with parametrically similar scalings. Together, our results show that the Bell-basis and SWAP-test primitives underlying most exponential quantum learning advantages are fundamentally fragile to noise unless the experimental system has latent noise-robust structure. Thus, realizing meaningful quantum advantages in future experiments will require understanding how noise-robust physical properties interface with available algorithmic techniques.",
    "published": "2025-12-11T18:56:32Z",
    "updated": "2025-12-11T18:56:32Z",
    "link": "http://arxiv.org/pdf/2512.10929v1.pdf",
    "category": [
      "quant-ph",
      "cs.CC",
      "cs.IT",
      "cs.LG"
    ],
    "authors": [
      "Jordan Cotler",
      "Weiyuan Gong",
      "Ishaan Kannan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06982v2",
    "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding",
    "summary": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline in which the LLM serves as a neural architecture design agent, leveraging language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.",
    "published": "2025-12-07T20:25:07Z",
    "updated": "2025-12-11T18:52:44Z",
    "link": "http://arxiv.org/pdf/2512.06982v2.pdf",
    "category": [
      "cs.LG",
      "eess.SY"
    ],
    "authors": [
      "Yu Yu",
      "Qian Xie",
      "Nairen Cao",
      "Li Jin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10925v1",
    "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation",
    "summary": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",
    "published": "2025-12-11T18:52:42Z",
    "updated": "2025-12-11T18:52:42Z",
    "link": "http://arxiv.org/pdf/2512.10925v1.pdf",
    "category": [
      "cs.LG",
      "cs.RO"
    ],
    "authors": [
      "Zamirddine Mari",
      "Mohamad Motasem Nawaf",
      "Pierre Drap"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.06120v2",
    "title": "If generative AI is the answer, what is the question?",
    "summary": "Beginning with text and images, generative AI has expanded to audio, video, computer code, and molecules. Yet, if generative AI is the answer, what is the question? We explore the foundations of generation as a distinct machine learning task with connections to prediction, compression, and decision-making. We survey five major generative model families: autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models. We then introduce a probabilistic framework that emphasizes the distinction between density estimation and generation. We review a game-theoretic framework with a two-player adversary-learner setup to study generation. We discuss post-training modifications that prepare generative models for deployment. We end by highlighting some important topics in socially responsible generation such as privacy, detection of AI-generated content, and copyright and IP. We adopt a task-first framing of generation, focusing on what generation is as a machine learning problem, rather than only on how models implement it.",
    "published": "2025-09-07T16:07:45Z",
    "updated": "2025-12-11T18:45:18Z",
    "link": "http://arxiv.org/pdf/2509.06120v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Ambuj Tewari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10907v1",
    "title": "Hermitian Yang--Mills connections on general vector bundles: geometry and physical Yukawa couplings",
    "summary": "We compute solutions to the Hermitian Yang-Mills equations on holomorphic vector bundles $V$ via an alternating optimisation procedure founded on geometric machine learning. The proposed method is fully general with respect to the rank and structure group of $V$, requiring only the ability to enumerate a basis of global sections for a given bundle. This enables us to compute the physically normalised Yukawa couplings in a broad class of heterotic string compactifications. Using this method, we carry out this computation in full for a heterotic compactification incorporating a gauge bundle with non-Abelian structure group.",
    "published": "2025-12-11T18:38:10Z",
    "updated": "2025-12-11T18:38:10Z",
    "link": "http://arxiv.org/pdf/2512.10907v1.pdf",
    "category": [
      "hep-th",
      "cs.LG"
    ],
    "authors": [
      "Challenger Mishra",
      "Justin Tan"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10906v1",
    "title": "Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets",
    "summary": "In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.",
    "published": "2025-12-11T18:36:15Z",
    "updated": "2025-12-11T18:36:15Z",
    "link": "http://arxiv.org/pdf/2512.10906v1.pdf",
    "category": [
      "math.OC",
      "cs.LG",
      "eess.SY"
    ],
    "authors": [
      "Feras Al Taha",
      "Eilyan Bitar"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10891v1",
    "title": "Iterative Compositional Data Generation for Robot Control",
    "summary": "Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",
    "published": "2025-12-11T18:20:49Z",
    "updated": "2025-12-11T18:20:49Z",
    "link": "http://arxiv.org/pdf/2512.10891v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Anh-Quan Pham",
      "Marcel Hussing",
      "Shubhankar P. Patankar",
      "Dani S. Bassett",
      "Jorge Mendez-Mendez",
      "Eric Eaton"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10886v1",
    "title": "Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields",
    "summary": "Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.\n  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.\n  The model accurately reconstructs loop temperatures (RMSE $<2^\\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.",
    "published": "2025-12-11T18:16:26Z",
    "updated": "2025-12-11T18:16:26Z",
    "link": "http://arxiv.org/pdf/2512.10886v1.pdf",
    "category": [
      "cs.LG",
      "cs.CE"
    ],
    "authors": [
      "Stefan Matthes",
      "Markus Schramm"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10878v1",
    "title": "Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes",
    "summary": "Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.",
    "published": "2025-12-11T18:06:49Z",
    "updated": "2025-12-11T18:06:49Z",
    "link": "http://arxiv.org/pdf/2512.10878v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xuan Zhao",
      "Zhuo Cao",
      "Arya Bangun",
      "Hanno Scharr",
      "Ira Assent"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10877v1",
    "title": "Guided Transfer Learning for Discrete Diffusion Models",
    "summary": "Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.",
    "published": "2025-12-11T18:05:55Z",
    "updated": "2025-12-11T18:05:55Z",
    "link": "http://arxiv.org/pdf/2512.10877v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Julian Kleutgens",
      "Claudio Battiloro",
      "Lingkai Kong",
      "Benjamin Grewe",
      "Francesca Dominici",
      "Mauricio Tec"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10874v1",
    "title": "A Differentiable Digital Twin of Distributed Link Scheduling for Contention-Aware Networking",
    "summary": "Many routing and flow optimization problems in wired networks can be solved efficiently using minimum cost flow formulations. However, this approach does not extend to wireless multi-hop networks, where the assumptions of fixed link capacity and linear cost structure collapse due to contention for shared spectrum resources. The key challenge is that the long-term capacity of a wireless link becomes a non-linear function of its network context, including network topology, link quality, and the traffic assigned to neighboring links. In this work, we pursue a new direction of modeling wireless network under randomized medium access control by developing an analytical network digital twin (NDT) that predicts link duty cycles from network context. We generalize randomized contention as finding a Maximal Independent Set (MIS) on the conflict graph using weighted Luby's algorithm, derive an analytical model of link duty cycles, and introduce an iterative procedure that resolves the circular dependency among duty cycle, link capacity, and contention probability. Our numerical experiments show that the proposed NDT accurately predicts link duty cycles and congestion patterns with up to a 5000x speedup over packet-level simulation, and enables us to optimize link scheduling using gradient descent for reduced congestion and radio footprint.",
    "published": "2025-12-11T18:04:30Z",
    "updated": "2025-12-11T18:04:30Z",
    "link": "http://arxiv.org/pdf/2512.10874v1.pdf",
    "category": [
      "cs.NI",
      "cs.LG",
      "eess.SP",
      "eess.SY"
    ],
    "authors": [
      "Zhongyuan Zhao",
      "Yujun Ming",
      "Kevin Chan",
      "Ananthram Swami",
      "Santiago Segarra"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.08128v2",
    "title": "Beyond Basic A/B testing: Improving Statistical Efficiency for Business Growth",
    "summary": "The standard A/B testing approaches are mostly based on t-test in large scale industry applications. These standard approaches however suffers from low statistical power in business settings, due to nature of small sample-size or non-Gaussian distribution or return-on-investment (ROI) consideration. In this paper, we (i) show the statistical efficiency of using estimating equation and U statistics, which can address these issues separately; and (ii) propose a novel doubly robust generalized U that allows flexible definition of treatment effect, and can handles small samples, distribution robustness, ROI and confounding consideration in one framework. We provide theoretical results on asymptotics and efficiency bounds, together with insights on the efficiency gain from theoretical analysis. We further conduct comprehensive simulation studies, apply the methods to multiple real A/B tests at a large SaaS company, and share results and learnings that are broadly useful.",
    "published": "2025-05-13T00:00:06Z",
    "updated": "2025-12-11T18:04:04Z",
    "link": "http://arxiv.org/pdf/2505.08128v2.pdf",
    "category": [
      "stat.ME",
      "cs.LG",
      "math.ST",
      "stat.CO"
    ],
    "authors": [
      "Changshuai Wei",
      "Phuc Nguyen",
      "Benjamin Zelditch",
      "Joyce Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10873v1",
    "title": "Physics-informed Polynomial Chaos Expansion with Enhanced Constrained Optimization Solver and D-optimal Sampling",
    "summary": "Physics-informed polynomial chaos expansions (PC$^2$) provide an efficient physically constrained surrogate modeling framework by embedding governing equations and other physical constraints into the standard data-driven polynomial chaos expansions (PCE) and solving via the Karush-Kuhn-Tucker (KKT) conditions. This approach improves the physical interpretability of surrogate models while achieving high computational efficiency and accuracy. However, the performance and efficiency of PC$^2$ can still be degraded with high-dimensional parameter spaces, limited data availability, or unrepresentative training data. To address this problem, this study explores two complementary enhancements to the PC$^2$ framework. First, a numerically efficient constrained optimization solver, straightforward updating of Lagrange multipliers (SULM), is adopted as an alternative to the conventional KKT solver. The SULM method significantly reduces computational cost when solving physically constrained problems with high-dimensionality and derivative boundary conditions that require a large number of virtual points. Second, a D-optimal sampling strategy is utilized to select informative virtual points to improve the stability and achieve the balance of accuracy and efficiency of the PC$^2$. The proposed methods are integrated into the PC$^2$ framework and evaluated through numerical examples of representative physical systems governed by ordinary or partial differential equations. The results demonstrate that the enhanced PC$^2$ has better comprehensive capability than standard PC$^2$, and is well-suited for high-dimensional uncertainty quantification tasks.",
    "published": "2025-12-11T18:03:29Z",
    "updated": "2025-12-11T18:03:29Z",
    "link": "http://arxiv.org/pdf/2512.10873v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Qitian Lu",
      "Himanshu Sharma",
      "Michael D. Shields",
      "Lukáš Novák"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10858v1",
    "title": "Scaling Behavior of Discrete Diffusion Language Models",
    "summary": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
    "published": "2025-12-11T17:54:10Z",
    "updated": "2025-12-11T17:54:10Z",
    "link": "http://arxiv.org/pdf/2512.10858v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Dimitri von Rütte",
      "Janis Fluri",
      "Omead Pooladzandi",
      "Bernhard Schölkopf",
      "Thomas Hofmann",
      "Antonio Orvieto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10849v1",
    "title": "Bayesian Symbolic Regression via Posterior Sampling",
    "summary": "Symbolic regression is a powerful tool for discovering governing equations directly from data, but its sensitivity to noise hinders its broader application. This paper introduces a Sequential Monte Carlo (SMC) framework for Bayesian symbolic regression that approximates the posterior distribution over symbolic expressions, enhancing robustness and enabling uncertainty quantification for symbolic regression in the presence of noise. Differing from traditional genetic programming approaches, the SMC-based algorithm combines probabilistic selection, adaptive tempering, and the use of normalized marginal likelihood to efficiently explore the search space of symbolic expressions, yielding parsimonious expressions with improved generalization. When compared to standard genetic programming baselines, the proposed method better deals with challenging, noisy benchmark datasets. The reduced tendency to overfit and enhanced ability to discover accurate and interpretable equations paves the way for more robust symbolic regression in scientific discovery and engineering design applications.",
    "published": "2025-12-11T17:38:20Z",
    "updated": "2025-12-11T17:38:20Z",
    "link": "http://arxiv.org/pdf/2512.10849v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Geoffrey F. Bomarito",
      "Patrick E. Leser"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.15881v3",
    "title": "T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders",
    "summary": "SHallow REcurrent Decoders (SHRED) are effective for system identification and forecasting from sparse sensor measurements. Such models are light-weight and computationally efficient, allowing them to be trained on consumer laptops. SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding respectively. Despite the relatively simple structure of SHRED, they are able to predict chaotic dynamical systems on different physical, spatial, and temporal scales directly from a sparse set of sensor measurements. In this work, we modify SHRED by leveraging transformers (T-SHRED) embedded with symbolic regression for the temporal encoding, circumventing auto-regressive long-term forecasting for physical data. This is achieved through a new sparse identification of nonlinear dynamics (SINDy) attention mechanism into T-SHRED to impose sparsity regularization on the latent space, which also allows for immediate symbolic interpretation. Symbolic regression improves model interpretability by learning and regularizing the dynamics of the latent space during training. We analyze the performance of T-SHRED on three different dynamical systems ranging from low-data to high-data regimes.",
    "published": "2025-06-18T21:14:38Z",
    "updated": "2025-12-11T17:28:30Z",
    "link": "http://arxiv.org/pdf/2506.15881v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Alexey Yermakov",
      "David Zoro",
      "Mars Liyao Gao",
      "J. Nathan Kutz"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10835v1",
    "title": "Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments",
    "summary": "This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.",
    "published": "2025-12-11T17:26:24Z",
    "updated": "2025-12-11T17:26:24Z",
    "link": "http://arxiv.org/pdf/2512.10835v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Atahan Cilan",
      "Atay Özgövde"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10825v1",
    "title": "An Elementary Proof of the Near Optimality of LogSumExp Smoothing",
    "summary": "We consider the design of smoothings of the (coordinate-wise) max function in $\\mathbb{R}^d$ in the infinity norm. The LogSumExp function $f(x)=\\ln(\\sum^d_i\\exp(x_i))$ provides a classical smoothing, differing from the max function in value by at most $\\ln(d)$. We provide an elementary construction of a lower bound, establishing that every overestimating smoothing of the max function must differ by at least $\\sim 0.8145\\ln(d)$. Hence, LogSumExp is optimal up to constant factors. However, in small dimensions, we provide stronger, exactly optimal smoothings attaining our lower bound, showing that the entropy-based LogSumExp approach to smoothing is not exactly optimal.",
    "published": "2025-12-11T17:17:48Z",
    "updated": "2025-12-11T17:17:48Z",
    "link": "http://arxiv.org/pdf/2512.10825v1.pdf",
    "category": [
      "math.ST",
      "cs.LG",
      "math.OC"
    ],
    "authors": [
      "Thabo Samakhoana",
      "Benjamin Grimmer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10819v1",
    "title": "Deep sets and event-level maximum-likelihood estimation for fast pile-up jet rejection in ATLAS",
    "summary": "Multiple proton-proton collisions (pile-up) occur at every bunch crossing at the LHC, with the mean number of interactions expected to reach 80 during Run 3 and up to 200 at the High-Luminosity LHC. As a direct consequence, events with multijet signatures will occur at increasingly high rates. To cope with the increased luminosity, being able to efficiently group jets according to their origin along the beamline is crucial, particularly at the trigger level. In this work, a novel uncertainty-aware jet regression model based on a Deep Sets architecture is introduced, DIPz, to regress on a jet origin position along the beamline. The inputs to the DIPz algorithm are the charged particle tracks associated to each jet. An event-level discriminant, the Maximum Log Product of Likelihoods (MLPL), is constructed by combining the DIPz per-jet predictions. MLPL is cut-optimized to select events compatible with targeted multi-jet signature selection. This combined approach provides a robust and computationally efficient method for pile-up rejection in multi-jet final states, applicable to real-time event selections at the ATLAS High Level Trigger.",
    "published": "2025-12-11T17:09:45Z",
    "updated": "2025-12-11T17:09:45Z",
    "link": "http://arxiv.org/pdf/2512.10819v1.pdf",
    "category": [
      "hep-ex",
      "cs.LG"
    ],
    "authors": [
      "Mohammed Aboelela"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.10439v2",
    "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration",
    "summary": "Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise in reducing this additional communication overhead. Local SGD consists of three parts: a local optimization process, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer optimizer in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to settings where we use momentum in the outer optimizer, and we show a similar role for the momentum-adjusted outer learning rate. We also study acceleration in the outer optimizer and show that it improves the convergence rate as a function of the number of communication rounds, improving upon the convergence rate of prior algorithms that apply acceleration locally. Finally, we also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory.",
    "published": "2025-09-12T17:47:58Z",
    "updated": "2025-12-11T17:09:43Z",
    "link": "http://arxiv.org/pdf/2509.10439v2.pdf",
    "category": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "authors": [
      "Ahmed Khaled",
      "Satyen Kale",
      "Arthur Douillard",
      "Chi Jin",
      "Rob Fergus",
      "Manzil Zaheer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.21429v2",
    "title": "Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort",
    "summary": "This study investigates the efficacy of using multimodal machine learning techniques to detect deception in dyadic interactions, focusing on the integration of data from both the deceiver and the deceived. We compare early and late fusion approaches, utilizing audio and video data - specifically, Action Units and gaze information - across all possible combinations of modalities and participants. Our dataset, newly collected from Swedish native speakers engaged in truth or lie scenarios on emotionally relevant topics, serves as the basis for our analysis. The results demonstrate that incorporating both speech and facial information yields superior performance compared to single-modality approaches. Moreover, including data from both participants significantly enhances deception detection accuracy, with the best performance (71%) achieved using a late fusion strategy applied to both modalities and participants. These findings align with psychological theories suggesting differential control of facial and vocal expressions during initial interactions. As the first study of its kind on a Scandinavian cohort, this research lays the groundwork for future investigations into dyadic interactions, particularly within psychotherapy settings.",
    "published": "2025-06-26T16:11:42Z",
    "updated": "2025-12-11T17:00:44Z",
    "link": "http://arxiv.org/pdf/2506.21429v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Thomas Jack Samuels",
      "Franco Rugolon",
      "Stephan Hau",
      "Lennart Högman"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10813v1",
    "title": "Quantum Approaches to Urban Logistics: From Core QAOA to Clustered Scalability",
    "summary": "The Traveling Salesman Problem (TSP) is a fundamental challenge in combinatorial optimization, widely applied in logistics and transportation. As the size of TSP instances grows, traditional algorithms often struggle to produce high-quality solutions within reasonable timeframes. This study investigates the potential of the Quantum Approximate Optimization Algorithm (QAOA), a hybrid quantum-classical method, to solve TSP under realistic constraints. We adopt a QUBO-based formulation of TSP that integrates real-world logistical constraints reflecting operational conditions, such as vehicle capacity, road accessibility, and time windows, while ensuring compatibility with the limitations of current quantum hardware. Our experiments are conducted in a simulated environment using high-performance computing (HPC) resources to assess QAOA's performance across different problem sizes and quantum circuit depths. In order to improve scalability, we propose clustering QAOA (Cl-QAOA), a hybrid approach combining classical machine learning with QAOA. This method decomposes large TSP instances into smaller sub-problems, making quantum optimization feasible even on devices with a limited number of qubits. The results offer a comprehensive evaluation of QAOA's strengths and limitations in solving constrained TSP scenarios. This study advances quantum optimization and lays groundwork for future large-scale applications.",
    "published": "2025-12-11T17:00:24Z",
    "updated": "2025-12-11T17:00:24Z",
    "link": "http://arxiv.org/pdf/2512.10813v1.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "F. Picariello",
      "G. Turati",
      "R. Antonelli",
      "I. Bailo",
      "S. Bonura",
      "G. Ciarfaglia",
      "S. Cipolla",
      "P. Cremonesi",
      "M. Ferrari Dacrema",
      "M. Gabusi",
      "I. Gentile",
      "V. Morreale",
      "A. Noto"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.12074v2",
    "title": "Extrapolating Jet Radiation with Autoregressive Transformers",
    "summary": "Generative networks are an exciting tool for fast LHC event fixed number of particles. Autoregressive transformers allow us to generate events containing variable numbers of particles, very much in line with the physics of QCD jet radiation, and offer the possibility to generalize to higher multiplicities. We show how transformers can learn a factorized likelihood for jet radiation and extrapolate in terms of the number of generated jets. For this extrapolation, bootstrapping training data and training with modifications of the likelihood loss can be used.",
    "published": "2024-12-16T18:46:43Z",
    "updated": "2025-12-11T16:24:23Z",
    "link": "http://arxiv.org/pdf/2412.12074v2.pdf",
    "category": [
      "hep-ph",
      "cs.LG"
    ],
    "authors": [
      "Anja Butter",
      "François Charton",
      "Javier Mariño Villadamigo",
      "Ayodele Ore",
      "Tilman Plehn",
      "Jonas Spinner"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10770v1",
    "title": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers",
    "summary": "Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \\SMILES\\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.",
    "published": "2025-12-11T16:08:32Z",
    "updated": "2025-12-11T16:08:32Z",
    "link": "http://arxiv.org/pdf/2512.10770v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Youjun Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.03405v2",
    "title": "Deep Operator BSDE: a Numerical Scheme to Approximate Solution Operators",
    "summary": "Motivated by dynamic risk measures and conditional $g$-expectations, in this work we propose a numerical method to approximate the solution operator given by a Backward Stochastic Differential Equation (BSDE). The main ingredients for this are the Wiener chaos decomposition and the classical Euler scheme for BSDEs. We show convergence of this scheme under very mild assumptions, and provide a rate of convergence in more restrictive cases. We then implement it using neural networks, and we present several numerical examples where we can check the accuracy of the method.",
    "published": "2024-12-04T15:36:20Z",
    "updated": "2025-12-11T15:57:23Z",
    "link": "http://arxiv.org/pdf/2412.03405v2.pdf",
    "category": [
      "math.NA",
      "cs.LG",
      "math.PR"
    ],
    "authors": [
      "Pere Díaz Lozano",
      "Giulia Di Nunno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.21615v2",
    "title": "Adapting to Change: A Comparison of Continual and Transfer Learning for Modeling Building Thermal Dynamics under Concept Drifts",
    "summary": "Transfer Learning (TL) is currently the most effective approach for modeling building thermal dynamics when only limited data are available. TL uses a pretrained model that is fine-tuned to a specific target building. However, it remains unclear how to proceed after initial fine-tuning, as more operational measurement data are collected over time. This challenge becomes even more complex when the dynamics of the building change, for example, after a retrofit or a change in occupancy. In Machine Learning literature, Continual Learning (CL) methods are used to update models of changing systems. TL approaches can also address this challenge by reusing the pretrained model at each update step and fine-tuning it with new measurement data. A comprehensive study on how to incorporate new measurement data over time to improve prediction accuracy and address the challenges of concept drifts (changes in dynamics) for building thermal dynamics is still missing.\n  Therefore, this study compares several CL and TL strategies, as well as a model trained from scratch, for thermal dynamics modeling during building operation. The methods are evaluated using 5--7 years of simulated data representative of single-family houses in Central Europe, including scenarios with concept drifts from retrofits and changes in occupancy. We propose a CL strategy (Seasonal Memory Learning) that provides greater accuracy improvements than existing CL and TL methods, while maintaining low computational effort. SML outperformed the benchmark of initial fine-tuning by 28.1\\% without concept drifts and 34.9\\% with concept drifts.",
    "published": "2025-08-29T13:29:54Z",
    "updated": "2025-12-11T15:37:19Z",
    "link": "http://arxiv.org/pdf/2508.21615v2.pdf",
    "category": [
      "eess.SY",
      "cs.LG"
    ],
    "authors": [
      "Fabian Raisch",
      "Max Langtry",
      "Felix Koch",
      "Ruchi Choudhary",
      "Christoph Goebel",
      "Benjamin Tischler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.14573v2",
    "title": "State-Space Models for Tabular Prior-Data Fitted Networks",
    "summary": "Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance. However, Transformers suffer from quadratic complexity with respect to sequence length, motivating the exploration of more efficient sequence models. In this work, we investigate the potential of using Hydra, a bidirectional linear-time structured state space model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies in SSM's inherent sensitivity to the order of input tokens - an undesirable property for tabular datasets where the row order is semantically meaningless. We investigate to what extent a bidirectional approach can preserve efficiency and enable symmetric context aggregation. Our experiments show that this approach reduces the order-dependence, achieving predictive performance competitive to the original TabPFN model.",
    "published": "2025-10-16T11:31:51Z",
    "updated": "2025-12-11T15:36:53Z",
    "link": "http://arxiv.org/pdf/2510.14573v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Felix Koch",
      "Marcel Wever",
      "Fabian Raisch",
      "Benjamin Tischler"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10745v1",
    "title": "PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography",
    "summary": "Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring.",
    "published": "2025-12-11T15:32:50Z",
    "updated": "2025-12-11T15:32:50Z",
    "link": "http://arxiv.org/pdf/2512.10745v1.pdf",
    "category": [
      "physics.med-ph",
      "cs.LG"
    ],
    "authors": [
      "Yaowen Zhang",
      "Libera Fresiello",
      "Peter H. Veltink",
      "Dirk W. Donker",
      "Ying Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10723v1",
    "title": "Generalized Spherical Neural Operators: Green's Function Formulation",
    "summary": "Neural operators offer powerful approaches for solving parametric partial differential equations, but extending them to spherical domains remains challenging due to the need to preserve intrinsic geometry while avoiding distortions that break rotational consistency. Existing spherical operators rely on rotational equivariance but often lack the flexibility for real-world complexity. We propose a general operator-design framework based on the designable spherical Green's function and its harmonic expansion, establishing a solid operator-theoretic foundation for spherical learning. Based on this, we propose an absolute and relative position-dependent Green's function that enables flexible balance of equivariance and invariance for real-world modeling. The resulting operator, Green's-function Spherical Neural Operator (GSNO) with a novel spectral learning method, can adapt to anisotropic, constraint-rich systems while retaining spectral efficiency. To exploit GSNO, we develop GSHNet, a hierarchical architecture that combines multi-scale spectral modeling with spherical up-down sampling, enhancing global feature representation. Evaluations on diffusion MRI, shallow water dynamics, and global weather forecasting, GSNO and GSHNet consistently outperform state-of-the-art methods. Our results position GSNO as a principled and general framework for spherical operator learning, bridging rigorous theory with real-world complexity.",
    "published": "2025-12-11T15:05:33Z",
    "updated": "2025-12-11T15:05:33Z",
    "link": "http://arxiv.org/pdf/2512.10723v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hao Tang",
      "Hao Chen",
      "Chao Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10720v1",
    "title": "Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality",
    "summary": "Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.",
    "published": "2025-12-11T14:59:14Z",
    "updated": "2025-12-11T14:59:14Z",
    "link": "http://arxiv.org/pdf/2512.10720v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Lingjing Kong",
      "Shaoan Xie",
      "Guangyi Chen",
      "Yuewen Sun",
      "Xiangchen Song",
      "Eric P. Xing",
      "Kun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.06158v3",
    "title": "ENMA: Tokenwise Autoregression for Generative Neural PDE Operators",
    "summary": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.",
    "published": "2025-06-06T15:25:14Z",
    "updated": "2025-12-11T14:55:09Z",
    "link": "http://arxiv.org/pdf/2506.06158v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Armand Kassaï Koupaï",
      "Lise Le Boudec",
      "Louis Serrano",
      "Patrick Gallinari"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10701v1",
    "title": "HybridVFL: Disentangled Feature Learning for Edge-Enabled Vertical Federated Multimodal Classification",
    "summary": "Vertical Federated Learning (VFL) offers a privacy-preserving paradigm for Edge AI scenarios like mobile health diagnostics, where sensitive multimodal data reside on distributed, resource-constrained devices. Yet, standard VFL systems often suffer performance limitations due to simplistic feature fusion. This paper introduces HybridVFL, a novel framework designed to overcome this bottleneck by employing client-side feature disentanglement paired with a server-side cross-modal transformer for context-aware fusion. Through systematic evaluation on the multimodal HAM10000 skin lesion dataset, we demonstrate that HybridVFL significantly outperforms standard federated baselines, validating the criticality of advanced fusion mechanisms in robust, privacy-preserving systems.",
    "published": "2025-12-11T14:41:19Z",
    "updated": "2025-12-11T14:41:19Z",
    "link": "http://arxiv.org/pdf/2512.10701v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Mostafa Anoosha",
      "Zeinab Dehghani",
      "Kuniko Paxton",
      "Koorosh Aslansefat",
      "Dhavalkumar Thakker"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10669v1",
    "title": "Learning by Analogy: A Causal Framework for Composition Generalization",
    "summary": "Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.\n  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.",
    "published": "2025-12-11T14:16:14Z",
    "updated": "2025-12-11T14:16:14Z",
    "link": "http://arxiv.org/pdf/2512.10669v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Lingjing Kong",
      "Shaoan Xie",
      "Yang Jiao",
      "Yetian Chen",
      "Yanhui Guo",
      "Simone Shao",
      "Yan Gao",
      "Guangyi Chen",
      "Kun Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10659v1",
    "title": "DCFO Additional Material",
    "summary": "Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.",
    "published": "2025-12-11T14:04:52Z",
    "updated": "2025-12-11T14:04:52Z",
    "link": "http://arxiv.org/pdf/2512.10659v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Tommaso Amico",
      "Pernille Matthews",
      "Lena Krieger",
      "Arthur Zimek",
      "Ira Assent"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10656v1",
    "title": "Token Sample Complexity of Attention",
    "summary": "As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.",
    "published": "2025-12-11T14:02:34Z",
    "updated": "2025-12-11T14:02:34Z",
    "link": "http://arxiv.org/pdf/2512.10656v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Léa Bohbot",
      "Cyril Letrouit",
      "Gabriel Peyré",
      "François-Xavier Vialard"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10653v1",
    "title": "Virtual camera detection: Catching video injection attacks in remote biometric systems",
    "summary": "Face anti-spoofing (FAS) is a vital component of remote biometric authentication systems based on facial recognition, increasingly used across web-based applications. Among emerging threats, video injection attacks -- facilitated by technologies such as deepfakes and virtual camera software -- pose significant challenges to system integrity. While virtual camera detection (VCD) has shown potential as a countermeasure, existing literature offers limited insight into its practical implementation and evaluation. This study introduces a machine learning-based approach to VCD, with a focus on its design and validation. The model is trained on metadata collected during sessions with authentic users. Empirical results demonstrate its effectiveness in identifying video injection attempts and reducing the risk of malicious users bypassing FAS systems.",
    "published": "2025-12-11T14:01:06Z",
    "updated": "2025-12-11T14:01:06Z",
    "link": "http://arxiv.org/pdf/2512.10653v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Daniyar Kurmankhojayev",
      "Andrei Shadrikov",
      "Dmitrii Gordin",
      "Mikhail Shkorin",
      "Danijar Gabdullin",
      "Aigerim Kambetbayeva",
      "Kanat Kuatov"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10637v1",
    "title": "Adaptive Intrusion Detection System Leveraging Dynamic Neural Models with Adversarial Learning for 5G/6G Networks",
    "summary": "Intrusion Detection Systems (IDS) are critical components in safeguarding 5G/6G networks from both internal and external cyber threats. While traditional IDS approaches rely heavily on signature-based methods, they struggle to detect novel and evolving attacks. This paper presents an advanced IDS framework that leverages adversarial training and dynamic neural networks in 5G/6G networks to enhance network security by providing robust, real-time threat detection and response capabilities. Unlike conventional models, which require costly retraining to update knowledge, the proposed framework integrates incremental learning algorithms, reducing the need for frequent retraining. Adversarial training is used to fortify the IDS against poisoned data. By using fewer features and incorporating statistical properties, the system can efficiently detect potential threats. Extensive evaluations using the NSL- KDD dataset demonstrate that the proposed approach provides better accuracy of 82.33% for multiclass classification of various network attacks while resisting dataset poisoning. This research highlights the potential of adversarial-trained, dynamic neural networks for building resilient IDS solutions.",
    "published": "2025-12-11T13:40:37Z",
    "updated": "2025-12-11T13:40:37Z",
    "link": "http://arxiv.org/pdf/2512.10637v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      " Neha",
      "Tarunpreet Bhatia"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10633v1",
    "title": "Supporting Migration Policies with Forecasts: Illegal Border Crossings in Europe through a Mixed Approach",
    "summary": "This paper presents a mixed-methodology to forecast illegal border crossings in Europe across five key migratory routes, with a one-year time horizon. The methodology integrates machine learning techniques with qualitative insights from migration experts. This approach aims at improving the predictive capacity of data-driven models through the inclusion of a human-assessed covariate, an innovation that addresses challenges posed by sudden shifts in migration patterns and limitations in traditional datasets. The proposed methodology responds directly to the forecasting needs outlined in the EU Pact on Migration and Asylum, supporting the Asylum and Migration Management Regulation (AMMR). It is designed to provide policy-relevant forecasts that inform strategic decisions, early warning systems, and solidarity mechanisms among EU Member States. By joining data-driven modeling with expert judgment, this work aligns with existing academic recommendations and introduces a novel operational tool tailored for EU migration governance. The methodology is tested and validated with known data to demonstrate its applicability and reliability in migration-related policy context.",
    "published": "2025-12-11T13:33:25Z",
    "updated": "2025-12-11T13:33:25Z",
    "link": "http://arxiv.org/pdf/2512.10633v1.pdf",
    "category": [
      "cs.LG",
      "cs.SI",
      "stat.AP"
    ],
    "authors": [
      "C. Bosco",
      "U. Minora",
      "D. de Rigo",
      "J. Pingsdorf",
      "R. Cortinovis"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.11840v2",
    "title": "GT-SNT: A Linear-Time Transformer for Large-Scale Graphs via Spiking Node Tokenization",
    "summary": "Graph Transformers (GTs), which integrate message passing and self-attention mechanisms simultaneously, have achieved promising empirical results in graph prediction tasks. However, the design of scalable and topology-aware node tokenization has lagged behind other modalities. This gap becomes critical as the quadratic complexity of full attention renders them impractical on large-scale graphs. Recently, Spiking Neural Networks (SNNs), as brain-inspired models, provided an energy-saving scheme to convert input intensity into discrete spike-based representations through event-driven spiking neurons. Inspired by these characteristics, we propose a linear-time Graph Transformer with Spiking Node Tokenization (GT-SNT) for node classification. By integrating multi-step feature propagation with SNNs, spiking node tokenization generates compact, locality-aware spike count embeddings as node tokens to avoid predefined codebooks and their utilization issues. The codebook guided self-attention leverages these tokens to perform node-to-token attention for linear-time global context aggregation. In experiments, we compare GT-SNT with other state-of-the-art baselines on node classification datasets ranging from small to large. Experimental results show that GT-SNT achieves comparable performances on most datasets and reaches up to 130x faster inference speed compared to other GTs.",
    "published": "2025-04-16T07:57:42Z",
    "updated": "2025-12-11T13:28:05Z",
    "link": "http://arxiv.org/pdf/2504.11840v2.pdf",
    "category": [
      "cs.NE",
      "cs.LG"
    ],
    "authors": [
      "Huizhe Zhang",
      "Jintang Li",
      "Yuchang Zhu",
      "Huazhen Zhong",
      "Liang Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.09500v2",
    "title": "Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions",
    "summary": "We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + σZ$, where $σ\\in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(σ^4)$ and $O(σ^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Ampère equation with higher-order accuracy, and can be implemented efficiently via score matching.\n  Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \\[ \\mathbf{T}^*(y) = y + σ^2 \\nabla \\log q(y), \\] with denoisers exhibiting less aggressive distributional shrinkage, \\[ \\mathbf{T}_1(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y), \\] \\[ \\mathbf{T}_2(y) = y + \\frac{σ^2}{2} \\nabla \\log q(y) - \\frac{σ^4}{8} \\nabla \\left( \\frac{1}{2} \\| \\nabla \\log q(y) \\|^2 + \\nabla \\cdot \\nabla \\log q(y) \\right) . \\]",
    "published": "2025-11-12T17:20:42Z",
    "updated": "2025-12-11T13:24:15Z",
    "link": "http://arxiv.org/pdf/2511.09500v2.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Tengyuan Liang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10602v1",
    "title": "Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification",
    "summary": "Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog \"Bayesian Machines\" operating at inherently low precision.",
    "published": "2025-12-11T12:51:42Z",
    "updated": "2025-12-11T12:51:42Z",
    "link": "http://arxiv.org/pdf/2512.10602v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hendrik Borras",
      "Yong Wu",
      "Bernhard Klein",
      "Holger Fröning"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10601v1",
    "title": "Multi-Objective Reward and Preference Optimization: Theory and Algorithms",
    "summary": "This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.",
    "published": "2025-12-11T12:51:21Z",
    "updated": "2025-12-11T12:51:21Z",
    "link": "http://arxiv.org/pdf/2512.10601v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Akhil Agnihotri"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10600v1",
    "title": "Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs",
    "summary": "Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor,\" which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.",
    "published": "2025-12-11T12:50:39Z",
    "updated": "2025-12-11T12:50:39Z",
    "link": "http://arxiv.org/pdf/2512.10600v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Han Yang",
      "Shaofeng Li",
      "Tian Dong",
      "Xiangyu Xu",
      "Guangchi Liu",
      "Zhen Ling"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2411.14013v4",
    "title": "Lightweight Model Attribution and Detection of Synthetic Speech via Audio Residual Fingerprints",
    "summary": "As speech generation technologies advance, so do risks of impersonation, misinformation, and spoofing. We present a lightweight, training-free approach for detecting synthetic speech and attributing it to its source model. Our method addresses three tasks: (1) single-model attribution in an open-world setting, (2) multi-model attribution in a closed-world setting, and (3) real vs. synthetic speech classification. The core idea is simple: we compute standardized average residuals--the difference between an audio signal and its filtered version--to extract model-agnostic fingerprints that capture synthesis artifacts. Experiments across multiple synthesis systems and languages show AUROC scores above 99%, with strong reliability even when only a subset of model outputs is available. The method maintains high performance under common audio distortions, including echo and moderate background noise, while data augmentation can improve results in more challenging conditions. In addition, out-of-domain detection is performed using Mahalanobis distances to in-domain residual fingerprints, achieving an F1 score of 0.91 on unseen models, reinforcing the method's efficiency, generalizability, and suitability for digital forensics and security applications.",
    "published": "2024-11-21T10:55:49Z",
    "updated": "2025-12-11T12:41:32Z",
    "link": "http://arxiv.org/pdf/2411.14013v4.pdf",
    "category": [
      "eess.AS",
      "cs.CR",
      "cs.LG"
    ],
    "authors": [
      "Matías Pizarro",
      "Mike Laszkiewicz",
      "Dorothea Kolossa",
      "Asja Fischer"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10589v1",
    "title": "THeGAU: Type-Aware Heterogeneous Graph Autoencoder and Augmentation",
    "summary": "Heterogeneous Graph Neural Networks (HGNNs) are effective for modeling Heterogeneous Information Networks (HINs), which encode complex multi-typed entities and relations. However, HGNNs often suffer from type information loss and structural noise, limiting their representational fidelity and generalization. We propose THeGAU, a model-agnostic framework that combines a type-aware graph autoencoder with guided graph augmentation to improve node classification. THeGAU reconstructs schema-valid edges as an auxiliary task to preserve node-type semantics and introduces a decoder-driven augmentation mechanism to selectively refine noisy structures. This joint design enhances robustness, accuracy, and efficiency while significantly reducing computational overhead. Extensive experiments on three benchmark HIN datasets (IMDB, ACM, and DBLP) demonstrate that THeGAU consistently outperforms existing HGNN methods, achieving state-of-the-art performance across multiple backbones.",
    "published": "2025-12-11T12:30:42Z",
    "updated": "2025-12-11T12:30:42Z",
    "link": "http://arxiv.org/pdf/2512.10589v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Ming-Yi Hong",
      "Miao-Chen Chiang",
      "Youchen Teng",
      "Yu-Hsiang Wang",
      "Chih-Yu Wang",
      "Che Lin"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2505.21356v4",
    "title": "Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations",
    "summary": "Perceptual voice quality assessment plays a vital role in diagnosing and monitoring voice disorders. Traditional methods, such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and the Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS) scales, rely on expert raters and are prone to inter-rater variability, emphasizing the need for objective solutions. This study introduces the Voice Quality Assessment Network (VOQANet), a deep learning framework that employs an attention mechanism and Speech Foundation Model (SFM) embeddings to extract high-level features. To further enhance performance, we propose VOQANet+, which integrates self-supervised SFM embeddings with low-level acoustic descriptors-namely jitter, shimmer, and harmonics-to-noise ratio (HNR). Unlike previous approaches that focus solely on vowel-based phonation (PVQD-A), our models are evaluated on both vowel-level and sentence-level speech (PVQD-S) to assess generalizability. Experimental results demonstrate that sentence-based inputs yield higher accuracy, particularly at the patient level. Overall, VOQANet consistently outperforms baseline models in terms of root mean squared error (RMSE) and Pearson correlation coefficient across CAPE-V and GRBAS dimensions, with VOQANet+ achieving even greater performance gains. Additionally, VOQANet+ maintains consistent performance under noisy conditions, suggesting enhanced robustness for real-world and telehealth applications. This work highlights the value of combining SFM embeddings with low-level features for accurate and robust pathological voice assessment.",
    "published": "2025-05-27T15:48:17Z",
    "updated": "2025-12-11T12:26:21Z",
    "link": "http://arxiv.org/pdf/2505.21356v4.pdf",
    "category": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "authors": [
      "Whenty Ariyanti",
      "Kuan-Yu Chen",
      "Sabato Marco Siniscalchi",
      "Hsin-Min Wang",
      "Yu Tsao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10582v1",
    "title": "Topology-Guided Quantum GANs for Constrained Graph Generation",
    "summary": "Quantum computing (QC) promises theoretical advantages, benefiting computational problems that would not be efficiently classically simulatable. However, much of this theoretical speedup depends on the quantum circuit design solving the problem. We argue that QC literature has yet to explore more domain specific ansatz-topologies, instead of relying on generic, one-size-fits-all architectures. In this work, we show that incorporating task-specific inductive biases -- specifically geometric priors -- into quantum circuit design can enhance the performance of hybrid Quantum Generative Adversarial Networks (QuGANs) on the task of generating geometrically constrained K4 graphs. We evaluate a portfolio of entanglement topologies and loss-function designs to assess their impact on both statistical fidelity and compliance with geometric constraints, including the Triangle and Ptolemaic inequalities. Our results show that aligning circuit topology with the underlying problem structure yields substantial benefits: the Triangle-topology QuGAN achieves the highest geometric validity among quantum models and matches the performance of classical Generative Adversarial Networks (GAN). Additionally, we showcase how specific architectural choices, such as entangling gate types, variance regularization and output-scaling govern the trade-off between geometric consistency and distributional accuracy, thus emphasizing the value of structured, task-aware quantum ansatz-topologies.",
    "published": "2025-12-11T12:22:18Z",
    "updated": "2025-12-11T12:22:18Z",
    "link": "http://arxiv.org/pdf/2512.10582v1.pdf",
    "category": [
      "quant-ph",
      "cs.LG"
    ],
    "authors": [
      "Tobias Rohe",
      "Markus Baumann",
      "Michael Poppel",
      "Gerhard Stenzel",
      "Maximilian Zorn",
      "Claudia Linnhoff-Popien"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10573v1",
    "title": "Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning",
    "summary": "The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a \"Minimal-Sufficient-Clean\" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.",
    "published": "2025-12-11T12:01:20Z",
    "updated": "2025-12-11T12:01:20Z",
    "link": "http://arxiv.org/pdf/2512.10573v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yi Huang",
      "Qingyun Sun",
      "Yisen Gao",
      "Haonan Yuan",
      "Xingcheng Fu",
      "Jianxin Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10570v1",
    "title": "Flexible Deep Neural Networks for Partially Linear Survival Data",
    "summary": "We propose a flexible deep neural network (DNN) framework for modeling survival data within a partially linear regression structure. The approach preserves interpretability through a parametric linear component for covariates of primary interest, while a nonparametric DNN component captures complex time-covariate interactions among nuisance variables. We refer to the method as FLEXI-Haz, a flexible hazard model with a partially linear structure. In contrast to existing DNN approaches for partially linear Cox models, FLEXI-Haz does not rely on the proportional hazards assumption. We establish theoretical guarantees: the neural network component attains minimax-optimal convergence rates based on composite Holder classes, and the linear estimator is root-n consistent, asymptotically normal, and semiparametrically efficient. Extensive simulations and real-data analyses demonstrate that FLEXI-Haz provides accurate estimation of the linear effect, offering a principled and interpretable alternative to modern methods based on proportional hazards. Code for implementing FLEXI-Haz, as well as scripts for reproducing data analyses and simulations, is available at: https://github.com/AsafBanana/FLEXI-Haz",
    "published": "2025-12-11T11:58:42Z",
    "updated": "2025-12-11T11:58:42Z",
    "link": "http://arxiv.org/pdf/2512.10570v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Asaf Ben Arie",
      "Malka Gorfine"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10547v1",
    "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
    "summary": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
    "published": "2025-12-11T11:23:50Z",
    "updated": "2025-12-11T11:23:50Z",
    "link": "http://arxiv.org/pdf/2512.10547v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Qingsen Ma",
      "Dianyun Wang",
      "Jiaming Lyu",
      "Yaoye Wang",
      "Lechen Ning",
      "Sujie Zhu",
      "Zhenbo Xu",
      "Liuyu Xiang",
      "Huining Li",
      "Huijia Wu",
      "Zhaofeng He"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2312.15187v3",
    "title": "IRG: Modular Synthetic Relational Database Generation with Complex Relational Schemas",
    "summary": "Relational databases (RDBs) are widely used by corporations and governments to store multiple related tables. Their relational schemas pose unique challenges to synthetic data generation for privacy-preserving data sharing, e.g., for collaborative analytical and data mining tasks, as well as software testing at various scales. Relational schemas typically include a set of primary and foreign key constraints to specify the intra-and inter-table entity relations, which also imply crucial intra-and inter-table data correlations in the RDBs. Existing synthetic RDB generation approaches often focus on the relatively simple and basic parent-child relations, failing to address the ubiquitous real-world complexities in relational schemas in key constraints like composite keys, intra-table correlations like sequential correlation, and inter-table data correlations like indirectly connected tables. In this paper, we introduce incremental relational generator (IRG), a modular framework designed to handle these real-world challenges. In IRG, each table is generated by learning context from a depth-first traversal of relational connections to capture indirect inter-table relationships and constructs different parts of a table through several classical generative and predictive modules to preserve complex key constraints and data correlations. Compared to 3 prior art algorithms across 10 real-world RDB datasets, IRG successfully handles the relational schemas and captures critical data relationships for all datasets while prior works are incapable of. The generated synthetic data also demonstrates better fidelity and utility than prior works, implying its higher potential as a replacement for the basis of analytical tasks and data mining applications. Code is available at: https://github.com/li-jiayu-ljy/irg.",
    "published": "2023-12-23T07:47:58Z",
    "updated": "2025-12-11T11:16:48Z",
    "link": "http://arxiv.org/pdf/2312.15187v3.pdf",
    "category": [
      "cs.DB",
      "cs.LG"
    ],
    "authors": [
      "Jiayu Li",
      "Zilong Zhao",
      "Milad Abdollahzadeh",
      "Biplab Sikdar",
      "Y. C. Tay"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10522v1",
    "title": "Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees",
    "summary": "Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA",
    "published": "2025-12-11T10:47:38Z",
    "updated": "2025-12-11T10:47:38Z",
    "link": "http://arxiv.org/pdf/2512.10522v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Zahra Rahiminasab",
      "Michael Yuhas",
      "Arvind Easwaran"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2501.02298v5",
    "title": "Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance",
    "summary": "Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the W2-distance rely on stringent assumptions about the data distribution. In this work, we present a novel framework for analyzing W2-convergence in SGMs, significantly relaxing traditional assumptions such as log-concavity and score regularity. Leveraging the regularization properties of the Ornstein--Uhlenbeck (OU) process, we show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton--Jacobi--Bellman equation governing the log-density of the forward process. Moreover, we establish that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity. Our approach circumvents the need for stringent regularity conditions on the score function and its estimators, relying instead on milder, more practical assumptions. We demonstrate the wide applicability of this framework through explicit computations on Gaussian mixture models, illustrating its versatility and potential for broader classes of data distributions.",
    "published": "2025-01-04T14:33:27Z",
    "updated": "2025-12-11T10:42:29Z",
    "link": "http://arxiv.org/pdf/2501.02298v5.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Marta Gentiloni-Silveri",
      "Antonio Ocello"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10506v1",
    "title": "Hyperspectral Image Data Reduction for Endmember Extraction",
    "summary": "Endmember extraction from hyperspectral images aims to identify the spectral signatures of materials present in a scene. Recent studies have shown that self-dictionary methods can achieve high extraction accuracy; however, their high computational cost limits their applicability to large-scale hyperspectral images. Although several approaches have been proposed to mitigate this issue, it remains a major challenge. Motivated by this situation, this paper pursues a data reduction approach. Assuming that the hyperspectral image follows the linear mixing model with the pure-pixel assumption, we develop a data reduction technique that removes pixels that do not contain endmembers. We analyze the theoretical properties of this reduction step and show that it preserves pixels that lie close to the endmembers. Building on this result, we propose a data-reduced self-dictionary method that integrates the data reduction with a self-dictionary method based on a linear programming formulation. Numerical experiments demonstrate that the proposed method can substantially reduce the computational time of the original self-dictionary method without sacrificing endmember extraction accuracy.",
    "published": "2025-12-11T10:27:40Z",
    "updated": "2025-12-11T10:27:40Z",
    "link": "http://arxiv.org/pdf/2512.10506v1.pdf",
    "category": [
      "eess.IV",
      "cs.LG",
      "eess.SP"
    ],
    "authors": [
      "Tomohiko Mizutani"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16685v4",
    "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections",
    "summary": "We address key challenges in Dataset Aggregation (DAgger) for real-world contact-rich manipulation: how to collect informative human correction data and how to effectively update policies with this new data. We introduce Compliant Residual DAgger (CR-DAgger), which contains two novel components: 1) a Compliant Intervention Interface that leverages compliance control, allowing humans to provide gentle, accurate delta action corrections without interrupting the ongoing robot policy execution; and 2) a Compliant Residual Policy formulation that learns from human corrections while incorporating force feedback and force control. Our system significantly enhances performance on precise contact-rich manipulation tasks using minimal correction data, improving base policy success rates by over 50\\% on two challenging tasks (book flipping and belt assembly) while outperforming both retraining-from-scratch and finetuning approaches. Through extensive real-world experiments, we provide practical guidance for implementing effective DAgger in real-world robot learning tasks. Result videos are available at: https://compliant-residual-dagger.github.io/",
    "published": "2025-06-20T01:57:47Z",
    "updated": "2025-12-11T10:15:43Z",
    "link": "http://arxiv.org/pdf/2506.16685v4.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Xiaomeng Xu",
      "Yifan Hou",
      "Zeyi Liu",
      "Shuran Song"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10485v1",
    "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
    "summary": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
    "published": "2025-12-11T10:04:54Z",
    "updated": "2025-12-11T10:04:54Z",
    "link": "http://arxiv.org/pdf/2512.10485v1.pdf",
    "category": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "authors": [
      "Chaomeng Lu",
      "Bert Lagaisse"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06356v2",
    "title": "DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction",
    "summary": "Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.",
    "published": "2025-12-06T09:06:08Z",
    "updated": "2025-12-11T09:53:17Z",
    "link": "http://arxiv.org/pdf/2512.06356v2.pdf",
    "category": [
      "cs.LG",
      "cs.SI"
    ],
    "authors": [
      "Yifan Song",
      "Fenglin Yu",
      "Yihong Luo",
      "Xingjian Tao",
      "Siya Qiu",
      "Kai Han",
      "Jing Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10457v1",
    "title": "Hybrid Physics-ML Model for Forward Osmosis Flux with Complete Uncertainty Quantification",
    "summary": "Forward Osmosis (FO) is a promising low-energy membrane separation technology, but challenges in accurately modelling its water flux (Jw) persist due to complex internal mass transfer phenomena. Traditional mechanistic models struggle with empirical parameter variability, while purely data-driven models lack physical consistency and rigorous uncertainty quantification (UQ). This study introduces a novel Robust Hybrid Physics-ML framework employing Gaussian Process Regression (GPR) for highly accurate, uncertainty-aware Jw prediction. The core innovation lies in training the GPR on the residual error between the detailed, non-linear FO physical model prediction (Jw_physical) and the experimental water flux (Jw_actual). Crucially, we implement a full UQ methodology by decomposing the total predictive variance (sigma2_total) into model uncertainty (epistemic, from GPR's posterior variance) and input uncertainty (aleatoric, analytically propagated via the Delta method for multi-variate correlated inputs). Leveraging the inherent strength of GPR in low-data regimes, the model, trained on a meagre 120 data points, achieved a state-of-the-art Mean Absolute Percentage Error (MAPE) of 0.26% and an R2 of 0.999 on the independent test data, validating a truly robust and reliable surrogate model for advanced FO process optimization and digital twin development.",
    "published": "2025-12-11T09:27:44Z",
    "updated": "2025-12-11T09:27:44Z",
    "link": "http://arxiv.org/pdf/2512.10457v1.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Shiv Ratn",
      "Shivang Rampriyan",
      "Bahni Ray"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10451v1",
    "title": "Metacognitive Sensitivity for Test-Time Dynamic Model Selection",
    "summary": "A key aspect of human cognition is metacognition - the ability to assess one's own knowledge and judgment reliability. While deep learning models can express confidence in their predictions, they often suffer from poor calibration, a cognitive bias where expressed confidence does not reflect true competence. Do models truly know what they know? Drawing from human cognitive science, we propose a new framework for evaluating and leveraging AI metacognition. We introduce meta-d', a psychologically-grounded measure of metacognitive sensitivity, to characterise how reliably a model's confidence predicts its own accuracy. We then use this dynamic sensitivity score as context for a bandit-based arbiter that performs test-time model selection, learning which of several expert models to trust for a given task. Our experiments across multiple datasets and deep learning model combinations (including CNNs and VLMs) demonstrate that this metacognitive approach improves joint-inference accuracy over constituent models. This work provides a novel behavioural account of AI models, recasting ensemble selection as a problem of evaluating both short-term signals (confidence prediction scores) and medium-term traits (metacognitive sensitivity).",
    "published": "2025-12-11T09:15:05Z",
    "updated": "2025-12-11T09:15:05Z",
    "link": "http://arxiv.org/pdf/2512.10451v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Le Tuan Minh Trinh",
      "Le Minh Vu Pham",
      "Thi Minh Anh Pham",
      "An Duc Nguyen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2412.00173v2",
    "title": "Enhanced Spatial Clustering of Single-Molecule Localizations with Graph Neural Networks",
    "summary": "Single-molecule localization microscopy generates point clouds corresponding to fluorophore localizations. Spatial cluster identification and analysis of these point clouds are crucial for extracting insights about molecular organization. However, this task becomes challenging in the presence of localization noise, high point density, or complex biological structures. Here, we introduce MIRO (Multifunctional Integration through Relational Optimization), an algorithm that uses recurrent graph neural networks to transform the point clouds in order to improve clustering efficiency when applying conventional clustering techniques. We show that MIRO supports simultaneous processing of clusters of different shapes and at multiple scales, demonstrating improved performance across varied datasets. Our comprehensive evaluation demonstrates MIRO's transformative potential for single-molecule localization applications, showcasing its capability to revolutionize cluster analysis and provide accurate, reliable details of molecular architecture. In addition, MIRO's robust clustering capabilities hold promise for applications in various fields such as neuroscience, for the analysis of neural connectivity patterns, and environmental science, for studying spatial distributions of ecological data.",
    "published": "2024-11-29T17:43:57Z",
    "updated": "2025-12-11T08:59:03Z",
    "link": "http://arxiv.org/pdf/2412.00173v2.pdf",
    "category": [
      "cs.LG",
      "physics.bio-ph",
      "physics.data-an",
      "q-bio.QM"
    ],
    "authors": [
      "Jesús Pineda",
      "Sergi Masó-Orriols",
      "Montse Masoliver",
      "Joan Bertran",
      "Mattias Goksör",
      "Giovanni Volpe",
      "Carlo Manzo"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.01681v2",
    "title": "Generalized Kernelized Bandits: A Novel Self-Normalized Bernstein-Like Dimension-Free Inequality and Regret Bounds",
    "summary": "We study the regret minimization problem in the novel setting of generalized kernelized bandits (GKBs), where we optimize an unknown function $f^*$ belonging to a reproducing kernel Hilbert space (RKHS) having access to samples generated by an exponential family (EF) reward model whose mean is a non-linear function $μ(f^*)$. This setting extends both kernelized bandits (KBs) and generalized linear bandits (GLBs), providing a unified view of both settings. We propose an optimistic regret minimization algorithm, GKB-UCB, and we explain why existing self-normalized concentration inequalities used for KBs and GLBs do not allow to provide tight regret guarantees. For this reason, we devise a novel self-normalized Bernstein-like dimension-free inequality that applies to a Hilbert space of functions with bounded norm, representing a contribution of independent interest. Based on it, we analyze GKB-UCB, deriving a regret bound of order $\\widetilde{O}( γ_T \\sqrt{T/κ_*})$, being $T$ the learning horizon, $γ_T$ the maximal information gain, and $κ_*$ a term characterizing the magnitude of the expected reward non-linearity. Our result is tight in its dependence on $T$, $γ_T$, and $κ_*$ for both KBs and GLBs. Finally, we present a tractable version GKB-UCB, Trac-GKB-UCB, which attains similar regret guarantees, and we discuss its time and space complexity.",
    "published": "2025-08-03T09:23:19Z",
    "updated": "2025-12-11T08:54:23Z",
    "link": "http://arxiv.org/pdf/2508.01681v2.pdf",
    "category": [
      "cs.LG",
      "stat.ML"
    ],
    "authors": [
      "Alberto Maria Metelli",
      "Simone Drago",
      "Marco Mussi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10427v1",
    "title": "The Operator Origins of Neural Scaling Laws: A Generalized Spectral Transport Dynamics of Deep Learning",
    "summary": "Modern deep networks operate in a rough, finite-regularity regime where Jacobian-induced operators exhibit heavy-tailed spectra and strong basis drift. In this work, we derive a unified operator-theoretoretic description of neural training dynamics directly from gradient descent. Starting from the exact evolution $\\dot e_t = -M(t)e_t$ in function space, we apply Kato perturbation theory to obtain a rigorous system of coupled mode ODEs and show that, after coarse-graining, these dynamics converge to a spectral transport--dissipation PDE \\[ \\partial_t g + \\partial_λ(v g) = -λg + S, \\] where $v$ captures eigenbasis drift and $S$ encodes nonlocal spectral coupling.\n  We prove that neural training preserves functional regularity, forcing the drift to take an asymptotic power-law form $v(λ,t)\\sim -c(t)λ^b$. In the weak-coupling regime -- naturally induced by spectral locality and SGD noise -- the PDE admits self-similar solutions with a resolution frontier, polynomial amplitude growth, and power-law dissipation. This structure yields explicit scaling-law exponents, explains the geometry of double descent, and shows that the effective training time satisfies $τ(t)=t^αL(t)$ for slowly varying $L$.\n  Finally, we show that NTK training and feature learning arise as two limits of the same PDE: $v\\equiv 0$ recovers lazy dynamics, while $v\\neq 0$ produces representation drift. Our results provide a unified spectral framework connecting operator geometry, optimization dynamics, and the universal scaling behavior of modern deep networks.",
    "published": "2025-12-11T08:38:46Z",
    "updated": "2025-12-11T08:38:46Z",
    "link": "http://arxiv.org/pdf/2512.10427v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Yizhou Zhang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10407v1",
    "title": "Supervised Learning of Random Neural Architectures Structured by Latent Random Fields on Compact Boundaryless Multiply-Connected Manifolds",
    "summary": "This paper introduces a new probabilistic framework for supervised learning in neural systems. It is designed to model complex, uncertain systems whose random outputs are strongly non-Gaussian given deterministic inputs. The architecture itself is a random object stochastically generated by a latent anisotropic Gaussian random field defined on a compact, boundaryless, multiply-connected manifold. The goal is to establish a novel conceptual and mathematical framework in which neural architectures are realizations of a geometry-aware, field-driven generative process. Both the neural topology and synaptic weights emerge jointly from a latent random field. A reduced-order parameterization governs the spatial intensity of an inhomogeneous Poisson process on the manifold, from which neuron locations are sampled. Input and output neurons are identified via extremal evaluations of the latent field, while connectivity is established through geodesic proximity and local field affinity. Synaptic weights are conditionally sampled from the field realization, inducing stochastic output responses even for deterministic inputs. To ensure scalability, the architecture is sparsified via percentile-based diffusion masking, yielding geometry-aware sparse connectivity without ad hoc structural assumptions. Supervised learning is formulated as inference on the generative hyperparameters of the latent field, using a negative log-likelihood loss estimated through Monte Carlo sampling from single-observation-per-input datasets. The paper initiates a mathematical analysis of the model, establishing foundational properties such as well-posedness, measurability, and a preliminary analysis of the expressive variability of the induced stochastic mappings, which support its internal coherence and lay the groundwork for a broader theory of geometry-driven stochastic learning.",
    "published": "2025-12-11T08:17:12Z",
    "updated": "2025-12-11T08:17:12Z",
    "link": "http://arxiv.org/pdf/2512.10407v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG"
    ],
    "authors": [
      "Christian Soize"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10401v1",
    "title": "Diffusion differentiable resampling",
    "summary": "This paper is concerned with differentiable resampling in the context of sequential Monte Carlo (e.g., particle filtering). We propose a new informative resampling method that is instantly pathwise differentiable, based on an ensemble score diffusion model. We prove that our diffusion resampling method provides a consistent estimate to the resampling distribution, and we show by experiments that it outperforms the state-of-the-art differentiable resampling methods when used for stochastic filtering and parameter estimation.",
    "published": "2025-12-11T08:08:55Z",
    "updated": "2025-12-11T08:08:55Z",
    "link": "http://arxiv.org/pdf/2512.10401v1.pdf",
    "category": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "authors": [
      "Jennifer Rosina Andersson",
      "Zheng Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10394v1",
    "title": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI",
    "summary": "Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.",
    "published": "2025-12-11T07:58:19Z",
    "updated": "2025-12-11T07:58:19Z",
    "link": "http://arxiv.org/pdf/2512.10394v1.pdf",
    "category": [
      "cs.RO",
      "cs.LG"
    ],
    "authors": [
      "Weifan Guan",
      "Huasen Xi",
      "Chenxiao Zhang",
      "Aosheng Li",
      "Qinghao Hu",
      "Jian Cheng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10390v1",
    "title": "Fitting magnetization data using continued fraction of straight lines",
    "summary": "Magnetization of a ferromagnetic substance in response to an externally applied magnetic field increases with the strength of the field. This is because at the microscopic level, magnetic moments in certain regions or domains of the substance increasingly align with the applied field, while the amount of misaligned domains decreases. The alignment of such magnetic domains with an applied magnetic field forms the physical basis for the nonlinearity of magnetization. In this paper, the nonlinear function is approximated as a combination of continued fraction of straight lines. The resulting fit is used to interpret the nonlinear behavior in both growing and shrinking magnetic domains. The continued fraction of straight lines used here is an algebraic expression which can be used to estimate parameters using nonlinear regression.",
    "published": "2025-12-11T07:57:17Z",
    "updated": "2025-12-11T07:57:17Z",
    "link": "http://arxiv.org/pdf/2512.10390v1.pdf",
    "category": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "physics.class-ph"
    ],
    "authors": [
      "Vijay Prakash S"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.06181v2",
    "title": "Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data",
    "summary": "Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.\n  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.\n  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.\n  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.\n  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.",
    "published": "2025-12-05T22:02:02Z",
    "updated": "2025-12-11T07:50:25Z",
    "link": "http://arxiv.org/pdf/2512.06181v2.pdf",
    "category": [
      "q-bio.QM",
      "cs.LG"
    ],
    "authors": [
      "Yanuo Zhou"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2506.16528v2",
    "title": "Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches",
    "summary": "Traditional ASR metrics like WER and CER fail to capture intelligibility, especially for dysarthric and dysphonic speech, where semantic alignment matters more than exact word matches. ASR systems struggle with these speech types, often producing errors like phoneme repetitions and imprecise consonants, yet the meaning remains clear to human listeners. We identify two key challenges: (1) Existing metrics do not adequately reflect intelligibility, and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR transcripts of dysarthric speech remains underexplored. To address this, we propose a novel metric integrating Natural Language Inference (NLI) scores, semantic similarity, and phonetic similarity. Our ASR evaluation metric achieves a 0.890 correlation with human judgments on Speech Accessibility Project data, surpassing traditional methods and emphasizing the need to prioritize intelligibility over error-based measures.",
    "published": "2025-06-19T18:21:19Z",
    "updated": "2025-12-11T07:36:11Z",
    "link": "http://arxiv.org/pdf/2506.16528v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Bornali Phukon",
      "Xiuwen Zheng",
      "Mark Hasegawa-Johnson"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10355v1",
    "title": "Better Prevent than Tackle: Valuing Defense in Soccer Based on Graph Neural Networks",
    "summary": "Evaluating defensive performance in soccer remains challenging, as effective defending is often expressed not through visible on-ball actions such as interceptions and tackles, but through preventing dangerous opportunities before they arise. Existing approaches have largely focused on valuing on-ball actions, leaving much of defenders' true impact unmeasured. To address this gap, we propose DEFCON (DEFensive CONtribution evaluator), a comprehensive framework that quantifies player-level defensive contributions for every attacking situation in soccer. Leveraging Graph Attention Networks, DEFCON estimates the success probability and expected value of each attacking option, along with each defender's responsibility for stopping it. These components yield an Expected Possession Value (EPV) for the attacking team before and after each action, and DEFCON assigns positive or negative credits to defenders according to whether they reduced or increased the opponent's EPV. Trained on 2023-24 and evaluated on 2024-25 Eredivisie event and tracking data, DEFCON's aggregated player credits exhibit strong positive correlations with market valuations. Finally, we showcase several practical applications, including in-game timelines of defensive contributions, spatial analyses across pitch zones, and pairwise summaries of attacker-defender interactions.",
    "published": "2025-12-11T07:12:23Z",
    "updated": "2025-12-11T07:12:23Z",
    "link": "http://arxiv.org/pdf/2512.10355v1.pdf",
    "category": [
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Hyunsung Kim",
      "Sangwoo Seo",
      "Hoyoung Choi",
      "Tom Boomstra",
      "Jinsung Yoon",
      "Chanyoung Park"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.19980v2",
    "title": "RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis",
    "summary": "Clinical diagnosis is a highly specialized discipline requiring both domain expertise and strict adherence to rigorous guidelines. While current AI-driven medical research predominantly focuses on knowledge graphs or natural text pretraining paradigms to incorporate medical knowledge, these approaches primarily rely on implicitly encoded knowledge within model parameters, neglecting task-specific knowledge required by diverse downstream tasks. To address this limitation, we propose Retrieval-Augmented Diagnosis (RAD), a novel framework that explicitly injects external knowledge into multimodal models directly on downstream tasks. Specifically, RAD operates through three key mechanisms: retrieval and refinement of disease-centered knowledge from multiple medical sources, a guideline-enhanced contrastive loss that constrains the latent distance between multi-modal features and guideline knowledge, and the dual transformer decoder that employs guidelines as queries to steer cross-modal fusion, aligning the models with clinical diagnostic workflows from guideline acquisition to feature extraction and decision-making. Moreover, recognizing the lack of quantitative evaluation of interpretability for multimodal diagnostic models, we introduce a set of criteria to assess the interpretability from both image and text perspectives. Extensive evaluations across four datasets with different anatomies demonstrate RAD's generalizability, achieving state-of-the-art performance. Furthermore, RAD enables the model to concentrate more precisely on abnormal regions and critical indicators, ensuring evidence-based, trustworthy diagnosis. Our code is available at https://github.com/tdlhl/RAD.",
    "published": "2025-09-24T10:36:14Z",
    "updated": "2025-12-11T07:09:18Z",
    "link": "http://arxiv.org/pdf/2509.19980v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Haolin Li",
      "Tianjie Dai",
      "Zhe Chen",
      "Siyuan Du",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2403.04162v2",
    "title": "Noisy Spiking Actor Network for Exploration",
    "summary": "As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.",
    "published": "2024-03-07T02:47:08Z",
    "updated": "2025-12-11T06:42:28Z",
    "link": "http://arxiv.org/pdf/2403.04162v2.pdf",
    "category": [
      "cs.LG",
      "cs.NE"
    ],
    "authors": [
      "Ding Chen",
      "Peixi Peng",
      "Tiejun Huang",
      "Yonghong Tian"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.20068v2",
    "title": "JITServe: SLO-aware LLM Serving with Imprecise Request Information",
    "summary": "The integration of Large Language Models (LLMs) into applications ranging from interactive chatbots to multi-agent systems has introduced a wide spectrum of service-level objectives (SLOs) for responsiveness. These include latency-sensitive requests emphasizing per-token latency in streaming chat, deadline-sensitive requests requiring rapid full responses to trigger external tools, and compound requests with evolving dependencies across multiple LLM calls. Despite-or perhaps, because of-this workload diversity and unpredictable request information (e.g., response lengths and dependencies), existing request schedulers have focused on aggregate performance, unable to ensure application-level SLO needs.\n  This paper presents JITServe, the first SLO-aware LLM serving system designed to maximize service goodput (e.g., the number of tokens meeting request SLOs) across diverse workloads. JITServe novelly schedules requests using imprecise request information and gradually relaxes this conservatism by refining request information estimates as generation progresses. It applies a grouped margin goodput maximization algorithm to allocate just enough serving bandwidth to satisfy each request's SLO just-in-time (JIT), maximizing residual capacity for others, while deciding the composition of requests in a batch to maximize efficiency and goodput with provable guarantees. Our evaluation across diverse realistic workloads, including chat, deep research, and agentic pipelines, shows that JITServe improves service goodput by 1.4x-6.3x, alternatively achieving 28.5%-83.2% resource savings, compared to state-of-the-art designs.",
    "published": "2025-04-24T05:55:21Z",
    "updated": "2025-12-11T06:24:21Z",
    "link": "http://arxiv.org/pdf/2504.20068v2.pdf",
    "category": [
      "cs.DC",
      "cs.LG",
      "eess.SY"
    ],
    "authors": [
      "Wei Zhang",
      "Zhiyu Wu",
      "Yi Mu",
      "Ning Rui",
      "Banruo Liu",
      "Nikhil Sarda",
      "Myungjin Lee",
      "Fan Lai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.08169v3",
    "title": "Bidirectional Representations Augmented Autoregressive Biological Sequence Generation",
    "summary": "Autoregressive (AR) models, common in sequence generation, are limited in many biological tasks such as de novo peptide sequencing and protein modeling by their unidirectional nature, failing to capture crucial global bidirectional token dependencies. Non-Autoregressive (NAR) models offer holistic, bidirectional representations but face challenges with generative coherence and scalability. To transcend this, we propose a hybrid framework enhancing AR generation by dynamically integrating rich contextual information from non-autoregressive mechanisms. Our approach couples a shared input encoder with two decoders: a non-autoregressive one learning latent bidirectional biological features, and an AR decoder synthesizing the biological sequence by leveraging these bidirectional features. A novel cross-decoder attention module enables the AR decoder to iteratively query and integrate these bidirectional features, enriching its predictions. This synergy is cultivated via a tailored training strategy with importance annealing for balanced objectives and cross-decoder gradient blocking for stable, focused learning. Evaluations on a demanding nine-species benchmark of de novo peptide sequencing show that our model substantially surpasses AR and NAR baselines. It uniquely harmonizes AR stability with NAR contextual awareness, delivering robust, superior performance on diverse downstream data. This research advances biological sequence modeling techniques and contributes a novel architectural paradigm for augmenting AR models with enhanced bidirectional understanding for complex sequence generation. Code is available at https://github.com/BEAM-Labs/denovo.",
    "published": "2025-10-09T12:52:55Z",
    "updated": "2025-12-11T06:21:38Z",
    "link": "http://arxiv.org/pdf/2510.08169v3.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Xiang Zhang",
      "Jiaqi Wei",
      "Zijie Qiu",
      "Sheng Xu",
      "Zhi Jin",
      "ZhiQiang Gao",
      "Nanqing Dong",
      "Siqi Sun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10325v1",
    "title": "Residual subspace evolution strategies for nonlinear inverse problems",
    "summary": "Nonlinear inverse problems often feature noisy, non-differentiable, or expensive residual evaluations that make Jacobian-based solvers unreliable. Popular derivative-free optimizers such as natural evolution strategies (NES) or Powell's NEWUOA still assume smoothness or expend many evaluations to maintain stability. Ensemble Kalman inversion (EKI) relies on empirical covariances that require preconditioning and scale poorly with residual dimension.\n  We introduce residual subspace evolution strategies (RSES), a derivative-free solver that samples Gaussian probes around the current iterate, builds a residual-only surrogate from their differences, and recombines the probes through a least-squares solve yielding an optimal update without forming Jacobians or covariances. Each iteration costs $k+1$ residual evaluations, where $k \\ll n$ for $n$-dimensional problems, with $O(k^3)$ linear algebra overhead.\n  Benchmarks on calibration, regression, and deconvolution problems demonstrate consistent misfit reduction in both deterministic and stochastic settings. RSES matches or surpasses xNES and NEWUOA while staying competitive with EKI under matched evaluation budgets, particularly when smoothness or covariance assumptions fail.",
    "published": "2025-12-11T06:20:13Z",
    "updated": "2025-12-11T06:20:13Z",
    "link": "http://arxiv.org/pdf/2512.10325v1.pdf",
    "category": [
      "math.OC",
      "cs.LG"
    ],
    "authors": [
      "Francesco Alemanno"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.15365v2",
    "title": "TranSimHub:A Unified Air-Ground Simulation Platform for Multi-Modal Perception and Decision-Making",
    "summary": "Air-ground collaborative intelligence is becoming a key approach for next-generation urban intelligent transportation management, where aerial and ground systems work together on perception, communication, and decision-making. However, the lack of a unified multi-modal simulation environment has limited progress in studying cross-domain perception, coordination under communication constraints, and joint decision optimization. To address this gap, we present TranSimHub, a unified simulation platform for air-ground collaborative intelligence. TranSimHub offers synchronized multi-view rendering across RGB, depth, and semantic segmentation modalities, ensuring consistent perception between aerial and ground viewpoints. It also supports information exchange between the two domains and includes a causal scene editor that enables controllable scenario creation and counterfactual analysis under diverse conditions such as different weather, emergency events, and dynamic obstacles. We release TranSimHub as an open-source platform that supports end-to-end research on perception, fusion, and control across realistic air and ground traffic scenes. Our code is available at https://github.com/Traffic-Alpha/TransSimHub.",
    "published": "2025-10-17T06:56:34Z",
    "updated": "2025-12-11T05:55:53Z",
    "link": "http://arxiv.org/pdf/2510.15365v2.pdf",
    "category": [
      "eess.SY",
      "cs.LG",
      "cs.MA"
    ],
    "authors": [
      "Maonan Wang",
      "Yirong Chen",
      "Yuxin Cai",
      "Aoyu Pang",
      "Yuejiao Xie",
      "Zian Ma",
      "Chengcheng Xu",
      "Kemou Jiang",
      "Ding Wang",
      "Laurent Roullet",
      "Chung Shue Chen",
      "Zhiyong Cui",
      "Yuheng Kan",
      "Michael Lepech",
      "Man-On Pun"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10309v1",
    "title": "Tracking large chemical reaction networks and rare events by neural networks",
    "summary": "Chemical reaction networks are widely used to model stochastic dynamics in chemical kinetics, systems biology and epidemiology. Solving the chemical master equation that governs these systems poses a significant challenge due to the large state space exponentially growing with system sizes. The development of autoregressive neural networks offers a flexible framework for this problem; however, its efficiency is limited especially for high-dimensional systems and in scenarios with rare events. Here, we push the frontier of neural-network approach by exploiting faster optimizations such as natural gradient descent and time-dependent variational principle, achieving a 5- to 22-fold speedup, and by leveraging enhanced-sampling strategies to capture rare events. We demonstrate reduced computational cost and higher accuracy over the previous neural-network method in challenging reaction networks, including the mitogen-activated protein kinase (MAPK) cascade network, the hitherto largest biological network handled by the previous approaches of solving the chemical master equation. We further apply the approach to spatially extended reaction-diffusion systems, the Schlögl model with rare events, on two-dimensional lattices, beyond the recent tensor-network approach that handles one-dimensional lattices. The present approach thus enables efficient modeling of chemical reaction networks in general.",
    "published": "2025-12-11T05:55:44Z",
    "updated": "2025-12-11T05:55:44Z",
    "link": "http://arxiv.org/pdf/2512.10309v1.pdf",
    "category": [
      "q-bio.MN",
      "cs.LG",
      "physics.bio-ph"
    ],
    "authors": [
      "Jiayu Weng",
      "Xinyi Zhu",
      "Jing Liu",
      "Linyuan Lü",
      "Pan Zhang",
      "Ying Tang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2507.11865v2",
    "title": "Optimizing Drivers' Discount Order Acceptance Strategies: A Policy-Improved Deep Deterministic Policy Gradient Framework",
    "summary": "The rapid expansion of platform integration has emerged as an effective solution to mitigate market fragmentation by consolidating multiple ride-hailing platforms into a single application. To address heterogeneous passenger preferences, third-party integrators provide Discount Express service delivered by express drivers at lower trip fares. For the individual platform, encouraging broader participation of drivers in Discount Express services has the potential to expand the accessible demand pool and improve matching efficiency, but often at the cost of reduced profit margins. This study aims to dynamically manage drivers' acceptance of Discount Express from the perspective of an individual platform. The lack of historical data under the new business model necessitates online learning. However, early-stage exploration through trial and error can be costly in practice, highlighting the need for reliable early-stage performance in real-world deployment. To address these challenges, this study formulates the decision regarding the proportion of drivers accepting discount orders as a continuous control task. In response to the high stochasticity and the opaque matching mechanisms employed by third-party integrator, we propose an innovative policy-improved deep deterministic policy gradient (pi-DDPG) framework. The proposed framework incorporates a refiner module to boost policy performance during the early training phase. A customized simulator based on a real-world dataset is developed to validate the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate that pi-DDPG achieves superior learning efficiency and significantly reduces early-stage training losses, enhancing its applicability to practical ride-hailing scenarios.",
    "published": "2025-07-16T03:24:54Z",
    "updated": "2025-12-11T05:55:14Z",
    "link": "http://arxiv.org/pdf/2507.11865v2.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Hanwen Dai",
      "Chang Gao",
      "Fang He",
      "Congyuan Ji",
      "Yanni Yang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10308v1",
    "title": "An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis",
    "summary": "Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.\n  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.\n  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\\% in Hartford and 13.8\\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.\n  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.",
    "published": "2025-12-11T05:54:22Z",
    "updated": "2025-12-11T05:54:22Z",
    "link": "http://arxiv.org/pdf/2512.10308v1.pdf",
    "category": [
      "cs.LG"
    ],
    "authors": [
      "Vasiliki Stoumpou",
      "Maciej Tysarowski",
      "Talhat Azemi",
      "Jawad Haider",
      "Howard L. Haronian",
      "Robert C. Hagberg",
      "Dimitris Bertsimas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.09366v2",
    "title": "Meta-learning three-factor plasticity rules for structured credit assignment with sparse feedback",
    "summary": "Biological neural networks learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via \\textbf{tangent-propagation through learning}. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.",
    "published": "2025-12-10T06:57:51Z",
    "updated": "2025-12-11T05:38:49Z",
    "link": "http://arxiv.org/pdf/2512.09366v2.pdf",
    "category": [
      "q-bio.NC",
      "cond-mat.dis-nn",
      "cs.LG",
      "physics.bio-ph"
    ],
    "authors": [
      "Dimitra Maoutsa"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2509.00614v3",
    "title": "RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models",
    "summary": "In the era of foundation models, fine-tuning pre-trained models for specific downstream tasks has become crucial. This drives the need for robust fine-tuning methods to address challenges such as model overfitting and sparse labeling. Molecular graph foundation models (MGFMs) face unique difficulties that complicate fine-tuning. These models are limited by smaller pre-training datasets and more severe data scarcity for downstream tasks, both of which require enhanced model generalization. Moreover, MGFMs must accommodate diverse objectives, including both regression and classification tasks. To better understand and improve fine-tuning techniques under these conditions, we classify eight fine-tuning methods into three mechanisms: weight-based, representation-based, and partial fine-tuning. We benchmark these methods on downstream regression and classification tasks across supervised and self-supervised pre-trained models in diverse labeling settings. This extensive evaluation provides valuable insights and informs the design of a refined robust fine-tuning method, ROFT-MOL. This approach combines the strengths of simple post-hoc weight interpolation with more complex weight ensemble fine-tuning methods, delivering improved performance across both task types while maintaining the ease of use inherent in post-hoc weight interpolation.",
    "published": "2025-08-30T21:35:57Z",
    "updated": "2025-12-11T05:11:29Z",
    "link": "http://arxiv.org/pdf/2509.00614v3.pdf",
    "category": [
      "cs.LG",
      "physics.chem-ph"
    ],
    "authors": [
      "Shikun Liu",
      "Deyu Zou",
      "Nima Shoghi",
      "Victor Fung",
      "Kai Liu",
      "Pan Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.14348v2",
    "title": "Enforcing hidden physics in physics-informed neural networks",
    "summary": "Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, ensuring that such frameworks fully reflect the physical structure embedded in the governing equations remains an open challenge, particularly for maintaining robustness across diverse scientific problems. In this work, we address this issue by introducing a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training, thereby recovering the missing physics associated with irreversible processes in the conventional PINN. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack growth, we observe substantial performance improvements over the conventional PINN, demonstrating that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks.",
    "published": "2025-11-18T10:52:37Z",
    "updated": "2025-12-11T05:08:35Z",
    "link": "http://arxiv.org/pdf/2511.14348v2.pdf",
    "category": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "authors": [
      "Nanxi Chen",
      "Sifan Wang",
      "Rujin Ma",
      "Airong Chen",
      "Chuanjie Cui"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10287v1",
    "title": "A Kernel-based Resource-efficient Neural Surrogate for Multi-fidelity Prediction of Aerodynamic Field",
    "summary": "Surrogate models provide fast alternatives to costly aerodynamic simulations and are extremely useful in design and optimization applications. This study proposes the use of a recent kernel-based neural surrogate, KHRONOS. In this work, we blend sparse high-fidelity (HF) data with low-fidelity (LF) information to predict aerodynamic fields under varying constraints in computational resources. Unlike traditional approaches, KHRONOS is built upon variational principles, interpolation theory, and tensor decomposition. These elements provide a mathematical basis for heavy pruning compared to dense neural networks. Using the AirfRANS dataset as a high-fidelity benchmark and NeuralFoil to generate low-fidelity counterparts, this work compares the performance of KHRONOS with three contemporary model architectures: a multilayer perceptron (MLP), a graph neural network (GNN), and a physics-informed neural network (PINN). We consider varying levels of high-fidelity data availability (0%, 10%, and 30%) and increasingly complex geometry parameterizations. These are used to predict the surface pressure coefficient distribution over the airfoil. Results indicate that, whilst all models eventually achieve comparable predictive accuracy, KHRONOS excels in resource-constrained conditions. In this domain, KHRONOS consistently requires orders of magnitude fewer trainable parameters and delivers much faster training and inference than contemporary dense neural networks at comparable accuracy. These findings highlight the potential of KHRONOS and similar architectures to balance accuracy and efficiency in multi-fidelity aerodynamic field prediction.",
    "published": "2025-12-11T05:05:10Z",
    "updated": "2025-12-11T05:05:10Z",
    "link": "http://arxiv.org/pdf/2512.10287v1.pdf",
    "category": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "authors": [
      "Apurba Sarker",
      "Reza T. Batley",
      "Darshan Sarojini",
      "Sourav Saha"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10778v1",
    "title": "Building Audio-Visual Digital Twins with Smartphones",
    "summary": "Digital twins today are almost entirely visual, overlooking acoustics-a core component of spatial realism and interaction. We introduce AV-Twin, the first practical system that constructs editable audio-visual digital twins using only commodity smartphones. AV-Twin combines mobile RIR capture and a visual-assisted acoustic field model to efficiently reconstruct room acoustics. It further recovers per-surface material properties through differentiable acoustic rendering, enabling users to modify materials, geometry, and layout while automatically updating both audio and visuals. Together, these capabilities establish a practical path toward fully modifiable audio-visual digital twins for real-world environments.",
    "published": "2025-12-11T16:14:32Z",
    "updated": "2025-12-11T16:14:32Z",
    "link": "http://arxiv.org/pdf/2512.10778v1.pdf",
    "category": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "authors": [
      "Zitong Lan",
      "Yiwei Tang",
      "Yuhan Wang",
      "Haowen Lai",
      "Yido Hao",
      "Mingmin Zhao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10572v1",
    "title": "DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting",
    "summary": "We propose DeMapGS, a structured Gaussian Splatting framework that jointly optimizes deformable surfaces and surface-attached 2D Gaussian splats. By anchoring splats to a deformable template mesh, our method overcomes topological inconsistencies and enhances editing flexibility, addressing limitations of prior Gaussian Splatting methods that treat points independently. The unified representation in our method supports extraction of high-fidelity diffuse, normal, and displacement maps, enabling the reconstructed mesh to inherit the photorealistic rendering quality of Gaussian Splatting. To support robust optimization, we introduce a gradient diffusion strategy that propagates supervision across the surface, along with an alternating 2D/3D rendering scheme to handle concave regions. Experiments demonstrate that DeMapGS achieves state-of-the-art mesh reconstruction quality and enables downstream applications for Gaussian splats such as editing and cross-object manipulation through a shared parametric surface.",
    "published": "2025-12-11T12:00:23Z",
    "updated": "2025-12-11T12:00:23Z",
    "link": "http://arxiv.org/pdf/2512.10572v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Shuyi Zhou",
      "Shengze Zhong",
      "Kenshi Takayama",
      "Takafumi Taketomi",
      "Takeshi Oishi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10424v1",
    "title": "Neural Hamiltonian Deformation Fields for Dynamic Scene Rendering",
    "summary": "Representing and rendering dynamic scenes with complex motions remains challenging in computer vision and graphics. Recent dynamic view synthesis methods achieve high-quality rendering but often produce physically implausible motions. We introduce NeHaD, a neural deformation field for dynamic Gaussian Splatting governed by Hamiltonian mechanics. Our key observation is that existing methods using MLPs to predict deformation fields introduce inevitable biases, resulting in unnatural dynamics. By incorporating physics priors, we achieve robust and realistic dynamic scene rendering. Hamiltonian mechanics provides an ideal framework for modeling Gaussian deformation fields due to their shared phase-space structure, where primitives evolve along energy-conserving trajectories. We employ Hamiltonian neural networks to implicitly learn underlying physical laws governing deformation. Meanwhile, we introduce Boltzmann equilibrium decomposition, an energy-aware mechanism that adaptively separates static and dynamic Gaussians based on their spatial-temporal energy states for flexible rendering. To handle real-world dissipation, we employ second-order symplectic integration and local rigidity regularization as physics-informed constraints for robust dynamics modeling. Additionally, we extend NeHaD to adaptive streaming through scale-aware mipmapping and progressive optimization. Extensive experiments demonstrate that NeHaD achieves physically plausible results with a rendering quality-efficiency trade-off. To our knowledge, this is the first exploration leveraging Hamiltonian mechanics for neural Gaussian deformation, enabling physically realistic dynamic scene rendering with streaming capabilities.",
    "published": "2025-12-11T08:36:30Z",
    "updated": "2025-12-11T08:36:30Z",
    "link": "http://arxiv.org/pdf/2512.10424v1.pdf",
    "category": [
      "cs.GR"
    ],
    "authors": [
      "Hai-Long Qin",
      "Sixian Wang",
      "Guo Lu",
      "Jincheng Dai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.02569v2",
    "title": "Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models",
    "summary": "This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.",
    "published": "2025-12-02T09:42:13Z",
    "updated": "2025-12-11T16:38:20Z",
    "link": "http://arxiv.org/pdf/2512.02569v2.pdf",
    "category": [
      "cs.HC",
      "cs.RO"
    ],
    "authors": [
      "Yuchong Zhang",
      "Yong Ma",
      "Danica Kragic"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10773v1",
    "title": "AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators",
    "summary": "Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests.",
    "published": "2025-12-11T16:10:32Z",
    "updated": "2025-12-11T16:10:32Z",
    "link": "http://arxiv.org/pdf/2512.10773v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Samaksh Ujjawal",
      "Shivansh Pratap Singh",
      "Naveen Sudheer Nair",
      "Rishabh Dev Yadav",
      "Wei Pan",
      "Spandan Roy"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10738v1",
    "title": "Distribution-Free Stochastic MPC for Joint-in-Time Chance-Constrained Linear Systems",
    "summary": "This work presents a stochastic model predictive control (MPC) framework for linear systems subject to joint-in-time chance constraints under unknown disturbance distributions. Unlike existing stochastic MPC formulations that rely on parametric or Gaussian assumptions or require expensive offline computations, the proposed method leverages conformal prediction (CP) as a streamlined tool to construct finite-sample confidence regions for the system's stochastic error trajectories with minimal computational effort. These regions enable the relaxation of probabilistic constraints while providing formal guarantees. By employing an indirect feedback mechanism and a probabilistic set-based formulation, we prove recursive feasibility of the relaxed optimization problem and establish chance constraint satisfaction in closed-loop. Furthermore, we extend the approach to the more general output feedback setting with unknown measurement noise distributions. Given available noise samples, we establish satisfaction of the joint chance constraints and recursive feasibility via output measurements alone. Numerical examples demonstrate the effectiveness and advantages of the proposed method compared to existing approaches.",
    "published": "2025-12-11T15:25:02Z",
    "updated": "2025-12-11T15:25:02Z",
    "link": "http://arxiv.org/pdf/2512.10738v1.pdf",
    "category": [
      "eess.SY",
      "cs.RO"
    ],
    "authors": [
      "Lukas Vogel",
      "Andrea Carron",
      "Eleftherios E. Vlahakis",
      "Dimos V. Dimarogonas"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10700v1",
    "title": "On the Stabilization of Rigid Formations on Regular Curves",
    "summary": "This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026",
    "published": "2025-12-11T14:41:19Z",
    "updated": "2025-12-11T14:41:19Z",
    "link": "http://arxiv.org/pdf/2512.10700v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Mohamed Elobaid",
      "Shinkyu Park",
      "Eric Feron"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10605v1",
    "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
    "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
    "published": "2025-12-11T12:58:36Z",
    "updated": "2025-12-11T12:58:36Z",
    "link": "http://arxiv.org/pdf/2512.10605v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Lihuang Chen",
      "Xiangyu Luo",
      "Jun Meng"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10595v1",
    "title": "Motion Planning for Safe Landing of a Human-Piloted Parafoil",
    "summary": "Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\\%-80\\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.",
    "published": "2025-12-11T12:39:48Z",
    "updated": "2025-12-11T12:39:48Z",
    "link": "http://arxiv.org/pdf/2512.10595v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Maximillian Fainkich",
      "Kiril Solovey",
      "Anna Clarke"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10540v1",
    "title": "Mr. Virgil: Learning Multi-robot Visual-range Relative Localization",
    "summary": "Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.",
    "published": "2025-12-11T11:16:53Z",
    "updated": "2025-12-11T11:16:53Z",
    "link": "http://arxiv.org/pdf/2512.10540v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Si Wang",
      "Zhehan Li",
      "Jiadong Lu",
      "Rong Xiong",
      "Yanjun Cao",
      "Yue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10531v1",
    "title": "Neural Ranging Inertial Odometry",
    "summary": "Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.",
    "published": "2025-12-11T11:03:26Z",
    "updated": "2025-12-11T11:03:26Z",
    "link": "http://arxiv.org/pdf/2512.10531v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Si Wang",
      "Bingqi Shen",
      "Fei Wang",
      "Yanjun Cao",
      "Rong Xiong",
      "Yue Wang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10481v1",
    "title": "Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks",
    "summary": "Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation\". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM\", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.",
    "published": "2025-12-11T09:59:08Z",
    "updated": "2025-12-11T09:59:08Z",
    "link": "http://arxiv.org/pdf/2512.10481v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Gaozhao Wang",
      "Xing Liu",
      "Zhenduo Ye",
      "Zhengxiong Liu",
      "Panfeng Huang"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10480v1",
    "title": "Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF",
    "summary": "Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.",
    "published": "2025-12-11T09:59:03Z",
    "updated": "2025-12-11T09:59:03Z",
    "link": "http://arxiv.org/pdf/2512.10480v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jiaqiang Zhang",
      "Xianjia Yu",
      "Sier Ha",
      "Paola Torrico Moron",
      "Sahar Salimpour",
      "Farhad Kerama",
      "Haizhou Zhang",
      "Tomi Westerlund"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10477v1",
    "title": "Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots",
    "summary": "In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. \"Swaddling\" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.",
    "published": "2025-12-11T09:55:49Z",
    "updated": "2025-12-11T09:55:49Z",
    "link": "http://arxiv.org/pdf/2512.10477v1.pdf",
    "category": [
      "cs.RO",
      "cs.NE"
    ],
    "authors": [
      "Timur Ishuov",
      "Michele Folgheraiter",
      "Madi Nurmanov",
      "Goncalo Gordo",
      "Richárd Farkas",
      "József Dombi"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10428v1",
    "title": "Design and Implementation of a High-Precision Wind-Estimation UAV with Onboard Sensors",
    "summary": "Accurate real-time wind vector estimation is essential for enhancing the safety, navigation accuracy, and energy efficiency of unmanned aerial vehicles (UAVs). Traditional approaches rely on external sensors or simplify vehicle dynamics, which limits their applicability during agile flight or in resource-constrained platforms. This paper proposes a real-time wind estimation method based solely on onboard sensors. The approach first estimates external aerodynamic forces using a disturbance observer (DOB), and then maps these forces to wind vectors using a thin-plate spline (TPS) model. A custom-designed wind barrel mounted on the UAV enhances aerodynamic sensitivity, further improving estimation accuracy. The system is validated through comprehensive experiments in wind tunnels, indoor and outdoor flights. Experimental results demonstrate that the proposed method achieves consistently high-accuracy wind estimation across controlled and real-world conditions, with speed RMSEs as low as \\SI{0.06}{m/s} in wind tunnel tests, \\SI{0.22}{m/s} during outdoor hover, and below \\SI{0.38}{m/s} in indoor and outdoor dynamic flights, and direction RMSEs under \\ang{7.3} across all scenarios, outperforming existing baselines. Moreover, the method provides vertical wind estimates -- unavailable in baselines -- with RMSEs below \\SI{0.17}{m/s} even during fast indoor translations.",
    "published": "2025-12-11T08:39:55Z",
    "updated": "2025-12-11T08:39:55Z",
    "link": "http://arxiv.org/pdf/2512.10428v1.pdf",
    "category": [
      "cs.ET",
      "cs.RO"
    ],
    "authors": [
      "Haowen Yu",
      "Na Fan",
      "Xing Liu",
      "Ximin Lyu"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2504.14894v2",
    "title": "Never too Cocky to Cooperate: An FIM and RL-based USV-AUV Collaborative System for Underwater Tasks in Extreme Sea Conditions",
    "summary": "This paper develops a novel unmanned surface vehicle (USV)-autonomous underwater vehicle (AUV) collaborative system designed to enhance underwater task performance in extreme sea conditions. The system integrates a dual strategy: (1) high-precision multi-AUV localization enabled by Fisher information matrix-optimized USV path planning, and (2) reinforcement learning-based cooperative planning and control method for multi-AUV task execution. Extensive experimental evaluations in the underwater data collection task demonstrate the system's operational feasibility, with quantitative results showing significant performance improvements over baseline methods. The proposed system exhibits robust coordination capabilities between USV and AUVs while maintaining stability in extreme sea conditions. To facilitate reproducibility and community advancement, we provide an open-source simulation toolkit available at: https://github.com/360ZMEM/USV-AUV-colab .",
    "published": "2025-04-21T06:47:46Z",
    "updated": "2025-12-11T08:28:12Z",
    "link": "http://arxiv.org/pdf/2504.14894v2.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Jingzehua Xu",
      "Guanwen Xie",
      "Jiwei Tang",
      "Yimian Ding",
      "Weiyi Liu",
      "Shuai Zhang",
      "Yi Li"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.19955v2",
    "title": "ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation",
    "summary": "Contact feedback is essential for contact-rich robotic manipulation, as it allows the robot to detect subtle interaction changes and adjust its actions accordingly. Six-axis force-torque sensors are commonly used to obtain contact feedback, but their high cost and fragility have discouraged many researchers from adopting them in contact-rich tasks. To offer a more cost-efficient and easy-accessible source of contact feedback, we present ShapeForce, a low-cost, plug-and-play soft wrist that provides force-like signals for contact-rich robotic manipulation. Inspired by how humans rely on relative force changes in contact rather than precise force magnitudes, ShapeForce converts external force and torque into measurable deformations of its compliant core, which are then estimated via marker-based pose tracking and converted into force-like signals. Our design eliminates the need for calibration or specialized electronics to obtain exact values, and instead focuses on capturing force and torque changes sufficient for enabling contact-rich manipulation. Extensive experiments across diverse contact-rich tasks and manipulation policies demonstrate that ShapeForce delivers performance comparable to six-axis force-torque sensors at an extremely low cost.",
    "published": "2025-11-25T06:01:04Z",
    "updated": "2025-12-11T08:21:48Z",
    "link": "http://arxiv.org/pdf/2511.19955v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Jinxuan Zhu",
      "Zihao Yan",
      "Yangyu Xiao",
      "Jingxiang Guo",
      "Chenrui Tie",
      "Xinyi Cao",
      "Yuhang Zheng",
      "Lin Shao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2511.21203v2",
    "title": "Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching",
    "summary": "In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.",
    "published": "2025-11-26T09:33:03Z",
    "updated": "2025-12-11T08:08:12Z",
    "link": "http://arxiv.org/pdf/2511.21203v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Fuyuki Tokuda",
      "Akira Seino",
      "Akinari Kobayashi",
      "Kai Tang",
      "Kazuhiro Kosuge"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2510.19766v2",
    "title": "SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas",
    "summary": "In this paper, we propose SEA, a novel approach for active robot exploration through semantic map prediction and a reinforcement learning-based hierarchical exploration policy. Unlike existing learning-based methods that rely on one-step waypoint prediction, our approach enhances the agent's long-term environmental understanding to facilitate more efficient exploration. We propose an iterative prediction-exploration framework that explicitly predicts the missing areas of the map based on current observations. The difference between the actual accumulated map and the predicted global map is then used to guide exploration. Additionally, we design a novel reward mechanism that leverages reinforcement learning to update the long-term exploration strategies, enabling us to construct an accurate semantic map within limited steps. Experimental results demonstrate that our method significantly outperforms state-of-the-art exploration strategies, achieving superior coverage ares of the global map within the same time constraints.",
    "published": "2025-10-22T16:51:36Z",
    "updated": "2025-12-11T07:43:51Z",
    "link": "http://arxiv.org/pdf/2510.19766v2.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Hongyu Ding",
      "Xinyue Liang",
      "Yudong Fang",
      "You Wu",
      "Jieqi Shi",
      "Jing Huo",
      "Wenbin Li",
      "Jing Wu",
      "Yu-Kun Lai",
      "Yang Gao"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10360v1",
    "title": "CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation",
    "summary": "Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.",
    "published": "2025-12-11T07:20:06Z",
    "updated": "2025-12-11T07:20:06Z",
    "link": "http://arxiv.org/pdf/2512.10360v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Liuyi Wang",
      "Zongtao He",
      "Jinlong Li",
      "Xiaoyan Qi",
      "Mengxian Hu",
      "Chenpeng Yao",
      "Chengju Liu",
      "Qijun Chen"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10349v1",
    "title": "Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing",
    "summary": "Tendon-driven under-actuated robotic fingers provide advantages for dexterous manipulation through reduced actuator requirements and simplified mechanical design. However, achieving both high load capacity and adaptive compliance in a compact form remains challenging. This paper presents an under-actuated tendon-driven robotic finger (UTRF) featuring a synchronous tendon routing that mechanically couples all joints with fixed angular velocity ratios, enabling the entire finger to be actuated by a single actuator. This approach significantly reduces the number of actuators required in multi-finger hands, resulting in a lighter and more compact structure without sacrificing stiffness or compliance. The kinematic and static models of the finger are derived, incorporating tendon elasticity to predict structural stiffness. A single-finger prototype was fabricated and tested under static loading, showing an average deflection prediction error of 1.0 mm (0.322% of total finger length) and a measured stiffness of 1.2x10^3 N/m under a 3 kg tip load. Integration into a five-finger robotic hand (UTRF-RoboHand) demonstrates effective object manipulation across diverse scenarios, confirming that the proposed routing achieves predictable stiffness and reliable grasping performance with a minimal actuator count.",
    "published": "2025-12-11T07:05:43Z",
    "updated": "2025-12-11T07:05:43Z",
    "link": "http://arxiv.org/pdf/2512.10349v1.pdf",
    "category": [
      "cs.RO"
    ],
    "authors": [
      "Quan Yuan",
      "Zhenting Du",
      "Daqian Cao",
      "Weibang Bai"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2512.10294v1",
    "title": "Lies We Can Trust: Quantifying Action Uncertainty with Inaccurate Stochastic Dynamics through Conformalized Nonholonomic Lie Groups",
    "summary": "We propose Conformal Lie-group Action Prediction Sets (CLAPS), a symmetry-aware conformal prediction-based algorithm that constructs, for a given action, a set guaranteed to contain the resulting system configuration at a user-defined probability. Our assurance holds under both aleatoric and epistemic uncertainty, non-asymptotically, and does not require strong assumptions about the true system dynamics, the uncertainty sources, or the quality of the approximate dynamics model. Typically, uncertainty quantification is tackled by making strong assumptions about the error distribution or magnitude, or by relying on uncalibrated uncertainty estimates - i.e., with no link to frequentist probabilities - which are insufficient for safe control. Recently, conformal prediction has emerged as a statistical framework capable of providing distribution-free probabilistic guarantees on test-time prediction accuracy. While current conformal methods treat robots as Euclidean points, many systems have non-Euclidean configurations, e.g., some mobile robots have SE(2). In this work, we rigorously analyze configuration errors using Lie groups, extending previous Euclidean Space theoretical guarantees to SE(2). Our experiments on a simulated JetBot, and on a real MBot, suggest that by considering the configuration space's structure, our symmetry-informed nonconformity score leads to more volume-efficient prediction regions which represent the underlying uncertainty better than existing approaches.",
    "published": "2025-12-11T05:26:56Z",
    "updated": "2025-12-11T05:26:56Z",
    "link": "http://arxiv.org/pdf/2512.10294v1.pdf",
    "category": [
      "cs.RO",
      "eess.SY"
    ],
    "authors": [
      "Luís Marques",
      "Maani Ghaffari",
      "Dmitry Berenson"
    ]
  }
]